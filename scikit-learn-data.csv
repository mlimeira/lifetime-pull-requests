pull_req_id,project_name,lang,github_id,created_at,merged_at,closed_at,lifetime_minutes,mergetime_minutes,merged_using,conflict,forward_links,intra_branch,description_length,num_commits,num_commits_open,num_pr_comments,num_issue_comments,num_commit_comments,num_comments,num_commit_comments_open,num_participants,files_added_open,files_deleted_open,files_modified_open,files_changed_open,src_files_open,doc_files_open,other_files_open,files_added,files_deleted,files_modified,files_changed,src_files,doc_files,other_files,src_churn_open,test_churn_open,src_churn,test_churn,new_entropy,entropy_diff,commits_on_files_touched,owner_files,files_name,commits_to_hottest_file,hotness,at_mentions_description,at_mentions_comments,friday_effect,words_description,assignee,prev_pull_reqs_project,project_succ_rate,perc_external_contribs,sloc,test_lines_per_kloc,test_cases_per_kloc,asserts_per_kloc,stars,team_size,project_age,workload,ci,requester,closer,auto_closed,merger,prev_pullreqs,requester_succ_rate,followers,following,requester_age,main_team_member,watcher_project,req_follows_integrator,integrator_follows_req,prior_interaction_issue_events,prior_interaction_issue_comments,prior_interaction_pr_events,prior_interaction_pr_comments,prior_interaction_commits,prior_interaction_commit_comments,first_response
13046148,scikit-learn/scikit-learn,python,6543,1458008507,1458009185,1458009185,11,11,commits_in_master,false,false,false,27,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.770100092805642,0.09781309834996424,1,p.e.strickland@gmail.com,doc/conf.py,1,0.0034965034965034965,0,0,false,Update copyright year in the doc footer (2015 → 2016) I just found the old version of confpy file that updates copyright year in the doc footer ,,3687,0.7130458367236235,0.1958041958041958,52230,532.7015125406855,39.230327398047095,135.82232433467357,10014,33,2036,431,travis,ohld,GaelVaroquaux,false,GaelVaroquaux,1,0.0,0,0,892,false,false,false,false,0,4,1,0,0,0,9
13015810,scikit-learn/scikit-learn,python,6531,1457910473,1457922189,1457922189,195,195,commits_in_master,false,false,false,24,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.283576132062365,0.08783670239760619,6,p.e.strickland@gmail.com,sklearn/linear_model/coordinate_descent.py,6,0.020833333333333332,0,0,false,change lasso_path to use return_n_iter argument As for title description currently lasso_path does not use the return_n_iter argument This simple pull request fixes it,,3680,0.7141304347826087,0.19444444444444445,52230,532.7015125406855,39.230327398047095,135.82232433467357,9981,33,2035,425,travis,gasagna,agramfort,false,agramfort,0,0,4,8,1788,false,false,false,false,0,0,0,0,0,0,195
13012205,scikit-learn/scikit-learn,python,6530,1457892572,1457894055,1457894055,24,24,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.287746235934975,0.08792221223791245,5,p.e.strickland@gmail.com,sklearn/linear_model/ridge.py,5,0.017421602787456445,0,0,false,Correcting Typo in sklearnlinear_modelRidge ,,3679,0.7140527317205763,0.1951219512195122,52230,532.7015125406855,39.230327398047095,135.82232433467357,9979,33,2035,425,travis,Arafatk,agramfort,false,agramfort,0,0,7,6,384,false,false,false,false,0,0,0,0,0,0,-1
12965108,scikit-learn/scikit-learn,python,6521,1457686285,1457733490,1457733490,786,786,commits_in_master,false,false,false,302,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.90183117450636,0.10051433172814009,7,t3kcit@gmail.com,README.rst,7,0.023728813559322035,0,0,true,Update Python version in README Please try to adhere to the guidelines below as much as possible when submitting your pull request- Please verify that your code satisfies the[code/documentation quality guidelines](http://scikit-learnorg/stable/developers/contributinghtml#coding-guidelines)- Please prefix the title of your pull request with [MRG] if thecontribution is complete and should be subjected to a detailed review- Incomplete contributions should be prefixed [WIP] to indicate a workin progress (and changed to [MRG] when it matures) WIPs may be usefulto: indicate you are working on something to avoid duplicated workrequest broad review of functionality or API or seek collaboratorsWIPs often benefit from the inclusion of a[task list](https://githubcom/blog/1375-task-lists-in-gfm-issues-pulls-comments)in the PR description- Documentation and high-coverage tests are necessary for enhancements tobe accepted- If you are adding an enhancement you may wish to provide evidence forits benefit with distinguishing examples in the code and benchmarksin the PR discussion- If your pull request addresses an issue please use the title to describethe issue and mention the issue number in the pull request description toensure a link is created to the original issuePlease be aware that we are a loose team of volunteers so patience isnecessary assistance handling other issues is very welcome We valueall user contributions no matter how minor they are If we are slow toreview either the pull request needs some benchmarking tinkeringconvincing etc or more likely the reviewers are simply busy In eithercase we ask for your understanding during the review processFor more information see[Why is my PR not getting any attention](http://scikit-learnorg/dev/faqhtml#why-is-my-pull-request-not-getting-any-attention)Thanks for contributing Please delete these guidelines before submitting your pull requestIf I am not mistaken those numbers are outdated with respected to what we test on Trevis,,3678,0.7139749864056553,0.18983050847457628,52230,532.7015125406855,39.230327398047095,135.82232433467357,9935,33,2033,425,travis,giorgiop,TomDLT,false,TomDLT,15,0.8666666666666667,2,7,1204,true,true,false,false,2,25,1,24,1,0,697
12884664,scikit-learn/scikit-learn,python,6508,1457468017,1457642641,1457642641,2910,2910,commits_in_master,false,false,false,39,2,1,2,16,0,18,0,4,0,0,2,2,1,0,1,0,0,2,2,1,0,1,2,0,4,0,9.889306424211306,0.20337582805013735,12,t3kcit@gmail.com,.travis.yml|build_tools/travis/install.sh,10,0.03278688524590164,0,0,false,[MRG] Temporary disable MKL in Travis to have master green on Travis until we fix #6279The main other change is to install nomkl when INSTALL_MKL is not trueI got the nomkl tip from this Continuum blog [post](https://wwwcontinuumio/blog/developer-blog/anaconda-25-release-now-mkl-optimizations),,3672,0.7148692810457516,0.18360655737704917,51765,533.932193567082,39.350912778904664,136.17308992562545,9884,34,2030,430,travis,lesteve,ogrisel,false,ogrisel,30,0.9,4,0,1413,true,false,false,false,4,85,7,63,8,0,5
12878369,scikit-learn/scikit-learn,python,6506,1457449998,,1457466983,283,,unknown,false,false,false,55,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.272459526651091,0.08786409852930221,7,trent@trenthauck.com,sklearn/manifold/t_sne.py,7,0.022950819672131147,0,0,false,[MRG] add error in the return function of tsne library Its important to have access to the error when calling modelfit_transform(X)An example is to use Bayesian optimisation to minimise the error of the resulting TSNE transformationmodel  TSNE(n_componentsn_components                 perplexityperplexity                 early_exaggerationearly_exaggeration                 learning_ratelearning_rate                 n_itern_iter                 random_staterandom_state                 angleangle                 verbose1)    _ error  modelfit_transform(X)    return error,,3671,0.715064015254699,0.18360655737704917,51765,533.932193567082,39.350912778904664,136.17308992562545,9876,34,2030,428,travis,philipperemy,philipperemy,true,,0,0,30,31,1019,false,false,false,false,0,0,0,0,0,0,209
12861259,scikit-learn/scikit-learn,python,6504,1457395320,,1457478145,1380,,unknown,false,false,false,5,1,1,0,3,0,3,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.685628656472729,0.09636101532990907,5,p.e.strickland@gmail.com,README.rst,5,0.016339869281045753,0,0,false,added scikit-learn logo and separator ,,3669,0.7154538021259199,0.1830065359477124,51765,533.932193567082,39.350912778904664,136.17308992562545,9863,34,2029,428,travis,philipgura,amueller,false,,0,0,0,0,740,false,false,false,false,0,0,0,0,0,0,1
12860996,scikit-learn/scikit-learn,python,6503,1457394690,1457478044,1457478044,1389,1389,commits_in_master,false,false,false,29,1,1,0,5,0,5,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.974490317179724,0.10230152085357203,5,p.e.strickland@gmail.com,README.rst,5,0.016339869281045753,0,0,false,Add Python and PyPi version badges Added Python and PyPi version badges to the top of the README per our discussions in #6495Should be ready for your review,,3668,0.7153762268266085,0.1830065359477124,51765,533.932193567082,39.350912778904664,136.17308992562545,9863,34,2029,428,travis,rhiever,amueller,false,amueller,0,0,1134,22,1399,false,false,true,false,1,6,0,0,0,0,24
12822815,scikit-learn/scikit-learn,python,6494,1457237317,1457646724,1457646724,6823,6823,commits_in_master,false,false,false,64,2,1,0,1,0,1,0,1,0,0,2,2,1,0,1,0,0,2,2,1,0,1,11,0,11,0,10.1179270234148,0.20807761181057702,12,t3kcit@gmail.com,.travis.yml|build_tools/travis/install.sh,10,0.03125,0,0,false,[MRG] MKL is always enabled for latest numpy in conda The master branch is currently failing on travis probably caused by the recent changes in the way conda is shipping MKL enabled builds of numpy / scipyThis is an attempt to fix the issue by bumping up to the latest versions of those packages I will merge as soon as travis is green,,3665,0.7156889495225103,0.178125,51765,533.932193567082,39.350912778904664,136.17308992562545,9839,36,2027,431,travis,ogrisel,ogrisel,true,ogrisel,153,0.8562091503267973,1247,124,2474,true,true,false,false,25,231,52,79,60,0,6760
12809312,scikit-learn/scikit-learn,python,6490,1457173768,1457282895,1457282895,1818,1818,commits_in_master,false,false,false,34,1,1,0,2,0,2,0,2,0,0,2,2,1,0,0,0,0,2,2,1,0,0,1,0,1,0,9.129013305217457,0.187740362461671,3,p.e.strickland@gmail.com,doc/modules/outlier_detection.rst|sklearn/ensemble/iforest.py,3,0.009174311926605505,0,1,false,[MRG] doc: add link to user guide in isolation forest docstring Added a link in the isolation forest docstring to the user guide section about isolation forest in novelty and outlier detection Addresses https://githubcom/scikit-learn/scikit-learn/issues/6485,,3663,0.7158067158067158,0.1743119266055046,51765,533.932193567082,39.350912778904664,136.17308992562545,9831,37,2027,423,travis,nelson-liu,agramfort,false,agramfort,16,0.4375,34,24,693,true,false,false,false,4,102,15,36,4,0,260
12759965,scikit-learn/scikit-learn,python,6481,1457031281,1457093032,1457093032,1029,1029,commits_in_master,false,false,false,18,2,2,2,0,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,8.75298727513889,0.17999986143821747,6,p.e.strickland@gmail.com,sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py,6,0.01812688821752266,0,0,false,DOC: Clarify the scoring argument in Logistic Regression Clarifies the allowed parameters for scoring in LogisticRegressionCV fixes #6480,,3661,0.7159246107620869,0.17220543806646527,51765,533.932193567082,39.350912778904664,136.17308992562545,9793,37,2025,420,travis,maniteja123,agramfort,false,agramfort,9,0.2222222222222222,4,16,793,true,true,false,false,8,139,12,41,5,0,160
12699884,scikit-learn/scikit-learn,python,6469,1456860697,1456861776,1456861776,17,17,commits_in_master,false,false,false,21,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.369828813368109,0.0898628730431031,8,t3kcit@gmail.com,sklearn/cross_validation.py,8,0.024096385542168676,0,0,false,[MRG] Corrected typos in cross_validationKFold docstring My first PR Trying to start small and limit screwups Thanks for your excellent module,,3654,0.7170224411603722,0.1686746987951807,51765,533.932193567082,39.350912778904664,136.17308992562545,9751,38,2023,416,travis,ralexx,agramfort,false,agramfort,0,0,0,0,738,false,true,false,false,0,0,0,0,0,0,-1
12657482,scikit-learn/scikit-learn,python,6462,1456721566,1456754372,1456754372,546,546,commits_in_master,false,false,false,62,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,30,0,4.409826621447769,0.0908609131001367,1,p.e.strickland@gmail.com,CONTRIBUTING.md,1,0.0030303030303030303,0,0,false,DOC: Align the code blocks in CONTRIBUTINGmd with tab Removes the spaces which were appearing at the start of the code on each line in CONTRIBUTINGmd by aligning the code text with the relevant tab (4 space) indentation levelIf you look at [CONTRIBUTINGmd](https://githubcom/scikit-learn/scikit-learn/blob/master/CONTRIBUTINGmd) now you can see there are currently 2 spaces before every $With this PR [they are gone](https://githubcom/scottclowe/scikit-learn/blob/doc-align-contrubting-code-indents/CONTRIBUTINGmd),,3651,0.7173377156943304,0.17272727272727273,51761,533.7223005737911,39.31531461911478,136.1449740151852,9714,39,2021,420,travis,scottclowe,TomDLT,false,TomDLT,1,1.0,6,11,1189,false,true,false,false,0,0,1,0,0,0,544
12657429,scikit-learn/scikit-learn,python,6461,1456721263,1456754198,1456754198,548,548,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.551027823522732,0.09377024973685827,2,nelson.liu.2009@gmail.com,build_tools/travis/install.sh,2,0.006060606060606061,0,0,false,MAINT: Fix typo in comment for travis/installsh Trivial PR Fixes a typo in a comment in a document nobody reads,,3650,0.7172602739726027,0.17272727272727273,51761,533.7223005737911,39.31531461911478,136.1449740151852,9714,39,2021,420,travis,scottclowe,TomDLT,false,TomDLT,0,0,6,11,1189,false,true,false,false,0,0,0,0,0,0,4
12634497,scikit-learn/scikit-learn,python,6458,1456601776,1456754698,1456754698,2548,2548,commits_in_master,false,false,false,16,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.347445935975782,0.0895756094978975,5,se.raschka@me.com,sklearn/ensemble/voting_classifier.py,5,0.015105740181268883,0,2,false,[MRG] DOC Add doc of attributes for VotingClassifier Ive added attribute estimators_  into VotingClassifiers doc,,3649,0.7171827898054262,0.17522658610271905,51761,533.7223005737911,39.31531461911478,136.1449740151852,9693,40,2020,418,travis,yenchenlin1994,agramfort,false,agramfort,28,0.5357142857142857,25,15,704,true,true,false,false,5,85,31,28,4,0,2545
12601353,scikit-learn/scikit-learn,python,6453,1456471055,1456481097,1456481097,167,167,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,2,0,0,8,8,2,0,1,0,0,8,8,2,0,1,6,0,6,0,34.81378269435482,0.7173095993896674,1,p.e.strickland@gmail.com,doc/modules/label_propagation.rst|doc/themes/scikit-learn/static/ML_MAPS_README.rst|doc/themes/scikit-learn/static/nature.css_t|doc/tutorial/machine_learning_map/ML_MAPS_README.txt|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/text_analytics/working_with_text_data.rst|sklearn/neighbors/nearest_centroid.py|sklearn/preprocessing/imputation.py,1,0.0029850746268656717,0,0,true,[MRG] DOC Fix misuse of its Fix the misuse of its in sklearn,,3647,0.7173018919659995,0.1791044776119403,51761,533.7223005737911,39.31531461911478,136.1449740151852,9683,40,2019,416,travis,yenchenlin1994,jakevdp,false,jakevdp,27,0.5185185185185185,25,15,703,true,true,true,false,5,85,30,28,4,0,69
12569681,scikit-learn/scikit-learn,python,6447,1456373733,1456428009,1456428009,904,904,commits_in_master,false,false,false,45,2,2,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,8.698359895454303,0.17974967301966244,1,p.e.strickland@gmail.com,sklearn/decomposition/pca.py|sklearn/decomposition/pca.py,1,0.0030120481927710845,0,1,false,[MRG]Correct PCA docs for whitening If the projection of the data on to each component has to have unit variance then we need to multiply by square_root(n_samples) / singular_valueNote that the variance along each component is given by (singular_value**2) / n_samplesAlso see: https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/pcapy#L622,,3644,0.7176180021953897,0.18072289156626506,51590,532.9908897073076,39.29056018608257,136.1310331459585,9662,39,2017,418,travis,MechCoder,jakevdp,false,jakevdp,87,0.8735632183908046,89,41,1345,true,true,false,false,31,293,39,278,24,0,5
12548497,scikit-learn/scikit-learn,python,6442,1456319801,1456362112,1456362112,705,705,commits_in_master,false,false,false,2,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.258062997233081,0.08799184501762328,41,yenchenlin1994@gmail.com,doc/whats_new.rst,41,0.12462006079027356,0,0,false,Fixed typo ,,3640,0.7181318681318681,0.17325227963525835,51590,532.9908897073076,39.29056018608257,136.1310331459585,9652,39,2017,417,travis,radarhere,agramfort,false,agramfort,0,0,4,0,1158,false,false,false,false,0,0,0,0,0,0,2
12533555,scikit-learn/scikit-learn,python,6436,1456271293,,1456309628,638,,unknown,false,false,false,48,2,2,3,5,0,8,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,40,28,40,28,17.072813905568363,0.35280558229638204,4,p.e.strickland@gmail.com,sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/tests/test_function_transformer.py|sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/tests/test_function_transformer.py,4,0.012232415902140673,0,3,false,feature: add get_feature_names() and tests to FunctionTransformer Adds get_feature_names() to FunctionTransformer as outlined in https://githubcom/scikit-learn/scikit-learn/issues/6425 Im not sure if I did this properly -- is the point of this function to maintain compatibility when get_feature_names() is implemented in Pipeline Seems quite trivial otherwise / maybe Im missing something,,3637,0.7187242232609293,0.16819571865443425,51590,532.9908897073076,39.29056018608257,136.1310331459585,9646,39,2016,417,travis,nelson-liu,nelson-liu,true,,13,0.5384615384615384,34,24,682,true,false,false,false,3,93,14,20,5,0,133
12526880,scikit-learn/scikit-learn,python,6432,1456256695,,1456309778,884,,unknown,false,false,false,18,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,6,17,6,8.827476542372999,0.18241801692152113,4,p.e.strickland@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,4,0.012345679012345678,0,0,false,[MRG] ENH Add get_feature_names for Binarizer This is a PR for #6425 Ive added get_feature_names for Binarizer,,3635,0.7191196698762036,0.16666666666666666,51590,532.9908897073076,39.29056018608257,136.1310331459585,9644,39,2016,417,travis,yenchenlin1994,yenchenlin1994,true,,24,0.5833333333333334,25,15,700,true,true,false,false,5,72,26,19,2,0,884
12520785,scikit-learn/scikit-learn,python,6427,1456238835,1456251623,1456251623,213,213,commits_in_master,false,false,false,46,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.789542083481428,0.09897493339609703,5,p.e.strickland@gmail.com,doc/modules/svm.rst,5,0.015432098765432098,0,0,false,doc: fix capitalization in svmrst citation to match paper Seems like the citation was inadvertently modified in https://githubcom/scikit-learn/scikit-learn/pull/6412 and not reverted back to match the capitalization in the paper Its not exactly proper capitalization but citations should generally stay true to the papers they correspond to,,3630,0.7198347107438017,0.16666666666666666,51590,532.9908897073076,39.29056018608257,136.1310331459585,9641,39,2016,413,travis,nelson-liu,agramfort,false,agramfort,12,0.5,34,24,682,true,false,false,false,3,91,13,20,3,0,-1
12518965,scikit-learn/scikit-learn,python,6426,1456232065,1456259426,1456259426,456,456,commits_in_master,false,false,false,14,2,2,4,1,0,5,0,3,0,0,32,32,24,0,0,0,0,32,32,24,0,0,80,24,80,24,277.87502136937456,5.742234018686158,66,tom.dupre-la-tour@m4x.org,doc/faq.rst|doc/modules/biclustering.rst|doc/modules/covariance.rst|doc/modules/cross_validation.rst|doc/modules/ensemble.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/whats_new.rst|examples/bicluster/plot_spectral_coclustering.py|examples/cluster/plot_cluster_comparison.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/ensemble/bagging.py|sklearn/ensemble/gradient_boosting.py|sklearn/exceptions.py|sklearn/externals/joblib/pool.py|sklearn/grid_search.py|sklearn/kernel_ridge.py|sklearn/linear_model/omp.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/_barnes_hut_tsne.pyx|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/tests/test_pairwise.py|sklearn/mixture/dpgmm.py|sklearn/model_selection/_search.py|sklearn/tree/_splitter.pxd|sklearn/tree/_tree.pyx|sklearn/utils/seq_dataset.pyx|sklearn/utils/weight_vector.pyx|doc/faq.rst|doc/modules/biclustering.rst|doc/modules/covariance.rst|doc/modules/cross_validation.rst|doc/modules/ensemble.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/whats_new.rst|examples/bicluster/plot_spectral_coclustering.py|examples/cluster/plot_cluster_comparison.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/ensemble/bagging.py|sklearn/ensemble/gradient_boosting.py|sklearn/exceptions.py|sklearn/externals/joblib/pool.py|sklearn/grid_search.py|sklearn/kernel_ridge.py|sklearn/linear_model/omp.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/_barnes_hut_tsne.pyx|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/tests/test_pairwise.py|sklearn/mixture/dpgmm.py|sklearn/model_selection/_search.py|sklearn/tree/_splitter.pxd|sklearn/tree/_tree.pyx|sklearn/utils/seq_dataset.pyx|sklearn/utils/weight_vector.pyx,39,0.009345794392523364,0,5,false,[MRG] DOC Remove redundant words in sklearn Ive removed some redundant words in sklearn,,3629,0.7197575089556352,0.16510903426791276,51590,532.9908897073076,39.29056018608257,136.1310331459585,9641,38,2016,415,travis,yenchenlin1994,GaelVaroquaux,false,GaelVaroquaux,22,0.5909090909090909,25,15,700,true,true,false,false,5,66,24,15,2,0,53
12492299,scikit-learn/scikit-learn,python,6418,1456153405,,1456228212,1246,,unknown,false,false,false,38,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.1925944273137254,0.0866391421599364,3,p.e.strickland@gmail.com,sklearn/svm/base.py,3,0.009287925696594427,0,4,false,[WIP] Change boolean array to 0-1 array (fixes #6416) Ive changed the first argument passed to _ovr_decision_function from boolean array to array contains 0 and 1 which will be treat correctly in sklearn/multiclasspyThis should fix #6416 ,,3626,0.7203530060672918,0.17027863777089783,51590,532.9908897073076,39.29056018608257,136.1310331459585,9623,38,2015,412,travis,yenchenlin1994,yenchenlin1994,true,,21,0.6190476190476191,25,15,699,true,true,false,false,5,60,22,4,2,0,1014
12489981,scikit-learn/scikit-learn,python,6417,1456143529,,1456155617,201,,unknown,false,false,false,18,5,4,4,2,0,6,0,2,0,6,79,85,34,0,7,0,6,79,85,34,0,7,118,10,118,10,446.7336188079092,9.231662680122403,108,tom.dupre-la-tour@m4x.org,AUTHORS.rst|doc/about.rst|doc/developers/advanced_installation.rst|doc/developers/performance.rst|doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/gaussian_process.rst|doc/modules/kernel_approximation.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/model_evaluation.rst|doc/modules/neighbors.rst|doc/modules/sgd.rst|doc/presentations.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/neighbors/plot_species_kde.py|sklearn/covariance/shrunk_covariance_.py|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/descr/linnerud.rst|sklearn/gaussian_process/gaussian_process.py|sklearn/linear_model/theil_sen.py|AUTHORS.rst|CONTRIBUTING.md|doc/README|doc/about.rst|doc/datasets/twenty_newsgroups.rst|doc/developers/advanced_installation.rst|doc/developers/contributing.rst|doc/developers/performance.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/cross_validation.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/model_persistence.rst|doc/modules/neural_networks_supervised.rst|doc/modules/preprocessing.rst|doc/modules/random_projection.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/presentations.rst|doc/related_projects.rst|doc/testimonials/testimonials.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/tutorial/statistical_inference/index.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/working_with_text_data.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/applications/wikipedia_principal_eigenvector.py|examples/calibration/plot_calibration.py|examples/datasets/plot_iris_dataset.py|examples/decomposition/plot_pca_iris.py|examples/linear_model/plot_iris_logistic.py|examples/manifold/plot_manifold_sphere.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/datasets/samples_generator.py|sklearn/feature_selection/univariate_selection.py|sklearn/isotonic.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/ransac.py|sklearn/linear_model/sgd_fast.pyx|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/random_projection.py|sklearn/tree/tree.py|sklearn/utils/linear_assignment_.py|AUTHORS.rst.orig|doc/modules/feature_extraction.rst.orig|doc/modules/linear_model.rst.orig|doc/modules/neighbors.rst.orig|doc/tutorial/statistical_inference/unsupervised_learning.rst.orig|sklearn/metrics/classification.py.orig,39,0.003076923076923077,0,0,false,[MRG] DOC: Fix broken linkcheck Continuation of PR https://githubcom/scikit-learn/scikit-learn/pull/5968 Ive rebased it on top of the current master,,3625,0.720551724137931,0.1753846153846154,51590,532.9908897073076,39.29056018608257,136.1310331459585,9618,38,2015,411,travis,nelson-liu,nelson-liu,true,,10,0.6,34,24,681,true,false,false,false,3,88,10,16,3,0,147
12477107,scikit-learn/scikit-learn,python,6414,1456091097,1456174701,1456174701,1393,1393,commits_in_master,false,false,false,27,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.856916620352337,0.10036723008270511,3,p.e.strickland@gmail.com,Makefile,3,0.00906344410876133,0,3,false,Fixes the make cython by adjusting the path to cythonizepy #4613 Fixes #4613 by adjusting the path of cythonizepy which was previously moved from sklearn/_build_utils/cythonizepy to build_tools/cythonizepy,,3624,0.7204746136865342,0.1782477341389728,51590,532.9908897073076,39.29056018608257,136.1310331459585,9610,38,2014,410,travis,ssaeger,TomDLT,false,TomDLT,3,0.6666666666666666,2,0,1561,false,false,false,false,0,3,0,0,0,0,183
12476293,scikit-learn/scikit-learn,python,6412,1456087611,1456238186,1456238186,2509,2509,commits_in_master,false,false,false,12,4,1,6,4,3,13,0,4,0,0,1,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,5.209283191356227,0.10764881621436816,1,p.e.strickland@gmail.com,doc/modules/svm.rst,1,0.0030211480362537764,0,1,false,Update svmrst Just two tiny typos and space errors in the references,,3623,0.7203974606679547,0.1782477341389728,51590,532.9908897073076,39.29056018608257,136.1310331459585,9610,38,2014,410,travis,Naereen,agramfort,false,agramfort,5,0.8,7,46,310,false,true,false,false,0,0,0,0,0,0,326
12451538,scikit-learn/scikit-learn,python,6411,1455969227,1456799641,1456799641,13840,13840,commits_in_master,false,false,false,26,2,2,57,20,0,77,0,10,2,0,1,3,2,0,0,2,0,1,3,2,0,0,92,0,92,0,23.21646676968174,0.47976373574456926,5,p.e.strickland@gmail.com,ISSUE_TEMPLATE.md|PULL_REQUEST_TEMPLATE.md|ISSUE_TEMPLATE.md|PULL_REQUEST_TEMPLATE.md|doc/faq.rst,5,0.0,0,12,false,DOC: add issue and pull request template Added a first draft issue and PR template as discussed at https://githubcom/scikit-learn/scikit-learn/issues/6394 please let me know what you think,,3622,0.7203202650469354,0.1729106628242075,51590,532.9908897073076,39.29056018608257,136.1310331459585,9596,38,2013,425,travis,nelson-liu,amueller,false,amueller,9,0.5555555555555556,34,24,679,true,false,false,false,3,85,9,9,3,0,222
12427897,scikit-learn/scikit-learn,python,6405,1455893876,,1455980153,1437,,unknown,false,false,false,326,1,1,0,1,0,1,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,0,214,0,214,0,4.484866120494471,0.09267649467190908,0,,sklearn/ensemble/ensamble_learning_for_structured_predictions.py,0,0.0,0,0,true,ensamble learning for structured predictions **Reference**Corinna Cortes Vitaly Kuznetsov Mehryar Mohri: Ensemble Methods for Structured Prediction 1134-1142**Motivation for the algorithm**Ensemble methods are general techniques in machine learning for combining several hypotheses to create a more accurate predictor These methods often significantly improve the performance in practice and additionally benefit from favorable learning guarantees However ensemble methods and their theory have been developed primarily for the binary classification problem or regression tasks These techniques do not readily apply to structured prediction problems While it is straightforward to combine scalar outputs for a classification or regression problem it is less clear how to combine structured predictions such as phonemic pronunciation hypotheses speech recognition lattices parse trees or outputs of several machine translation systems The main goal of this article is to present a learning algorithm for dealing with structured predictions and the theoretical guarantees for designing accurate ensembles of structured prediction tasks**Short Description:**First the algorithm splits the dataset into L substructuresA given set of P experts (ie classification algorithms such as SVM and RandomForest)  learn every substructure separately and then the algorithm create an internal graphExpert h_ij means that this kind of expert is from type i and learn substructure j Now our problem is to find the best “Expert Path”For each substructure we want to choose the best expert to be the predictorHere we will choose h_j1 (ie expert from type j for substructure 1)  to be the expert predictor for substructure 1 we iteratively continue to choose the best expert for every substructure iteratively and greedily In order to predict new instance the algorithm will split the row vector to L substructures and “walk” through the expert pathand then each subtract goes to its corresponding expert for prediction according to the “Expert Path”after each expert finishes to classify its substructure - the algorithm will count the majority class of the whole predictions and report it,,3620,0.7207182320441989,0.17663817663817663,51594,532.9495677791991,39.28751405202156,136.1204791254797,9588,38,2012,410,travis,galdreiman,jnothman,false,,0,0,0,0,1038,false,false,false,false,0,0,0,0,0,0,1437
12423663,scikit-learn/scikit-learn,python,6404,1455877287,1456328063,1456328063,7512,7512,commits_in_master,false,false,false,23,3,2,3,5,0,8,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.042882542579804,0.18686458709356027,5,t3kcit@gmail.com,doc/modules/clustering.rst|doc/modules/clustering.rst,5,0.014245014245014245,1,6,true,DOC: Added memory workaround to dbscan doc Added a small section to the dbscan doc as discussed on the mailing list by @jnothman,,3619,0.720641061066593,0.17663817663817663,51594,532.9495677791991,39.28751405202156,136.1204791254797,9585,38,2012,419,travis,dsquareindia,jnothman,false,jnothman,12,0.4166666666666667,1,0,480,true,true,false,false,5,82,13,24,9,0,6632
12405223,scikit-learn/scikit-learn,python,6396,1455826121,1455896489,1455896489,1172,1172,commits_in_master,false,false,false,17,4,4,7,8,0,15,0,5,0,0,6,6,12,0,3,0,0,6,6,12,0,3,1108,0,1108,0,50.93313910006779,1.0524966602588988,20,t3kcit@gmail.com,build_tools/circle/build_doc.sh|.travis.yml|appveyor.yml|build_tools/appveyor/install.ps1|build_tools/appveyor/requirements.txt|build_tools/appveyor/run_with_env.cmd|build_tools/circle/build_doc.sh|build_tools/circle/check_build_doc.py|build_tools/circle/push_doc.sh|build_tools/cythonize.py|build_tools/travis/after_success.sh|build_tools/travis/install.sh|build_tools/travis/test_script.sh|build_tools/windows/windows_testing_downloader.ps1|circle.yml|setup.py|sklearn/_build_utils/__init__.py|.travis.yml|appveyor.yml|build_tools/appveyor/install.ps1|build_tools/appveyor/requirements.txt|build_tools/appveyor/run_with_env.cmd|build_tools/circle/build_doc.sh|build_tools/circle/check_build_doc.py|build_tools/circle/push_doc.sh|build_tools/cythonize.py|build_tools/travis/after_success.sh|build_tools/travis/install.sh|build_tools/travis/test_script.sh|build_tools/windows/windows_testing_downloader.ps1|circle.yml|setup.py|sklearn/_build_utils/__init__.py,9,0.0,0,5,false,Enh: Reorganize build files This PR addresses issue https://githubcom/scikit-learn/scikit-learn/issues/6359 and gets rid of some redundant code in_build_utils/__init__py,,3614,0.7213613724405091,0.18051575931232092,51605,532.6615638019572,39.25976165100281,136.03333010367214,9576,37,2011,410,travis,nelson-liu,GaelVaroquaux,false,GaelVaroquaux,7,0.5714285714285714,34,24,677,false,false,false,false,2,76,7,8,1,0,31
12397359,scikit-learn/scikit-learn,python,6393,1455802738,1455851208,1455851208,807,807,commits_in_master,false,false,false,17,2,2,3,3,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,38,0,38,9.130087556904982,0.1886666879619252,3,yenchenlin1994@gmail.com,sklearn/manifold/tests/test_locally_linear.py|sklearn/manifold/tests/test_locally_linear.py,3,0.008771929824561403,0,3,false,Add test for local linear embedding Test the errors raised when parameter passed to LLE is invalid,,3612,0.721483942414175,0.16374269005847952,51605,532.6615638019572,39.25976165100281,136.03333010367214,9574,36,2011,406,travis,yenchenlin1994,jakevdp,false,jakevdp,19,0.631578947368421,25,15,695,true,true,true,false,5,51,20,4,1,0,368
12393259,scikit-learn/scikit-learn,python,6391,1455784821,1455797544,1455797544,212,212,commits_in_master,false,false,false,18,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,0,1,4.510449536393505,0.09320304964038381,1,p.e.strickland@gmail.com,sklearn/manifold/tests/test_locally_linear.py,1,0.0029411764705882353,0,1,false,Remove redundant assignment of variable tol variable tol is then assigned to 01 again in line 51:https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/manifold/tests/test_locally_linearpy#L51,,3611,0.7214068125173082,0.16470588235294117,51603,532.5271786524039,39.26128325872527,136.03860240683682,9573,36,2011,405,travis,yenchenlin1994,GaelVaroquaux,false,GaelVaroquaux,18,0.6111111111111112,25,15,695,true,true,false,false,5,49,19,4,1,0,212
12379496,scikit-learn/scikit-learn,python,6384,1455747024,1455750492,1455750492,57,57,commits_in_master,false,false,false,9,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.654899840636357,0.09618845061571418,1,p.e.strickland@gmail.com,examples/linear_model/plot_sparse_recovery.py,1,0.0029498525073746312,0,0,false,[MRG] Fix broken link of Wainwrights paper Fix #6383,,3608,0.7217294900221729,0.16519174041297935,51520,532.4534161490683,39.30512422360249,135.9666149068323,9569,36,2010,406,travis,yenchenlin1994,MechCoder,false,MechCoder,17,0.5882352941176471,25,15,694,true,true,false,false,5,46,18,4,1,0,-1
12370624,scikit-learn/scikit-learn,python,6379,1455725018,1456795404,1456795404,17839,17839,commits_in_master,false,false,false,9,2,1,8,17,0,25,0,6,0,0,2,5,2,0,0,0,0,5,5,4,0,0,11,12,33,38,8.532272484514477,0.17631016869123578,8,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,8,0.023529411764705882,0,1,false,[MRG] fix StratifiedShuffleSplit train and test overlap Fix #6121,,3605,0.7220527045769765,0.16470588235294117,51519,532.4637512374076,39.30588714843068,135.96925406160835,9567,36,2010,431,travis,lesteve,amueller,false,amueller,28,0.9285714285714286,4,0,1393,true,false,false,false,3,65,6,24,10,0,4
12362105,scikit-learn/scikit-learn,python,6375,1455691521,,1455808409,1948,,unknown,false,false,false,31,1,1,0,7,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,32,7,32,8.926624687943235,0.18553092456487177,0,,sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,0,0.0,0,2,false,Backport 017X -- FIX in randomized_svd flip sign Cherry picks the fix from this commit ( https://githubcom/scikit-learn/scikit-learn/commit/cfa498ca684297ea76b9e0938f6cd863570a0cff ) in master Also see the PR ( https://githubcom/scikit-learn/scikit-learn/pull/6135 ) this was taken from,,3603,0.7224535109630863,0.16374269005847952,46545,538.1458803308626,39.76796648404769,137.75915780427545,9559,37,2010,409,travis,jakirkham,ogrisel,false,,3,0.6666666666666666,13,8,1163,false,false,false,false,0,4,2,0,0,0,29
12355484,scikit-learn/scikit-learn,python,6372,1455674621,1456448104,1456448104,12891,12891,commits_in_master,false,false,false,9,5,1,20,16,0,36,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,29,16,66,55,8.761081271232374,0.18103825442191143,5,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,5,0.014492753623188406,0,5,false,[MRG] add get_feature_names to PolynomialFeatures Fixes #6185 replaces #6216,,3600,0.7227777777777777,0.1681159420289855,51519,532.4637512374076,39.30588714843068,135.96925406160835,9553,38,2009,428,travis,amueller,jakevdp,false,jakevdp,359,0.8523676880222841,1286,40,1943,true,true,true,false,77,436,32,175,33,0,11
12348137,scikit-learn/scikit-learn,python,6371,1455657110,1456777669,1456777669,18675,18675,commits_in_master,false,false,false,31,2,2,2,4,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,24,0,24,9.307436927951203,0.19232810225276034,3,se.raschka@me.com,sklearn/ensemble/tests/test_voting_classifier.py|sklearn/ensemble/tests/test_voting_classifier.py,3,0.008595988538681949,0,3,false,Add test for voting classifier‘s init errors Ive added tests for voting classifier to test different errors it will raise when invalid arguments are passed into __init__ and fit is called,,3599,0.7227007502083912,0.166189111747851,51519,532.4637512374076,39.30588714843068,135.96925406160835,9548,39,2009,430,travis,yenchenlin1994,TomDLT,false,TomDLT,16,0.5625,25,15,693,true,true,false,false,5,43,17,4,1,0,26
12334815,scikit-learn/scikit-learn,python,6366,1455614121,1455620634,1455620634,108,108,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.418939081817155,0.09131032240132018,1,p.e.strickland@gmail.com,doc/modules/sgd.rst,1,0.002898550724637681,0,0,false,DOC: very minor typo ,,3597,0.7228245760355853,0.16231884057971013,51519,532.4637512374076,39.30588714843068,135.96925406160835,9538,39,2009,401,travis,yarikoptic,agramfort,false,agramfort,11,0.8181818181818182,94,8,2623,false,true,false,true,0,0,1,0,0,0,-1
12311845,scikit-learn/scikit-learn,python,6363,1455545187,1455799975,1455799975,4246,4246,commits_in_master,false,false,false,48,16,13,0,4,0,4,0,2,0,0,6,7,5,0,1,0,0,7,7,6,0,1,133,38,157,38,74.07630309095485,1.5306663237483493,21,tom.dupre-la-tour@m4x.org,sklearn/utils/seq_dataset.pxd|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/tests/test_gpr.py|.travis.yml|continuous_integration/install.sh|.travis.yml|continuous_integration/install.sh|sklearn/tests/test_isotonic.py|sklearn/tests/test_isotonic.py|.travis.yml|continuous_integration/install.sh|sklearn/tests/test_isotonic.py|sklearn/tests/test_isotonic.py|sklearn/feature_selection/univariate_selection.py|sklearn/utils/seq_dataset.pxd|sklearn/gaussian_process/tests/test_gpr.py,7,0.017241379310344827,0,2,false,[WIP] Add numpy dev wheel to travis build matrix (without conda) This is a follow-up on #6332 to try to simplify the configuration to not rely on the setup of a conda env when testing against travis-dev-wheelsThis also tries to no require the sudo based legacy env,,3595,0.7229485396383867,0.16379310344827586,51515,532.5050956032223,39.30893914393866,135.97981170532856,9524,39,2008,414,travis,ogrisel,ogrisel,true,ogrisel,150,0.8666666666666667,1247,124,2455,true,true,false,false,18,185,44,58,57,0,1400
12309892,scikit-learn/scikit-learn,python,6362,1455536756,1455538666,1455538666,31,31,commits_in_master,false,false,false,17,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.2649672253912945,0.08812862966476576,1,p.e.strickland@gmail.com,sklearn/dummy.py,1,0.002881844380403458,0,0,false,DummyClassifier: Added doc information about default strategy As title It was missing from the current docstring ],,3594,0.7228714524207012,0.1642651296829971,51515,532.5050956032223,39.30893914393866,135.97981170532856,9522,39,2008,401,travis,hlin117,glouppe,false,glouppe,19,0.631578947368421,18,20,1336,true,true,false,false,4,56,8,24,7,0,31
12309574,scikit-learn/scikit-learn,python,6360,1455534882,1455545910,1455545910,183,183,commits_in_master,false,false,false,7,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.711097866140292,0.09734719570358587,1,p.e.strickland@gmail.com,examples/applications/plot_outlier_detection_housing.py,1,0.002881844380403458,0,0,false,Fixed legend text and pep8 warnings http://scikit-learnorg/stable/auto_examples/applications/plot_outlier_detection_housinghtml,,3593,0.7227943222933482,0.1642651296829971,51515,532.5050956032223,39.30893914393866,135.97981170532856,9522,39,2008,401,travis,alyssaq,agramfort,false,agramfort,0,0,57,7,1185,false,false,false,false,0,0,0,0,0,0,183
12288468,scikit-learn/scikit-learn,python,6354,1455415024,1455580420,1455580420,2756,2756,commits_in_master,false,false,false,87,3,3,3,6,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,0,26,0,21.921870830681428,0.4529799020480646,1,p.e.strickland@gmail.com,setup.py|sklearn/_build_utils/cythonize.py|setup.py|setup.py|sklearn/_build_utils/cythonize.py,1,0.0028169014084507044,0,0,false,BLD: fix build eissue with pip and ATLAS Issue introduced by gh-5492 (auto-cythonizing)On current master pip installs are failing when compiling against ATLAS (see https://githubcom/numpy/numpy/issues/7235) This is due to a distutils-setuptools issue with monkeypatching that is triggered if Cython is importedThe second issue fixed here is that pip install scikit-learn pip install  and python setuppy develop behave differently from python setuppy build python setuppy install and python setuppy build_ext -i Thats the main reason that no one has run into this build issue yet,,3590,0.7231197771587744,0.16619718309859155,51515,532.5050956032223,39.30893914393866,135.97981170532856,9509,40,2006,401,travis,rgommers,GaelVaroquaux,false,GaelVaroquaux,1,1.0,64,13,2426,false,false,false,false,0,0,0,0,0,0,6
12277808,scikit-learn/scikit-learn,python,6349,1455352575,1455896577,1455896577,9066,9066,commits_in_master,false,false,false,27,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.363345020251435,0.09016148725144688,14,yenchenlin1994@gmail.com,sklearn/model_selection/_split.py,14,0.03943661971830986,0,0,false,Remove redundant assignment of parameter shuffle In _BaseKFoldhttps://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_splitpy#L286already set selfshuffle  shuffleHowever KFold and StratifiedKFold reassign it again in its __init__ which seems redundant,,3588,0.7232441471571907,0.16901408450704225,51514,532.4960204992818,39.309702216873085,135.96303917381684,9505,40,2006,420,travis,yenchenlin1994,GaelVaroquaux,false,GaelVaroquaux,13,0.6153846153846154,25,15,690,true,true,false,false,4,35,14,3,1,0,7641
12262558,scikit-learn/scikit-learn,python,6343,1455310526,1455642787,1455642787,5537,5537,commits_in_master,false,false,false,84,2,2,2,6,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,38,0,38,0,8.370765526941474,0.17296830153466533,1,p.e.strickland@gmail.com,sklearn/manifold/_barnes_hut_tsne.pyx|sklearn/manifold/_barnes_hut_tsne.pyx,1,0.0028328611898017,3,5,true,Finish Fix memory leak in Barnes-Hut SNE PR A minor tidying up of #5983 as requested by @ogrisel and because @AlexanderFabisch did not have the bandwith to tackle itCloses #5983 and fixes #5916I used the following snippet (from @ogrisel in #5983) to make sure the memory leak was fixedpythonimport psutilimport numpy as npfrom sklearnmanifold import TSNEX  nprandomrand(10 10)p  psutilProcess()for i in range(100000):    TSNE()fit(X)    if i % 100  0:        print(pmemory_info()rss / 1e6),,3585,0.7235704323570432,0.16997167138810199,51505,532.3949131152316,39.29715561595962,135.9673818075915,9501,40,2005,406,travis,lesteve,GaelVaroquaux,false,GaelVaroquaux,27,0.9259259259259259,4,0,1388,true,false,false,false,3,59,5,24,10,0,3823
12250601,scikit-learn/scikit-learn,python,6339,1455264216,1455286095,1455286095,364,364,commits_in_master,false,false,false,12,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.607780296146635,0.09521182250678169,2,t3kcit@gmail.com,sklearn/metrics/tests/test_ranking.py,2,0.0056657223796034,0,0,true,Remove redundant that in test_rankingpy Remove redundant that appeared in the comment,,3583,0.7236952274630198,0.16997167138810199,51472,532.6196767174386,39.322350015542426,136.054553932235,9490,40,2005,397,travis,yenchenlin1994,agramfort,false,agramfort,11,0.6363636363636364,25,15,689,true,true,false,false,3,30,12,3,1,0,-1
12233509,scikit-learn/scikit-learn,python,6337,1455221007,,1455326781,1762,,unknown,false,false,false,23,1,1,0,8,0,8,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.174934413336201,0.08652310451139648,7,p.e.strickland@gmail.com,sklearn/feature_selection/univariate_selection.py,7,0.019830028328611898,0,0,false,MAINT: Fix errors in tests for Unvariate Selection Make sure percentile is an integer Prevent error caused by slice using floating point values,,3581,0.724099413571628,0.16997167138810199,51398,532.7639207751274,39.378964162029646,136.23098175026266,9481,41,2004,401,travis,maniteja123,maniteja123,true,,7,0.2857142857142857,4,16,772,true,true,false,false,5,104,8,34,2,0,329
12233293,scikit-learn/scikit-learn,python,6336,1455220499,1455405854,1455405854,3089,3089,commits_in_master,false,false,false,21,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,4.4206366656608225,0.0916151417871583,3,p.e.strickland@gmail.com,sklearn/metrics/ranking.py,3,0.0084985835694051,0,0,false,DOC: trival PR: incorrect phrasing of inputs to roc The phrasing was confusing some users that would feed in binary predictions,,3580,0.7240223463687151,0.16997167138810199,51398,532.7639207751274,39.378964162029646,136.23098175026266,9480,41,2004,403,travis,GaelVaroquaux,agramfort,false,agramfort,61,0.8032786885245902,717,3,2180,true,true,false,true,27,175,18,38,16,0,53
12230254,scikit-learn/scikit-learn,python,6333,1455211885,1455731983,1455731983,8668,8668,commits_in_master,false,false,false,55,2,1,0,6,0,6,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,5,0,3.8677546637807927,0.08015698161143732,7,t3kcit@gmail.com,sklearn/preprocessing/label.py,7,0.01977401129943503,0,1,false,Fix test_label_binarizer on Windows Fix #6280 Looks like fill is not very friendly with unicode numpy scalar on Windowspythonimport numpy as nparr  npempty(3 dtypeU3)unicode_scalar  nparray([asd])[0]arrfill(unicode_scalartolist())  # works with python strprint(arr)arrfill(unicode_scalar)  # fills the array with some garbageprint(arr)  # UnicodeDecodeErrorThis pattern was used [here](https://githubcom/scikit-learn/scikit-learn/blob/74475bc929960d98939a12fc2fb68c0e006895be/sklearn/preprocessing/labelpy#L614),,3579,0.7239452360994691,0.1694915254237288,51398,532.7639207751274,39.378964162029646,136.23098175026266,9476,41,2004,414,travis,lesteve,TomDLT,false,TomDLT,26,0.9230769230769231,4,0,1387,true,false,false,false,3,56,4,24,9,0,6
12210981,scikit-learn/scikit-learn,python,6332,1455146964,,1455626412,7990,,unknown,false,true,false,8,7,2,2,26,0,28,0,4,0,0,2,6,1,0,1,0,0,6,6,5,0,1,64,0,68,19,20.072815349404365,0.41597841569679217,5,t3kcit@gmail.com,.travis.yml|continuous_integration/install.sh|.travis.yml|continuous_integration/install.sh,5,0.014164305949008499,0,6,false,Add numpy dev wheel to travis build matrix ,,3578,0.7241475684740079,0.17563739376770537,51429,532.3261195045596,39.355227595325594,136.09053257889516,9465,42,2003,413,travis,amueller,ogrisel,false,,358,0.8547486033519553,1286,40,1937,true,true,true,false,80,445,31,183,31,0,237
12210859,scikit-learn/scikit-learn,python,6331,1455146661,1455161021,1455161021,239,239,commits_in_master,false,false,false,32,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.3020147795226,0.08915273751576,1,p.e.strickland@gmail.com,doc/modules/model_evaluation.rst,1,0.002840909090909091,0,0,false,Fixed typo in one word The markdown on the model_evaluationsrst has a small typo in the word objects Theres no issue for this but heres a fixAll scorer ojects followhttp://scikit-learnorg/stable/modules/model_evaluationhtml#scoring-parameter,,3577,0.7240704500978473,0.17613636363636365,51429,532.3261195045596,39.355227595325594,136.09053257889516,9465,42,2003,401,travis,joerobmunoz,MechCoder,false,MechCoder,0,0,5,10,722,false,true,false,false,0,0,0,0,0,0,-1
12199499,scikit-learn/scikit-learn,python,6328,1455115775,1455227037,1455227037,1854,1854,commits_in_master,false,false,false,51,2,2,3,8,0,11,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,23,10,23,18.10961114008508,0.3752927758829163,15,yenchenlin1994@gmail.com,sklearn/model_selection/_split.py|sklearn/model_selection/tests/test_split.py|sklearn/model_selection/_split.py|sklearn/model_selection/tests/test_split.py,11,0.03133903133903134,4,1,false,[MRG] FIX/NRT signature expects the init to not be a c-defined method (Py27) Fixes #6304 The NRT makes sure there are no errors raised at __repr__Do we need a more exhaustive test which will also check the contents Or is it too much to have**cc:** @kingjr @amueller @agramfort @MechCoder,,3576,0.7239932885906041,0.1737891737891738,51393,532.6795477983383,39.38279532232016,136.18586188780574,9453,42,2003,406,travis,rvraghav93,TomDLT,false,TomDLT,35,0.6285714285714286,29,43,859,true,false,false,false,9,305,9,164,8,0,115
12195771,scikit-learn/scikit-learn,python,6327,1455096973,1455402484,1455402484,5091,5091,commits_in_master,false,false,false,24,2,2,2,0,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,6,5,6,17.813471901462446,0.36915576299783354,7,yenchenlin1994@gmail.com,sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py,7,0.02,0,0,false,Normalize LDA transforms return value (fixes #6320) Normalize output of LDAs transform function since it represents document topic distribution for XFixes issue #6320 ,,3575,0.7239160839160839,0.1742857142857143,51393,532.6795477983383,39.38279532232016,136.18586188780574,9452,42,2003,411,travis,yenchenlin1994,larsmans,false,larsmans,10,0.6,25,15,687,true,true,false,false,3,29,11,1,1,0,296
12190372,scikit-learn/scikit-learn,python,6324,1455076034,1455131842,1455131842,930,930,commits_in_master,false,false,false,39,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,1,3,1,6.726221918437718,0.13939013677092557,7,yenchenlin1994@gmail.com,sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py,7,0.02005730659025788,0,0,false,Fix joblib error in LatentDirichletAllocation (#6258) The joblib error in #6258 is caused by parallel parameter not passed to _e_step when we evaluate perplexityThis PR fixed it and set evaluate_every1 in test_lda_multi_jobs test to make sure it works,,3574,0.7238388360380527,0.17478510028653296,51393,532.6795477983383,39.38279532232016,136.18586188780574,9448,42,2002,403,travis,chyikwei,ogrisel,false,ogrisel,6,0.6666666666666666,17,1,1232,false,true,false,false,0,2,0,0,1,0,3
12181037,scikit-learn/scikit-learn,python,6319,1455052937,1455060410,1455060410,124,124,commits_in_master,false,false,false,21,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,3.6468081985453216,0.07557435411854953,3,p.e.strickland@gmail.com,doc/modules/ensemble.rst,3,0.008771929824561403,0,0,false,fix math in soft VotingClassifier example There was a brief error in the ensemble module documentation for VotingClassifier with soft voting,,3573,0.7237615449202351,0.17543859649122806,51389,532.7210103329506,39.38586078732803,136.1964622779194,9443,42,2002,406,travis,gjreda,TomDLT,false,TomDLT,0,0,327,42,1667,false,false,false,false,0,0,0,0,0,0,124
12173421,scikit-learn/scikit-learn,python,6315,1455030196,,1455032433,37,,unknown,false,false,false,12,4,4,0,0,0,0,0,0,0,0,3,3,1,0,2,0,0,3,3,1,0,2,17,0,17,0,30.374003935099147,0.6332606826879191,0,,appveyor.yml|.travis.yml|continuous_integration/install.sh|appveyor.yml|.travis.yml|continuous_integration/install.sh,0,0.0,0,0,false,[MRG] backport CI fixes to 017X branch To speed-up the release process,,3571,0.7241669000280033,0.1749271137026239,46472,538.5608538474781,39.830435531072474,137.97555517300742,9434,42,2002,407,travis,ogrisel,ogrisel,true,,149,0.87248322147651,1247,124,2449,true,true,false,false,16,154,39,51,50,0,-1
12170527,scikit-learn/scikit-learn,python,6314,1455016725,1455038950,1455038950,370,370,commits_in_master,false,false,false,19,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.1274036031053605,0.08553393429949557,5,p.e.strickland@gmail.com,sklearn/manifold/t_sne.py,5,0.014577259475218658,0,0,false,Removes X after raise Because of the raise it doesnt look like this code block will ever be reached,,3570,0.7240896358543417,0.1749271137026239,51390,532.7106440941818,39.38509437633781,136.19381202568593,9432,42,2002,406,travis,tshauck,TomDLT,false,TomDLT,0,0,17,75,1958,false,true,false,false,0,0,0,0,0,0,126
12164435,scikit-learn/scikit-learn,python,6312,1454990524,,1454992538,33,,unknown,false,false,false,95,1,1,0,0,0,0,0,0,3,0,0,3,3,0,0,3,0,0,3,3,0,0,964,0,964,0,13.946764154339444,0.28902470501559485,0,,sklearn/feature_selection/boruta_py.py|sklearn/feature_selection/mi.py|sklearn/feature_selection/mifs.py,0,0.0,0,0,false,Added Boruta and mutual information based feature selection modules Dear FS module maintainersI have implemented the Boruta algorithm (https://m2icmedupl/boruta/) with a sklearn like interface Heres my original repo: https://githubcom/danielhomola/boruta_pyand heres a blog post I wrote about this FS method: http://danielhomolacom/2015/05/08/borutapy-an-all-relevant-feature-selection-method/I also implemented 3 famous mutual information based FS methods (JMI MRMR plus the pretty recent JMIM) and wrapped them up in a single module with parallelized execution Heres my repo: https://githubcom/danielhomola/mifsand heres a blog post I wrote about this method: http://danielhomolacom/2016/01/31/mifs-parallelized-mutual-information-based-feature-selection-module/Looking forward to hear from you guysBest wishesDan,,3568,0.7244955156950673,0.17251461988304093,51390,532.7106440941818,39.38509437633781,136.19381202568593,9429,42,2001,405,travis,danielhomola,danielhomola,true,,0,0,1,0,885,false,false,false,false,0,0,0,0,0,0,-1
12160245,scikit-learn/scikit-learn,python,6311,1454979840,1455032202,1455032202,872,872,commits_in_master,false,false,false,9,2,2,0,4,0,4,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.442315465641597,0.1956771055938536,8,t3kcit@gmail.com,README.rst|README.rst,8,0.023391812865497075,0,0,false,DOC: Use SVG badge for Travis CI [ci skip] ,,3567,0.7244182786655453,0.17251461988304093,51390,532.7106440941818,39.38509437633781,136.19381202568593,9427,42,2001,407,travis,jakirkham,agramfort,false,agramfort,1,1.0,13,8,1154,false,false,false,false,0,1,0,0,0,0,67
12150670,scikit-learn/scikit-learn,python,6307,1454954752,,1455385139,7173,,unknown,false,false,false,56,2,1,0,4,0,4,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,3,0,18,0,4.412829303061875,0.09144899912682355,8,tom.dupre-la-tour@m4x.org,sklearn/linear_model/ridge.py,8,0.023391812865497075,4,6,false,Fix for passing pandas series in sample_weights in RidgeCV leading to an error(#5606) Currently there seems to be no tests for checking pandas compatibility and as @jnothman mentioned in the issue perhaps tests for various cases checking for compatibility needs to be added @jakevdp @amueller @jnothman reviews please and let me know what should be done ,,3566,0.7246214245653393,0.17251461988304093,51390,532.3798404358824,39.36563533761432,136.0965168320685,9424,42,2001,418,travis,kaichogami,kaichogami,true,,5,0.2,18,29,754,true,false,false,false,1,20,6,7,2,0,67
12123072,scikit-learn/scikit-learn,python,6299,1454807235,1454812489,1454812489,87,87,commits_in_master,false,false,false,21,2,1,0,2,0,2,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,5.037994459345753,0.10440433074805938,4,tom.duprelatour@orange.fr,appveyor.yml,4,0.011834319526627219,0,0,false,[WIP] try to fix appveyor failures Recently appveyor started to fail with pip refusing to install the locally built scikit-learn wheel,,3562,0.7251544076361595,0.17455621301775148,51390,532.360381397159,39.36563533761432,136.115975870792,9405,42,1999,403,travis,ogrisel,ogrisel,true,ogrisel,148,0.8716216216216216,1247,124,2446,true,true,false,false,15,151,36,49,46,0,87
12119297,scikit-learn/scikit-learn,python,6296,1454788447,1454850420,1454850420,1032,1032,commits_in_master,false,false,false,27,3,3,0,1,0,1,0,1,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,8.550895227097445,0.17720601427160992,0,,doc/modules/decomposition.rst|doc/tutorial/machine_learning_map/index.rst,0,0.0,0,0,false,Correct a couple of documentation typos I found a couple of typos a while ago and forgot to create a PR Hopefully better late than neverPaul,,3561,0.7250772254984554,0.17559523809523808,51390,532.1074138937536,39.36563533761432,136.077057793345,9402,42,1999,404,travis,pestrickland,agramfort,false,agramfort,0,0,2,1,959,false,false,false,false,0,0,0,0,0,0,1032
12116802,scikit-learn/scikit-learn,python,6294,1454769141,1455074334,1455074334,5086,5086,commits_in_master,false,false,false,35,2,2,2,0,0,2,0,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,17.310623310487088,0.35873981377852765,47,tom.dupre-la-tour@m4x.org,doc/index.rst|doc/whats_new.rst|doc/index.rst|doc/whats_new.rst,41,0.12130177514792899,0,0,false,Typo fixes in the current changelog Finally found the time to read through the latest changelog Not that important but a few minor typos caught my eye and I thought Id just send them along,,3560,0.725,0.1804733727810651,51390,532.1074138937536,39.36563533761432,136.077057793345,9399,42,1999,409,travis,rasbt,amueller,false,amueller,8,1.0,1460,43,854,true,true,true,false,0,10,2,8,1,0,4970
12116612,scikit-learn/scikit-learn,python,6293,1454767686,1455067451,1455067451,4996,4996,commits_in_master,false,false,false,13,2,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,4,0,4.495739799868408,0.09316827185676463,0,,sklearn/neighbors/binary_tree.pxi,0,0.0,0,3,false,[MRG] DOC Remove redundant and in binary_treepxi Remove redundant and in the doc,,3559,0.7249227311042428,0.1804733727810651,51390,532.1074138937536,39.36563533761432,136.077057793345,9399,42,1999,409,travis,yenchenlin1994,amueller,false,amueller,7,0.7142857142857143,25,15,683,false,true,false,false,3,17,8,0,0,0,2858
12116581,scikit-learn/scikit-learn,python,6292,1454767370,1455067465,1455067465,5001,5001,commits_in_master,false,false,false,18,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.093084342110809,0.08482376910016202,8,toastedcornflakes@gmail.com,sklearn/model_selection/_split.py,8,0.023668639053254437,0,0,false,Fix check_cv error message ValueError message in check_cv should be or an iterable instead of or and iterable,,3558,0.7248454187745925,0.1804733727810651,51390,532.1074138937536,39.36563533761432,136.077057793345,9399,42,1999,408,travis,yenchenlin1994,amueller,false,amueller,6,0.6666666666666666,25,15,683,false,true,false,false,3,17,7,0,0,0,-1
12115042,scikit-learn/scikit-learn,python,6291,1454757596,1455133567,1455133567,6266,6266,commit_sha_in_comments,false,false,false,51,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.349337684506461,0.09013428130311946,0,,examples/applications/svm_gui.py,0,0.0,0,1,false,Adding Python 3 tkinter compatibility to svm_guipy example modified:   examples/applications/svm_guipyFor some reason the Tkinter package is not compatible with Python 3try:        import Tkinter as Tk #works with Python 2except ImportError:    import tkinter as Tk #works with Python 3 I added in an additional import statement to add compatibility,,3557,0.7247680629744166,0.1804733727810651,51390,532.1074138937536,39.36563533761432,136.077057793345,9398,42,1999,408,travis,vatsg,ogrisel,false,ogrisel,0,0,0,0,98,false,false,false,false,0,0,0,0,1,0,5166
12104982,scikit-learn/scikit-learn,python,6289,1454716565,,1454747171,510,,unknown,false,false,false,25,2,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,27,0,4.376046930459717,0.09068779516264822,2,jakevdp@gmail.com,sklearn/neighbors/dist_metrics.pyx,2,0.0058309037900874635,0,0,true,Modify checks in PyFunc distance Modify pilot checks in PyFunc distance since original check which first calls dist function with 10-d vectors is not needed,,3556,0.7249718785151856,0.17784256559766765,51390,532.1074138937536,39.36563533761432,136.077057793345,9394,42,1998,400,travis,yenchenlin1994,yenchenlin1994,true,,5,0.8,25,15,682,false,true,false,false,3,11,5,0,0,0,19
12094421,scikit-learn/scikit-learn,python,6286,1454686130,1454689557,1454689557,57,57,commits_in_master,false,false,false,47,2,2,0,1,1,2,0,1,0,0,3,3,2,0,0,0,0,3,3,2,0,0,100,44,100,44,18.50851613294472,0.3835620260689614,41,tom.dupre-la-tour@m4x.org,sklearn/tests/test_isotonic.py|doc/whats_new.rst|sklearn/isotonic.py|sklearn/tests/test_isotonic.py,39,0.005698005698005698,1,0,true,[MRG+2] Fix isotonic performance issue at prediction time This is a followup for #6206 as @jarfa is AFK for 2 weeks I has already been reviewed there I just want to make sure that the changes I made to address my last comment do not break CI,,3554,0.7250984805852561,0.18233618233618235,51358,532.030063475992,39.370692005140384,136.1423731453717,9390,43,1998,400,travis,ogrisel,ogrisel,true,ogrisel,147,0.8707482993197279,1247,124,2445,true,true,false,false,16,139,31,49,42,0,57
12092064,scikit-learn/scikit-learn,python,6285,1454675908,1455067636,1455067636,6528,6528,commits_in_master,false,false,false,39,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.448386486693273,0.092186327705483,0,,sklearn/feature_extraction/dict_vectorizer.py,0,0.0,2,0,true,[MRG] Doc: Add doc in DictVectorizer when categorical features are numeric values Ive added a paragraph to clarify the use of DictVectorizer when users categorical features are represented as numeric valuesSolve issue #4413 Please review - @amueller @jnothman ,,3553,0.7250211089220377,0.1807909604519774,51358,532.030063475992,39.370692005140384,136.1423731453717,9389,43,1998,408,travis,yenchenlin1994,amueller,false,amueller,4,0.75,25,15,682,false,true,false,false,2,9,4,0,0,0,64
12062852,scikit-learn/scikit-learn,python,6277,1454566519,1454598121,1454598121,526,526,commits_in_master,false,false,false,28,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.624414129064772,0.09583296935816851,0,,doc/modules/scaling_strategies.rst,0,0.0,0,0,false,Added LatentDirichletAllocation to the Incremental learning page Since the LatentDirichletAllocation has a partial_fit method now I thought it should be added to the incremental learning page at http://scikit-learnorg/stable/modules/scaling_strategieshtml#incremental-learning,,3549,0.725556494787264,0.1679790026246719,51352,531.7027574388534,39.3558186633432,136.119333229475,9371,44,1997,401,travis,rasbt,ogrisel,false,ogrisel,7,1.0,1460,43,852,true,true,true,false,0,8,1,8,1,0,280
12046817,scikit-learn/scikit-learn,python,6272,1454527321,1454592975,1454592975,1094,1094,commits_in_master,false,false,false,8,1,1,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,24,5,24,8.841116173910265,0.18321684063546506,0,,sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/tests/test_dist_metrics.py,0,0.0,0,1,false,BUG: fix pickling of DistanceMetric instances (fixes #6269) ,,3547,0.7256836763462081,0.16957605985037408,51352,531.7027574388534,39.3558186633432,136.119333229475,9369,46,1996,402,travis,jakevdp,agramfort,false,agramfort,53,0.8679245283018868,1874,0,1729,false,true,false,true,1,21,2,18,1,0,-1
12042298,scikit-learn/scikit-learn,python,6270,1454512471,1454800246,1454800246,4796,4796,commits_in_master,false,false,false,8,16,16,0,6,0,6,0,1,0,0,10,10,9,0,1,0,0,10,10,9,0,1,22,466,22,466,89.58086103948271,1.8564223542556393,12,tom.duprelatour@orange.fr,sklearn/gaussian_process/tests/test_kernels.py|sklearn/model_selection/tests/test_split.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/utils/tests/test_extmath.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/utils/tests/test_extmath.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/model_selection/tests/test_split.py|.travis.yml|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/gaussian_process/tests/test_kernels.py,7,0.004987531172069825,0,0,false,[MRG] Reduce test running time Followup on #5711,,3546,0.7256063169768754,0.16957605985037408,51357,531.6509920750823,39.35198707089589,136.10608096267305,9368,47,1996,412,travis,ogrisel,ogrisel,true,ogrisel,146,0.8698630136986302,1247,124,2443,true,true,false,false,14,108,23,37,45,0,31
12017315,scikit-learn/scikit-learn,python,6268,1454435315,1454801463,1454801463,6102,6102,commits_in_master,false,false,false,20,2,2,0,12,0,12,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,161,0,161,18.176232216764422,0.3766738052280041,0,,sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_spectral_embedding.py,0,0.0,0,1,false,[MRG] FIX performance issue in _graph_connected_component This is a reworked version of the fix in #5713 with a new test,,3545,0.7255289139633286,0.16464891041162227,51357,531.6509920750823,39.35198707089589,136.10608096267305,9352,47,1995,411,travis,ogrisel,ogrisel,true,ogrisel,145,0.8689655172413793,1247,124,2442,true,true,false,false,14,100,22,37,45,0,1
12016857,scikit-learn/scikit-learn,python,6267,1454433649,1454623020,1454623020,3156,3156,commits_in_master,false,false,false,17,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.227718928925178,0.08761282082014392,5,t3kcit@gmail.com,sklearn/decomposition/online_lda.py,5,0.012135922330097087,0,0,false,[MRG] Fix typo in LDA parameters doc Ive removed redundant and in parameter evaluate_every s doc string,,3544,0.725451467268623,0.1650485436893204,51357,531.6509920750823,39.35198707089589,136.10608096267305,9352,47,1995,403,travis,yenchenlin1994,ogrisel,false,ogrisel,2,1.0,25,15,679,false,true,false,false,1,2,2,0,0,0,-1
12008480,scikit-learn/scikit-learn,python,6264,1454399328,1454598245,1454598245,3315,3315,commits_in_master,false,false,false,29,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.906929047225306,0.10168838151712396,0,,examples/tree/plot_tree_regression_multioutput.py,0,0.0,0,1,false,Fix axis label in the plot of multioutput regression tree example In this plot X- and Y-axis are used to describe target-1 and target-2 respectivelySolve issue #6262 ,,3543,0.725373976855772,0.162291169451074,51357,531.6509920750823,39.35198707089589,136.10608096267305,9347,48,1995,402,travis,yenchenlin1994,ogrisel,false,ogrisel,1,1.0,25,15,679,false,true,false,false,1,2,1,0,0,0,199
11990679,scikit-learn/scikit-learn,python,6260,1454352335,1455224032,1455224032,14528,14528,commits_in_master,false,false,false,72,2,2,2,11,0,13,0,2,3,0,9,12,9,1,2,3,0,9,12,9,1,2,516,380,516,380,104.85816669455527,2.1730204690959583,21,toastedcornflakes@gmail.com,.travis.yml|circle.yml|doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/feature_extraction.rst|doc/themes/scikit-learn/layout.html|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning_fixture.py|examples/cluster/plot_face_compress.py|examples/cluster/plot_face_segmentation.py|examples/cluster/plot_face_ward_segmentation.py|examples/decomposition/plot_image_denoising.py|sklearn/feature_extraction/tests/test_image.py|.travis.yml|circle.yml|continuous_integration/circle/build_doc.sh|continuous_integration/circle/check_build_doc.py|continuous_integration/circle/push_doc.sh|doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/feature_extraction.rst|doc/themes/scikit-learn/layout.html|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning_fixture.py|examples/cluster/plot_face_compress.py|examples/cluster/plot_face_segmentation.py|examples/cluster/plot_face_ward_segmentation.py|examples/decomposition/plot_image_denoising.py|sklearn/feature_extraction/tests/test_image.py,8,0.0,0,5,false,[WIP] Updated examples and tests that use scipys lena This is a followup on #5920:- squashed all the commits into one (and removed the annoying merged commit in the middle) to cleanup the history and make it easier to cherry-pick this fix to 0171 later- fixed a dtype related error in the denoising example with recent numpyI have not fixed the circle ci configuration yet Working on it next,,3542,0.7252964426877471,0.16203703703703703,51357,531.6509920750823,39.35198707089589,136.10608096267305,9338,48,1994,415,travis,ogrisel,amueller,false,amueller,144,0.8680555555555556,1247,124,2441,true,true,false,true,16,97,20,32,44,0,11931
11966091,scikit-learn/scikit-learn,python,6254,1454230812,1455827848,1455827848,26617,26617,commits_in_master,false,false,false,44,3,2,1,9,0,10,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,320,0,333,0,9.046112969795196,0.18746617938900878,2,mks542@nyu.edu,sklearn/_build_utils/cythonize.py|sklearn/_build_utils/cythonize.py,2,0.004608294930875576,4,2,false,[MRG] ENH/MAINT Check for changes in pxd files too Cleanup code The cython code should compile when there is a change in the corresponding pxd file tooAlso cleaned up the variable names and simplified a bitPlease review - @arthurmensch @amueller @ogrisel @MechCoder ,,3541,0.7252188647274781,0.16129032258064516,51357,531.6509920750823,39.35198707089589,136.10608096267305,9321,48,1993,431,travis,rvraghav93,ogrisel,false,ogrisel,34,0.6176470588235294,29,43,849,true,false,true,true,10,315,12,158,12,0,6539
11959785,scikit-learn/scikit-learn,python,6253,1454199814,,1454199870,0,,unknown,false,false,false,5,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.528299949930438,0.09384175209566481,2,nelson.liu.2009@gmail.com,doc/faq.rst,2,0.004597701149425287,0,0,false,Rewordin algorithm requirements in FAQ ,,3540,0.7254237288135593,0.16091954022988506,51357,531.6509920750823,39.35198707089589,136.10608096267305,9317,48,1992,398,travis,halwai,halwai,true,,2,0.5,4,22,497,false,true,false,false,2,16,4,13,0,0,-1
11917179,scikit-learn/scikit-learn,python,6244,1454036460,1454071705,1454071705,587,587,commits_in_master,false,false,false,18,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.424167073017179,0.09168376662926937,4,t3kcit@gmail.com,sklearn/datasets/samples_generator.py,4,0.00904977375565611,0,0,false,Fixes make_sparse_spd_matrix documentation Updates the documentation of the parameter alpha in make_sparse_spd_matrix() to reflect its effect in sparsity,,3537,0.7257562906417868,0.1583710407239819,51357,531.6509920750823,39.35198707089589,136.10608096267305,9288,48,1990,396,travis,harrymvr,agramfort,false,agramfort,1,1.0,12,21,2483,false,false,false,false,0,0,0,0,0,0,-1
11907796,scikit-learn/scikit-learn,python,6241,1454015959,1454525320,1454525320,8489,8489,commits_in_master,false,false,false,42,2,1,1,4,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,21,0,4.744035242049904,0.09831375477023505,0,,examples/tree/plot_iris.py,0,0.0,0,1,false,remove shuffling and standardization and fix layout The example shuffles and standardises the dataset AFAIK this is not necessary for decision trees This PR removes that code and fixes the layout so that legends and labels do not overlap with other figures,,3536,0.7256787330316742,0.15873015873015872,51355,531.0096387888229,39.35351961834291,136.11138155973129,9285,48,1990,404,travis,louridas,ogrisel,false,ogrisel,0,0,72,0,1687,false,false,false,false,0,0,0,0,0,0,1165
11877241,scikit-learn/scikit-learn,python,6238,1453915935,1453991650,1453991650,1261,1261,commits_in_master,false,false,false,11,1,1,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.231413989652514,0.08769109588748521,4,nextrush@163.com,sklearn/feature_extraction/text.py,4,0.009029345372460496,0,0,false,Typo The text should say negative value for max_df *or* min_df,,3534,0.7258064516129032,0.1580135440180587,51355,531.0096387888229,39.35351961834291,136.11138155973129,9265,49,1989,400,travis,amlankar,ogrisel,false,ogrisel,0,0,0,4,618,false,false,false,false,0,0,0,0,0,0,68
11850472,scikit-learn/scikit-learn,python,6235,1453834325,1453980117,1453980117,2429,2429,commits_in_master,false,false,false,16,3,1,1,5,0,6,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,12,21,24,42,9.342571851135117,0.19361409134279653,0,,sklearn/datasets/base.py|sklearn/datasets/tests/test_base.py,0,0.0,0,1,false,[MRG] Fix reading of Bunch pickles that have been generated with scikit-learn  017 Fix #6196,,3532,0.7259343148357871,0.15349887133182843,51356,520.7181244645221,38.9243710569359,132.83744839940806,9247,49,1988,406,travis,lesteve,MechCoder,false,MechCoder,25,0.92,4,0,1371,true,false,false,false,2,50,4,14,7,0,1
11843416,scikit-learn/scikit-learn,python,6232,1453804545,1454598959,1454598959,13240,13240,commits_in_master,false,false,false,63,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.155704007991027,0.08612220148984366,0,,sklearn/datasets/twenty_newsgroups.py,0,0.0,0,0,false,MAINT: Print info message for fetch_20newsgroups Minor change to give a simple message to inform the user about downloading the dataset and that it is going to take some time See issue #1346  I have added the message when there is no cache copy and when it downloads a new one Please let me know if something more is expected here Thanks,,3530,0.7260623229461757,0.1493212669683258,51356,520.7181244645221,38.9243710569359,132.83744839940806,9236,49,1988,412,travis,maniteja123,ogrisel,false,ogrisel,6,0.16666666666666666,4,16,756,true,false,false,false,3,74,6,31,2,0,13240
11841699,scikit-learn/scikit-learn,python,6230,1453794522,1453991490,1453991490,3282,3282,commits_in_master,false,false,false,43,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.2870069458746665,0.08884330435253042,2,gael.varoquaux@normalesup.org,sklearn/cluster/hierarchical.py,2,0.0045045045045045045,0,0,false,Make doc for parameter in hierarchicalpy consistent Doc for parameter children in function _hc_cut  is different from itself in other functions in hierarchicalpyI think it is more clear for code reader if docs for parameter children are consistent throughout all functions,,3529,0.7259846982147917,0.14864864864864866,51356,520.7181244645221,38.9243710569359,132.83744839940806,9235,49,1988,405,travis,yenchenlin1994,ogrisel,false,ogrisel,0,0,25,15,672,false,true,false,false,0,2,0,0,0,0,1172
11833822,scikit-learn/scikit-learn,python,6229,1453770852,1453779740,1453779740,148,148,github,false,false,false,10,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,5,1,5,8.87059178175491,0.18383299617614124,4,olologin@gmail.com,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,4,0.00904977375565611,0,0,false,[MRG+1] LabelEncoder now raises error for 0-D arrays Fixes #6226,,3528,0.7259070294784581,0.1493212669683258,51355,520.6503748417875,38.92512900399182,132.80109044883653,9231,49,1987,401,travis,dsquareindia,amueller,false,amueller,11,0.36363636363636365,1,0,455,true,true,false,false,5,66,12,15,7,0,115
11832644,scikit-learn/scikit-learn,python,6227,1453767901,1453860128,1453860128,1537,1537,commits_in_master,false,false,false,138,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,16,21,16,21,9.010785821101889,0.1867383593056,11,t3kcit@gmail.com,sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py,11,0.024886877828054297,0,0,false,[MRG + 1] classification_report format supports long string labels Heres an improvement to the sklearnmetricsclassification_report output when y_true and y_pred args contain long* string labelsAssumption: no target_names arg is provided*long in this case meaning having a length greater than the default row-title string (currently  avg / total ) which appears in the last row of the report**Demo:**pythonfrom __future__ import print_functionfrom sklearnmetricsclassification import classification_reportoutput  classification_report([a b*20 a] [b*20 b*20 a])print(output)before format enhancement:             precision    recall  f1-score   support          a       100      050      067         2bbbbbbbbbbbbbbbbbbbb       050      100      067         1avg / total       083      067      067         3after format enhancement:                      precision    recall  f1-score   support                   a       100      050      067         2bbbbbbbbbbbbbbbbbbbb       050      100      067         1         avg / total       083      067      067         3**Classification test cases:**nosetests sklearn/metrics/tests/test_classificationpy,,3527,0.7258293166997448,0.1493212669683258,51355,520.6503748417875,38.92512900399182,132.80109044883653,9230,49,1987,405,travis,strixcuriosus,GaelVaroquaux,false,GaelVaroquaux,0,0,138,29,802,false,false,false,false,0,0,0,0,0,0,203
11826119,scikit-learn/scikit-learn,python,6225,1453752808,1454034237,1454034237,4690,4690,commits_in_master,false,false,false,43,5,4,7,6,0,13,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,11,149,11,149,17.892584793399823,0.37080361184827576,6,t3kcit@gmail.com,sklearn/metrics/ranking.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/ranking.py,3,0.006787330316742082,0,3,false,[MRG] make sure that AUC is always a float Even when computed on memory mapped data as reported in #6147Lets make sure that the new tests pass under AppVeyor before merging as deletion of mmap files tends to be troublesome under Windows,,3526,0.7257515598411798,0.1493212669683258,51355,520.6503748417875,38.92512900399182,132.80109044883653,9229,49,1987,404,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,143,0.8671328671328671,1247,124,2434,true,true,true,true,20,112,21,29,41,0,458
11823696,scikit-learn/scikit-learn,python,6223,1453746385,,1453750942,75,,unknown,false,false,false,68,1,1,0,0,0,0,0,2,1,0,1,2,2,0,0,1,0,1,2,2,0,0,274,0,274,0,9.31744883373418,0.19309360389531813,8,t3kcit@gmail.com,sklearn/externals/joblib/_backends.py|sklearn/externals/joblib/parallel.py,8,0.018223234624145785,0,1,false,Implemented the possibility to add custom Parallel backends Moreover the default backend (multiprocessing) can be overwrittenSome context:Ive been experimenting to see if I can get scikit-learn to run on top of YARN I dont want to add all the hdfs/yarn dependencies into this project so I made a small change to the Parallel code such that I can now add a YARN backend to it easily,,3525,0.7259574468085106,0.14350797266514806,51355,520.6503748417875,38.92512900399182,132.80109044883653,9226,49,1987,399,travis,NielsZeilemaker,NielsZeilemaker,true,,0,0,3,0,1056,false,false,false,false,0,0,0,0,0,0,-1
11811146,scikit-learn/scikit-learn,python,6222,1453688850,,1453845488,2610,,unknown,false,false,false,37,2,2,0,2,0,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,151,6,151,6,8.91129872980811,0.18467660169986716,0,,sklearn/manifold/mds.py|sklearn/manifold/tests/test_mds.py,0,0.0,0,0,false,Mds extend Extended MDS object to include a transform method for out-of-sample points as described here:http://papersnipscc/paper/2461-out-of-sample-extensions-for-lle-isomap-mds-eigenmaps-and-spectral-clusteringpdfSMACOF algorithm is used in the non-extendible case but not for the extendible case which requires eigen-decomposition of (dis)similarity matrix,,3524,0.7261634506242906,0.14482758620689656,51355,520.6503748417875,38.92512900399182,132.80109044883653,9202,48,1986,405,travis,webdrone,webdrone,true,,0,0,1,1,949,false,false,false,false,0,1,0,0,0,0,830
11803331,scikit-learn/scikit-learn,python,6218,1453645518,1453992184,1453992184,5777,5777,commits_in_master,false,false,false,10,2,2,8,3,0,11,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.236563525257669,0.19141734722950562,2,t3kcit@gmail.com,doc/modules/clustering.rst|doc/modules/clustering.rst,2,0.004524886877828055,0,0,false,update note on Input Data in clustring docs Resolves #6193 ,,3521,0.7264981539335416,0.1425339366515837,51355,520.6503748417875,38.92512900399182,132.80109044883653,9195,48,1986,403,travis,mth4saurabh,ogrisel,false,ogrisel,7,0.8571428571428571,9,6,916,true,true,false,false,0,15,6,10,1,0,278
11792472,scikit-learn/scikit-learn,python,6216,1453582652,,1455695035,35206,,unknown,false,true,false,39,1,1,3,5,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.393714316477108,0.09105476692110842,18,trev.stephens@gmail.com,sklearn/preprocessing/data.py,18,0.0400890868596882,0,3,false,ENH: Add feature_names_ property to PolynomialFeatures Added the property of feature_names_ for PolynomialFeatures in preprocessing See issue #6185 I am not totally sure this is the expected solution Please let me know if something else needs to be done,,3519,0.7269110542767832,0.1403118040089087,51355,520.6503748417875,38.92512900399182,132.80109044883653,9180,48,1985,432,travis,maniteja123,maniteja123,true,,4,0.25,4,16,753,true,false,false,false,3,62,4,27,2,0,6947
11789994,scikit-learn/scikit-learn,python,6214,1453566513,1453780905,1453780905,3573,3573,commits_in_master,false,false,false,11,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.020080523287512,0.08331163159116128,2,nickjljames@gmail.com,sklearn/linear_model/randomized_l1.py,2,0.004310344827586207,0,0,false,[MRG] DOC : minor fix in RandomizedLasso API docs Fixes #5790 ,,3517,0.7270400909866364,0.14870689655172414,51355,520.6503748417875,38.92512900399182,132.80109044883653,9173,51,1985,403,travis,mth4saurabh,amueller,false,amueller,6,0.8333333333333334,9,6,915,true,true,false,false,0,15,5,10,1,0,-1
11787566,scikit-learn/scikit-learn,python,6213,1453544660,1453571019,1453571019,439,439,commits_in_master,false,false,false,23,2,2,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.577646791634757,0.19848591996918957,42,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|doc/whats_new.rst,42,0.08713692946058091,0,0,false,Added link to my profile in whats_newrst :P Sorry for forgetting to include it in #6182 Should link to my profile from [here](http://scikit-learnorg/dev/whats_new#bug-fixes),,3516,0.726962457337884,0.14730290456431536,51355,520.6503748417875,38.92512900399182,132.80109044883653,9166,54,1985,394,travis,dsquareindia,agramfort,false,agramfort,9,0.3333333333333333,1,0,453,true,true,false,false,5,63,10,15,7,0,439
11781483,scikit-learn/scikit-learn,python,6212,1453515714,1453541590,1453541590,431,431,commits_in_master,false,false,false,10,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.355871859985264,0.09027032948149423,14,tom.dupre-la-tour@m4x.org,sklearn/linear_model/ridge.py,14,0.029045643153526972,0,0,true,FIX: doc KernelRidge closes https://githubcom/scikit-learn/scikit-learn/issues/6210#issuecomment-174003862Lets see whether I managed,,3515,0.7268847795163584,0.14730290456431536,51351,520.4572452337833,38.92816108741796,132.7724873907032,9160,54,1984,394,travis,kingjr,MechCoder,false,MechCoder,0,0,5,4,937,false,true,false,false,0,2,0,0,0,0,-1
11774673,scikit-learn/scikit-learn,python,6208,1453496698,1453499622,1453499622,48,48,commits_in_master,false,false,false,15,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.939118723469339,0.10235786784386847,1,gael.varoquaux@normalesup.org,AUTHORS.rst,1,0.002074688796680498,0,0,true,[MRG] DOC add jmetzen and tomdlt to authorsrst This PR supplements recently merged #5219 :wink: ,,3514,0.7268070574843484,0.14937759336099585,51351,520.4572452337833,38.92816108741796,132.7724873907032,9159,54,1984,394,travis,TomDLT,agramfort,false,agramfort,27,0.6296296296296297,7,4,338,true,false,true,true,26,104,22,53,16,0,-1
11754858,scikit-learn/scikit-learn,python,6206,1453425531,1453711524,1453711524,4766,4766,merged_in_comments,false,true,false,179,8,2,22,27,0,49,0,9,1,0,1,5,2,0,0,1,1,3,5,3,0,0,64,0,189,67,9.252079761613972,0.19230450535141336,0,,examples/fast_isotonic.py|sklearn/isotonic.py,0,0.0,0,14,false,Much faster prediction with isotonic regression This change add an optional parameter to IsotonicRegressionfit() fast_predict Setting this to True speeds up prediction by 3 orders of magnitude on my tests doesnt have a meaningful effect on training time and has no effect at all on the values that are predicted Unless a user cares about storing the fitted values of the training data its an unqualified improvement However in order to avoid breaking legacy code that depends values like selfX_ and selfy_ I left the default value of fast_predict to False (in other words no speedup)This is my first contribution to sklearn so please let me know if I need anything else to get this merged into master Sample output from the examples/fast_isotonicpy script (also in this pull request) is below:Training sample size: 100000Prediction sample size: 100000Training the old model took 003661 secondsTraining the new model took 003391 secondsPredicting with the old model took 360884 secondsPredicting with the new model took 000439 secondsMaximum absolute difference between new and old predictions: 0000000,,3513,0.726729291204099,0.18796992481203006,51209,518.3463844246129,38.84082876056943,132.71104688628952,9118,54,1983,417,travis,jarfa,jarfa,true,jarfa,0,0,4,11,849,false,false,false,false,0,0,0,0,0,0,10
11730861,scikit-learn/scikit-learn,python,6201,1453343288,1453907523,1453907523,9403,9403,commits_in_master,false,false,false,47,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,33,5,33,9.193507023148,0.19108707080826243,2,gael.varoquaux@normalesup.org,sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py,2,0.0033003300330033004,0,0,false,[MRG] TST: More tests for Ledoit-Wolf Fixes #6195Indeed #6195 was not a bug: the code in scikit-learn is correctHowever it is fairly hard to convinced oneself that it is the caseThis commit adds tests that are easier to read and relate to thepublication,,3512,0.7266514806378133,0.21287128712871287,51209,518.3463844246129,38.84082876056943,132.71104688628952,9079,59,1982,406,travis,GaelVaroquaux,ogrisel,false,ogrisel,60,0.8,716,3,2158,true,true,true,true,28,204,26,44,25,1,727
11728463,scikit-learn/scikit-learn,python,6200,1453337091,,1453853869,8612,,unknown,false,false,false,61,2,1,6,5,0,11,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,34,19,50,9.438087283631036,0.19617067224953505,11,t3kcit@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,7,0.01155115511551155,2,2,false,Fixed error in multiclasspy where it failed in multiple classes Following up the recent PR #6087 implementing the feature of partial_fit in OvO and OvR it contained logical error in tests(for which I am solely responsible) where it incorrectly tested for mini-batches which doesnt contain all target classesRegardless I have addressed the issue here @MechCoder @GaelVaroquaux please have a look ,,3511,0.7268584448874964,0.21287128712871287,51209,518.3463844246129,38.84082876056943,132.71104688628952,9079,59,1982,407,travis,kaichogami,kaichogami,true,,1,1.0,18,29,735,true,false,false,false,0,10,1,5,1,0,184
11728359,scikit-learn/scikit-learn,python,6199,1453336843,,1453844413,8459,,unknown,false,false,false,32,1,1,2,9,0,11,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,105,0,105,0,4.478043194120122,0.09307614110292874,4,manojkumarsivaraj334@gmail.com,sklearn/utils/validation.py,4,0.006600660066006601,1,1,false,DOC: Add Raises Section Added Raises section to utilsvalidation This was an idea by @rvraghav93 which seemed helpful If feasible can do it for other modules too Otherwise please close the PR,,3510,0.7270655270655271,0.21287128712871287,51209,518.3463844246129,38.84082876056943,132.71104688628952,9079,59,1982,408,travis,maniteja123,maniteja123,true,,3,0.3333333333333333,4,16,750,true,false,false,false,3,55,3,26,2,0,624
11704002,scikit-learn/scikit-learn,python,6194,1453255828,,1456885153,60488,,unknown,false,false,false,40,3,1,5,12,0,17,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,48,0,4.539766638591739,0.09436108547985779,4,nextrush@163.com,sklearn/feature_extraction/text.py,4,0.006389776357827476,0,2,false,Support new scipy sparse array indices which can now be  2^31 ( 2^63)This is needed for very large training setsFeature indices (based on the number of distinct features) are unlikely to need 4 bytes per value however,,3509,0.7272727272727273,0.21884984025559107,51213,517.8763204655068,38.83779509109015,132.64210259113895,9052,60,1981,451,travis,mannby,mannby,true,,0,0,1,12,1888,false,false,false,false,0,0,0,0,0,0,933
11691836,scikit-learn/scikit-learn,python,6188,1453223693,,1453902122,11307,,unknown,false,false,false,43,1,1,2,8,0,10,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,16,14,16,8.78666782063254,0.18281462139099466,15,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,15,0.02332814930015552,0,1,false,Make sure that KFold indices are shuffled At the moment KFold returns indices sorted in ascending order ([0 2 4]) regardless of the shuffle setting This commit fixes this and adds a testThis may break existing tests that rely on this behaviour,,3508,0.7274800456100342,0.2192846034214619,51178,518.2304896635272,38.86435577787331,132.73281488139435,9044,62,1981,406,travis,maciejkula,maciejkula,true,,1,1.0,41,0,1215,false,false,false,false,0,0,0,0,0,0,197
11688795,scikit-learn/scikit-learn,python,6187,1453212480,1453212542,1453212542,1,1,commits_in_master,false,false,false,21,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.967528888810505,0.10335393578659977,13,tom.dupre-la-tour@m4x.org,doc/modules/linear_model.rst,13,0.02,0,0,false,[MRG] DOC: Update link to a paper Original [link to paper](http://booksnipscc/papers/files/nips20/NIPS2007_0976pdf) no longer works so updated it to the [correct one](http://papersnipscc/paper/3372-a-new-view-of-automatic-relevance-determinationpdf),,3507,0.7274023381807813,0.22,51178,518.2304896635272,38.86435577787331,132.73281488139435,9038,64,1981,391,travis,mth4saurabh,agramfort,false,agramfort,5,0.8,9,6,911,true,true,false,false,0,15,4,10,1,0,-1
11676660,scikit-learn/scikit-learn,python,6182,1453162713,,1453540432,6295,,unknown,false,false,false,10,2,2,13,16,0,29,0,4,0,0,5,5,4,0,0,0,0,5,5,4,0,0,26,60,26,60,39.32830237215247,0.8182609701707998,69,tom.dupre-la-tour@m4x.org,sklearn/cross_validation.py|sklearn/model_selection/_split.py|sklearn/model_selection/tests/test_split.py|sklearn/tests/test_cross_validation.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/model_selection/_split.py|sklearn/model_selection/tests/test_split.py|sklearn/tests/test_cross_validation.py,52,0.013533834586466165,0,15,false,SKF raises error if n_labelsn_folds for individual classes Addresses #6177,,3506,0.7276098117512835,0.22406015037593985,51172,528.3553505823497,39.33791917454858,136.01188149769405,9017,65,1980,398,travis,dsquareindia,MechCoder,false,,8,0.375,1,0,448,true,true,false,false,5,44,9,10,7,0,2
11671214,scikit-learn/scikit-learn,python,6179,1453147469,1453294710,1453294710,2454,2454,commits_in_master,false,false,false,43,2,2,4,6,0,10,0,5,0,0,12,12,11,0,0,0,0,12,12,11,0,0,290,182,290,182,53.994830431114245,1.1234113772491638,57,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_compat.py|sklearn/externals/joblib/format_stack.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/my_exceptions.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/testing.py,52,0.0029985007496251873,0,5,false,[MRG] Joblib 094 Here is a code sync for joblib 094 In particular it solves a bug that can cause silently wrong results as report in #6063 Therefore I would like to make backport this in the maintenance branch to release 0171 ASAP,,3503,0.7279474735940622,0.22338830584707647,51172,528.3553505823497,39.33791917454858,136.01188149769405,9012,65,1980,393,travis,ogrisel,ogrisel,true,ogrisel,142,0.8661971830985915,1247,124,2427,true,true,false,false,30,185,30,71,52,1,48
11623549,scikit-learn/scikit-learn,python,6174,1452896454,,1452897224,12,,unknown,false,false,false,4,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,Merge remote-tracking branch refs/remotes/scikit-learn/master  ,,3500,0.7285714285714285,0.23668639053254437,51195,527.7273171208126,39.28117980271511,135.87264381287235,8943,65,1977,389,travis,RPGOne,glouppe,false,,3,0.6666666666666666,0,0,133,false,true,false,false,0,0,3,0,0,0,-1
11612693,scikit-learn/scikit-learn,python,6171,1452857633,1452862203,1452862203,76,76,commits_in_master,false,false,true,2,250,250,0,0,0,0,0,0,5,0,206,211,174,2,8,5,0,206,211,174,2,8,3061,986,3061,986,1461.2277642644513,36.05990468763818,0,,sklearn/linear_model/tests/test_logistic.py|doc/themes/scikit-learn/layout.html|doc/whats_new.rst|doc/modules/calibration.rst|doc/modules/gaussian_process.rst|doc/tutorial/basic/tutorial.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|doc/modules/gaussian_process.rst|doc/modules/cross_validation.rst|doc/themes/scikit-learn/layout.html|sklearn/ensemble/bagging.py|sklearn/tests/test_discriminant_analysis.py|doc/modules/model_evaluation.rst|sklearn/utils/multiclass.py|sklearn/ensemble/iforest.py|sklearn/tests/test_multiclass.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_ridge.py|appveyor.yml|continuous_integration/appveyor/requirements.txt|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/tests/test_multiclass.py|sklearn/dummy.py|doc/related_projects.rst|examples/preprocessing/plot_robust_scaling.py|doc/related_projects.rst|examples/decomposition/plot_pca_iris.py|sklearn/ensemble/voting_classifier.py|doc/related_projects.rst|examples/applications/topics_extraction_with_nmf_lda.py|doc/related_projects.rst|doc/modules/gaussian_process.rst|examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_ridge_path.py|sklearn/decomposition/pca.py|doc/modules/neural_networks_supervised.rst|README.rst|doc/related_projects.rst|README.rst|README.rst|doc/developers/advanced_installation.rst|doc/whats_new.rst|sklearn/linear_model/randomized_l1.py|doc/data_transforms.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/neural_network/rbm.py|.mailmap|doc/whats_new.rst|examples/applications/topics_extraction_with_nmf_lda.py|doc/sphinxext/numpy_ext/docscrape.py|doc/tutorial/machine_learning_map/pyparsing.py|README.rst|doc/developers/advanced_installation.rst|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|benchmarks/bench_plot_randomized_svd.py|sklearn/model_selection/_split.py|doc/datasets/index.rst|doc/modules/clustering.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/ensemble/plot_partial_dependence.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/ensemble/voting_classifier.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py|sklearn/metrics/classification.py|sklearn/metrics/regression.py|sklearn/preprocessing/data.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/feature_extraction/text.py|sklearn/linear_model/bayes.py|doc/about.rst|doc/images/nyu_short_color.png|doc/index.rst|doc/themes/scikit-learn/static/img/nyu_short_color.png|sklearn/preprocessing/label.py|sklearn/pipeline.py|examples/cluster/plot_segmentation_toy.py|.travis.yml|continuous_integration/after_success.sh|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/decomposition/online_lda.py|doc/modules/ensemble.rst|sklearn/ensemble/forest.py|.gitignore|doc/whats_new.rst|examples/linear_model/plot_ols.py|examples/linear_model/plot_ols.py|sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py|doc/modules/linear_model.rst|doc/whats_new.rst|examples/linear_model/plot_logistic_multinomial.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sag.py|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/utils/tests/test_seq_dataset.py|examples/svm/plot_svm_anova.py|doc/related_projects.rst|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|doc/whats_new.rst|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_extraction/text.py|sklearn/decomposition/kernel_pca.py|sklearn/datasets/descr/breast_cancer.rst|doc/themes/scikit-learn/layout.html|sklearn/tree/tests/test_tree.py|doc/support.rst|doc/support.rst|sklearn/datasets/kddcup99.py|sklearn/datasets/tests/test_kddcup99.py|sklearn/utils/tests/test_metaestimators.py|sklearn/linear_model/logistic.py|examples/ensemble/plot_partial_dependence.py|sklearn/feature_extraction/stop_words.py|sklearn/grid_search.py|examples/applications/plot_prediction_latency.py|examples/svm/plot_svm_anova.py|doc/developers/performance.rst|sklearn/decomposition/nmf.py|examples/plot_kernel_ridge_regression.py|examples/cluster/plot_mini_batch_kmeans.py|README.rst|circle.yml|doc/faq.rst|doc/modules/neighbors.rst|doc/modules/neighbors.rst|examples/neighbors/plot_nearest_centroid.py|sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/tests/test_function_transformer.py|doc/whats_new.rst|examples/cluster/plot_mini_batch_kmeans.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/model_selection/tests/test_split.py|sklearn/model_selection/_split.py|sklearn/metrics/scorer.py|examples/applications/plot_out_of_core_classification.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|doc/modules/isotonic.rst|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/tests/test_score_objects.py|examples/decomposition/plot_image_denoising.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|COPYING|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/linear_model/coordinate_descent.py|sklearn/multiclass.py|Makefile|appveyor.yml|benchmarks/bench_isotonic.py|continuous_integration/test_script.sh|doc/datasets/twenty_newsgroups_fixture.py|doc/themes/scikit-learn/static/css/bootstrap.css|doc/tutorial/machine_learning_map/parse_path.py|doc/tutorial/machine_learning_map/pyparsing.py|examples/applications/plot_out_of_core_classification.py|examples/applications/plot_stock_market.py|examples/applications/wikipedia_principal_eigenvector.py|examples/calibration/plot_calibration_curve.py|examples/calibration/plot_compare_calibration.py|examples/cluster/plot_kmeans_silhouette_analysis.py|examples/cross_decomposition/plot_compare_cross_decomposition.py|examples/ensemble/plot_voting_decision_regions.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/model_selection/plot_learning_curve.py|examples/neural_networks/plot_mlp_alpha.py|examples/plot_kernel_approximation.py|examples/text/document_clustering.py|setup.cfg|sklearn/calibration.py|sklearn/cluster/birch.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_validation.py|sklearn/datasets/california_housing.py|sklearn/datasets/kddcup99.py|sklearn/datasets/lfw.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_lfw.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/tests/test_pca.py|sklearn/ensemble/forest.py|sklearn/ensemble/iforest.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/voting_classifier.py|sklearn/ensemble/weight_boosting.py|sklearn/exceptions.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/odict.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gpc.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/regression.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_pairwise.py|sklearn/mixture/dpgmm.py|sklearn/model_selection/_split.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neural_network/_base.py|sklearn/neural_network/_stochastic_optimizers.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_function_transformer.py|sklearn/src/cblas/atlas_level2.h|sklearn/src/cblas/cblas_dasum.c|sklearn/src/cblas/cblas_errprn.c|sklearn/svm/base.py|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_isotonic.py|sklearn/tests/test_kernel_approximation.py|sklearn/tree/tests/test_tree.py|sklearn/utils/estimator_checks.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/testing.py|sklearn/utils/tests/test_class_weight.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_random.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/validation.py|examples/ensemble/plot_partial_dependence.py|sklearn/datasets/california_housing.py|examples/ensemble/plot_partial_dependence.py|examples/ensemble/plot_partial_dependence.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/datasets/lfw.py|sklearn/multiclass.py|doc/developers/advanced_installation.rst|doc/modules/classes.rst|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_isomap.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/model_selection/_search.py|sklearn/model_selection/_split.py|sklearn/model_selection/_validation.py|sklearn/datasets/lfw.py|sklearn/base.py|sklearn/metrics/base.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/neural_network/multilayer_perceptron.py|examples/linear_model/plot_ridge_path.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/feature_selection/univariate_selection.py|sklearn/utils/extmath.py|doc/whats_new.rst|sklearn/datasets/lfw.py|doc/whats_new.rst|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|doc/index.rst|doc/whats_new.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|doc/whats_new.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py,0,0.0,0,0,true,Master #6167,,3498,0.7287021154945683,0.2445414847161572,32482,407.36407856659076,33.40311557170125,105.96638138045687,8932,65,1977,390,travis,RPGOne,glouppe,false,glouppe,2,0.5,0,0,133,false,true,false,false,0,0,2,0,0,0,-1
11612661,scikit-learn/scikit-learn,python,6170,1452857469,1452862181,1452862181,78,78,commits_in_master,false,false,true,1,250,250,0,2,0,2,0,1,5,0,206,211,174,2,8,5,0,206,211,174,2,8,3061,986,3061,986,1461.2277642644513,31.344357352979767,0,,sklearn/linear_model/tests/test_logistic.py|doc/themes/scikit-learn/layout.html|doc/whats_new.rst|doc/modules/calibration.rst|doc/modules/gaussian_process.rst|doc/tutorial/basic/tutorial.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|doc/modules/gaussian_process.rst|doc/modules/cross_validation.rst|doc/themes/scikit-learn/layout.html|sklearn/ensemble/bagging.py|sklearn/tests/test_discriminant_analysis.py|doc/modules/model_evaluation.rst|sklearn/utils/multiclass.py|sklearn/ensemble/iforest.py|sklearn/tests/test_multiclass.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_ridge.py|appveyor.yml|continuous_integration/appveyor/requirements.txt|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/tests/test_multiclass.py|sklearn/dummy.py|doc/related_projects.rst|examples/preprocessing/plot_robust_scaling.py|doc/related_projects.rst|examples/decomposition/plot_pca_iris.py|sklearn/ensemble/voting_classifier.py|doc/related_projects.rst|examples/applications/topics_extraction_with_nmf_lda.py|doc/related_projects.rst|doc/modules/gaussian_process.rst|examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_ridge_path.py|sklearn/decomposition/pca.py|doc/modules/neural_networks_supervised.rst|README.rst|doc/related_projects.rst|README.rst|README.rst|doc/developers/advanced_installation.rst|doc/whats_new.rst|sklearn/linear_model/randomized_l1.py|doc/data_transforms.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/neural_network/rbm.py|.mailmap|doc/whats_new.rst|examples/applications/topics_extraction_with_nmf_lda.py|doc/sphinxext/numpy_ext/docscrape.py|doc/tutorial/machine_learning_map/pyparsing.py|README.rst|doc/developers/advanced_installation.rst|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|benchmarks/bench_plot_randomized_svd.py|sklearn/model_selection/_split.py|doc/datasets/index.rst|doc/modules/clustering.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/ensemble/plot_partial_dependence.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/ensemble/voting_classifier.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py|sklearn/metrics/classification.py|sklearn/metrics/regression.py|sklearn/preprocessing/data.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/feature_extraction/text.py|sklearn/linear_model/bayes.py|doc/about.rst|doc/images/nyu_short_color.png|doc/index.rst|doc/themes/scikit-learn/static/img/nyu_short_color.png|sklearn/preprocessing/label.py|sklearn/pipeline.py|examples/cluster/plot_segmentation_toy.py|.travis.yml|continuous_integration/after_success.sh|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/decomposition/online_lda.py|doc/modules/ensemble.rst|sklearn/ensemble/forest.py|.gitignore|doc/whats_new.rst|examples/linear_model/plot_ols.py|examples/linear_model/plot_ols.py|sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py|doc/modules/linear_model.rst|doc/whats_new.rst|examples/linear_model/plot_logistic_multinomial.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sag.py|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/utils/tests/test_seq_dataset.py|examples/svm/plot_svm_anova.py|doc/related_projects.rst|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|doc/whats_new.rst|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_extraction/text.py|sklearn/decomposition/kernel_pca.py|sklearn/datasets/descr/breast_cancer.rst|doc/themes/scikit-learn/layout.html|sklearn/tree/tests/test_tree.py|doc/support.rst|doc/support.rst|sklearn/datasets/kddcup99.py|sklearn/datasets/tests/test_kddcup99.py|sklearn/utils/tests/test_metaestimators.py|sklearn/linear_model/logistic.py|examples/ensemble/plot_partial_dependence.py|sklearn/feature_extraction/stop_words.py|sklearn/grid_search.py|examples/applications/plot_prediction_latency.py|examples/svm/plot_svm_anova.py|doc/developers/performance.rst|sklearn/decomposition/nmf.py|examples/plot_kernel_ridge_regression.py|examples/cluster/plot_mini_batch_kmeans.py|README.rst|circle.yml|doc/faq.rst|doc/modules/neighbors.rst|doc/modules/neighbors.rst|examples/neighbors/plot_nearest_centroid.py|sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/tests/test_function_transformer.py|doc/whats_new.rst|examples/cluster/plot_mini_batch_kmeans.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/model_selection/tests/test_split.py|sklearn/model_selection/_split.py|sklearn/metrics/scorer.py|examples/applications/plot_out_of_core_classification.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|doc/modules/isotonic.rst|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/tests/test_score_objects.py|examples/decomposition/plot_image_denoising.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|COPYING|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/linear_model/coordinate_descent.py|sklearn/multiclass.py|Makefile|appveyor.yml|benchmarks/bench_isotonic.py|continuous_integration/test_script.sh|doc/datasets/twenty_newsgroups_fixture.py|doc/themes/scikit-learn/static/css/bootstrap.css|doc/tutorial/machine_learning_map/parse_path.py|doc/tutorial/machine_learning_map/pyparsing.py|examples/applications/plot_out_of_core_classification.py|examples/applications/plot_stock_market.py|examples/applications/wikipedia_principal_eigenvector.py|examples/calibration/plot_calibration_curve.py|examples/calibration/plot_compare_calibration.py|examples/cluster/plot_kmeans_silhouette_analysis.py|examples/cross_decomposition/plot_compare_cross_decomposition.py|examples/ensemble/plot_voting_decision_regions.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/model_selection/plot_learning_curve.py|examples/neural_networks/plot_mlp_alpha.py|examples/plot_kernel_approximation.py|examples/text/document_clustering.py|setup.cfg|sklearn/calibration.py|sklearn/cluster/birch.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_validation.py|sklearn/datasets/california_housing.py|sklearn/datasets/kddcup99.py|sklearn/datasets/lfw.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_lfw.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/tests/test_pca.py|sklearn/ensemble/forest.py|sklearn/ensemble/iforest.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/voting_classifier.py|sklearn/ensemble/weight_boosting.py|sklearn/exceptions.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/odict.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gpc.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/regression.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_pairwise.py|sklearn/mixture/dpgmm.py|sklearn/model_selection/_split.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neural_network/_base.py|sklearn/neural_network/_stochastic_optimizers.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_function_transformer.py|sklearn/src/cblas/atlas_level2.h|sklearn/src/cblas/cblas_dasum.c|sklearn/src/cblas/cblas_errprn.c|sklearn/svm/base.py|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_isotonic.py|sklearn/tests/test_kernel_approximation.py|sklearn/tree/tests/test_tree.py|sklearn/utils/estimator_checks.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/testing.py|sklearn/utils/tests/test_class_weight.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_random.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/validation.py|examples/ensemble/plot_partial_dependence.py|sklearn/datasets/california_housing.py|examples/ensemble/plot_partial_dependence.py|examples/ensemble/plot_partial_dependence.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/datasets/lfw.py|sklearn/multiclass.py|doc/developers/advanced_installation.rst|doc/modules/classes.rst|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_isomap.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/model_selection/_search.py|sklearn/model_selection/_split.py|sklearn/model_selection/_validation.py|sklearn/datasets/lfw.py|sklearn/base.py|sklearn/metrics/base.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/neural_network/multilayer_perceptron.py|examples/linear_model/plot_ridge_path.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/feature_selection/univariate_selection.py|sklearn/utils/extmath.py|doc/whats_new.rst|sklearn/datasets/lfw.py|doc/whats_new.rst|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|doc/index.rst|doc/whats_new.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|doc/whats_new.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py,0,0.0,0,0,true,Master ,,3497,0.7286245353159851,0.2445414847161572,43905,514.3605511900695,39.47158637968341,132.99168659605968,8932,65,1977,390,travis,RPGOne,glouppe,false,glouppe,1,0.0,0,0,133,false,true,false,false,0,0,1,0,0,0,78
11608222,scikit-learn/scikit-learn,python,6167,1452836805,,1452862215,423,,unknown,false,false,false,5,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,Merge remote-tracking branch refs/remotes/scikit-learn/master RANSAC,,3495,0.7290414878397711,0.24489795918367346,51172,527.2219182365357,39.279293363558196,135.69921050574533,8926,65,1977,389,travis,RPGOne,glouppe,false,,0,0,0,0,133,false,true,false,false,0,0,0,0,0,0,-1
11602161,scikit-learn/scikit-learn,python,6166,1452820244,1457662170,1457662170,80698,80698,merged_in_comments,false,false,false,77,18,2,44,38,0,82,0,7,0,0,2,13,2,0,0,4,1,12,17,11,0,0,152,40,729,227,18.419743771614534,0.3832414416001695,14,t3kcit@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,10,0.014619883040935672,0,10,false,[WIP] Multitarget regression meta estimator This adds a meta estimator to multiclass which allows you to perform multi target regression with regressors that do not natively support this Inspired by OvR fits one estimator per targetI needed this personally and wanted to get feedback if this would be a welcome addition If yes I will write more tests functionality and documentation Or maybe there is already a different way to do this with regressors like GradientBoostingRegressor,,3494,0.7289639381797367,0.24561403508771928,51172,527.2219182365357,39.279293363558196,135.69921050574533,8923,65,1976,467,travis,betatim,MechCoder,false,MechCoder,9,0.6666666666666666,47,45,1426,true,false,false,false,1,25,3,13,2,0,111
11572483,scikit-learn/scikit-learn,python,6160,1452721293,,1452728976,128,,unknown,false,false,false,75,4,1,0,1,0,1,0,5,0,0,3,5,3,0,0,0,0,5,5,5,0,0,105,0,272,197,4.574806805882057,0.09518371740140674,4,t3kcit@gmail.com,sklearn/model_selection/__init__.py|sklearn/model_selection/_search.py|sklearn/model_selection/tests/test_search.py,4,0.004273504273504274,0,11,false,Refactor model_selection_search to include unsupervised estimators This generalises BaseSearchCV as BaseSearch so that it can be used on unsupervised learning where cross-validation does not make sense The approach is to use a pair of mixins SearchClusterMixin and SearchCVMixin to add _fit methods appropriate to each type of searchThe existing GridSearchCV and RandomizedSearchCV now inherit BaseSearch and SearchCVMixin while the new GridSearchCluster and RandomizedSearchCluster inherit BaseSearch and SearchClusterMixinSee #6154 for a little more background,,3493,0.7291726309762382,0.2535612535612536,51157,526.770529937252,39.271262974763964,135.69990421643178,8907,66,1975,390,travis,jamiebull1,jamiebull1,true,,1,0.0,3,0,1040,false,false,false,false,0,3,2,0,0,0,36
11571319,scikit-learn/scikit-learn,python,6159,1452720814,,1452721027,3,,unknown,false,false,false,74,1,1,0,0,0,0,0,0,0,0,3,3,3,0,0,0,0,3,3,3,0,0,105,0,105,0,4.574806805882057,0.09518371740140674,4,t3kcit@gmail.com,sklearn/model_selection/__init__.py|sklearn/model_selection/_search.py|sklearn/model_selection/tests/test_search.py,4,0.004273504273504274,0,0,false,Refactor model_selection_search to include unsupervised estimators This generalises BaseSearchCV as BaseSearch so that it can be used on unsupervised learning where cross-validation does not make sense The approach is to use a pair of mixins SearchClusterMixin and SearchCVMixin to add _fit methods appropriate to each type of searchThe existing GridSearchCV and RandomizedSearchCV inherit BaseSearch and SearchCVMixin while the new GridSearchCluster and RandomizedSearchCluster inherit BaseSearch and SearchClusterMixinSee #6154 for a little more background,,3492,0.729381443298969,0.2535612535612536,51157,526.770529937252,39.271262974763964,135.69990421643178,8907,66,1975,390,travis,jamiebull1,jamiebull1,true,,0,0,3,0,1040,false,false,false,false,0,3,0,0,0,0,-1
11561946,scikit-learn/scikit-learn,python,6157,1452691683,1452698269,1452698269,109,109,commits_in_master,false,false,false,28,1,1,0,2,0,2,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.489734401387217,0.09341370175145398,5,t3kcit@gmail.com,doc/index.rst,5,0.007112375533428165,0,1,false,[MRG] Fix FAQ link at the bottom of main page On scikit-learnorg the FAQ link at the bottom (see snapshot below) links to http://scikit-learnorg/stable/faq/ instead of http://scikit-learnorg/stable/faqhtml[scikit-learn-faq](https://cloudgithubusercontentcom/assets/1680079/12288808/bdb62304-b9d7-11e5-9d03-704f5c7cc569png),,3491,0.7293039243769693,0.2574679943100996,51157,526.770529937252,39.271262974763964,135.69990421643178,8899,66,1975,391,travis,lesteve,jnothman,false,jnothman,24,0.9166666666666666,4,0,1358,true,false,false,false,6,70,8,27,20,0,109
11543456,scikit-learn/scikit-learn,python,6155,1452632576,1455790616,1455790616,52634,52634,merged_in_comments,false,false,false,64,2,1,1,12,0,13,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,18,0,3.5600991585494435,0.07407165129847916,1,peter.fischer@fau.de,sklearn/linear_model/cd_fast.pyx,1,0.001422475106685633,0,4,false,Change comment formula of enet_coordinate_descent_gram I changed the formula in the comment of enet_coordinate_descent_gram function Indeed the function solves w^T Q w - 2 q^T w + alpha norm(w 1) + beta norm(w 2)^2 instead of w^T Q w - q^T w + alpha norm(w 1) + beta norm(w 2)^2 (there was a factor 2 missing in front of the linear term q^T w),,3490,0.7292263610315186,0.25604551920341395,51157,526.770529937252,39.271262974763964,135.69990421643178,8888,66,1974,441,travis,edwinENSAE,MechCoder,false,MechCoder,0,0,0,0,400,false,false,false,false,0,0,0,0,1,0,11
11499756,scikit-learn/scikit-learn,python,6153,1452463755,,1455146815,44717,,unknown,false,false,false,8,2,2,4,3,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,8.826795914513424,0.1836516913457252,7,thomas@1stdibs.com,sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py,7,0.009817671809256662,0,1,false,Doc: Fix redundant doc for f_regression Addresses #6149,,3489,0.7294353683003726,0.26928471248246844,51146,526.1212998083917,39.24060532592969,135.53357056270283,8853,67,1972,427,travis,dsquareindia,MechCoder,false,,5,0.6,1,0,440,true,true,false,false,4,29,6,5,5,0,464
11496566,scikit-learn/scikit-learn,python,6152,1452443594,1454199269,1454199269,29261,29261,commits_in_master,false,false,false,2,3,1,20,20,0,40,0,8,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.537337872244593,0.09440456948963927,3,peter.fischer@fau.de,doc/faq.rst,3,0.004225352112676056,0,8,false,ISSUE #5996 ,,3488,0.7293577981651376,0.2704225352112676,51146,526.1212998083917,39.24060532592969,135.53357056270283,8847,66,1972,416,travis,halwai,halwai,true,halwai,1,0.0,4,22,477,false,true,false,false,2,11,2,9,0,0,32
11494868,scikit-learn/scikit-learn,python,6151,1452428157,1453051951,1453051951,10396,10396,commits_in_master,false,false,false,79,2,2,5,11,3,19,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,190,40,190,40,22.55428116873593,0.46926658144474814,64,tom.duprelatour@orange.fr,sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/metrics/cluster/unsupervised.py|doc/whats_new.rst|sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/metrics/cluster/unsupervised.py,64,0.0,0,3,false,[MRG] MAINT: Refactor and speed up silhoutte_score (samples) A rewrite of metricssilhouette_score Able to get at least 2 times speedup in most cases     n_samples10000 n_labels6     In this branch: 134s in master:232s     n_samples10000 n_labels4     In this branch: 144s in master:239s     n_samples1000 n_labels6     In this branch: 115ms in master:287ms     n_samples1000 n_labels4     In this branch: 110ms in master:249ms     n_samples100 n_labels6     In this branch: 187ms in master:713ms     n_samples100 n_labels4     In this branch: 154ms in master:584msAlso fixed a bug related to non-encoded labels,,3487,0.7292801835388586,0.272984441301273,51124,525.5066113762616,39.21837101948204,135.35717080040686,8846,66,1972,400,travis,MechCoder,GaelVaroquaux,false,GaelVaroquaux,86,0.872093023255814,88,41,1300,true,true,false,false,22,246,34,293,33,1,153
11492694,scikit-learn/scikit-learn,python,6150,1452408287,1452473731,1452473731,1090,1090,commits_in_master,false,false,false,12,2,2,1,5,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,0,22,0,18.412614512176273,0.38309465963233696,17,thomas@1stdibs.com,sklearn/feature_selection/univariate_selection.py|sklearn/utils/extmath.py|sklearn/feature_selection/univariate_selection.py|sklearn/utils/extmath.py,11,0.015558698727015558,0,1,false,[MRG] Cosmit: Use row_norms in f_regression Also row_norms now supports all matrices,,3486,0.7292025243832473,0.272984441301273,51124,525.5066113762616,39.21837101948204,135.35717080040686,8845,66,1972,395,travis,MechCoder,MechCoder,true,MechCoder,85,0.8705882352941177,88,41,1300,true,true,false,false,22,244,33,293,33,1,0
11487619,scikit-learn/scikit-learn,python,6148,1452378595,1452431281,1452431281,878,878,commits_in_master,false,false,false,10,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.439768532546744,0.09237425862022913,2,ragvrv@gmail.com,examples/linear_model/plot_ridge_path.py,2,0.002824858757062147,0,3,false,[MRG] Make ridge path with ill-conditioned matrices clearer See https://githubcom/scikit-learn/scikit-learn/pull/5470#issuecomment-149954279,,3485,0.7291248206599713,0.2740112994350282,51124,525.5066113762616,39.21837101948204,135.35717080040686,8842,66,1971,393,travis,MechCoder,agramfort,false,agramfort,84,0.8690476190476191,88,41,1299,true,true,true,false,20,239,32,294,33,1,0
11486165,scikit-learn/scikit-learn,python,6146,1452370154,1452711812,1452711812,5694,5694,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.243143742679822,0.08828326399806208,15,t3kcit@gmail.com,sklearn/neural_network/multilayer_perceptron.py,15,0.0211864406779661,0,0,false,[MRG] MultiLayerPerceptron: fix copypasta in error message ,,3484,0.7290470723306545,0.2740112994350282,51124,525.5066113762616,39.21837101948204,135.35717080040686,8842,66,1971,394,travis,toastedcornflakes,TomDLT,false,TomDLT,3,0.6666666666666666,7,25,700,false,true,false,false,3,4,3,0,0,0,-1
11476109,scikit-learn/scikit-learn,python,6144,1452305417,,1455139469,47234,,unknown,false,false,false,9,3,1,12,13,2,27,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,69,0,4.133648759572007,0.08600510256774417,9,toastedcornflakes@gmail.com,sklearn/model_selection/_split.py,9,0.012569832402234637,0,9,true,doc:added multilabel y support for all splitters Fix #6138,,3482,0.7294658242389431,0.28212290502793297,51124,525.5066113762616,39.21837101948204,135.35717080040686,8830,66,1970,425,travis,Sentient07,MechCoder,false,,4,0.25,13,12,753,true,true,true,true,1,16,4,5,4,0,2679
11466431,scikit-learn/scikit-learn,python,6140,1452275749,,1452811867,8935,,unknown,false,false,false,34,7,1,19,10,0,29,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,21,0,78,100,4.269854492316929,0.08884046568889106,2,t3kcit@gmail.com,sklearn/linear_model/ransac.py,2,0.0028011204481792717,0,7,true,Added sample_weight parameter to ransacfit Resolved issue #6113 and #5871For estimators that used sample_weight parameter in the fit method it was requested that similar should be applied to ransacRegressor This has been intergrated,,3480,0.7298850574712644,0.2815126050420168,51128,525.4654983570646,39.21530276951964,135.34658112971366,8822,66,1970,395,travis,imaculate,MechCoder,false,,0,0,16,101,920,false,true,false,false,0,2,0,0,1,0,491
11464309,scikit-learn/scikit-learn,python,6139,1452267085,1452473681,1452473681,3443,3443,commits_in_master,false,false,false,20,2,1,4,3,0,7,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,35,0,51,0,4.682180159585468,0.0974194944032262,5,putintsevnikita@gmail.com,sklearn/datasets/lfw.py,5,0.0070126227208976155,0,2,true,[MRG] Fixed doc in lfwpy Fixes #6136 Added dependence statement for data target and pairs Fixed minor line length issues,,3479,0.7298074159241161,0.28190743338008417,51128,525.4654983570646,39.21530276951964,135.34658112971366,8820,66,1970,396,travis,dsquareindia,MechCoder,false,MechCoder,4,0.5,1,0,438,false,true,false,false,3,22,5,2,3,0,1124
11454475,scikit-learn/scikit-learn,python,6135,1452222352,,1454966138,45729,,unknown,false,false,false,16,2,2,14,16,0,30,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,56,12,56,17.839961892724162,0.37118583257550436,13,t3kcit@gmail.com,sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,11,0.015341701534170154,0,13,false,FIX #5608 in randomized_svd flip sign Flip sign according to u in both cases of transpose,,3478,0.7300172512938471,0.28870292887029286,51128,525.3090283210765,39.19574401502112,135.30746362071665,8813,66,1969,430,travis,dsquareindia,ogrisel,false,,3,0.6666666666666666,1,0,437,false,true,false,false,2,18,4,1,3,0,528
11453172,scikit-learn/scikit-learn,python,6134,1452218077,1452222319,1452222319,70,70,commit_sha_in_comments,false,false,false,13,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.551195549901527,0.09469408809125163,5,t3kcit@gmail.com,doc/developers/advanced_installation.rst,5,0.00697350069735007,0,1,false,FIX #6132 broken link Fixed the broken nose link in advanced installation docs,,3477,0.7299396031061259,0.28870292887029286,51128,525.3090283210765,39.19574401502112,135.30746362071665,8812,66,1969,391,travis,dsquareindia,dsquareindia,true,dsquareindia,2,0.5,1,0,437,false,true,false,false,1,18,2,1,2,0,70
11426343,scikit-learn/scikit-learn,python,6125,1452123524,1455334882,1455334882,53522,53522,commits_in_master,false,false,false,6,2,1,4,20,0,24,0,5,0,0,1,3,1,0,0,0,0,3,3,2,0,0,15,0,32,13,4.392269047510448,0.09138739646430517,11,t3kcit@gmail.com,sklearn/decomposition/nmf.py,11,0.014945652173913044,0,10,false,inverse_transformation for NMF decomposition Fixes #6118 ,,3474,0.7302820955670697,0.3016304347826087,51128,525.3090283210765,39.19574401502112,135.30746362071665,8789,65,1968,431,travis,AnishShah,GaelVaroquaux,false,GaelVaroquaux,2,0.5,11,14,1098,false,false,false,false,0,5,1,2,0,0,621
11407421,scikit-learn/scikit-learn,python,6119,1452047984,1452231161,1452231161,3052,3052,commits_in_master,false,false,false,18,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.498756186862547,0.09360303089504908,3,peter.fischer@fau.de,sklearn/datasets/lfw.py,3,0.003947368421052632,0,1,false,Issue #6117 Change shape of target in fetch_lfw_pairs docstring to 2200 for consistence with data and pairs shape,,3472,0.7304147465437788,0.30657894736842106,51128,525.3090283210765,39.19574401502112,135.30746362071665,8779,65,1967,392,travis,Qwasser,MechCoder,false,MechCoder,0,0,0,0,974,false,false,false,false,1,1,0,0,0,0,12
11380512,scikit-learn/scikit-learn,python,6111,1451947247,1452094713,1452094713,2457,2457,commits_in_master,false,false,false,42,1,1,0,2,0,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,8.296603706505879,0.17262288289501274,13,t3kcit@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/base.py,13,0.017082785808147174,0,0,false,Baggings base_estimator_ attribute is a not a list Changed documentation to reflect that base_estimator_ is a single estimator object not a list of them I dont think it is ever a list at least I couldnt find any instances where it was,,3468,0.7309688581314879,0.3114323258869908,51128,525.3090283210765,39.19574401502112,135.30746362071665,8760,66,1966,388,travis,betatim,glouppe,false,glouppe,8,0.625,47,45,1416,false,false,true,true,1,26,2,13,0,0,2457
11357195,scikit-learn/scikit-learn,python,6108,1451807886,1451829810,1451829810,365,365,commits_in_master,false,false,false,19,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.518573055224898,0.09401537640858179,16,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py,16,0.020969855832241154,0,0,false,Remove extra + in document In ElasticNets document there are some typoshttp://scikit-learnorg/stable/modules/generated/sklearnlinear_modelElasticNethtmlThis PR remove those extra +,,3467,0.7308912604557254,0.31585845347313235,51137,525.2165750826213,39.1888456499208,135.2836498034691,8731,66,1965,390,travis,chezou,GaelVaroquaux,false,GaelVaroquaux,0,0,39,5,1633,false,false,false,false,0,0,0,0,0,0,365
11342515,scikit-learn/scikit-learn,python,6107,1451686219,1451687175,1451687175,15,15,commits_in_master,false,false,false,19,1,1,0,0,0,0,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.806925665423629,0.10001496496339855,1,peter.fischer@fau.de,COPYING,1,0.0012804097311139564,0,2,true,First pull request of 2016 Happy new year guys:beer: :beers: :cocktail: :sake: :tada: :ramen: :tropical_drink: :pizza: :curry: :dizzy: ,,3466,0.7308136180034622,0.31241997439180536,51137,525.2165750826213,39.1888456499208,135.2836498034691,8718,66,1963,391,travis,MechCoder,agramfort,false,agramfort,83,0.8674698795180723,88,41,1291,true,true,true,false,16,246,28,281,29,1,-1
11326440,scikit-learn/scikit-learn,python,6105,1451542800,,1451567981,419,,unknown,false,false,false,7,1,1,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Merge pull request #1 from scikit-learn/master Update,,3465,0.731024531024531,0.3181242078580482,51136,524.9530663329161,39.1700563204005,135.26673967459323,8701,67,1962,392,travis,samyhhu,agramfort,false,,0,0,2,1,1358,false,false,false,false,0,0,0,0,0,0,25
11322537,scikit-learn/scikit-learn,python,6104,1451523878,1451600592,1451600592,1278,1278,commits_in_master,false,false,false,57,3,3,0,8,0,8,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,29,2,29,18.77421394128043,0.39062483324021097,13,t3kcit@gmail.com,sklearn/metrics/tests/test_classification.py|sklearn/metrics/classification.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py,12,0.015286624203821656,0,2,false,Enable pandas input to log_loss Solves issue #5715Since the shape of y_pred is (n_samples n_classes) I suppose the option of ensure2d in check_array needs to be True(default) Also added a regression test using the same doc_test exampleThis is my first contribution Hence please do suggest if something needs to be changed or added Thanks ,,3464,0.73094688221709,0.3159235668789809,51136,524.9530663329161,39.1700563204005,135.26673967459323,8693,67,1961,391,travis,maniteja123,agramfort,false,agramfort,1,0.0,4,16,729,false,false,false,false,1,12,1,0,0,0,949
11317493,scikit-learn/scikit-learn,python,6102,1451502249,1452022466,1452022466,8670,8670,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.406965889272082,0.09169333645479905,0,,examples/decomposition/plot_image_denoising.py,0,0.0,0,0,false,Remove useless normalization Fixes #6075,,3462,0.7310803004043905,0.31639135959339265,51136,524.9530663329161,39.1700563204005,135.26673967459323,8692,67,1961,394,travis,toastedcornflakes,TomDLT,false,TomDLT,1,1.0,7,25,690,false,true,false,false,2,2,1,0,0,0,6888
11289245,scikit-learn/scikit-learn,python,6097,1451344373,1451415379,1451415379,1183,1183,commits_in_master,false,false,false,24,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.383977015195685,0.09121498792913553,0,,doc/modules/isotonic.rst,0,0.0,0,0,false,fixed target link to isotonic regression example [Current link](http://scikit-learnorg/stable/auto_examples/images/plot_isotonic_regressionhtml) leads to page note found so I fixed the link to the correct [example link](http://scikit-learnorg/stable/auto_examples/plot_isotonic_regressionhtml),,3461,0.7310026004045074,0.32254047322540474,51137,524.9428007118133,39.16929033772024,135.26409449126857,8664,68,1959,391,travis,mrandrewandrade,agramfort,false,agramfort,0,0,30,94,937,false,false,false,false,0,1,0,0,0,0,1183
11272957,scikit-learn/scikit-learn,python,6092,1451225652,,1451515461,4830,,unknown,false,false,false,92,1,1,0,4,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,266,0,266,0,8.889446674192921,0.1849577686403852,17,t3kcit@gmail.com,sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py,17,0.020833333333333332,0,0,false,Implement penaltyfactor of glmnet R package to have different penalty… Implement penaltyfactor of glmnet R package to have different penalty for each variableDifferent penalties are allowed for each of the variables as described in the coordinate decent paper section 26 (page 8)[1] I added the parameter l1_weights in the elastic net classes (enet_path) This parameter corresponds to the penaltyfactor parameter in the glmnet packageglmnet docs: https://cranr-projectorg/web/packages/glmnet/glmnetpdf[1] Friedman Jerome Trevor Hastie and Rob Tibshirani “Regularization paths for generalized linear models via coordinate descent” Journal of statistical software 331 (2010): 1,,3459,0.731425267418329,0.3247549019607843,51136,524.9530663329161,39.1700563204005,135.26673967459323,8645,68,1958,395,travis,doaa-altarawy,doaa-altarawy,true,,0,0,0,5,743,false,true,false,false,0,0,0,0,0,0,22
11262241,scikit-learn/scikit-learn,python,6088,1451104141,1451359631,1451359631,4258,4258,commits_in_master,false,false,false,14,3,1,0,9,0,9,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,5,0,5,8,4.467265641992994,0.09294790951000315,1,t3kcit@gmail.com,sklearn/metrics/scorer.py,1,0.001215066828675577,0,7,true,Use kwargs instead of functoolspartial to define some scorers Another attempt to solve #6078,,3456,0.7317708333333334,0.3219927095990279,51136,524.9530663329161,39.1700563204005,135.26673967459323,8627,68,1956,396,travis,duboism,jnothman,false,jnothman,0,0,1,0,964,false,false,false,false,0,0,0,0,0,0,478
11259505,scikit-learn/scikit-learn,python,6087,1451074831,1452437976,1452437976,22719,22719,commits_in_master,false,false,false,84,6,1,39,19,0,58,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,106,23,331,81,9.13528519335722,0.1900727938626512,7,t3kcit@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,7,0.00850546780072904,0,11,true,Added partial fit method for OvR and OvO * Works only on condition if all target classes dataset chunks are passed in each iteration In case any class in some iteration is missing we would get incorrect results * This can be overcome if we store each estimator in dictionary and train the relevant estimator in each iteration I am new to scikit and machine learning in general so please go easy on me and let me know what could be done Thank you ,,3455,0.7316931982633864,0.3219927095990279,51136,524.9530663329161,39.1700563204005,135.26673967459323,8622,68,1956,410,travis,kaichogami,GaelVaroquaux,false,GaelVaroquaux,0,0,18,29,709,false,false,false,false,0,2,0,0,0,0,23
11251708,scikit-learn/scikit-learn,python,6084,1450992262,1450993205,1450993205,15,15,commits_in_master,false,false,false,27,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,20,17,20,17,9.051742232897107,0.1883344334525508,8,t3kcit@gmail.com,sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py,6,0.007255139056831923,0,0,false,Kullback-Leibler divergence as a parameter of TSNE This is a rebased version of #3804 to run travis on the resolution of the conflicts Will merge if green,,3453,0.7318273964668405,0.3252720677146312,51135,528.7180991493107,39.522831719956976,135.71917473354844,8606,69,1955,390,travis,ogrisel,ogrisel,true,ogrisel,141,0.8652482269503546,1239,124,2402,true,true,false,false,47,244,45,130,91,2,-1
11245031,scikit-learn/scikit-learn,python,6081,1450943859,,1451159794,3598,,unknown,false,false,false,7,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.957633296793606,0.08234421679129218,1,t3kcit@gmail.com,sklearn/metrics/scorer.py,1,0.001201923076923077,0,0,false,solve the scorer name issue for #6078 ,,3452,0.7320393974507532,0.3245192307692308,51135,528.7180991493107,39.522831719956976,135.71917473354844,8594,69,1955,392,travis,fayeshine,fayeshine,true,,2,0.0,1,0,285,false,true,false,false,1,13,3,0,0,0,-1
11186015,scikit-learn/scikit-learn,python,6072,1450667671,1452987186,1452987186,38658,38658,merged_in_comments,false,false,false,56,2,2,2,11,0,13,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,56,0,56,0,9.708947584781486,0.20200853004351316,2,t3kcit@gmail.com,examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_diabetes.py,2,0.0023014959723820483,2,6,false,[MRG] ENH/FIX std error is computed for mean across 3 sets of scores not len(X) Fixes #6059 Also* adds a fill between the error margins* sets size in inches to make the x/y labels visible* sets random_stateThe new plot looks like this -[](https://iimgurcom/6iGIw17png)The old plot was - [](http://scikit-learnorg/stable/_images/plot_cv_diabetes_001png)@agramfort @eyaler,,3449,0.7323861988982314,0.34062140391254314,51135,528.7180991493107,39.522831719956976,135.71917473354844,8527,71,1951,412,travis,rvraghav93,MechCoder,false,MechCoder,32,0.625,29,43,807,true,false,true,true,10,306,22,110,47,0,601
11177713,scikit-learn/scikit-learn,python,6069,1450605964,1450638993,1450638993,550,550,commits_in_master,false,false,false,76,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.49108848994053,0.09344341647716178,2,zheng.qm@163.com,examples/cluster/plot_mini_batch_kmeans.py,2,0.002306805074971165,0,3,false,FIX: Cluster centers after fit in KMeans and MBKMeans need not be ordered in the same way After printing out k_means_cluster_centers and mbk_means_cluster_centers I get    In [3]: mbk_means_cluster_centers    Out[3]:      array([[ 095903103  100100039]       [-100953167 -096610056]       [ 113870981 -110478529]])    In [4]: k_means_cluster_centers    Out[4]:     array([[-107262225 -100554224]       [ 107510478 -106937206]       [ 096786467  10173955 ]]) which means that this (https://githubcom/scikit-learn/scikit-learn/blob/master/examples/cluster/plot_mini_batch_kmeanspy#L107) line will not work the way it is supposed toI get this[kmeans_after](https://cloudgithubusercontentcom/assets/1867024/11916630/47145f3c-a6ad-11e5-84d0-329c0d7c2a0fpng)as compared to this in master[kmeans_before](https://cloudgithubusercontentcom/assets/1867024/11916638/6fdeec0c-a6ad-11e5-8b6d-a47da072a01apng),,3448,0.732308584686775,0.3414071510957324,51137,528.6974206543208,39.521285957330306,135.71386667188142,8517,71,1951,388,travis,MechCoder,agramfort,false,agramfort,82,0.8658536585365854,88,41,1279,true,true,true,false,17,274,29,292,44,1,0
11176570,scikit-learn/scikit-learn,python,6068,1450594589,1450639786,1450639786,753,753,commits_in_master,false,false,false,65,1,1,0,2,0,2,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,9.557997795332534,0.19887024808658682,3,peter.fischer@fau.de,doc/modules/neighbors.rst|examples/neighbors/plot_nearest_centroid.py,3,0.003468208092485549,0,1,false,doc/fix: edited inaccuracies in nearest centroid classification example I was going to completely rework the example but decided that would be better suited for a separate PR as I havent found a suitable dataset where a changing the shrink threshold would also result in a noticeable change in the plot / large performance gains For now Ive simply fixed the previously existing inaccuracies Addresses https://githubcom/scikit-learn/scikit-learn/issues/5931#issuecomment-161731646,,3447,0.7322309254424136,0.3421965317919075,51134,528.3764227324285,39.46493526811906,135.66315954159657,8516,71,1951,389,travis,nelson-liu,agramfort,false,agramfort,4,0.75,34,24,617,false,false,false,false,1,30,4,2,1,0,753
11171222,scikit-learn/scikit-learn,python,6067,1450560236,1450561438,1450561438,20,20,commits_in_master,false,false,false,22,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.987821367931844,0.10378003404134953,1,peter.fischer@fau.de,doc/modules/neighbors.rst,1,0.0011547344110854503,0,0,false,Fixes #6066 Fixes the link to Bawa et al LSH Forest: Self-Tuning Indexes for Similarity Search WWW05 in the Nearest Neighbor section,,3446,0.7321532211259432,0.3418013856812933,51134,528.3764227324285,39.46493526811906,135.66315954159657,8511,72,1950,389,travis,IsaacHaze,agramfort,false,agramfort,0,0,2,2,1044,false,false,false,false,0,0,0,0,0,0,20
11152427,scikit-learn/scikit-learn,python,6062,1450463211,1452243774,1452243774,29676,29676,commit_sha_in_comments,false,false,false,35,3,1,1,6,0,7,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,4,15,4.237379809375583,0.08816583201845214,0,,sklearn/manifold/isomap.py,0,0.0,0,1,true,BUG: fix initialization in Isomap to work correctly with GridSearch Currently if grid_search changes the number of neighbors for Isomap this wont be correctly reflected in the grid search results This PR fixes that bug,,3444,0.7322880371660859,0.3418013856812933,51134,528.3764227324285,39.46493526811906,135.66315954159657,8494,72,1949,405,travis,jakevdp,MechCoder,false,MechCoder,52,0.8653846153846154,1864,0,1682,true,true,false,false,2,52,9,26,3,0,377
11150296,scikit-learn/scikit-learn,python,6061,1450454257,,1450454617,6,,unknown,false,false,true,1,54,54,0,0,0,0,0,0,3,0,106,109,86,1,3,3,0,106,109,86,1,3,1266,513,1266,513,739.5881494420595,15.388378545220677,390,yoshiki@ucsd.edu,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|appveyor.yml|sklearn/__init__.py|continuous_integration/appveyor/requirements.txt|examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_random_forest_embedding.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_discriminant_analysis.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/discriminant_analysis.py|circle.yml|sklearn/ensemble/forest.py|sklearn/utils/class_weight.py|sklearn/linear_model/base.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/tests/test_common.py|doc/whats_new.rst|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/externals/joblib/format_stack.py|sklearn/utils/estimator_checks.py|doc/whats_new.rst|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|examples/model_selection/plot_roc.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py|sklearn/linear_model/ridge.py|doc/modules/svm.rst|sklearn/externals/joblib/format_stack.py|doc/whats_new.rst|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|sklearn/utils/testing.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/manifold/tests/test_t_sne.py|sklearn/linear_model/tests/test_ridge.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_theil_sen.py|sklearn/tests/test_cross_validation.py|sklearn/linear_model/ridge.py|doc/documentation.rst|doc/themes/scikit-learn/layout.html|sklearn/feature_selection/rfe.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/feature_selection/rfe.py|sklearn/metrics/regression.py|doc/datasets/rcv1.rst|doc/modules/decomposition.rst|doc/modules/feature_selection.rst|doc/modules/multiclass.rst|doc/whats_new.rst|examples/applications/face_recognition.py|sklearn/cross_decomposition/pls_.py|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/descr/diabetes.rst|sklearn/datasets/descr/digits.rst|sklearn/datasets/descr/iris.rst|sklearn/linear_model/base.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py|doc/whats_new.rst|sklearn/decomposition/dict_learning.py|doc/whats_new.rst|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_validation.py|sklearn/datasets/rcv1.py|sklearn/datasets/samples_generator.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/nmf.py|sklearn/decomposition/online_lda.py|sklearn/discriminant_analysis.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/voting_classifier.py|sklearn/feature_selection/from_model.py|sklearn/grid_search.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|sklearn/manifold/t_sne.py|sklearn/metrics/classification.py|sklearn/metrics/pairwise.py|sklearn/metrics/ranking.py|sklearn/mixture/gmm.py|sklearn/naive_bayes.py|sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/data.py|sklearn/svm/classes.py|sklearn/tree/tree.py|examples/applications/plot_prediction_latency.py|examples/applications/plot_stock_market.py|examples/classification/plot_lda_qda.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/decomposition/plot_pca_iris.py|examples/decomposition/plot_sparse_coding.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/linear_model/plot_sgd_separating_hyperplane.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/semi_supervised/plot_label_propagation_structure.py|doc/datasets/index.rst|doc/modules/clustering.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/ensemble/plot_partial_dependence.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ransac.py|sklearn/linear_model/ridge.py|sklearn/metrics/regression.py|sklearn/svm/classes.py|sklearn/tree/tree.py|doc/index.rst|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/ensemble/voting_classifier.py|sklearn/metrics/classification.py|sklearn/preprocessing/data.py|Makefile|setup32.cfg|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/testing.py|sklearn/utils/tests/test_extmath.py|README.rst|doc/developers/advanced_installation.rst|doc/developers/index.rst|doc/install.rst|sklearn/__init__.py|sklearn/tests/test_discriminant_analysis.py|doc/documentation.rst|doc/themes/scikit-learn/layout.html,85,0.006928406466512702,0,0,true,017x ,,3443,0.7325007261109497,0.34295612009237875,51134,528.3764227324285,39.46493526811906,135.66315954159657,8494,72,1949,387,travis,suhkrish,suhkrish,true,,0,0,0,0,0,false,false,false,false,0,0,0,0,0,0,-1
11145010,scikit-learn/scikit-learn,python,6060,1450426544,1450451618,1450451618,417,417,commits_in_master,false,false,false,16,3,3,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,8.925107979271429,0.18570135850298283,1,peter.fischer@fau.de,doc/faq.rst|doc/faq.rst,1,0.0011560693641618498,0,0,true,DOC: Add deep learning to faq Added information about scikit-learn and deep learning to the FAQ,,3442,0.7324230098779779,0.34335260115606936,51134,528.3764227324285,39.46493526811906,135.66315954159657,8485,72,1949,387,travis,nelson-liu,agramfort,false,agramfort,3,0.6666666666666666,34,24,615,false,false,false,false,1,30,3,2,1,0,199
11121823,scikit-learn/scikit-learn,python,6058,1450342651,1455068834,1455068834,78769,78769,commits_in_master,false,false,false,31,1,1,0,1,0,1,0,1,0,0,2,2,1,1,0,0,0,2,2,1,1,0,2,0,2,0,9.273372518246425,0.1929475675299521,9,t3kcit@gmail.com,doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/layout.html,9,0.010380622837370242,0,0,false,FIX #5485 Replaced missing plot_gp_regression with plot_gpr_co2 in ca… Fixes #5485 Replaced the now-missing plot_gp_regression with plot_gpr_co2 (the Mauna Loa example) in carousel as discussed on the thread for this issue ,,3441,0.7323452484742807,0.34371395617070355,51134,528.3764227324285,39.46493526811906,135.66315954159657,8458,72,1948,440,travis,DavidThaler,amueller,false,amueller,1,1.0,9,3,1219,false,true,false,false,1,5,1,0,0,0,78769
11119572,scikit-learn/scikit-learn,python,6057,1450332212,,1451939742,26792,,unknown,false,false,false,29,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.851498989732945,0.10094331130353462,0,,examples/applications/plot_out_of_core_classification.py,0,0.0,0,1,false,Fix6016: Decpredated color_cycle with backward compatability Fixing the deprecate warning message by using prop_cycle instead of color_cycleAdded If condition to ensure backward compatability with mpl versions before 15,,3440,0.7325581395348837,0.34371395617070355,51134,528.3764227324285,39.46493526811906,135.66315954159657,8456,72,1948,399,travis,malzantot,TomDLT,false,,0,0,2,3,1405,false,false,false,false,0,1,0,0,0,0,338
11106760,scikit-learn/scikit-learn,python,6055,1450295673,1451922800,1451922800,27118,27118,commit_sha_in_comments,false,false,false,51,5,2,1,7,0,8,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,151,0,285,0,9.517383939406052,0.1980246238171608,9,t3kcit@gmail.com,examples/ensemble/plot_partial_dependence.py|sklearn/datasets/california_housing.py,6,0.006896551724137931,0,1,false,[MRG] Fix fetch_california_housing Fix #5953I used http://wwwdccfcuppt/~ltorgo/Regression/cal_housingtgzI still has an old cal_housingpkz (ie before libstatcmuedu got broken)  in ~/scikit_learn_data so I double-checked that the content of the pickles were matching exactlyHere are the plots from examples/ensemble/plot_partial_dependencepy:[figure_1](https://cloudgithubusercontentcom/assets/1680079/11844040/edd15922-a40c-11e5-8c72-b819e7689c53png)[figure_2](https://cloudgithubusercontentcom/assets/1680079/11844049/f519ccaa-a40c-11e5-8ca1-b26ff607df87png)They match the ones from the [stable doc](http://scikit-learnorg/stable/auto_examples/ensemble/plot_partial_dependencehtml)[](http://scikit-learnorg/stable/_images/plot_partial_dependence_001png)[](http://scikit-learnorg/stable/_images/plot_partial_dependence_002png),,3439,0.7324803722012213,0.34367816091954023,51134,528.3764227324285,39.46493526811906,135.66315954159657,8442,72,1947,400,travis,lesteve,ogrisel,false,ogrisel,23,0.9130434782608695,4,0,1330,true,false,false,false,4,62,7,19,19,0,1027
11100400,scikit-learn/scikit-learn,python,6051,1450272535,1450272771,1450272771,3,3,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.697273985232888,0.09773439241307666,7,t3kcit@gmail.com,circle.yml,7,0.007990867579908675,0,0,false,[MRG] fix data home dir in circleyml ,,3438,0.7324025596276905,0.3447488584474886,51134,528.3764227324285,39.46493526811906,135.66315954159657,8434,72,1947,386,travis,ogrisel,ogrisel,true,ogrisel,140,0.8642857142857143,1238,124,2394,true,true,false,false,47,272,52,139,97,2,-1
11100098,scikit-learn/scikit-learn,python,6050,1450270886,1450271778,1450271778,14,14,commits_in_master,false,false,false,47,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.67734910307631,0.09731984763538988,9,t3kcit@gmail.com,README.rst,9,0.010285714285714285,0,0,false,MAINT fix the coveralls badge This PR has 2 purpose: - check that the coveralls hook is now reporting coverage status in PR once travis is done- try to fix the coveralls badge that show up as gray on the README of the project on github,,3437,0.7323247017748036,0.34514285714285714,51134,528.3764227324285,39.46493526811906,135.66315954159657,8434,72,1947,386,travis,ogrisel,ogrisel,true,ogrisel,139,0.8633093525179856,1238,124,2394,true,true,false,false,47,270,50,139,96,2,14
11099152,scikit-learn/scikit-learn,python,6048,1450265665,1450603583,1450603583,5631,5631,commits_in_master,false,false,false,34,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.638007749654688,0.09650122963533127,0,,examples/cluster/plot_mini_batch_kmeans.py,0,0.0,0,1,false,Fix bug in mini batch kmeans example Fix the bug in #6047 The example plots all the points that are labelled differently between the two algorithms after this fixBefore:[image](https://cloudgithubusercontentcom/assets/2503869/11833872/05b0aa1c-a402-11e5-8c61-746c9d98f889png)After:[image](https://cloudgithubusercontentcom/assets/2503869/11833861/eb79b1d4-a401-11e5-9a14-673cea1e7938png),,3436,0.7322467986030268,0.34668192219679633,51133,528.3867561066239,39.465707077621104,135.6658126845677,8431,72,1947,393,travis,zhengqm,MechCoder,false,MechCoder,0,0,19,59,1165,false,false,false,false,0,0,0,0,0,0,2
11097319,scikit-learn/scikit-learn,python,6046,1450255189,1450269190,1450269190,233,233,commits_in_master,false,false,false,62,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.830052812764321,0.10049703683010237,2,t3kcit@gmail.com,examples/plot_kernel_ridge_regression.py,2,0.002280501710376283,0,0,false,FIX Changed zorder so support vectors are clearly visible In the plot:http://scikit-learnorg/dev/_images/plot_kernel_ridge_regression_001png the points for the support vectors do not appear to match the legend This is because they are also data points and the data is plotted on top of them I fixed it by setting zorder in case the lines get moved around It looks like this now:[after](https://cloudgithubusercontentcom/assets/12852502/11831482/6924465c-a363-11e5-8c0c-748ca889d9e4png),,3435,0.7321688500727802,0.34777651083238315,51133,528.3867561066239,39.465707077621104,135.6658126845677,8426,72,1947,387,travis,DavidThaler,agramfort,false,agramfort,0,0,9,3,1218,false,true,false,false,0,4,0,0,0,0,233
11096084,scikit-learn/scikit-learn,python,6044,1450249049,1450269388,1450269388,338,338,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,4,0,4,0,8.884957642592251,0.18486587001322344,16,tom.dupre-la-tour@m4x.org,doc/developers/performance.rst|sklearn/decomposition/nmf.py,14,0.01596351197263398,0,0,false,DOC: few minor typos spotted ,,3434,0.7320908561444379,0.34777651083238315,51133,528.3867561066239,39.465707077621104,135.6658126845677,8425,72,1947,386,travis,yarikoptic,agramfort,false,agramfort,10,0.8,94,8,2561,false,true,false,true,0,0,0,0,0,0,338
11088477,scikit-learn/scikit-learn,python,6042,1450225115,1450225835,1450225835,12,12,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.512435434764429,0.09388845571978226,4,t3kcit@gmail.com,examples/svm/plot_svm_anova.py,4,0.004550625711035267,0,0,false,Fix omitted word in SVM-Anova example ,,3433,0.732012816778328,0.34812286689419797,51133,528.3867561066239,39.465707077621104,135.6658126845677,8421,73,1946,385,travis,kputnam,agramfort,false,agramfort,1,1.0,57,100,2083,false,false,false,false,0,0,1,0,0,0,-1
11085971,scikit-learn/scikit-learn,python,6041,1450218035,1451311385,1451311385,18222,18222,commit_sha_in_comments,false,false,false,33,4,1,3,12,0,15,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,30,0,4.938260607986484,0.10274857430675126,0,,examples/applications/plot_out_of_core_classification.py,0,0.0,0,6,false,Fix in plot_out_of_core_classificationpyAdded a conditional statement to use prop_cycle or color_cycle depend… …ing on the matplotlib version Just added conditions to check which one to use from color_cycle or prop_cycle Fix for #6016 ,,3432,0.7319347319347319,0.3469387755102041,51130,528.4177586544104,39.46802268726775,135.6737727361627,8419,74,1946,401,travis,dsquareindia,ogrisel,false,ogrisel,0,0,1,0,414,false,false,false,false,0,3,0,0,1,0,31
11084137,scikit-learn/scikit-learn,python,6040,1450213559,1455195491,1455195491,83032,83032,commits_in_master,false,false,false,20,1,1,0,6,0,6,0,4,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.336584158637813,0.09022971346240001,7,t3kcit@gmail.com,circle.yml,7,0.007936507936507936,0,9,false,install latex required for docbuild on circleci Fixes rest of #5995This is untested Not sure how to test it,,3431,0.7318566015738852,0.3469387755102041,51130,528.4177586544104,39.46802268726775,135.6737727361627,8418,74,1946,437,travis,amueller,ogrisel,false,ogrisel,357,0.8543417366946778,1273,40,1880,true,true,true,false,143,1058,103,419,134,5,1433
11072286,scikit-learn/scikit-learn,python,6036,1450166706,1450302495,1450302495,2263,2263,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.292129064459984,0.08930475264926327,12,t3kcit@gmail.com,sklearn/grid_search.py,12,0.013590033975084938,0,0,false,Fix minor typo in RandomizedSearchCV documentation ,,3428,0.7322053675612602,0.3465458663646659,51130,528.4177586544104,39.46802268726775,135.6737727361627,8402,74,1946,389,travis,kputnam,TomDLT,false,TomDLT,0,0,57,100,2083,false,false,false,false,0,0,0,0,0,0,-1
11070130,scikit-learn/scikit-learn,python,6034,1450157389,1450158301,1450158301,15,15,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.879630407276234,0.0807220468279657,0,,sklearn/feature_extraction/stop_words.py,0,0.0,0,0,false,I guess it is some small mistake And we need to fix it,,3426,0.7323409223584355,0.34619750283768447,51130,528.4177586544104,39.46802268726775,135.6737727361627,8397,74,1946,382,travis,alexloginov,amueller,false,amueller,0,0,1,6,1210,false,false,false,false,0,0,0,0,0,0,15
11065670,scikit-learn/scikit-learn,python,6028,1450144403,1450212459,1450212459,1134,1134,merged_in_comments,false,false,false,81,1,1,1,9,0,10,0,2,0,0,8,8,7,0,0,0,0,8,8,7,0,0,30,23,30,23,36.47458569109684,0.7589133255233117,22,tom.duprelatour@orange.fr,doc/modules/model_evaluation.rst|examples/model_selection/plot_underfitting_overfitting.py|examples/plot_kernel_ridge_regression.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/model_selection/tests/test_validation.py|sklearn/tests/test_cross_validation.py,11,0.0022727272727272726,0,5,false,Deprecated negative valued scorers 1) deprecated X  mean_squared_error mean_absolute_error median_absolute_error log_loss when called print Function returns negative of X so that greaterbetter This function is deprecated in version018 and will be removed in a future version 020 Use neg_* instead2) replaced X_scorer by neg_X_scorer throughout scikit-learn source code as well as tests Documented in doc/modules/model_evaluationrstStill need to do:3) add a new test that checks if you use deprecated one you get deprecation warning and same behavior,,3423,0.7326906222611744,0.3465909090909091,51130,528.4177586544104,39.46802268726775,135.6737727361627,8392,74,1945,385,travis,lvrzhn,amueller,false,amueller,0,0,0,0,460,false,false,false,false,0,2,0,0,0,0,14
11032333,scikit-learn/scikit-learn,python,6020,1449977407,,1450030933,892,,unknown,false,false,false,30,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.5552649771246845,0.094871984791626,2,tom.dupre-la-tour@m4x.org,examples/linear_model/plot_logistic_multinomial.py,2,0.0022396416573348264,0,0,false,Change solver to lbgfs since sag does not support multinomial logistic regression Fixes the error ValueError: Solver sag does not support a multinomial backend that appears when running the examples/linear_model/plot_logistic_multinomialpy,,3421,0.7331189710610932,0.3426651735722284,51125,527.9608801955991,39.45232273838631,135.43276283618582,8370,76,1943,380,travis,statwonk,statwonk,true,,0,0,44,218,1505,false,false,false,false,0,0,0,0,0,0,576
11023853,scikit-learn/scikit-learn,python,6018,1449920860,1450143527,1450143527,3711,3711,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,3.831784934912479,0.0798041848710558,0,,sklearn/utils/tests/test_metaestimators.py,0,0.0,0,0,false,Provide self argument to instance method ,,3420,0.7330409356725146,0.3418708240534521,51125,527.9608801955991,39.45232273838631,135.43276283618582,8362,78,1943,382,travis,proinsias,amueller,false,amueller,1,0.0,0,7,1229,false,true,true,false,0,0,1,0,0,0,-1
10962020,scikit-learn/scikit-learn,python,5996,1449696148,,1452442229,45768,,unknown,false,false,false,5,5,1,32,13,0,45,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.439252395556952,0.09265197874674173,1,peter.fischer@fau.de,doc/faq.rst,1,0.0010775862068965517,0,5,false,Reorganizing algorithm requirements in FAQ ,,3406,0.7345860246623606,0.3415948275862069,51138,527.7093355234854,39.44229340216668,135.32011420079002,8307,81,1940,418,travis,halwai,halwai,true,,0,0,4,22,445,false,true,false,false,2,3,0,0,0,0,1492
10961675,scikit-learn/scikit-learn,python,5994,1449695353,1449709861,1449709861,241,241,commits_in_master,false,false,false,14,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.759143557585983,0.0993284518337976,2,fabio.ticconi@gmail.com,doc/support.rst,2,0.0021551724137931034,0,1,false,adding 017 support link As requested in #5923 Im adding the 017 support link,,3405,0.7345080763582966,0.3415948275862069,51138,527.7093355234854,39.44229340216668,135.32011420079002,8307,81,1940,381,travis,fabioticconi,agramfort,false,agramfort,4,0.5,7,17,878,true,true,false,false,0,10,8,0,4,0,35
10960178,scikit-learn/scikit-learn,python,5993,1449691712,,1449692511,13,,unknown,false,false,false,37,2,1,0,7,0,7,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.845248425980535,0.10112615260026198,2,fabio.ticconi@gmail.com,doc/support.rst,2,0.002162162162162162,0,2,false,adding missing support links and removing old ones As requested here: https://githubcom/scikit-learn/scikit-learn/pull/5923 Im adding 016 and 017 support links to the /dev website as well as removing the old links ( 014) which are not supported anymore,,3404,0.7347238542890717,0.3427027027027027,51138,527.7093355234854,39.44229340216668,135.32011420079002,8307,81,1940,382,travis,fabioticconi,fabioticconi,true,,3,0.6666666666666666,7,17,878,true,true,false,false,0,5,4,0,3,0,4
10950610,scikit-learn/scikit-learn,python,5990,1449648870,1449684950,1449684950,601,601,commits_in_master,false,false,false,7,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.6945462801420845,0.09797887856790026,21,t3kcit@gmail.com,sklearn/tree/tests/test_tree.py,21,0.022532188841201718,0,2,false,Fixed unit test in sklearn/tree/tests/test_treepy Fixes #5989,,3403,0.7346459006758742,0.34227467811158796,51138,527.7093355234854,39.44229340216668,135.32011420079002,8299,81,1940,382,travis,jblackburne,agramfort,false,agramfort,2,1.0,1,0,595,false,false,false,false,0,4,0,0,0,0,539
10927410,scikit-learn/scikit-learn,python,5983,1449579100,,1455642787,101061,,unknown,false,false,false,61,1,1,5,5,0,10,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.140590813749836,0.08641812393027079,4,t3kcit@gmail.com,sklearn/manifold/_barnes_hut_tsne.pyx,4,0.004232804232804233,1,3,false,Fix memory leak in Barnes-Hut SNE That seems to fix #5916 for me @vighneshbirodkar still seems to have problemsHere is a test program:pythonimport numpy as npfrom sklearnmanifold import TSNEX  nprandomrand(10 10)t  TSNE()tfit(X)You can run it with valgrind --leak-checkfull --track-originsyes python test_tsnepy 2 logfile and search for barnes in the logfile,,3401,0.7350779182593354,0.3492063492063492,51139,527.6990164062653,39.441522125970394,135.31746807720134,8279,81,1939,455,travis,AlexanderFabisch,GaelVaroquaux,false,,12,0.8333333333333334,36,27,1629,false,true,true,false,1,5,1,0,1,0,393
10924024,scikit-learn/scikit-learn,python,5982,1449562192,,1449646053,1397,,unknown,false,false,false,16,2,2,2,1,0,3,0,2,0,0,57,57,54,1,2,0,0,57,57,54,1,2,96,76,96,76,320.5621009870883,6.690440233422177,192,trev.stephens@gmail.com,Makefile|appveyor.yml|benchmarks/bench_mnist.py|continuous_integration/test_script.sh|doc/sphinxext/numpy_ext/numpydoc.py|doc/themes/scikit-learn/static/css/bootstrap.css|examples/svm/plot_svm_kernels.py|examples/svm/plot_svm_margin.py|setup.cfg|setup.py|sklearn/calibration.py|sklearn/cluster/birch.py|sklearn/datasets/california_housing.py|sklearn/datasets/tests/test_lfw.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/format_stack.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/classification.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/src/cblas/atlas_level2.h|sklearn/src/cblas/cblas_dasum.c|sklearn/src/cblas/cblas_errprn.c|sklearn/svm/src/libsvm/svm.h|Makefile|appveyor.yml|benchmarks/bench_isotonic.py|continuous_integration/test_script.sh|doc/datasets/twenty_newsgroups_fixture.py|doc/themes/scikit-learn/static/css/bootstrap.css|doc/tutorial/machine_learning_map/parse_path.py|doc/tutorial/machine_learning_map/pyparsing.py|examples/calibration/plot_calibration_curve.py|examples/cluster/plot_kmeans_silhouette_analysis.py|examples/cross_decomposition/plot_compare_cross_decomposition.py|setup.cfg|sklearn/calibration.py|sklearn/cluster/birch.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/datasets/lfw.py|sklearn/datasets/tests/test_lfw.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/tests/test_pca.py|sklearn/ensemble/tests/test_bagging.py|sklearn/externals/joblib/parallel.py|sklearn/externals/odict.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/tests/test_approximate.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_function_transformer.py|sklearn/src/cblas/atlas_level2.h|sklearn/src/cblas/cblas_dasum.c|sklearn/src/cblas/cblas_errprn.c|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_kernel_approximation.py|sklearn/utils/testing.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_random.py|sklearn/utils/tests/test_sparsefuncs.py,36,0.0031746031746031746,0,0,false,General spelling fixes - general spelling fixes - removal of unused imports- several expression augmentations,,3400,0.7352941176470589,0.3492063492063492,51139,527.6990164062653,39.441522125970394,135.31746807720134,8274,81,1939,386,travis,seales,seales,true,,2,0.5,9,3,898,true,true,false,false,0,1,3,0,2,0,1194
10923073,scikit-learn/scikit-learn,python,5981,1449558009,1449783441,1449783441,3757,3757,commits_in_master,false,false,false,11,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.40655802280035,0.09196911611157745,5,ragvrv@gmail.com,sklearn/decomposition/kernel_pca.py,5,0.005291005291005291,0,0,false,Fix Kernel PCA docstring to reflect how remove_zero_eig defaults to false ,,3399,0.735216240070609,0.3492063492063492,51139,527.6990164062653,39.441522125970394,135.31746807720134,8273,81,1939,388,travis,seales,amueller,false,amueller,1,0.0,9,3,898,true,true,false,false,0,1,2,0,2,0,1237
10922490,scikit-learn/scikit-learn,python,5980,1449555522,1449581510,1449581510,433,433,commits_in_master,false,false,false,13,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.360296505341854,0.09100359362245466,6,t3kcit@gmail.com,sklearn/feature_extraction/text.py,6,0.006349206349206349,0,0,false,removed the doc line Only applies if analyzer  word from CountVectorizer analyzer ,,3398,0.735138316656857,0.3492063492063492,51139,527.6990164062653,39.441522125970394,135.31746807720134,8273,81,1939,386,travis,dshieble,agramfort,false,agramfort,0,0,0,0,809,false,false,false,false,0,2,0,0,0,0,-1
10919102,scikit-learn/scikit-learn,python,5978,1449544152,,1449546323,36,,unknown,false,false,false,16,1,1,0,0,0,0,0,0,0,0,30,30,27,1,2,0,0,30,30,27,1,2,46,18,46,18,129.88845276590507,2.710897288126302,124,tom.duprelatour@orange.fr,Makefile|appveyor.yml|benchmarks/bench_mnist.py|continuous_integration/test_script.sh|doc/sphinxext/numpy_ext/numpydoc.py|doc/themes/scikit-learn/static/css/bootstrap.css|doc/tutorial/machine_learning_map/pyparsing.py|examples/svm/plot_svm_kernels.py|examples/svm/plot_svm_margin.py|setup.cfg|setup.py|sklearn/calibration.py|sklearn/cluster/birch.py|sklearn/datasets/california_housing.py|sklearn/datasets/tests/test_lfw.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/format_stack.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/classification.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/src/cblas/atlas_level2.h|sklearn/src/cblas/cblas_dasum.c|sklearn/src/cblas/cblas_errprn.c|sklearn/svm/src/libsvm/svm.h,31,0.004232804232804233,0,0,false,General spelling fixes - general spelling fixes - removal of unused imports- several expression agumentations,,3397,0.7353547247571387,0.3492063492063492,51139,527.6990164062653,39.441522125970394,135.31746807720134,8270,81,1938,382,travis,seales,seales,true,,0,0,9,3,897,true,true,false,false,0,0,0,0,2,0,-1
10903733,scikit-learn/scikit-learn,python,5971,1449480638,1451934556,1451934556,40898,40898,commit_sha_in_comments,false,false,false,116,1,1,1,4,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.335398146694453,0.09048394086273104,12,t3kcit@gmail.com,sklearn/neural_network/multilayer_perceptron.py,12,0.01251303441084463,0,1,false,DOC clarifies that sparse matrices are scipy Currently the language states that the MLP operates on numpy sparse matrices while the rest of scikit-learns documentation uses different language clearly stating that they use scipysparse matrices I was rather confused when I went to add this into my standard list of algorithms to test given that my library works only with scipysparse matrices I was quite happy to find that this does indeed work with scipysparse matrices I copied the language directly from another algorithm so it should be highly consistent Thanks for the awesome new MLP This is much more user-friendly when a user is already working with scikit-learn than trying to integrate a third-party option ,,3394,0.7357100766057749,0.3461939520333681,51139,527.6990164062653,39.441522125970394,135.31746807720134,8261,80,1938,411,travis,ClimbsRocks,ogrisel,false,ogrisel,3,0.6666666666666666,122,95,627,false,true,false,false,0,2,2,0,1,0,2588
10896773,scikit-learn/scikit-learn,python,5969,1449444725,1450422570,1450422570,16297,16297,commit_sha_in_comments,false,false,false,47,50,8,7,13,0,20,0,3,2,0,5,18,6,0,0,3,2,14,19,13,0,1,274,93,3750,642,36.858122270346605,0.7692654094846867,48,tom.dupre-la-tour@m4x.org,sklearn/utils/estimator_checks.py|sklearn/manifold/isomap.py|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_landmark_isomap.py|sklearn/manifold/__init__.py|examples/manifold/plot_compare_isomaps.py|sklearn/tests/test_common.py|examples/manifold/plot_compare_isomaps.py,43,0.0,0,9,false,Landmark Isomap First implementation for Landmark Isomap (L-Isomap) References issue #5935ps: LandmarkIsomap wasnt included in _check_transformer test because its result depends on the landmarks which are randomly selected during fitting With this behavior the test would always fail because of inconsistent results from transformerfit and clone_transformerfit_transform,,3393,0.735632183908046,0.34691745036572624,51134,527.4963820549928,39.425822349121916,135.29158681112372,8246,80,1937,406,travis,lucasdavid,lucasdavid,true,lucasdavid,2,0.5,18,16,1344,true,true,false,false,0,7,3,0,28,0,1392
10892200,scikit-learn/scikit-learn,python,5968,1449419192,,1454961891,92378,,unknown,false,false,false,4,4,4,4,11,0,15,0,6,0,0,80,80,34,0,1,0,0,80,80,34,0,1,236,20,236,20,898.3060851883797,18.748535082618208,286,vmehta94@gmail.com,AUTHORS.rst|CONTRIBUTING.md|doc/README|doc/about.rst|doc/datasets/twenty_newsgroups.rst|doc/developers/advanced_installation.rst|doc/developers/contributing.rst|doc/developers/performance.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/cross_validation.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/model_persistence.rst|doc/modules/neural_networks_supervised.rst|doc/modules/preprocessing.rst|doc/modules/random_projection.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/presentations.rst|doc/related_projects.rst|doc/testimonials/testimonials.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/tutorial/statistical_inference/index.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/working_with_text_data.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/applications/wikipedia_principal_eigenvector.py|examples/calibration/plot_calibration.py|examples/datasets/plot_iris_dataset.py|examples/decomposition/plot_pca_iris.py|examples/linear_model/plot_iris_logistic.py|examples/manifold/plot_manifold_sphere.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/datasets/samples_generator.py|sklearn/feature_selection/univariate_selection.py|sklearn/isotonic.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/ransac.py|sklearn/linear_model/sgd_fast.pyx|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/random_projection.py|sklearn/tree/tree.py|sklearn/utils/linear_assignment_.py|AUTHORS.rst|doc/about.rst|doc/developers/advanced_installation.rst|doc/developers/performance.rst|doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/gaussian_process.rst|doc/modules/kernel_approximation.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/model_evaluation.rst|doc/modules/neighbors.rst|doc/modules/sgd.rst|doc/presentations.rst|doc/support.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/neighbors/plot_species_kde.py|sklearn/covariance/shrunk_covariance_.py|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/descr/linnerud.rst|sklearn/gaussian_process/gaussian_process.py|sklearn/linear_model/theil_sen.py|AUTHORS.rst|doc/about.rst|doc/developers/advanced_installation.rst|doc/developers/performance.rst|doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/gaussian_process.rst|doc/modules/kernel_approximation.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/model_evaluation.rst|doc/modules/neighbors.rst|doc/modules/sgd.rst|doc/presentations.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/neighbors/plot_species_kde.py|sklearn/covariance/shrunk_covariance_.py|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/descr/linnerud.rst|sklearn/gaussian_process/gaussian_process.py|sklearn/linear_model/theil_sen.py|AUTHORS.rst|CONTRIBUTING.md|doc/README|doc/about.rst|doc/datasets/twenty_newsgroups.rst|doc/developers/advanced_installation.rst|doc/developers/contributing.rst|doc/developers/performance.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/cross_validation.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/model_persistence.rst|doc/modules/neural_networks_supervised.rst|doc/modules/preprocessing.rst|doc/modules/random_projection.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/presentations.rst|doc/related_projects.rst|doc/testimonials/testimonials.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/finding_help.rst|doc/tutorial/statistical_inference/index.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/working_with_text_data.rst|doc/whats_new.rst|examples/applications/plot_species_distribution_modeling.py|examples/applications/wikipedia_principal_eigenvector.py|examples/calibration/plot_calibration.py|examples/datasets/plot_iris_dataset.py|examples/decomposition/plot_pca_iris.py|examples/linear_model/plot_iris_logistic.py|examples/manifold/plot_manifold_sphere.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/datasets/samples_generator.py|sklearn/feature_selection/univariate_selection.py|sklearn/isotonic.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/ransac.py|sklearn/linear_model/sgd_fast.pyx|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/random_projection.py|sklearn/tree/tree.py|sklearn/utils/linear_assignment_.py,96,0.002103049421661409,0,7,false,[WIP] Fix broken/redirection linkcheck ,,3392,0.7358490566037735,0.3491062039957939,51134,527.4963820549928,39.425822349121916,135.29158681112372,8245,80,1937,452,travis,sieben,sieben,true,,18,0.7777777777777778,35,136,2006,true,true,false,false,1,19,20,6,5,0,295
10886706,scikit-learn/scikit-learn,python,5966,1449376173,1449408944,1449408944,546,546,commits_in_master,false,false,false,54,3,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,22,0,8.84072478669271,0.18451472126829882,12,t3kcit@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py,12,0.012658227848101266,0,0,false,[Needs Review] Added information about gamma parameter for pairwise kernels I noticed that some of the kernels in sklearnpairwisemetrics were missing default parameter information All of them didnt state the default parameter value for gamma and the polynomial_kernel missed this parameter description completelyThis is just a quick patch to fix up some docstrings,,3391,0.7357711589501622,0.3491561181434599,51134,527.4963820549928,39.425822349121916,135.29158681112372,8238,80,1936,381,travis,hlin117,agramfort,false,agramfort,15,0.6666666666666666,18,20,1264,true,true,false,false,6,64,11,23,29,2,546
10880414,scikit-learn/scikit-learn,python,5964,1449345093,,1449440599,1591,,unknown,false,false,false,13,26,24,0,3,0,3,0,2,2,0,12,15,13,0,0,2,0,13,15,14,0,0,522,109,524,131,124.72247558788511,2.6030818126032624,25,t3kcit@gmail.com,sklearn/neighbors/graph.py|sklearn/manifold/isomap.py|sklearn/neighbors/graph.py|sklearn/manifold/isomap.py|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/mean_shift_.py|sklearn/manifold/locally_linear.py|sklearn/neighbors/graph.py|sklearn/semi_supervised/label_propagation.py|sklearn/manifold/locally_linear.py|sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/semi_supervised/label_propagation.py|sklearn/cluster/dbscan_.py|sklearn/decomposition/kernel_pca.py|sklearn/manifold/isomap.py|doc/modules/pipeline.rst|sklearn/manifold/__init__.py|sklearn/manifold/isomap.py|examples/manifold/plot_isomaps.py|sklearn/manifold/isomap.py|examples/manifold/plot_compare_isomaps.py|sklearn/manifold/isomap.py|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_landmark_isomap.py|sklearn/manifold/tests/test_landmark_isomap.py,12,0.003171247357293869,0,0,false,Landmark isomap First implementation for LandmarkIsomap done by sub-classing Isomap References issue #5935,,3390,0.7359882005899705,0.3498942917547569,51134,527.4963820549928,39.425822349121916,135.29158681112372,8231,79,1936,382,travis,lucasdavid,lucasdavid,true,,1,1.0,18,16,1343,true,true,false,false,0,4,1,0,22,0,138
10872658,scikit-learn/scikit-learn,python,5962,1449287405,1449321431,1449321431,567,567,github,false,false,false,15,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.714881898189263,0.0984042411075171,13,villu.ruusmann@gmail.com,doc/related_projects.rst,13,0.013800424628450107,0,0,true,Add link to multiisotonic Added a link to multiisotonic regression in the Related Projects doc,,3388,0.7361275088547816,0.351380042462845,51134,527.4963820549928,39.425822349121916,135.29158681112372,8221,79,1935,382,travis,alexfields,agramfort,false,agramfort,0,0,2,1,772,false,true,false,false,0,1,0,0,0,0,566
10816563,scikit-learn/scikit-learn,python,5960,1449254724,,1450791387,25611,,unknown,false,false,false,10,4,1,2,15,0,17,0,4,2,0,0,5,0,0,1,2,0,3,5,3,0,1,0,0,63,9,8.754251460263642,0.18289795395891573,0,,sklearn/datasets/data/yeast.csv|sklearn/datasets/descr/yeast.rst,0,0.0,0,13,true,[WIP] Added Yeast Dataset Add the multi-label yeast dataset #5403 ,,3387,0.7363448479480366,0.35100742311770944,51062,527.1043045709139,39.44224668050605,135.32568250362303,8211,79,1935,412,travis,kshitij10496,kshitij10496,true,,0,0,46,311,100,false,true,false,false,0,2,0,0,0,0,2662
10775654,scikit-learn/scikit-learn,python,5954,1449186808,1449195614,1449195614,146,146,commits_in_master,false,false,false,18,2,1,1,2,0,3,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,8,0,4.6966915502211375,0.09812547162895825,0,,examples/linear_model/plot_ols.py,0,0.0,0,1,false,residual sum of squares should be computed by summing the squared dif… …ferences instead of taking the mean,,3385,0.7364844903988184,0.35157894736842105,51062,527.1043045709139,39.44224668050605,135.32568250362303,8197,79,1934,380,travis,KjellSwedin,agramfort,false,agramfort,0,0,0,0,885,false,false,false,false,0,0,0,0,0,0,120
10745164,scikit-learn/scikit-learn,python,5952,1449108259,,1449108504,4,,unknown,false,false,false,16,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.356214763749283,0.09101207171882582,3,peter.fischer@fau.de,sklearn/pipeline.py,3,0.003134796238244514,0,0,false,Fixes #5943 Added documentation to explicitly state that make_pipeline will lowercase the given class name,,3384,0.7367021276595744,0.35318704284221525,51062,527.1043045709139,39.44224668050605,135.32568250362303,8185,79,1933,380,travis,EricFalkenberg,EricFalkenberg,true,,0,0,17,22,1110,false,false,false,false,1,0,0,0,0,0,-1
10744650,scikit-learn/scikit-learn,python,5951,1449106893,,1449622649,8595,,unknown,false,false,false,35,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.565473837979878,0.09538400995065405,21,tom.dupre-la-tour@m4x.org,sklearn/linear_model/logistic.py,21,0.021920668058455117,1,0,false,FIX LabelEncoder to correctly handle string labels (Issue #5868) Can we just make a simple change like this It works with @dan-vines example this way If yes should we add some tests for itThanks,,3383,0.736919893585575,0.35281837160751567,51062,527.1043045709139,39.44224668050605,135.32568250362303,8184,79,1933,391,travis,r0fls,r0fls,true,,0,0,0,0,1264,false,false,false,false,0,0,0,0,0,0,2783
10723646,scikit-learn/scikit-learn,python,5949,1449053451,1449409420,1449409420,5932,5932,commits_in_master,false,false,false,24,1,0,0,3,0,3,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,[DOC] plot_svm_anovapy-and-all-CPUs typo fix Fix of typo: If you want to use all cpus - its better to leave default n_jobs or use -1,,3382,0.7368421052631579,0.35360824742268043,51062,527.1043045709139,39.44224668050605,135.32568250362303,8175,79,1933,383,travis,olologin,agramfort,false,agramfort,7,0.42857142857142855,2,2,894,true,true,false,false,4,25,5,1,4,0,50
10696406,scikit-learn/scikit-learn,python,5946,1449001014,1450022823,1450022823,17030,17030,commits_in_master,false,false,false,21,2,2,3,23,0,26,0,5,1,0,1,2,2,0,0,1,0,1,2,2,0,0,31,41,31,41,13.771698627479411,0.2877247544369116,4,t3kcit@gmail.com,sklearn/datasets/kddcup99.py|sklearn/datasets/kddcup99.py|sklearn/datasets/tests/test_kddcup99.py,4,0.00408997955010225,0,4,false,[MRG] BUG: Fix fetch_kddcup99 for Python 3 This is a small fix to account for str / bytes issues in fetch_kddcup99,,3381,0.7367642709257616,0.3507157464212679,51062,527.1043045709139,39.44224668050605,135.32568250362303,8159,79,1932,398,travis,nmayorov,agramfort,false,agramfort,8,0.375,8,0,615,true,false,false,false,1,41,2,30,8,0,1460
10803802,scikit-learn/scikit-learn,python,5945,1448981998,1455076120,1455076120,101568,101568,commits_in_master,false,false,false,52,3,3,11,5,0,16,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,14.228802116097633,0.2972747738333936,11,t3kcit@gmail.com,doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst,11,0.011213047910295617,0,2,false,Improve mlp docs Fixes #5583 - [x] Add images linking to MLP examples- [x] Fix and complete the list of all the relevant examples  - [x] Add warning about MLP implementation here not being applicable to deep architectures or convnets or any serious neural nets & point user to appropriate resources ,,3380,0.7366863905325444,0.3516819571865443,51062,527.1043045709139,39.44224668050605,135.32568250362303,8151,80,1932,447,travis,mth4saurabh,amueller,false,amueller,3,1.0,9,6,862,true,false,false,false,0,3,2,5,1,0,10982
10697870,scikit-learn/scikit-learn,python,5944,1448973499,1449374379,1449374379,6681,6681,commits_in_master,false,false,false,30,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.3356315695342005,0.09058203801991487,3,peter.fischer@fau.de,sklearn/pipeline.py,3,0.0030581039755351682,0,2,false,Fixed make_pipeline documentation to provide more details about component naming In this PR I clarified that make_pipeline sets the components of the pipelines names to their type lowercased Addresses https://githubcom/scikit-learn/scikit-learn/issues/5943,,3379,0.7366084640426162,0.3516819571865443,51062,527.1043045709139,39.44224668050605,135.32568250362303,8147,80,1932,383,travis,nelson-liu,TomDLT,false,TomDLT,2,0.5,34,24,598,false,false,false,false,0,18,2,2,1,0,488
10800183,scikit-learn/scikit-learn,python,5942,1448965008,1449791547,1449791547,13775,13775,commits_in_master,false,false,false,77,1,1,0,1,0,1,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,11,3,11,9.038345419136,0.18883333033790486,21,trev.stephens@gmail.com,sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py,19,0.019348268839103868,0,0,false,LDA: explained_variance_ratio_ can be found using svd solver Previously the only way to get the explained_variance_ratio_ from Linear Discriminant Analysis was through using the eigen solver option A small modification to the source lets you also find the explained_variance_ratio_s from the SVD solverOverall this is just an incremental change to the source code## PR includes* Changed source code to extract explained_variance_ratio_ from the SVD solver* Change to docstring for LDA* Test case,,3378,0.7365304914150385,0.3513238289205703,51062,527.1043045709139,39.44224668050605,135.32568250362303,8144,80,1932,389,travis,hlin117,agramfort,false,agramfort,14,0.6428571428571429,18,20,1260,true,true,false,false,6,62,10,23,29,2,13685
10837443,scikit-learn/scikit-learn,python,5936,1448855726,1448880618,1448880618,414,414,github,false,false,false,19,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.355134455017988,0.09098951167569806,3,t3kcit@gmail.com,sklearn/preprocessing/label.py,3,0.0029791459781529296,0,0,false,[MRG+1] MultiLabelBinarizer check_is_fitted validation for transform() and inverse_transform() Im not sure if i need to add test for this,,3377,0.7364524726088244,0.34955312810327704,51062,527.1043045709139,39.44224668050605,135.32568250362303,8120,80,1930,375,travis,olologin,GaelVaroquaux,false,GaelVaroquaux,6,0.3333333333333333,2,2,891,true,true,false,false,4,25,4,1,4,0,16
10706427,scikit-learn/scikit-learn,python,5925,1448569900,1449783329,1449783329,20223,20223,commits_in_master,false,false,false,148,1,0,5,10,0,15,0,3,0,0,0,3,0,0,0,1,0,3,4,3,0,1,0,0,44,0,0,0.0,0,,,0,0.0,1,3,false,[MRG+1] Fix for coveralls not sending coverage report coveralls needs to be run from a git checkout so keeping git when copying folder in continuous_integration/installshTalked with @arthurmensch and he said that excluding git from the copy was just to save a bit of time (~10s)For reference the error from coveralls from this Travis [log](https://travis-ciorg/scikit-learn/scikit-learn/jobs/93359303#L2977):Traceback (most recent call last):  File /home/travis/build/scikit-learn/scikit-learn/testvenv/bin/coveralls line 11 in module    sysexit(main())  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/clipy line 62 in main    result  coverallzwear()  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/apipy line 89 in wear    json_string  selfcreate_report()  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/apipy line 107 in create_report    data  selfcreate_data()  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/apipy line 157 in create_data    self_dataupdate(selfgit_info())  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/apipy line 197 in git_info    rev  run_command(git rev-parse --abbrev-ref HEAD)strip()  File /home/travis/build/scikit-learn/scikit-learn/testvenv/local/lib/python27/site-packages/coveralls/apipy line 240 in run_command    STDERR: %s % (cmdreturncode stdout stderr))AssertionError: command return code 128 STDOUT: STDERR: fatal: Not a git repository (or any of the parent directories): git,,3375,0.7365925925925926,0.35009671179883944,51062,527.1043045709139,39.44224668050605,135.32568250362303,8089,80,1927,392,travis,lesteve,amueller,false,amueller,22,0.9090909090909091,4,0,1310,true,false,false,false,5,48,6,15,15,0,117
10703526,scikit-learn/scikit-learn,python,5923,1448564510,1449691624,1449691624,18785,18785,commits_in_master,false,false,false,6,3,2,0,8,0,8,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.628137506849448,0.20115560361917897,0,,doc/support.rst|doc/support.rst,0,0.0,0,2,false,[MRG+1] adding missing 016 doc link ,,3374,0.7365145228215768,0.35009671179883944,51059,527.1352748780822,39.44456413169079,135.33363363951506,8089,80,1927,390,travis,fabioticconi,TomDLT,false,TomDLT,2,0.5,7,17,865,true,true,false,false,0,2,2,0,3,0,1
10756764,scikit-learn/scikit-learn,python,5920,1448503742,,1454352392,97477,,unknown,false,true,false,84,38,1,13,56,0,69,0,5,0,0,7,12,5,1,0,1,0,12,13,6,1,2,115,168,241,206,33.16010729279,0.6927855816368431,22,tom.dupre-la-tour@m4x.org,doc/modules/clustering.rst|doc/modules/decomposition.rst|doc/modules/feature_extraction.rst|doc/themes/scikit-learn/layout.html|doc/tutorial/statistical_inference/unsupervised_learning.rst|examples/cluster/plot_face_compress.py|examples/cluster/plot_face_segmentation.py|examples/cluster/plot_face_ward_segmentation.py|examples/decomposition/plot_image_denoising.py|sklearn/feature_extraction/tests/test_image.py,10,0.0028957528957528956,0,28,false,updated examples and tests that use scipymisclena() scipymisclena() will be removed in scipy version 017 so this PR changes all tests and examples that use it to use scipymiscface() instead Addresses issue https://githubcom/scikit-learn/scikit-learn/issues/5739 Ive verified that all the scripts work on my local machine except for what was formerly plot_lena_segmentationpy (now plot_face_segmentationpy) plot_face_segmentationpy runs extremely slowly I ran it for around 6 hours during as I was running make html before I gave up and interrupted it This is probably related to bug https://githubcom/scikit-learn/scikit-learn/issues/1966,,3372,0.7369513641755635,0.3503861003861004,51054,527.1869001449445,39.44842715556078,135.34688760919812,8086,80,1926,442,travis,nelson-liu,ogrisel,false,,1,1.0,34,24,592,false,false,false,false,0,1,1,0,1,0,79
10709832,scikit-learn/scikit-learn,python,5919,1448502923,1448577545,1448577546,1243,1243,github,false,false,false,38,1,1,0,8,0,8,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,2,false,[MRG+2] Fixed plot_stock_marketpy example to work with matplotlib 14x+ Ive replaced matplotlib function financequotes_historical_yahoo() (deprecated as of matplotlib 14x) with financequotes_historical_yahoo_ochl() enabling the example to work for the latest version of matplotlib (150) / anything beyond version 14x,,3371,0.7368733313556808,0.3503861003861004,51059,527.1352748780822,39.44456413169079,135.33363363951506,8086,80,1926,379,travis,nelson-liu,TomDLT,false,TomDLT,0,0,34,24,592,false,false,false,false,0,1,0,0,1,0,68
10703618,scikit-learn/scikit-learn,python,5918,1448467195,,1448564541,1622,,unknown,false,false,false,5,4,3,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.786605627793149,0.1000024316760242,0,,doc/support.rst,0,0.0,0,0,false,adding missing 016 doc link ,,3370,0.7370919881305638,0.3484119345524543,51054,527.1869001449445,39.44842715556078,135.34688760919812,8085,81,1926,377,travis,fabioticconi,fabioticconi,true,,1,1.0,7,17,864,true,true,false,false,0,1,1,0,1,0,1570
10647986,scikit-learn/scikit-learn,python,5914,1448389070,1450602114,1450602114,36884,36884,commit_sha_in_comments,false,false,false,118,4,1,7,8,0,15,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,10,10,47,8.83438770255282,0.18518327964391126,4,t3kcit@gmail.com,sklearn/preprocessing/_function_transformer.py|sklearn/preprocessing/tests/test_function_transformer.py,4,0.0038095238095238095,0,0,false,Keyword arguments in FunctionTransformer This PR enhances the FunctionTransformer class to store optional keyword arguments for the embedded functionThe motivation is to reduce the need for lambda functions in many simple cases which should make code more readable and allow objects to be serializedFor example rather than doing:python F  FunctionTransformer(lambda x : nparound(x decimals3)) X_rounded  Ftransform(X)one can do:python F2  FunctionTransformer(nparound kw_argsdict(decimals3)) X_rounded  F2transform(X)In the latter case the F2 object can be serialized and its parameters are available for inspection:python FFunctionTransformer(accept_sparseFalse          funcfunction lambda at 0x7f6c4bd90f28 kw_argsNone          pass_yFalse validateTrue) F2FunctionTransformer(accept_sparseFalse          funcfunction around at 0x7f6c44173c80          kw_args{decimals: 3} pass_yFalse validateTrue),,3369,0.7370139507272188,0.3476190476190476,51054,527.1869001449445,39.44842715556078,135.34688760919812,8080,81,1925,410,travis,bmcfee,MechCoder,false,MechCoder,1,1.0,180,141,1473,false,true,false,false,0,1,0,0,0,0,10626
10643408,scikit-learn/scikit-learn,python,5913,1448382680,1448384033,1448384033,22,22,commits_in_master,false,false,false,64,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.32645921695373,0.09068968195682689,2,gael.varoquaux@normalesup.org,sklearn/linear_model/bayes.py,2,0.0019047619047619048,1,0,false,DOC: fix shape lambda_ attribute in BayesianRidge supersedes #5912 I run into problems during rebase and started over with a new PR @agramfort Attributes for BayesianRidge currently read:    lambda_ : array shape  (n_features)       estimated precisions of the weightsbut should be    lambda_ : float       estimated precision of the weightsARDRegression estimates lambda_ for each feature However BayesianRidge estimates only one lambda_BestMatthias,,3368,0.7369358669833729,0.3476190476190476,51054,527.1869001449445,39.44842715556078,135.34688760919812,8079,81,1925,375,travis,mekman,agramfort,false,agramfort,2,0.5,9,4,2233,false,true,false,true,0,0,1,0,0,0,22
10642320,scikit-learn/scikit-learn,python,5912,1448379082,,1448382703,60,,unknown,false,false,false,48,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.383365312984234,0.09188252707372706,2,gael.varoquaux@normalesup.org,sklearn/linear_model/bayes.py,2,0.0019047619047619048,0,0,false,DOC: fix shape lambda_ attribute in BayesianRidge Attributes for BayesianRidge currently read:    lambda_ : array shape  (n_features)       estimated precisions of the weightsbut should be    lambda_ : float       estimated precision of the weightsARDRegression estimates lambda_ for each feature However BayesianRidge estimates only one lambda_BestMatthias,,3367,0.7371547371547371,0.3476190476190476,51054,527.1869001449445,39.44842715556078,135.34688760919812,8079,81,1925,375,travis,mekman,mekman,true,,1,1.0,9,4,2233,false,true,false,false,0,0,0,0,0,0,9
10638648,scikit-learn/scikit-learn,python,5910,1448335514,1448410913,1448410913,1256,1256,commits_in_master,false,false,false,69,1,0,1,3,0,4,0,3,0,0,0,2,0,0,0,2,0,2,4,0,0,2,0,0,0,0,0,0.0,0,,,0,0.0,1,0,false,[MRG+1] website: added NYU funding to front-page footer and the about section ping @GaelVaroquaux [nyu_funding_frontpage](https://cloudgithubusercontentcom/assets/449558/11353228/5a9b1db4-920f-11e5-8502-0aa150f35514png)[nyu_funding_about](https://cloudgithubusercontentcom/assets/449558/11353227/5a9b1954-920f-11e5-9b3f-d8901ef059a4png)Screenshots of the changesI changed the footer on the front page as I needed to make the central part wider for the logo I didnt want to break symmetry and there are only 12 cells in bootstrap so I had to shorten the text on the right to avoid an ugly line-break,,3366,0.7370766488413547,0.34695817490494296,51054,527.1869001449445,39.44842715556078,135.34688760919812,8068,81,1924,374,travis,amueller,amueller,true,amueller,356,0.8539325842696629,1270,40,1858,true,true,false,false,160,1206,115,561,126,6,2
11320563,scikit-learn/scikit-learn,python,5907,1448295484,1452292904,1452292904,66623,66623,commit_sha_in_comments,false,false,false,20,1,1,2,6,0,8,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,89,0,89,0,18.176311779686095,0.38100530983590386,20,vighneshbirodkar@nyu.edu,sklearn/base.py|sklearn/metrics/base.py|sklearn/utils/__init__.py|sklearn/utils/validation.py,19,0.008538899430740038,2,1,false,[MRG] MAINT Deprecate wisely ) Sorry for the sloppy work last timeThis deprecation method is more elegant@GaelVaroquaux @amueller,,3365,0.736998514115899,0.34629981024667933,51054,527.1869001449445,39.44842715556078,135.34688760919812,8068,81,1924,417,travis,rvraghav93,MechCoder,false,MechCoder,29,0.6551724137931034,29,43,780,true,false,true,true,5,230,20,95,47,0,53665
10614921,scikit-learn/scikit-learn,python,5895,1448064701,1448069341,1448069341,77,77,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.35187887658016,0.0912234143099522,3,t3kcit@gmail.com,sklearn/model_selection/_split.py,3,0.002890173410404624,0,0,true,Grammatical fix in module_selection ,,3361,0.7375781017554299,0.34778420038535646,50930,528.4704496367564,39.48556842725309,135.53897506381307,8054,81,1921,373,travis,toastedcornflakes,agramfort,false,agramfort,0,0,7,25,650,false,true,false,false,0,0,0,0,0,0,-1
10609120,scikit-learn/scikit-learn,python,5894,1448048476,,1448279439,3849,,unknown,false,false,false,3,9,9,0,2,2,4,2,1,1,0,7,8,7,0,0,1,0,7,8,7,0,0,334,20,334,20,70.25522000461773,1.4726790941742416,49,yoshiki@ucsd.edu,sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|examples/plot_predictive_standard_deviation.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.py|examples/gaussian_process/plot_gp_regression.py|sklearn/gaussian_process/tests/test_gaussian_process.py|examples/plot_predictive_standard_deviation.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/gaussian_process/gaussian_process.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py,22,0.0019230769230769232,0,1,true,Variance ensemble regressors ,,3360,0.7377976190476191,0.34807692307692306,50930,528.4704496367564,39.48556842725309,135.53897506381307,8050,81,1921,374,travis,lynochka,glouppe,false,,0,0,0,0,337,false,false,false,false,0,0,0,0,0,0,3849
10601751,scikit-learn/scikit-learn,python,5892,1448020764,1448561129,1448561129,9006,9006,commits_in_master,false,false,false,8,2,2,2,2,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,9.338110849265917,0.195744033582227,13,t3kcit@gmail.com,sklearn/decomposition/online_lda.py|sklearn/decomposition/online_lda.py,13,0.012452107279693486,0,0,true,Add user guide link to lda Fixes #5878 ,,3358,0.7379392495533056,0.34770114942528735,50930,528.4704496367564,39.48556842725309,135.53897506381307,8047,81,1921,380,travis,mth4saurabh,TomDLT,false,TomDLT,2,1.0,9,6,851,false,false,false,false,1,4,1,5,0,0,819
10599512,scikit-learn/scikit-learn,python,5891,1448007186,,1448080523,1222,,unknown,false,false,false,4,2,2,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.429307781471993,0.09284646382095907,7,t3kcit@gmail.com,sklearn/feature_extraction/text.py,7,0.006641366223908918,0,4,true,TfIdfVectorizer vocabulary_ attribute documentation ,,3357,0.7381590705987489,0.34629981024667933,50930,528.4704496367564,39.48556842725309,135.53897506381307,8045,82,1921,373,travis,fayeshine,fayeshine,true,,0,0,1,0,251,false,false,false,false,0,2,0,0,0,0,212
10595091,scikit-learn/scikit-learn,python,5890,1447988491,1449694717,1449694717,28437,28437,commits_in_master,false,false,false,19,1,1,0,3,0,3,0,2,0,0,3,3,0,1,0,0,0,3,3,0,1,0,0,0,0,0,13.238117456785925,0.277495368158997,11,t3kcit@gmail.com,doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html,9,0.004743833017077799,0,1,false,[MRG] add 0170 release to news fix versions in menu Fixes to the website navigation for the dev website,,3356,0.7380810488676997,0.34629981024667933,50930,528.4704496367564,39.48556842725309,135.53897506381307,8044,82,1920,394,travis,amueller,amueller,true,amueller,355,0.8535211267605634,1270,40,1854,true,true,false,false,159,1187,112,497,124,6,27388
10588934,scikit-learn/scikit-learn,python,5889,1447971967,1455653753,1455653753,128029,128029,commit_sha_in_comments,false,false,false,65,2,2,2,10,0,12,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,52,0,52,0,29.145203411158057,0.610937742130153,0,,doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py|doc/tutorial/text_analytics/data/twenty_newsgroups/fetch_data.py|doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py|doc/tutorial/text_analytics/data/twenty_newsgroups/fetch_data.py,0,0.0,0,5,false,Read web pages as UTF-8 by default Use with-blocks to open files On windows the default encoding for open(filename) is cp1252 Calling open(filename)read() on a UTF-8 encoded file (eg the wikipedia pages being fetched) fails with an error when it encounters multi-byte unicode code pointsThis was a blocker bug I also wrapped the file ops in with-blocks as is the preferred style these days,,3355,0.7380029806259314,0.34629981024667933,50930,528.4704496367564,39.48556842725309,135.53897506381307,8039,81,1920,449,travis,SmedbergM,ogrisel,false,ogrisel,0,0,0,0,978,false,false,false,false,0,0,0,0,0,0,13
10586580,scikit-learn/scikit-learn,python,5887,1447965843,,1447971578,95,,unknown,false,false,false,2,1,1,4,5,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.014536004242033,0.0841521210119122,0,,doc/sphinxext/numpy_ext/docscrape_sphinx.py,0,0.0,0,2,false,Unreachable code ,,3354,0.7382230172927847,0.3446969696969697,50930,528.4704496367564,39.48556842725309,135.53897506381307,8039,82,1920,370,travis,sieben,agramfort,false,,17,0.8235294117647058,35,136,1989,true,true,false,false,0,14,19,3,3,0,15
10586056,scikit-learn/scikit-learn,python,5886,1447964372,1448071576,1448071576,1786,1786,commits_in_master,false,false,false,4,2,2,6,7,0,13,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,15,0,15,0,9.182596618595,0.19248425746712886,2,olivier.grisel@ensta.org,benchmarks/bench_plot_randomized_svd.py|benchmarks/bench_plot_randomized_svd.py,2,0.001893939393939394,0,3,false,With statement is safer ,,3353,0.7381449448255294,0.3446969696969697,50930,528.4704496367564,39.48556842725309,135.53897506381307,8038,82,1920,375,travis,sieben,amueller,false,amueller,16,0.8125,35,136,1989,true,true,true,false,0,13,18,3,3,0,3
10586030,scikit-learn/scikit-learn,python,5885,1447964308,1447967271,1447967271,49,49,commits_in_master,false,true,false,2,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,0,6,0,7.776824636860305,0.16301666923349709,2,remy.leone@gmail.com,doc/sphinxext/numpy_ext/docscrape.py|doc/tutorial/machine_learning_map/pyparsing.py,2,0.001893939393939394,0,2,false,dicthas_key() deprecated ,,3352,0.7380668257756563,0.3446969696969697,50930,528.4704496367564,39.48556842725309,135.53897506381307,8038,82,1920,370,travis,sieben,amueller,false,amueller,15,0.8,35,136,1989,true,true,true,false,0,13,17,3,3,0,8
10578173,scikit-learn/scikit-learn,python,5882,1447938703,1448562913,1448562913,10403,10403,commits_in_master,false,false,false,30,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.637068820075785,0.09720156353691937,7,vighneshbirodkar@nyu.edu,examples/applications/topics_extraction_with_nmf_lda.py,7,0.006548175865294668,0,1,false,#5877 fix confusing n_samples and n_features Fix #5877 by adding n_samples to actual use and n_features to both NMF and LDA Fix a few PEP8 warnings to the old code,,3350,0.7382089552238806,0.3461178671655753,50930,528.4704496367564,39.48556842725309,135.53897506381307,8034,81,1920,379,travis,LeiG,TomDLT,false,TomDLT,0,0,3,0,1083,false,true,false,false,0,1,0,0,0,0,508
10565195,scikit-learn/scikit-learn,python,5880,1447890014,1455198745,1455198745,121812,121812,commits_in_master,false,false,false,29,2,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,9,4.411328523754619,0.09246962821158897,2,gael.varoquaux@normalesup.org,sklearn/cluster/spectral.py,2,0.0018656716417910447,0,0,false,DOC: clarify terms in documentation of Spectral Clustering Quick clarificaiton of terms: this seems to be a common confusion (see eg [this question](http://stackoverflowcom/questions/33773916/precomputed-distances-for-spectral-clustering-with-scikit-learn) and [this question](http://stackoverflowcom/questions/20530804/using-the-class-sklearn-cluster-spectralclustering-with-parameter-affinity-prec) on Stack Overflow),,3349,0.7381307853090475,0.3460820895522388,50930,528.4704496367564,39.48556842725309,135.53897506381307,8030,81,1919,442,travis,jakevdp,glouppe,false,glouppe,51,0.8627450980392157,1857,0,1652,true,true,false,false,2,46,8,19,2,0,39
10540183,scikit-learn/scikit-learn,python,5872,1447807047,1455284086,1455284086,124617,124617,commits_in_master,false,false,false,8,5,5,0,4,0,4,0,3,0,0,10,10,9,0,0,0,0,10,10,9,0,0,229,28,229,28,62.32867146284252,1.3065240854946656,32,t3kcit@gmail.com,sklearn/neighbors/graph.py|sklearn/manifold/isomap.py|sklearn/neighbors/graph.py|sklearn/manifold/isomap.py|doc/modules/pipeline.rst|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/decomposition/kernel_pca.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/neighbors/graph.py|sklearn/semi_supervised/label_propagation.py,14,0.0064995357474466105,0,1,false,Parallel NearestNeighbors execution in Isomap Patches issue #5867,,3347,0.738273080370481,0.34540389972144847,50930,528.4704496367564,39.48556842725309,135.53897506381307,8021,81,1918,445,travis,lucasdavid,glouppe,false,glouppe,0,0,18,16,1325,true,true,false,false,0,1,0,0,2,0,1236
10519449,scikit-learn/scikit-learn,python,5865,1447739542,1447754533,1447754533,249,249,commits_in_master,false,false,false,24,1,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.911246689776775,0.1029487980820211,5,peter.fischer@fau.de,.mailmap,5,0.00458295142071494,0,0,false,Adding email to the mailmap Following up with the conversation in #5840 I wanted to add my name to the mailmapThanks-Henry Lin,,3345,0.7384155455904334,0.34097158570119157,50930,528.4704496367564,39.48556842725309,135.53897506381307,8007,80,1918,364,travis,hlin117,mblondel,false,mblondel,13,0.6153846153846154,18,20,1246,true,true,false,false,6,60,9,12,25,2,-1
10518151,scikit-learn/scikit-learn,python,5863,1447735060,1455205369,1455205369,124505,124505,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,5,3,5,9.02090918550509,0.18909491151971106,10,t3kcit@gmail.com,sklearn/utils/class_weight.py|sklearn/utils/tests/test_class_weight.py,9,0.008249312557286892,0,0,false,[MRG] Fix for missing classes found in y - Fixes #4327 Fixes #4327,,3343,0.7385581812743045,0.34097158570119157,50930,528.4704496367564,39.48556842725309,135.53897506381307,8007,80,1917,443,travis,trevorstephens,glouppe,false,glouppe,22,0.8181818181818182,142,51,826,true,true,true,false,2,33,5,12,5,0,3872
10517138,scikit-learn/scikit-learn,python,5860,1447731923,1447732056,1447732056,2,2,commits_in_master,false,false,false,17,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.607153844766894,0.09657445041915741,5,ragvrv@gmail.com,sklearn/neural_network/rbm.py,5,0.004591368227731864,0,0,false,[MRG] Fix grammar in rbm docstring I noticed a small typo in the docstring and fixed it,,3340,0.7389221556886227,0.34067952249770433,50930,528.4704496367564,39.48556842725309,135.53897506381307,8006,80,1917,360,travis,pwgerman,amueller,false,amueller,0,0,0,0,643,false,false,false,false,0,0,0,0,0,0,-1
10516006,scikit-learn/scikit-learn,python,5858,1447728526,1448027281,1448027281,4979,4979,commits_in_master,false,false,false,14,3,1,0,6,0,6,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,4,0,4.26685463617312,0.08944115073884669,18,t3kcit@gmail.com,sklearn/ensemble/gradient_boosting.py,18,0.016544117647058824,0,1,false,Change super call to pass presort value to super Fix parameter passing issuehttps://githubcom/scikit-learn/scikit-learn/issues/5857,,3338,0.7390653085680048,0.3391544117647059,50930,528.4704496367564,39.48556842725309,135.53897506381307,8006,79,1917,375,travis,mcculloh,glouppe,false,glouppe,1,0.0,0,0,459,false,false,false,false,0,4,1,0,0,0,20
10515576,scikit-learn/scikit-learn,python,5855,1447727325,1447754580,1447754580,454,454,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.382583501474027,0.09186704141683279,1,gael.varoquaux@normalesup.org,doc/data_transforms.rst,1,0.0009191176470588235,0,0,false,[MRG] minor grammatical fix ,,3336,0.7392086330935251,0.3391544117647059,50930,528.4704496367564,39.48556842725309,135.53897506381307,8006,79,1917,363,travis,pieteradejong,mblondel,false,mblondel,2,0.0,34,49,1248,false,false,false,false,0,10,3,4,0,0,-1
10514294,scikit-learn/scikit-learn,python,5852,1447724016,1449950076,1449950076,37101,37101,commits_in_master,false,false,false,21,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.206918143310647,0.08818477119232473,12,ragvrv@gmail.com,sklearn/linear_model/randomized_l1.py,12,0.011039558417663294,0,0,false,[MRG] fix for #5725  makes sure numpy array is not compared to string which will fail in future versions of numpy,,3334,0.7393521295740851,0.33854645814167433,50930,528.4704496367564,39.48556842725309,135.53897506381307,8004,79,1917,399,travis,nickyj88,TomDLT,false,TomDLT,0,0,2,12,440,false,false,false,false,0,0,0,0,0,0,24
10514052,scikit-learn/scikit-learn,python,5850,1447723345,1448560704,1448560704,13955,13955,commits_in_master,false,false,false,57,4,3,2,3,0,5,0,3,0,0,3,3,1,0,0,0,0,3,3,1,0,0,8,0,16,0,17.259344082392012,0.36178771656288805,26,yoshiki@ucsd.edu,doc/modules/ensemble.rst|sklearn/ensemble/forest.py|doc/about.rst|doc/about.rst,21,0.007359705611775529,0,1,false,Update to Out of Bag Score Documentation for Random Forest Many have been confused about whether the score represents the models accuracy or its error examples in issue Meaning of oob_score_ in RandomForestClassifier #5838 and Stack overflow post here: http://stackoverflowcom/questions/31438476/parameter-oob-score-in-scikit-learn-equals-accuracy-or-errorThe score represents the accuracy Ive made some changes in the code documentation to make this clear,,3333,0.7392739273927392,0.33854645814167433,50930,528.4704496367564,39.48556842725309,135.53897506381307,8004,79,1917,380,travis,JeremyNixon,TomDLT,false,TomDLT,0,0,8,14,880,false,false,false,false,0,0,0,0,0,0,5762
10512963,scikit-learn/scikit-learn,python,5847,1447720408,1451931621,1451931621,70186,70186,commit_sha_in_comments,false,true,false,56,1,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.683481262506609,0.09817442673968184,2,t3kcit@gmail.com,examples/ensemble/plot_partial_dependence.py,2,0.0018433179723502304,0,0,false,[MRG] Fix for uneven grids in Partial Dependence Plots example Fixes #5846  - The problem was not in the code base but the shape of Z The output of the example wont change but a user can change the grid_resolution param such that it exceeds the number of unique values and then the shapes become invalid,,3331,0.7394175923146202,0.3391705069124424,50930,528.4704496367564,39.48556842725309,135.53897506381307,8004,79,1917,411,travis,trevorstephens,ogrisel,false,ogrisel,21,0.8095238095238095,142,51,826,true,true,true,false,2,30,4,12,4,0,18
10512833,scikit-learn/scikit-learn,python,5846,1447720023,1447722887,1447722887,47,47,commits_in_master,false,false,false,70,3,3,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,14.421691262084959,0.3023053137003709,108,trev.stephens@gmail.com,doc/whats_new.rst|doc/whats_new.rst|doc/whats_new.rst,108,0.09953917050691244,0,0,false,Adding my name (Henry Lin) to 0170 change log Hi I worked on PR #5431 (associated with issue #5322) I wanted to associate one of the bug fixes with my name Specifically the line that reads: All regressors now consistently handle and warn when given y that is of shape (n_samples 1) By Andreas MüllerI made this change to whats_newrst I also fixed some hyperlinks in the whats_newrst,,3330,0.7393393393393394,0.3391705069124424,50930,528.4704496367564,39.48556842725309,135.53897506381307,8004,79,1917,353,travis,hlin117,amueller,false,amueller,12,0.5833333333333334,18,20,1245,true,true,true,false,6,58,8,12,25,2,27
10511996,scikit-learn/scikit-learn,python,5841,1447717735,,1447727148,156,,unknown,false,true,false,4,2,2,8,2,0,10,0,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,8.910376932127535,0.18677797525294812,10,t3kcit@gmail.com,doc/modules/multiclass.rst|doc/data_transforms.rst,10,0.009216589861751152,0,2,false,[MRG] docs: dataset transformation ,,3327,0.7400060114217012,0.3391705069124424,50930,528.4704496367564,39.48556842725309,135.53897506381307,8004,79,1917,356,travis,pieteradejong,pieteradejong,true,,1,0.0,34,49,1248,false,false,false,false,0,7,1,0,0,0,78
10510874,scikit-learn/scikit-learn,python,5835,1447714383,,1447715503,18,,unknown,false,false,false,14,3,3,0,0,0,0,0,1,1,1,1,3,0,0,1,1,1,1,3,0,0,1,0,0,0,0,6.6756310810584765,0.1399335581048616,11,t3kcit@gmail.com,tmp.junk|tmp.junk|doc/modules/cross_validation.rst,11,0.01014760147601476,0,1,false,Cross validation docs This fixes the issue at #5770 (formatting issues in cross-validation docs),,3322,0.741119807344973,0.33948339483394835,50930,528.4704496367564,39.48556842725309,135.53897506381307,8003,79,1917,348,travis,lazarillo,GaelVaroquaux,false,,0,0,5,0,1726,true,false,false,false,1,2,0,0,1,0,-1
10510837,scikit-learn/scikit-learn,python,5834,1447714252,1447977089,1447977089,4380,4380,commits_in_master,false,false,false,6,7,1,4,7,0,11,0,5,0,0,1,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,4.893888344155446,0.10258493925895577,6,t3kcit@gmail.com,README.rst,6,0.005535055350553505,0,3,false,added cython to requirements in documentation ,,3321,0.7410418548629931,0.33948339483394835,50930,528.4704496367564,39.48556842725309,135.53897506381307,8003,79,1917,372,travis,hugobowne,amueller,false,amueller,0,0,5,2,943,false,true,false,false,0,1,0,0,0,0,2
10509978,scikit-learn/scikit-learn,python,5833,1447712027,1452311645,1452311645,76660,76660,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.808183728184383,0.10078841220208011,11,t3kcit@gmail.com,doc/modules/neural_networks_supervised.rst,11,0.010156971375807941,0,0,false,added warm_start flag to example ,,3320,0.7409638554216867,0.3397968605724838,50930,528.4704496367564,39.48556842725309,135.53897506381307,8003,78,1917,416,travis,welch,MechCoder,false,MechCoder,0,0,6,0,1353,false,false,false,false,0,1,0,0,0,0,-1
10509570,scikit-learn/scikit-learn,python,5831,1447710851,1447880252,1447880252,2823,2823,commits_in_master,false,false,false,9,2,2,0,6,0,6,0,4,1,0,1,2,0,0,0,1,0,1,2,0,0,0,0,0,0,0,14.174866385067164,0.29713138201294054,15,villu.ruusmann@gmail.com,doc/related_projects.rst|hist.txt|doc/related_projects.rst,15,0.013850415512465374,0,1,false,add docs add Lasagne to related projects in docs,,3319,0.740885808978608,0.3397968605724838,50930,528.4704496367564,39.48556842725309,135.53897506381307,8003,78,1917,367,travis,codevig,agramfort,false,agramfort,0,0,7,126,1409,false,false,true,false,0,0,0,0,0,0,9
10509461,scikit-learn/scikit-learn,python,5829,1447710493,1449639440,1449639440,32149,32149,commits_in_master,false,false,false,10,2,1,11,8,0,19,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.677084899656589,0.09804033860176897,108,trev.stephens@gmail.com,doc/whats_new.rst,108,0.0997229916897507,0,0,false,repaired dead link Added current link for contributor Alexandre Passos,,3318,0.7408077154912598,0.3397968605724838,50930,528.4704496367564,39.48556842725309,135.53897506381307,8003,78,1917,393,travis,joshuacook,amueller,false,amueller,0,0,4,4,1295,false,false,false,false,0,2,0,0,0,0,195
10508750,scikit-learn/scikit-learn,python,5828,1447708629,1448067076,1448067076,5974,5974,commits_in_master,false,false,false,10,2,2,8,3,0,11,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.858503259879964,0.20665243809797887,108,trev.stephens@gmail.com,doc/whats_new.rst|doc/whats_new.rst,108,0.0997229916897507,0,0,false,Add MLP to whats new Fixes #5640 updated whats new,,3317,0.7407295749170938,0.3397968605724838,50930,528.4704496367564,39.48556842725309,135.53897506381307,8002,78,1917,375,travis,mth4saurabh,amueller,false,amueller,1,1.0,9,6,847,false,false,false,false,1,1,0,1,0,0,150
10493725,scikit-learn/scikit-learn,python,5823,1447643956,1452250483,1452250483,76775,76775,commit_sha_in_comments,false,false,false,8,2,1,5,2,0,7,0,4,0,0,3,6,3,0,0,0,0,6,6,6,0,0,42,0,160,0,13.103092684632351,0.2746650255645436,47,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/model_selection/_search.py|sklearn/model_selection/_validation.py,43,0.0027752081406105457,2,0,false,DOC Reworded cv documentation Fixes https://githubcom/scikit-learn/scikit-learn/issues/5822@jnothman @amueller ,,3314,0.7410983705491853,0.3404255319148936,50930,528.4704496367564,39.48556842725309,135.53897506381307,7987,78,1916,410,travis,rvraghav93,MechCoder,false,MechCoder,27,0.6666666666666666,29,43,772,true,false,true,true,5,181,18,68,44,0,1
10484096,scikit-learn/scikit-learn,python,5819,1447581678,1447597742,1447597742,267,267,commits_in_master,false,false,false,92,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,27,0,27,0,4.567180568609459,0.09573671187591148,10,t3kcit@gmail.com,sklearn/decomposition/pca.py,10,0.00919963201471941,0,0,false,Added documentation to PCA docstring Addresses #5751 Some things that I added:* Mentioned that components_ is sorted by explained_variance_* Added the explained_variance_ attribute to the docstring Apparently this was being stored before but it wasnt documented* One doctest with pcaexplained_variance_* Some very minor punctuation changesOne thing I was hoping to add to the documentation was how to smooth data using PCA To do this one would say pcainverse_transform(pcatransform(X)) Is there any interest in me writing an example in the docstring of how to smooth data using PCA,,3313,0.7410202233625113,0.3376264949402024,50930,528.4704496367564,39.46593363440016,135.51934027096013,7984,78,1916,340,travis,hlin117,GaelVaroquaux,false,GaelVaroquaux,10,0.6,18,20,1244,true,true,true,false,6,50,6,11,25,2,51
10445762,scikit-learn/scikit-learn,python,5809,1447388905,1447544384,1447544384,2591,2591,commits_in_master,false,false,false,36,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.712080523432824,0.09877228438979521,13,villu.ruusmann@gmail.com,doc/related_projects.rst,13,0.011786038077969175,0,0,false,Update related_projectsrst to point to the hdbscan clustering library The hdbscan library (https://githubcom/lmcinnes/hdbscan) provides high performance implementations of HDBSCAN and Robust Single Linkage inheriting the sklearn estimator interface These are new but very powerful clustering algorithms,,3310,0.7413897280966767,0.3318223028105168,50928,528.3733898837575,39.4478479421929,135.48539114043356,7959,80,1913,345,travis,lmcinnes,agramfort,false,agramfort,0,0,13,6,211,false,true,false,false,0,0,0,0,0,0,2486
10440921,scikit-learn/scikit-learn,python,5806,1447375653,1447411167,1447411167,591,591,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.752596425672043,0.09962155854764977,5,peter.fischer@fau.de,examples/applications/topics_extraction_with_nmf_lda.py,5,0.004533091568449683,0,1,false,Remove exit call in topic extraction example Addresses #5767 This call came from https://githubcom/scikit-learn/scikit-learn/commit/47a85f3f3470dc07fa4a6f621e9ebaef4525827bWas most likely introduced for debugging ,,3309,0.7413115744938048,0.3318223028105168,50928,528.3733898837575,39.4478479421929,135.48539114043356,7953,80,1913,343,travis,vighneshbirodkar,glouppe,false,glouppe,7,0.5714285714285714,14,1,977,true,false,false,false,3,31,7,52,51,0,552
10435004,scikit-learn/scikit-learn,python,5802,1447360320,1447611978,1447611978,4194,4194,commits_in_master,false,false,false,30,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.02625794032402,0.06343448609189747,4,t3kcit@gmail.com,sklearn/ensemble/voting_classifier.py,4,0.003616636528028933,0,8,false,Voting Classifier: predictions are float but npbincount can only be used on integers This downcast manually the array of predictions to integers so that npbincount does not trigger an error,,3307,0.7414575143634714,0.3318264014466546,50938,528.2696611567003,39.44010365542424,135.45879304252227,7951,80,1913,345,travis,alvarouc,glouppe,false,glouppe,0,0,6,4,507,false,false,false,false,0,0,0,0,0,0,1115
10434738,scikit-learn/scikit-learn,python,5801,1447359649,1447360948,1447360948,21,21,commits_in_master,false,false,false,8,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.9091815483012695,0.1029031281500657,0,,examples/decomposition/plot_pca_iris.py,0,0.0,0,1,false,Remove unused code from plotting tutorial Addresses #5766,,3306,0.7413793103448276,0.3318264014466546,50938,528.2696611567003,39.44010365542424,135.45879304252227,7951,80,1913,342,travis,hlin117,GaelVaroquaux,false,GaelVaroquaux,9,0.5555555555555556,18,20,1241,true,true,true,false,5,49,5,11,25,2,8
10433524,scikit-learn/scikit-learn,python,5800,1447356720,1449693249,1449693249,38942,38942,commits_in_master,false,false,false,41,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.36796860901181,0.09155856818620609,2,gael.varoquaux@normalesup.org,examples/preprocessing/plot_robust_scaling.py,2,0.0018083182640144665,0,1,false,Fixed transformation of test set via RobustScaler in example RobustScaler should have followed the same approach of StandardScaler: fit_transform on train and only transform on testResults are similar:    Testset accuracy using standard scaler: 0545    Testset accuracy using robust scaler:   0705,,3305,0.7413010590015129,0.3318264014466546,50938,528.2696611567003,39.44010365542424,135.45879304252227,7951,80,1913,395,travis,fabioticconi,TomDLT,false,TomDLT,0,0,7,17,851,false,true,false,false,0,0,0,0,0,0,968
10430531,scikit-learn/scikit-learn,python,5798,1447348587,1447374641,1447374641,434,434,commits_in_master,false,false,false,19,3,1,6,3,0,9,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.670254778384606,0.09789489780019742,9,peter.fischer@fau.de,doc/related_projects.rst,9,0.008144796380090498,0,0,false,Added a related PMML serialization project Opened a new pull request regarding documentation changes as suggested in PR #5151,,3304,0.7412227602905569,0.33122171945701356,50938,528.2696611567003,39.44010365542424,135.45879304252227,7949,80,1913,341,travis,vruusmann,agramfort,false,agramfort,0,0,39,3,843,false,false,false,false,0,2,0,0,0,0,10
10426310,scikit-learn/scikit-learn,python,5796,1447333447,1447343761,1447343761,171,171,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.212422378267049,0.08829807986956376,5,t3kcit@gmail.com,sklearn/dummy.py,5,0.0045207956600361665,0,0,false,memory issue in DummyClassifier(strategymost_frequent) fixes #5717,,3303,0.7411444141689373,0.3309222423146474,50938,528.2696611567003,39.44010365542424,135.45879304252227,7946,80,1913,339,travis,max-moroz,agramfort,false,agramfort,0,0,0,0,1321,false,false,false,false,0,1,0,0,0,0,37
10418647,scikit-learn/scikit-learn,python,5794,1447298193,1454668463,1454668463,122837,122837,commits_in_master,false,true,false,19,2,1,26,14,0,40,0,7,0,0,3,3,2,0,0,0,0,3,3,2,0,0,12,13,29,40,13.59554046400035,0.2849809468190241,110,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/ensemble/tests/test_voting_classifier.py|sklearn/ensemble/voting_classifier.py,108,0.004512635379061372,0,7,false,raise NotFittedError in VotingClassifier Raise a NotFittedError if predict predict_proba or transform is called on the unfitted VottingClassifier object,,3302,0.7410660205935796,0.3312274368231047,50938,528.2696611567003,39.44010365542424,135.45879304252227,7941,80,1912,438,travis,rasbt,MechCoder,false,MechCoder,6,1.0,1425,43,767,false,true,false,false,1,12,1,0,0,0,501
10414016,scikit-learn/scikit-learn,python,5791,1447284483,1447445273,1447445273,2679,2679,commits_in_master,false,false,false,93,1,1,0,8,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,2,15,2,9.015268791903528,0.1889722474033012,41,trev.stephens@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,34,0.030685920577617327,0,1,false,[MRG] Scaling a sparse matrix along axis 0 should accept a csc by default For scaling a csc_matrix along axis 0 one pass is made across n_featuresFor scaling a csc_matrix along axis 0 two passes are made across non_zero_values and a pass across n_featuresSo the csc_matrix should be accepted by default This can be validated by this quick benchmark    X  nprandomrand(10000 10000)    csr  sparsecsr_matrix(X)    csc  sparsecsc_matrix(X)    %timeit csr_mean_variance_axis0(csr)    10 loops best of 3: 837 ms per loop    %timeit csc_mean_variance_axis0(csc)    10 loops best of 3: 412 ms per loop,,3301,0.7409875795213572,0.3312274368231047,50938,528.2696611567003,39.44010365542424,135.45879304252227,7940,80,1912,342,travis,MechCoder,GaelVaroquaux,false,GaelVaroquaux,81,0.8641975308641975,88,41,1240,true,true,false,false,18,267,33,230,49,1,2
10406342,scikit-learn/scikit-learn,python,5788,1447262412,,1447262831,6,,unknown,false,false,false,19,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,0,6,0,8.579087955651254,0.17250691358808148,2,t3kcit@gmail.com,doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,2,0.0017985611510791368,0,0,false,Fix outdated usage of module model_selection in text_analytics tutorial solutions Fixes #5787 by replacing model_selection with the correct modules,,3300,0.7412121212121212,0.3318345323741007,50776,529.5809043642666,39.52654797542146,135.73341736253346,7931,80,1912,340,travis,aakashjain,aakashjain,true,,0,0,7,4,637,false,false,false,false,1,0,0,0,0,0,-1
10392887,scikit-learn/scikit-learn,python,5784,1447204814,1449473219,1449473219,37806,37806,commits_in_master,false,false,false,20,2,1,19,7,0,26,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,56,16,112,33,9.027519453959165,0.18152333689998407,10,t3kcit@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,10,0.009017132551848512,0,6,false,[MRG] Parallelize embarassingly parallel loop in RFECVfit I also took the opportunity to clean up a bit of the code,,3298,0.741358399029715,0.3318304779080252,50776,529.0097683945171,39.50685363163699,135.6743343311801,7917,79,1911,393,travis,MechCoder,MechCoder,true,MechCoder,80,0.8625,88,41,1239,true,true,false,false,18,259,31,221,46,1,1
10392501,scikit-learn/scikit-learn,python,5783,1447203903,1447541722,1447541722,5630,5630,commits_in_master,false,false,false,10,3,1,7,4,0,11,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,13,4.564880492597583,0.09178959334200562,9,tom.dupre-la-tour@m4x.org,sklearn/utils/multiclass.py,9,0.008115419296663661,0,0,false,BUG :#5782  check_classification_targets returns y instead of y_type Please review,,3297,0.7412799514710343,0.3309287646528404,50776,529.0097683945171,39.50685363163699,135.6743343311801,7917,79,1911,347,travis,varun-kr,MechCoder,false,MechCoder,0,0,0,9,1129,false,true,false,false,1,0,0,0,0,0,19
10477456,scikit-learn/scikit-learn,python,5780,1447177788,,1452779078,93354,,unknown,false,false,false,9,2,2,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,8.588966610079206,0.17270499890238725,3,t3kcit@gmail.com,sklearn/model_selection/_split.py|sklearn/model_selection/_split.py,3,0.0026905829596412557,0,0,false,[MRG + 1] Fix inspectgetargspec() DeprecationWarning in cross-validation repr() ,,3296,0.741504854368932,0.3291479820627803,50776,529.0097683945171,39.50685363163699,135.6743343311801,7913,80,1911,418,travis,mkurnikov,mkurnikov,true,,0,0,0,0,1153,false,false,false,false,0,0,0,0,0,0,6021
10366072,scikit-learn/scikit-learn,python,5773,1447100939,1449797472,1449797472,44942,44942,commits_in_master,false,false,false,19,1,1,0,3,0,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,4.4664706312202815,0.08981078172515215,11,trev.stephens@gmail.com,sklearn/tests/test_discriminant_analysis.py,11,0.00980392156862745,0,1,false,[MRG] Fix import of reload for python 33 Forward-port () of da4f480a6adf5fed30a42500fe0e5a21c404ac2a to fix the Python33 buildFixes #5738,,3294,0.7416514875531269,0.3306595365418895,50776,529.0097683945171,39.50685363163699,135.6743343311801,7902,80,1910,393,travis,amueller,agramfort,false,agramfort,354,0.8531073446327684,1266,40,1844,true,true,true,false,151,1216,111,498,122,9,1209
10358731,scikit-learn/scikit-learn,python,5771,1447076437,1447715220,1447715220,10646,10646,commits_in_master,false,false,false,32,1,1,3,2,0,5,0,3,0,0,2,2,0,1,0,0,0,2,2,0,1,0,0,0,0,0,9.125534342965395,0.18349417583229236,13,t3kcit@gmail.com,doc/modules/cross_validation.rst|doc/themes/scikit-learn/layout.html,11,0.009812667261373774,0,0,false,DOC - Minor documentation fixes - Fixes #5770 and #5761 Fixes #5770 and #5761- Fixes formatting issues in warning after LOLO- Fixes Error 404 Page not found in contributing section,,3293,0.7415730337078652,0.32917038358608386,50776,529.0097683945171,39.50685363163699,135.6743343311801,7899,80,1910,356,travis,bhargav,GaelVaroquaux,false,GaelVaroquaux,2,1.0,8,22,1944,true,true,false,false,1,4,2,0,1,0,399
10345561,scikit-learn/scikit-learn,python,5763,1447007858,,1453924747,115281,,unknown,false,true,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,3,4.139995103472932,0.08324608301398756,11,trev.stephens@gmail.com,sklearn/tests/test_discriminant_analysis.py,11,0.00980392156862745,0,0,false,Added an else statement before importing the reload module ,,3291,0.7420237010027347,0.32887700534759357,50776,529.0097683945171,39.50685363163699,135.6743343311801,7881,80,1909,435,travis,Sentient07,TomDLT,false,,2,0.5,13,12,692,false,true,false,false,1,7,2,4,1,0,1551
10338269,scikit-learn/scikit-learn,python,5755,1446951142,1447197214,1447197214,4101,4101,commits_in_master,false,false,false,16,2,2,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,12,0,12,0,24.830664205115085,0.4992893667863186,45,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py,41,0.0125,2,0,false,[MRG] MNT 019 -- 020 These changes will be released in 020 only @amueller or @ogrisel ,,3288,0.7423965936739659,0.32857142857142857,50776,529.0097683945171,39.50685363163699,135.6743343311801,7864,80,1908,340,travis,rvraghav93,MechCoder,false,MechCoder,26,0.6538461538461539,29,43,764,true,false,true,true,5,189,20,80,48,0,7
10338105,scikit-learn/scikit-learn,python,5754,1446950379,1447548641,1447548641,9971,9971,commits_in_master,false,false,false,13,2,1,2,24,0,26,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,0,36,0,9.17600544903733,0.18450903739151198,0,,examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_ridge_path.py,0,0.0,2,6,false,MNT fix for mpl 15 set_color_cycle -- set_prop_cycle(color ) Ping #5726 @agramfort @amueller ,,3287,0.7423182233039245,0.32857142857142857,50776,529.0097683945171,39.50685363163699,135.6743343311801,7864,80,1908,347,travis,rvraghav93,agramfort,false,agramfort,25,0.64,29,43,764,true,false,false,false,5,189,19,80,48,0,669
10334782,scikit-learn/scikit-learn,python,5753,1446932467,1446991058,1446991058,976,976,commits_in_master,false,false,false,13,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.444383893836697,0.08936666380643653,6,peter.fischer@fau.de,doc/tutorial/basic/tutorial.rst,6,0.005357142857142857,0,1,false,grammatical fix in tutorialrst under basic category __is it__ should be __it is__,,3286,0.7422398052343274,0.32857142857142857,50776,529.0097683945171,39.50685363163699,135.6743343311801,7863,80,1908,333,travis,staranjeet,agramfort,false,agramfort,0,0,40,78,921,false,false,false,false,0,0,0,0,0,0,-1
10330548,scikit-learn/scikit-learn,python,5752,1446903317,1447534227,1447534227,10515,10515,commits_in_master,false,false,false,20,3,1,1,5,0,6,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.965592317007897,0.09984698642472498,20,t3kcit@gmail.com,doc/modules/gaussian_process.rst,20,0.017825311942959002,0,1,false,Proper fix for issue #5694 Added [RW2006]s corresponding reference with link to the books website and PDF version available online,,3285,0.7421613394216134,0.32887700534759357,50776,529.0097683945171,39.50685363163699,135.6743343311801,7861,80,1908,349,travis,hrjn,agramfort,false,agramfort,1,0.0,1,0,1127,false,false,false,false,1,3,2,0,0,0,417
10316083,scikit-learn/scikit-learn,python,5749,1446835558,1446905721,1446905721,1169,1169,commits_in_master,false,false,false,8,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.261692932202446,0.08569313974699531,2,gael.varoquaux@normalesup.org,doc/modules/calibration.rst,2,0.0017699115044247787,0,0,true,Improve wording for best choice of calibration method ,,3284,0.7420828258221681,0.32654867256637166,50776,529.0097683945171,39.50685363163699,135.6743343311801,7851,81,1907,330,travis,ethanwhite,jnothman,false,jnothman,1,1.0,99,27,1660,false,false,false,false,0,0,0,0,0,0,1169
10310006,scikit-learn/scikit-learn,python,5748,1446811809,,1449811739,49998,,unknown,false,false,false,12,2,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.486192246984415,0.09020732025243444,4,t3kcit@gmail.com,doc/install.rst,4,0.0035335689045936395,0,0,true,Disambiguate some unclear installation note Sorry for the cross-posting Reference to scikit-learn/scikit-learngithubio#3,,3283,0.742308863844045,0.32420494699646646,50776,529.0097683945171,39.50685363163699,135.6743343311801,7848,81,1907,391,travis,alvations,alvations,true,,0,0,49,14,1514,false,false,false,false,1,1,0,0,0,0,49979
10303694,scikit-learn/scikit-learn,python,5743,1446778661,1452222933,1452222933,90737,90737,commit_sha_in_comments,false,false,false,6,3,1,3,5,0,8,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.235759665960727,0.08517168188536785,32,t3kcit@gmail.com,doc/modules/classes.rst,32,0.028268551236749116,0,4,false,cosine_similarity cosine_distances sigmoid_kernel added to classesrst ,,3281,0.7424565681194758,0.32420494699646646,50776,529.0097683945171,39.50685363163699,135.6743343311801,7845,81,1906,412,travis,Sentient07,MechCoder,false,MechCoder,1,0.0,12,12,689,false,true,true,true,1,6,1,4,1,0,13896
10302056,scikit-learn/scikit-learn,python,5742,1446773826,1447100649,1447100649,5447,5447,commits_in_master,false,false,false,8,1,1,0,2,0,2,0,2,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,4.767581613549888,0.09586543538222661,5,t3kcit@gmail.com,doc/themes/scikit-learn/layout.html,5,0.00441696113074205,0,1,false,[MRG] fix link to pdf documentation Fixes #5741,,3280,0.7423780487804879,0.32420494699646646,50776,529.0097683945171,39.50685363163699,135.6743343311801,7845,81,1906,336,travis,amueller,amueller,true,amueller,353,0.8526912181303116,1265,40,1840,true,true,false,false,150,1244,111,502,123,9,6
10300452,scikit-learn/scikit-learn,python,5740,1446769258,1446773154,1446773154,64,64,commits_in_master,false,false,false,36,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,3,4.698383659569715,0.09447401165791919,22,tom.dupre-la-tour@m4x.org,sklearn/linear_model/tests/test_logistic.py,22,0.01950354609929078,0,0,false,BUG: use pre-parsed scipy version This avoids a test failure when using a development install of scipy: sp_version  tuple([int(s) for s in scipy__version__split()])ValueError: invalid literal for int() with base 10: dev0+1072f2e,,3279,0.7422994815492528,0.325354609929078,50776,529.0097683945171,39.50685363163699,135.6743343311801,7843,81,1906,327,travis,perimosocordiae,amueller,false,amueller,7,0.7142857142857143,49,48,2371,false,true,false,false,0,0,0,0,0,0,10
10287376,scikit-learn/scikit-learn,python,5733,1446728731,1447284216,1447284216,9258,9258,commits_in_master,false,false,false,7,2,2,0,6,0,6,0,3,0,0,4,4,2,0,1,0,0,4,4,2,0,1,17,0,17,0,28.34335386564676,0.5710023578764851,14,toma.moral@gmail.com,continuous_integration/install.sh|continuous_integration/test_script.sh|appveyor.yml|continuous_integration/appveyor/requirements.txt|continuous_integration/install.sh|continuous_integration/test_script.sh,7,0.00530035335689046,0,0,false,MAINT nose-timer added to travis See https://githubcom/scikit-learn/scikit-learn/issues/5639#issuecomment-152673773,,3278,0.7422208663819402,0.32597173144876324,50776,528.497715456121,39.50685363163699,135.69402867496456,7838,81,1906,346,travis,giorgiop,TomDLT,false,TomDLT,13,0.9230769230769231,2,7,1077,true,true,false,false,7,235,16,80,83,4,650
10285101,scikit-learn/scikit-learn,python,5731,1446716717,1446718997,1446718997,38,38,commits_in_master,false,false,false,38,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,3.631269860930239,0.07315519949086903,24,t3kcit@gmail.com,sklearn/tree/tree.py,24,0.021238938053097345,0,1,false,removes extra spaces from error message I unfortunately got to know this error message too well when my data-formatting pipeline encountered a bug :)There was a space both to end one line and to start the next ,,3277,0.7421422032346658,0.32654867256637166,50776,528.497715456121,39.50685363163699,135.69402867496456,7835,81,1906,330,travis,ClimbsRocks,glouppe,false,glouppe,1,1.0,122,95,595,false,true,false,false,0,1,0,0,0,0,37
10284033,scikit-learn/scikit-learn,python,5730,1446708593,1450222443,1450222443,58564,58564,commits_in_master,false,false,false,4,3,2,5,22,0,27,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,66,0,9.221132335928045,0.1857680099252416,2,gael.varoquaux@normalesup.org,examples/applications/plot_prediction_latency.py|examples/applications/plot_prediction_latency.py,2,0.0017683465959328027,0,5,false,Bugfix/5720 Fix for https://githubcom/scikit-learn/scikit-learn/issues/5720,,3276,0.7420634920634921,0.32625994694960214,50776,528.497715456121,39.50685363163699,135.69402867496456,7833,81,1906,405,travis,hdmetor,TomDLT,false,TomDLT,0,0,5,8,901,false,false,false,false,0,0,0,0,0,0,314
10281259,scikit-learn/scikit-learn,python,5728,1446697360,1448069795,1448069795,22873,22873,commits_in_master,false,true,false,28,2,2,0,6,0,6,0,4,0,0,22,22,19,0,0,0,0,22,22,19,0,0,816,0,816,0,180.69838791987718,3.640331653172886,259,trev.stephens@gmail.com,doc/datasets/index.rst|doc/modules/clustering.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/ensemble/plot_partial_dependence.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py|sklearn/metrics/regression.py|sklearn/svm/classes.py|sklearn/tree/tree.py|doc/datasets/index.rst|doc/modules/clustering.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/ensemble/plot_partial_dependence.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/decomposition/nmf.py|sklearn/ensemble/voting_classifier.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py|sklearn/metrics/classification.py|sklearn/metrics/regression.py|sklearn/preprocessing/data.py|sklearn/svm/classes.py|sklearn/tree/tree.py,105,0.011494252873563218,0,4,false,[MRG] DOC some fixes to docbuild This makes at least 017X build without sphinx warningsI already merged it into 017X this is the cherry-picked version for master,,3275,0.7419847328244275,0.32625994694960214,50776,528.497715456121,39.50685363163699,135.69402867496456,7833,81,1905,381,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,352,0.8522727272727273,1265,40,1839,true,true,true,false,145,1250,108,504,112,9,1229
10280259,scikit-learn/scikit-learn,python,5727,1446693875,1446694754,1446694754,14,14,commits_in_master,false,false,false,33,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.770100092805642,0.09609796577378162,2,gael.varoquaux@normalesup.org,doc/conf.py,2,0.001774622892635315,0,0,false,Update copyright year in the doc footer (2014 → 2015) Simply updating copyright year in the doc footer (present on every page) from *2010 - 2014* to *2010 - 2015*[2010-2014](https://cloudgithubusercontentcom/assets/11994719/10955360/842ae53a-8353-11e5-9dec-7233ce12ce3fpng)↓[2010-2015](https://cloudgithubusercontentcom/assets/11994719/10955362/86194378-8353-11e5-9654-9f9b80d9d5abpng),,3274,0.741905925473427,0.32741792369121564,50776,528.497715456121,39.50685363163699,135.69402867496456,7832,81,1905,329,travis,Naereen,GaelVaroquaux,false,GaelVaroquaux,4,0.75,7,46,201,false,true,true,false,0,5,4,0,0,1,14
10277320,scikit-learn/scikit-learn,python,5721,1446685056,1449860574,1449860574,52925,52925,commits_in_master,false,false,false,12,2,1,4,5,0,9,0,5,0,0,10,10,10,0,0,0,0,10,10,10,0,0,38,0,38,0,46.842709499867695,0.9436968198090898,13,t3kcit@gmail.com,examples/applications/plot_prediction_latency.py|examples/classification/plot_lda_qda.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/decomposition/plot_pca_iris.py|examples/decomposition/plot_sparse_coding.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/linear_model/plot_sgd_separating_hyperplane.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py,4,0.0017793594306049821,0,1,false,[MRG]  Dont use deprecated 1d X (or deprecated matplotlib stuff) in examples ,,3273,0.7418270699663917,0.32829181494661924,50713,529.1542602488514,39.5559324039201,135.86259933350422,7830,81,1905,398,travis,amueller,agramfort,false,agramfort,351,0.8518518518518519,1265,40,1839,true,true,true,false,142,1233,106,504,106,9,692
10268628,scikit-learn/scikit-learn,python,5714,1446661364,1446755316,1446755316,1565,1565,commits_in_master,false,false,false,11,1,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.106325101008758,0.08272635085786334,3,t3kcit@gmail.com,doc/model_selection.rst,3,0.0026833631484794273,0,0,false,[MRG] DOC Add the learning curve documentation to model_selectionrst Fixes #5693 ,,3271,0.7419749312136961,0.3300536672629696,50713,529.1542602488514,39.5559324039201,135.86259933350422,7824,81,1905,333,travis,rvraghav93,glouppe,false,glouppe,24,0.625,28,43,761,true,false,false,false,5,184,18,81,48,0,49
10266753,scikit-learn/scikit-learn,python,5712,1446655134,,1446753062,1632,,unknown,false,true,false,23,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,9.483216010597062,0.1910496211712114,12,t3kcit@gmail.com,sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py,12,0.010771992818671455,0,0,false,[MRG] MLP: train_test_split is now in model_selection This fixes a spurious warning of the multi-layer perceptron which was still importing train_test_split from cross_validation ,,3269,0.7424288773325176,0.33123877917414724,50713,529.1542602488514,39.5559324039201,135.86259933350422,7821,81,1905,333,travis,glouppe,glouppe,true,,62,0.9516129032258065,165,26,1819,true,true,false,false,35,241,60,134,74,0,1632
10263297,scikit-learn/scikit-learn,python,5711,1446641805,1454514399,1454514399,131209,131209,merged_in_comments,false,true,false,48,9,1,4,32,0,36,0,8,0,0,1,7,1,0,0,0,0,7,7,7,0,0,0,21,0,241,4.579804480847728,0.09226510396119164,23,g.louppe@gmail.com,sklearn/gaussian_process/tests/test_gpc.py,23,0.020535714285714286,0,13,false,[WIP]: TEST runtime down to 4:30 min on an old laptop See #5639Runtime comparison is on a laptop configured withMac OsX4 GB 1600 MHz DDR3 18 GHz Intel Core i5Tests runs in master inreal5m28285suser5m59565ssys  0m8353s,,3268,0.7423500611995104,0.33035714285714285,50713,529.1542602488514,39.5559324039201,135.86259933350422,7817,82,1905,434,travis,giorgiop,ogrisel,false,ogrisel,12,0.9166666666666666,2,7,1076,true,true,true,false,7,219,15,73,66,4,14
10255463,scikit-learn/scikit-learn,python,5708,1446604399,1446605181,1446605181,13,13,commits_in_master,false,false,false,43,2,2,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.468556565497593,0.19075429139093436,106,yanlend@users.noreply.github.com,doc/whats_new.rst|doc/whats_new.rst,106,0.0944741532976827,0,1,false,[MRG] Add missing whatsnew entries for 017 This adds some whatsnew entries for 017I went through the diff of 017 with 016I added entries on all things that were previously deprecated and now removed as I felt that might be helpful,,3267,0.7422711968166513,0.32976827094474154,50712,529.1646947468055,39.55671241520744,135.86527843508438,7813,83,1904,327,travis,amueller,amueller,true,amueller,350,0.8514285714285714,1264,40,1838,true,true,false,false,143,1231,103,498,99,9,3
10249502,scikit-learn/scikit-learn,python,5706,1446587526,1449809624,1449809624,53701,53701,commits_in_master,false,false,false,29,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,6,0,6,4.585185674478898,0.0923664380148579,13,rvraghav93@gmail.com,sklearn/metrics/tests/test_ranking.py,13,0.01161751563896336,0,0,false,Added warning tests for test_roc_curve method Author forgot to add assertion for couple of cases which raised warnings Included assert_warns with expected warning value and it fixed the issue,,3265,0.7424196018376723,0.33065236818588023,50717,528.4618569710353,39.49366090265592,135.7138632016878,7813,83,1904,404,travis,shawpan,amueller,false,amueller,1,1.0,0,0,1144,false,false,false,false,1,5,1,0,0,0,1400
10246226,scikit-learn/scikit-learn,python,5704,1446579092,,1446639111,1000,,unknown,false,false,false,10,13,12,0,5,0,5,0,2,0,0,8,8,5,0,0,0,0,8,8,5,0,0,20,10,20,10,70.50919683051933,1.4203750558615578,95,vighneshbirodkar@nyu.edu,doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|sklearn/manifold/tests/test_t_sne.py|sklearn/linear_model/ridge.py|doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|sklearn/utils/validation.py|doc/modules/multiclass.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/modules/gaussian_process.rst,27,0.016100178890876567,0,0,false,Fix #5694 Fixed issue #5694 : added reference for [RW2006],,3263,0.7428746552252529,0.32737030411449014,50717,528.4618569710353,39.49366090265592,135.7138632016878,7813,83,1904,332,travis,hrjn,hrjn,true,,0,0,1,0,1123,false,false,false,false,1,1,0,0,0,0,101
10245617,scikit-learn/scikit-learn,python,5703,1446577574,,1453081747,108402,,unknown,false,false,false,9,2,1,11,9,0,20,0,4,0,0,1,3,1,0,0,0,0,3,3,3,0,0,0,5,0,66,4.576949972327968,0.09220053361616218,1,t3kcit@gmail.com,sklearn/model_selection/tests/test_validation.py,1,0.0008944543828264759,1,2,false,Reduce warnings in the model_selection tests Fix #5669@amueller ,,3262,0.7431023911710607,0.32737030411449014,50717,528.4618569710353,39.49366090265592,135.7138632016878,7813,83,1904,420,travis,rvraghav93,MechCoder,false,,23,0.6521739130434783,28,43,760,true,false,true,true,5,178,17,81,46,0,124
10244086,scikit-learn/scikit-learn,python,5701,1446573335,1454531652,1454531652,132638,132638,commits_in_master,false,false,false,20,2,1,2,6,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,10,0,4.287079745189939,0.08636123238211992,58,ragvrv@gmail.com,sklearn/gaussian_process/kernels.py,58,0.05132743362831858,0,4,false,Fixed string comparison on arrays in new Gaussian process This PR fixes the issue described in described in issue #5663 ,,3261,0.7430236123888377,0.3238938053097345,50717,528.4618569710353,39.49366090265592,135.7138632016878,7813,83,1904,438,travis,ziky90,ogrisel,false,ogrisel,0,0,6,10,1249,false,true,false,false,0,0,0,0,0,0,31
10243706,scikit-learn/scikit-learn,python,5700,1446572334,1446662972,1446662972,1510,1510,commits_in_master,false,false,false,25,1,1,0,4,0,4,0,3,0,0,3,3,0,0,0,0,0,3,3,0,0,0,0,0,0,0,12.517185866962919,0.25215290166688464,35,t3kcit@gmail.com,doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst,31,0.006194690265486726,0,1,false,DOC - neural network module only links to unsupervised not supervised docs PR for #5681 neural network module only links to unsupervised not supervised docs,,3260,0.7429447852760737,0.3238938053097345,50717,528.4618569710353,39.49366090265592,135.7138632016878,7812,83,1904,335,travis,bhargav,glouppe,false,glouppe,1,1.0,8,22,1938,true,true,true,false,0,2,1,0,1,0,2
10242252,scikit-learn/scikit-learn,python,5699,1446567111,1457468415,1457468415,181688,181688,commits_in_master,false,false,false,22,2,1,0,10,0,10,0,4,0,0,2,2,1,0,0,0,0,2,2,1,0,0,16,0,34,0,9.428656523797123,0.1899359110397789,13,t3kcit@gmail.com,doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py,10,0.0088339222614841,0,3,false,Fix MLP batch_size test warning for default value Initializing batch_size to auto and setting it to n_samples valuefor the default case,,3259,0.7428659097882786,0.32332155477031804,50717,528.4618569710353,39.49366090265592,135.7138632016878,7811,82,1904,473,travis,bhargav,amueller,false,amueller,0,0,8,22,1938,false,true,false,false,0,1,0,0,0,0,2
10237963,scikit-learn/scikit-learn,python,5698,1446548117,1446622278,1446622278,1236,1236,commits_in_master,false,false,false,57,3,3,1,6,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,12.307320058461993,0.24792525232648108,11,peter.fischer@fau.de,sklearn/decomposition/dict_learning.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/dict_learning.py,11,0.009708737864077669,0,0,false,Fix 5687: Floating point indexing in MiniBatchDictionaryLearning The regularization parameter is not an integer so it must be rounded to be passed as n_nonzero_coefs which is done with the other algorithms which need integers but not orthogonal_mp_gram I made the parameters verbose to make it easier to read without referring to the definition as wellCloses #5687,,3258,0.7427869858809085,0.323036187113857,50717,528.4618569710353,39.49366090265592,135.7138632016878,7808,82,1904,333,travis,gclenaghan,amueller,false,amueller,2,1.0,2,3,398,true,true,false,false,0,10,2,0,7,0,621
10231961,scikit-learn/scikit-learn,python,5695,1446521066,1446596697,1446596697,1260,1260,commits_in_master,false,false,false,17,2,1,0,8,0,8,0,4,0,0,24,25,11,0,0,0,0,25,25,12,0,0,151,0,302,8,106.13582645712428,2.1380569799328426,294,yanlend@users.noreply.github.com,doc/datasets/index.rst|doc/datasets/rcv1.rst|doc/modules/decomposition.rst|doc/modules/feature_selection.rst|doc/modules/gaussian_process.rst|doc/modules/multiclass.rst|doc/modules/neural_networks_supervised.rst|doc/modules/outlier_detection.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/gaussian_process/plot_gpr_co2.py|sklearn/cross_decomposition/pls_.py|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/descr/diabetes.rst|sklearn/datasets/descr/digits.rst|sklearn/datasets/descr/iris.rst|sklearn/datasets/kddcup99.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/linear_model/base.py|sklearn/model_selection/_validation.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/preprocessing/data.py,104,0.007042253521126761,0,1,false,[MRG] DOC some fixes to the doc build Minor fixes for sphinx more to come tomorrow maybe,,3256,0.7429361179361179,0.3248239436619718,50717,528.4618569710353,39.49366090265592,135.7138632016878,7802,82,1903,333,travis,amueller,amueller,true,amueller,349,0.8510028653295129,1264,40,1837,true,true,false,false,145,1241,102,502,89,9,560
10228982,scikit-learn/scikit-learn,python,5692,1446512534,1446752014,1446752014,3991,3991,commits_in_master,false,false,false,22,3,2,4,6,0,10,0,3,1,0,7,8,7,0,1,1,0,7,8,7,0,1,11,30,21,30,37.232073059777576,0.7500235231775015,61,t3kcit@gmail.com,sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/testing.py|sklearn/utils/tests/test_extmath.py|Makefile|setup32.cfg,30,0.013204225352112676,2,2,false,[MRG] Skip 32 bit tests that fail skip doctests on 32bit Fixes #5534 #5177For some definition of fixesping @ogrisel @GaelVaroquaux ,,3255,0.7428571428571429,0.3248239436619718,50713,528.4838207165815,39.49677597460217,135.72456766509575,7801,82,1903,340,travis,amueller,amueller,true,amueller,348,0.8505747126436781,1264,40,1837,true,true,false,false,143,1234,101,500,87,9,7
10228494,scikit-learn/scikit-learn,python,5691,1446511168,1446556853,1446556853,761,761,commits_in_master,false,false,false,19,75,70,0,1,2,3,2,2,0,0,100,101,78,0,0,0,0,101,101,79,0,0,452,222,456,222,514.0532756462487,10.355374203368038,307,yanlend@users.noreply.github.com,README.rst|examples/decomposition/plot_incremental_pca.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_sparse_coding.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/modules/svm.rst|doc/modules/neural_networks_supervised.rst|doc/modules/sgd.rst|continuous_integration/appveyor/requirements.txt|sklearn/manifold/t_sne.py|sklearn/ensemble/tests/test_forest.py|doc/modules/outlier_detection.rst|doc/developers/contributing.rst|doc/model_selection.rst|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/ensemble.rst|doc/modules/grid_search.rst|doc/modules/learning_curve.rst|doc/modules/model_evaluation.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py|doc/tutorial/text_analytics/working_with_text_data.rst|examples/applications/face_recognition.py|examples/calibration/plot_calibration.py|examples/calibration/plot_calibration_curve.py|examples/classification/plot_classifier_comparison.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/covariance/plot_covariance_estimation.py|examples/decomposition/plot_pca_vs_fa_model_selection.py|examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_partial_dependence.py|examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_digits.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/feature_stacker.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/linear_model/plot_sgd_comparison.py|examples/missing_values.py|examples/mixture/plot_gmm_classifier.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_learning_curve.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_underfitting_overfitting.py|examples/model_selection/plot_validation_curve.py|examples/model_selection/randomized_search.py|examples/neighbors/plot_digits_kde_sampling.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/plot_cv_predict.py|examples/plot_digits_pipe.py|examples/plot_kernel_ridge_regression.py|examples/preprocessing/plot_function_transformer.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_anova.py|examples/svm/plot_svm_scale_c.py|sklearn/linear_model/tests/test_logistic.py|sklearn/metrics/tests/test_pairwise.py|sklearn/externals/joblib/parallel.py|sklearn/neural_network/multilayer_perceptron.py|doc/whats_new.rst|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|sklearn/utils/testing.py|sklearn/linear_model/tests/test_logistic.py|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/gaussian_process/gaussian_process.py|sklearn/tests/test_pipeline.py|doc/modules/tree.rst|doc/developers/contributing.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/sag.py|doc/tutorial/machine_learning_map/pyparsing.py|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/arrayfuncs.pyx|examples/neural_networks/plot_mlp_alpha.py|examples/tree/unveil_tree_structure.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/model_selection/_split.py|sklearn/cross_validation.py|sklearn/ensemble/forest.py|sklearn/manifold/tests/test_t_sne.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_theil_sen.py|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx|doc/modules/multiclass.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/multiclass.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/tests/test_mlp.py,102,0.003518029903254178,0,0,false,neural network tests print fix Removed verbose from the tests in test_mlp which doesnt test for verbosity Issue #5662,,3254,0.7427781192378611,0.3245382585751979,50713,528.4838207165815,39.49677597460217,135.72456766509575,7801,82,1903,327,travis,IshankGulati,IshankGulati,true,IshankGulati,1,0.0,2,12,333,true,false,false,false,0,2,2,0,4,0,8
10228316,scikit-learn/scikit-learn,python,5690,1446510808,,1446510823,0,,unknown,false,false,false,19,8,8,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.659388143280433,0.09386129817332133,4,t3kcit@gmail.com,sklearn/neural_network/tests/test_mlp.py,4,0.003518029903254178,0,0,false,neural network tests print fix Removed verbose from the tests in test_mlp which doesnt test for verbosity Issue #5662 ,,3253,0.7430064555794651,0.3245382585751979,50713,528.4838207165815,39.49677597460217,135.72456766509575,7801,82,1903,323,travis,IshankGulati,IshankGulati,true,,0,0,2,12,333,true,false,false,false,0,2,0,0,4,0,-1
10228070,scikit-learn/scikit-learn,python,5689,1446510082,,1447716504,20107,,unknown,false,false,false,31,12,2,11,7,0,18,0,5,0,0,2,5,2,0,0,1,0,5,6,4,0,0,163,0,695,55,13.444029575862004,0.2708239858683493,18,ragvrv@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/__init__.py|sklearn/ensemble/gradient_boosting.py,16,0.014072119613016711,0,13,false,[WIP] Gradient Boosting Classifier CV A reincarnation of #1036 with early stopping I will add the documentation and an example soonThe early stopping API is inspired by that of [xgboost](http://xgboostreadthedocsorg/en/latest/python/python_apihtml),,3252,0.7432349323493235,0.3245382585751979,50713,528.4838207165815,39.49677597460217,135.72456766509575,7801,82,1903,362,travis,vighneshbirodkar,vighneshbirodkar,true,,6,0.6666666666666666,14,1,967,true,false,false,false,2,28,6,48,51,0,18
10227604,scikit-learn/scikit-learn,python,5688,1446508861,1446591987,1446591987,1385,1385,commits_in_master,false,false,false,11,2,1,4,12,0,16,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,23,24,46,8.641754918483574,0.17408430253107893,27,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,22,0.019349164467897976,0,3,false,[MRG] fix 1 sparse row scaling in robust scaler Fixes #5502,,3251,0.7431559520147647,0.3245382585751979,50713,528.4838207165815,39.49677597460217,135.72456766509575,7801,82,1903,333,travis,amueller,amueller,true,amueller,347,0.8501440922190202,1264,40,1837,true,true,false,false,143,1229,100,499,87,9,0
10226032,scikit-learn/scikit-learn,python,5684,1446505028,1446589750,1446589750,1412,1412,commits_in_master,false,false,false,8,1,1,0,5,0,5,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.485007244509334,0.09034845913824999,8,t3kcit@gmail.com,sklearn/feature_selection/rfe.py,8,0.007042253521126761,0,1,false,[MRG] Dont use floats to index numpy arrays ,,3250,0.7430769230769231,0.3248239436619718,50712,528.4942419940054,39.49755481937214,135.727244044802,7801,82,1903,334,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,346,0.8497109826589595,1264,40,1837,true,true,true,false,140,1224,98,499,85,9,830
10223764,scikit-learn/scikit-learn,python,5682,1446498610,1446589569,1446589569,1515,1515,commits_in_master,false,false,false,170,2,2,0,7,0,7,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,2,4,2,12.729921607035593,0.25643856061869225,27,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,22,0.019349164467897976,0,2,false,OneHotEncoder warn fix - fixes #5671 nosetests -sv sklearn/preprocessing/tests/test_datapy -m test_one_hot_encoder_sparse now passes without warning locally on py2710 with numpy 1101    enc  OneHotEncoder(n_values[3 2 2])    X  [[1 0 1] [0 1 1]]    X_trans  encfit_transform(X)    X_too_large  [[0 2 1] [0 1 1]]    enctransform(X_too_large)Used to give:    /home/trev/virtualenvs/ve/lib/python27/site-packages/sklearn/preprocessing/datapy:1819: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0 dimension is 2 but corresponding boolean dimension is 6      during transform % X[~mask])    ValueError: unknown categorical feature present [[0 1 1]] during transformNow:    ValueError: unknown categorical feature present [2] during transformAnd    enc  OneHotEncoder(n_values[3 2 2])    X  [[1 0 1] [0 1 1]]    X_trans  encfit_transform(X)    X_too_large  [[0 2 2] [0 1 1]]    enctransform(X_too_large)Used to give:    /home/trev/virtualenvs/ve/lib/python27/site-packages/sklearn/preprocessing/datapy:1819: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0 dimension is 2 but corresponding boolean dimension is 6      during transform % X[~mask])          IndexError: index 2 is out of bounds for axis 0 with size 2Now:    ValueError: unknown categorical feature present [2 2] during transform,,3249,0.7429978454909203,0.3245382585751979,50712,528.4942419940054,39.49755481937214,135.727244044802,7800,82,1903,335,travis,trevorstephens,GaelVaroquaux,false,GaelVaroquaux,20,0.8,140,51,812,true,true,true,false,2,24,3,0,4,0,1
10223104,scikit-learn/scikit-learn,python,5680,1446496972,1446598416,1446598416,1690,1690,commits_in_master,false,false,false,11,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,17,3,17,9.006623814413043,0.18143444629538508,17,trev.stephens@gmail.com,sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py,15,0.013215859030837005,0,0,false,[MRG] FIX port LDA covariance fix to decomposition module Fixes #5092,,3248,0.7429187192118226,0.3251101321585903,50712,528.4942419940054,39.49755481937214,135.727244044802,7800,82,1903,333,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,345,0.8492753623188406,1264,40,1837,true,true,true,false,138,1195,95,496,80,9,1686
10218564,scikit-learn/scikit-learn,python,5678,1446484095,1447264958,1447264958,13014,13014,commits_in_master,false,false,false,89,8,1,21,18,0,39,0,6,0,0,1,3,1,0,0,0,0,3,3,3,0,0,39,0,171,37,4.188670997935609,0.0843789214342258,4,goix.nicolas@gmail.com,sklearn/ensemble/iforest.py,4,0.003511852502194908,1,8,false,IsolationForest max_samples warning and calculation In response to #5672Introduces max_samplesauto as the new default to make it possible to check if the user set a non default value Only warn about max_samplesn_samples if the user set the value explicitlyFixes a bug where max_depth was not recalculated when max_samples has changedNow 256 appears as a magic value in both fit and predict which isnt nice Ill think about how to make it nicer suggestions welcometodo:* [ ] add test that max_depth recalculation works/cc @ngoix,,3247,0.7428395441946412,0.3257243195785777,50712,528.4942419940054,39.49755481937214,135.727244044802,7799,82,1903,351,travis,betatim,glouppe,false,glouppe,7,0.5714285714285714,46,45,1353,true,false,true,true,1,9,1,0,1,0,34
10216611,scikit-learn/scikit-learn,python,5675,1446476000,1455202187,1455202187,145436,145436,commits_in_master,false,false,false,10,4,1,2,6,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,3,0,0,50,0,126,0,9.157428622306863,0.18447231971769512,8,t3kcit@gmail.com,doc/modules/mixture.rst|examples/mixture/plot_gmm_clustering.py|sklearn/mixture/gmm.py,8,0.001759014951627089,0,1,false,Make clear that GMM is a clustering algorithm See #5674 ,,3246,0.7427603203943315,0.32629727352682497,50712,528.4745227953937,39.49755481937214,135.727244044802,7798,82,1903,432,travis,AlexanderFabisch,glouppe,false,glouppe,11,0.8181818181818182,36,27,1593,false,true,false,false,0,1,0,0,0,0,186
10211873,scikit-learn/scikit-learn,python,5673,1446450494,1446506399,1446506399,931,931,commits_in_master,false,false,false,11,2,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,9,0,4.346476618467658,0.0875575920789231,23,tom.duprelatour@orange.fr,sklearn/linear_model/ridge.py,23,0.020122484689413824,0,0,false,FIX we shouldnt warn the user if the solver is auto ,,3245,0.7426810477657936,0.32458442694663164,50704,528.6367939413063,39.50378668349637,135.7683811928053,7794,82,1903,330,travis,amueller,amueller,true,amueller,344,0.8488372093023255,1264,40,1837,true,true,false,false,138,1190,94,496,79,9,285
10211745,scikit-learn/scikit-learn,python,5668,1446449604,1446594735,1446594735,2418,2418,commits_in_master,false,false,false,21,2,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,27,0,46,0,4.327880723141706,0.08718298709189198,8,peter.fischer@fau.de,sklearn/metrics/regression.py,8,0.00699912510936133,0,1,false,MRG dont compare things that can be arrays to strings Fixes the remainder of #5413This will break on future numpy,,3244,0.7426017262638718,0.32458442694663164,50704,528.6367939413063,39.50378668349637,135.7683811928053,7794,82,1903,340,travis,amueller,amueller,true,amueller,343,0.8483965014577259,1264,40,1837,true,true,false,false,136,1190,93,496,79,9,2337
10211675,scikit-learn/scikit-learn,python,5667,1446449152,1446464999,1446464999,264,264,commits_in_master,false,false,false,8,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,16,0,16,4.48932364306247,0.09043517376336566,3,ragvrv@gmail.com,sklearn/linear_model/tests/test_theil_sen.py,3,0.0026246719160104987,0,0,false,[MRG] close /dev/null in the theil sen tests ,,3243,0.7425223558433549,0.32458442694663164,50704,528.6367939413063,39.50378668349637,135.7683811928053,7794,82,1903,325,travis,amueller,glouppe,false,glouppe,342,0.847953216374269,1264,40,1837,true,true,true,true,136,1190,92,496,79,9,197
10211633,scikit-learn/scikit-learn,python,5666,1446448903,1446483562,1446483562,577,577,commits_in_master,false,false,false,16,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,3,4.292047595480421,0.0864611467025182,8,tom.duprelatour@orange.fr,sklearn/linear_model/tests/test_ridge.py,8,0.0070052539404553416,0,1,false,[MRG] catch warnings in tests that the solver is changed to SAG in the sparse case ,,3242,0.742442936458976,0.3248686514886165,50704,528.6367939413063,39.50378668349637,135.7683811928053,7794,82,1903,325,travis,amueller,agramfort,false,agramfort,341,0.8475073313782991,1264,40,1837,true,true,true,false,136,1190,91,496,78,9,310
10211571,scikit-learn/scikit-learn,python,5665,1446448456,1446465729,1446465729,287,287,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,11,0,11,4.493984818421579,0.0905290708037793,5,peter.fischer@fau.de,sklearn/linear_model/tests/test_base.py,5,0.0043821209465381246,0,0,false,[MRG] dont use the deprecated residuals property of ols ,,3241,0.7423634680654119,0.32515337423312884,50704,528.6367939413063,39.50378668349637,135.7683811928053,7794,82,1903,324,travis,amueller,arjoly,false,arjoly,340,0.8470588235294118,1264,40,1837,true,true,false,true,136,1190,90,496,77,9,207
10210106,scikit-learn/scikit-learn,python,5661,1446439515,1446498608,1446498608,984,984,commits_in_master,false,false,false,11,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.458115877200925,0.08980650896820958,6,t3kcit@gmail.com,sklearn/manifold/tests/test_t_sne.py,6,0.0052585451358457495,0,1,false,[MRG] TST make tsne tests 32bit save Fixes some of #5164,,3240,0.7422839506172839,0.32515337423312884,50704,528.6367939413063,39.50378668349637,135.7683811928053,7791,82,1902,326,travis,amueller,amueller,true,amueller,339,0.8466076696165191,1264,40,1836,true,true,false,false,134,1174,88,491,76,9,1
10209604,scikit-learn/scikit-learn,python,5659,1446436275,1446436353,1446436353,1,1,commits_in_master,false,false,false,19,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.300323565514845,0.08662793816491654,20,t3kcit@gmail.com,sklearn/ensemble/forest.py,20,0.01757469244288225,0,0,false,DOC: Add defaults for oob_score in docstrings Add the default (False) to a few docstrings in the forestpy module,,3239,0.7422043840691571,0.3260105448154657,50703,528.7852789775753,39.504565804784725,135.77105891170146,7790,82,1902,322,travis,ElDeveloper,amueller,false,amueller,1,1.0,33,4,1894,false,false,false,false,0,0,0,0,0,0,1
10209499,scikit-learn/scikit-learn,python,5658,1446435653,1446460123,1446460123,407,407,commits_in_master,false,false,false,10,2,2,0,3,0,3,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,63,0,63,0,17.52175957985175,0.3529673714778269,128,yanlend@users.noreply.github.com,doc/whats_new.rst|sklearn/cross_validation.py|sklearn/model_selection/_split.py|sklearn/cross_validation.py,102,0.029876977152899824,0,1,false,[MRG] Train test split conserves type doc Continuation of #5476,,3238,0.7421247683755404,0.3260105448154657,50703,528.7852789775753,39.504565804784725,135.77105891170146,7790,82,1902,327,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,338,0.8461538461538461,1264,40,1836,true,true,true,false,133,1163,85,489,73,9,66
10208211,scikit-learn/scikit-learn,python,5657,1446429991,1446483500,1446483500,891,891,commits_in_master,false,false,false,24,2,2,0,6,0,6,0,6,0,0,4,4,4,0,0,0,0,4,4,4,0,0,34,0,34,0,17.3881447366035,0.3502756340559395,2,gael.varoquaux@normalesup.org,sklearn/datasets/_svmlight_format.pyx|sklearn/utils/graph_shortest_path.pyx|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx,2,0.001762114537444934,0,4,false,Remove cython warnings  Non-trivial type declarators in shared declaration (eg mix of pointers and values) Each pointer declaration should be on its own line,,3237,0.7420451034908866,0.32687224669603526,50703,528.7852789775753,39.504565804784725,135.77105891170146,7789,82,1902,328,travis,sieben,agramfort,false,agramfort,14,0.7857142857142857,34,136,1971,true,true,false,false,0,10,16,3,3,0,596
10207734,scikit-learn/scikit-learn,python,5655,1446427420,1446430702,1446430702,54,54,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,8.62057773236948,0.17365729969163482,7,t3kcit@gmail.com,examples/neural_networks/plot_mlp_alpha.py|examples/tree/unveil_tree_structure.py,4,0.0035211267605633804,0,0,false,Migrate examples to avoid deprecated module ,,3236,0.7419653893695921,0.3265845070422535,50703,528.7852789775753,39.504565804784725,135.77105891170146,7789,83,1902,324,travis,sieben,agramfort,false,agramfort,13,0.7692307692307693,34,136,1971,true,true,false,false,0,10,15,3,2,0,5
10207454,scikit-learn/scikit-learn,python,5654,1446426021,1446465891,1446465891,664,664,commits_in_master,false,false,false,3,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,0,6,0,8.154039424442036,0.16425911487472053,1,gael.varoquaux@normalesup.org,sklearn/linear_model/sgd_fast.pyx|sklearn/utils/arrayfuncs.pyx,1,0.0008802816901408451,0,0,false,Simplify chain comparaison ,,3235,0.7418856259659969,0.3265845070422535,50703,528.7852789775753,39.504565804784725,135.77105891170146,7789,83,1902,327,travis,sieben,arjoly,false,arjoly,12,0.75,34,136,1971,true,true,false,false,0,10,14,3,2,0,663
10206058,scikit-learn/scikit-learn,python,5652,1446418634,1446427835,1446427835,153,153,commits_in_master,false,false,false,3,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.138445372971976,0.08336694717027018,0,,doc/tutorial/machine_learning_map/pyparsing.py,0,0.0,0,0,false,Useless trailing semicolon ,,3233,0.7420352613671513,0.32629727352682497,50703,528.7852789775753,39.504565804784725,135.77105891170146,7788,83,1902,323,travis,sieben,GaelVaroquaux,false,GaelVaroquaux,11,0.7272727272727273,34,136,1971,true,true,true,false,0,10,13,3,2,0,36
10193607,scikit-learn/scikit-learn,python,5650,1446327170,1446431956,1446431956,1746,1746,commits_in_master,false,false,false,23,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.579793303375323,0.09225769883188321,2,gael.varoquaux@normalesup.org,sklearn/linear_model/ransac.py,2,0.0017497812773403325,0,0,false,Fixed user guide link in ransacpy #5621 Updated one line and it works No chance of side effects So a very simple PR,,3232,0.7419554455445545,0.3228346456692913,50703,528.7852789775753,39.504565804784725,135.77105891170146,7774,84,1901,325,travis,shawpan,amueller,false,amueller,0,0,0,0,1141,false,false,false,false,1,2,0,0,0,0,1746
10191094,scikit-learn/scikit-learn,python,5649,1446312860,,1446360709,797,,unknown,false,false,false,54,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.134798634676362,0.08329349861429582,13,ragvrv@gmail.com,sklearn/base.py,13,0.011343804537521814,0,1,false,ENH: add check_estimator test to BaseEstimator This is an idea I had to more easily expose the check_estimator() utility: this PR gives every sklearn estimator a check_estimator class method which will run the (very useful) tests in sklearnutilsestimator_checksMainly opened this PR to start a discussion of whether something like this would be useful,,3231,0.7421850820179511,0.3219895287958115,50703,528.7852789775753,39.504565804784725,135.77105891170146,7774,84,1901,322,travis,jakevdp,jakevdp,true,,50,0.88,1852,0,1634,true,true,false,false,2,46,6,19,2,0,22
10189932,scikit-learn/scikit-learn,python,5648,1446303954,1446338498,1446338498,575,575,commits_in_master,false,false,false,32,2,1,5,5,1,11,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,12,4,16,4,17.1848779237623,0.3461809931853819,13,ragvrv@gmail.com,sklearn/externals/joblib/format_stack.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/odict.py|sklearn/linear_model/sag.py,7,0.0034812880765883376,0,1,false,Default mutable argument Default argument values are evaluated only once at function definitiontime which means that modifying the default value of the argument willaffect all subsequent calls of the function,,3230,0.7421052631578947,0.32201914708442125,50703,528.7852789775753,39.504565804784725,135.77105891170146,7772,84,1901,323,travis,sieben,agramfort,false,agramfort,10,0.7,34,136,1970,true,true,false,false,0,8,12,1,2,0,4
10189862,scikit-learn/scikit-learn,python,5647,1446303440,1446409177,1446409177,1762,1762,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.631245862955702,0.09329419153491987,8,tdhopper@gmail.com,doc/developers/contributing.rst,8,0.006962576153176675,0,0,false,Redirecting to easy and open issues ,,3229,0.7420253948590895,0.32201914708442125,50703,528.7852789775753,39.504565804784725,135.77105891170146,7772,84,1901,321,travis,sieben,agramfort,false,agramfort,9,0.6666666666666666,34,136,1970,true,true,false,false,0,8,11,1,2,0,1751
10189640,scikit-learn/scikit-learn,python,5646,1446301518,1446302103,1446302103,9,9,commits_in_master,false,false,false,5,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.256486815942785,0.085744854715669,9,peter.fischer@fau.de,doc/modules/tree.rst,9,0.0078397212543554,0,0,false,Fix formatting issue in treerst ,,3228,0.7419454770755886,0.32229965156794427,50703,528.7852789775753,39.504565804784725,135.77105891170146,7772,84,1901,320,travis,mscherer,glouppe,false,glouppe,0,0,10,0,2046,false,false,false,false,0,0,0,0,0,0,9
10189054,scikit-learn/scikit-learn,python,5645,1446296593,1446296758,1446296758,2,2,commits_in_master,false,false,false,3,1,1,0,2,0,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,1,1,1,8.48121224825103,0.17084996385941,15,peter.fischer@fau.de,sklearn/gaussian_process/gaussian_process.py|sklearn/tests/test_pipeline.py,12,0.010462074978204011,0,0,false,remove useless pass ,,3227,0.7418655097613883,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7772,84,1901,323,travis,sieben,glouppe,false,glouppe,8,0.625,34,136,1970,true,true,false,false,0,2,8,0,2,0,2
10184591,scikit-learn/scikit-learn,python,5644,1446261721,1446437693,1446437693,2932,2932,commits_in_master,false,false,false,58,1,1,0,5,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,35,12,35,8.265476164109925,0.16650406363904008,16,peter.fischer@fau.de,sklearn/tree/export.py|sklearn/tree/tests/test_export.py,11,0.009523809523809525,0,1,true,fix rounding adjust tests for 32 bit export_graphviz Fingers crossed this fixes #5164 Had to change one of the tests to get a stable tree between 64/32 bit which was unrelated to the issue but encountered on a vagrant 32bit box npround should fix the issue seen on the referenced issue with computation of alpha at https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/tree/exportpy#L161  ,,3226,0.7417854928704277,0.3203463203463203,50704,528.7945724203219,39.50378668349637,135.7683811928053,7770,85,1900,326,travis,trevorstephens,amueller,false,amueller,18,0.8333333333333334,140,51,809,true,true,true,false,2,20,1,0,3,0,21
10183648,scikit-learn/scikit-learn,python,5643,1446257954,,1446301045,718,,unknown,false,false,false,2,3,3,1,5,0,6,0,3,0,0,21,21,20,0,0,0,0,21,21,20,0,0,148,62,148,62,170.42687205636713,3.433167816016764,183,trev.stephens@gmail.com,doc/modules/outlier_detection.rst|benchmarks/bench_plot_approximate_neighbors.py|doc/sphinxext/gen_rst.py|examples/applications/svm_gui.py|examples/bicluster/bicluster_newsgroups.py|examples/model_selection/plot_underfitting_overfitting.py|sklearn/cluster/mean_shift_.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/discriminant_analysis.py|sklearn/feature_extraction/text.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/learning_curve.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_ranking.py|sklearn/svm/tests/test_svm.py|sklearn/utils/random.py|sklearn/utils/tests/test_random.py|benchmarks/bench_plot_approximate_neighbors.py|doc/sphinxext/gen_rst.py|examples/applications/svm_gui.py|examples/bicluster/bicluster_newsgroups.py|examples/model_selection/plot_underfitting_overfitting.py|sklearn/cluster/mean_shift_.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/discriminant_analysis.py|sklearn/feature_extraction/text.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/learning_curve.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_ranking.py|sklearn/svm/tests/test_svm.py|sklearn/utils/random.py|sklearn/utils/tests/test_random.py,58,0.00779896013864818,0,0,true,Enumerate pattern ,,3225,0.742015503875969,0.32062391681109187,50704,528.7945724203219,39.50378668349637,135.7683811928053,7769,85,1900,322,travis,sieben,sieben,true,,7,0.7142857142857143,34,136,1969,true,true,false,false,0,1,7,0,2,0,658
10178700,scikit-learn/scikit-learn,python,5637,1446242626,1446513210,1446513210,4509,4509,commits_in_master,false,false,false,77,2,2,2,6,0,8,0,2,0,0,2,2,1,0,0,0,0,2,2,1,0,0,12,0,12,0,17.888874704651702,0.36034056650060203,28,t3kcit@gmail.com,doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|doc/modules/preprocessing.rst|sklearn/preprocessing/data.py,22,0.019130434782608695,0,3,true,DOC: Correct confusing docs of n_values in OneHotEncoder The current doc states that n_values is the maximum value of every categorical feature However it should be the number of values per categorical featurefrom sklearnpreprocessing import OneHotEncoderX  [[1 2 3] [0 2 0]]        onehotenc  OneHotEncoder(n_values[1 2 3])        onehotencfit(X)        ValueError: column index exceeds matrix dimensions        onehotenc  OneHotEncoder(n_values[2 3 4]) # worksAlso added docs on when it is better to explicitly set these values,,3222,0.7423960273122284,0.3217391304347826,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,85,1900,336,travis,MechCoder,amueller,false,amueller,79,0.8607594936708861,87,41,1228,true,true,false,false,17,227,26,172,42,1,94
10177829,scikit-learn/scikit-learn,python,5636,1446240385,1446305775,1446305775,1089,1089,commits_in_master,false,false,false,43,1,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,19,0,19,4.56806739512528,0.0920158489647391,18,tom.dupre-la-tour@m4x.org,sklearn/linear_model/tests/test_logistic.py,18,0.015665796344647518,0,1,true,[MRG] decrease tolerance in test_logistic for appveyor failure This should fix appveyor failures described in #5598I dont get where does this regression comes fromso I just decreased the tol with SAG solver from 1e-7 to 1e-10Other changes are mainly cosmetic,,3221,0.7423160509158646,0.32201914708442125,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,85,1900,328,travis,TomDLT,GaelVaroquaux,false,GaelVaroquaux,24,0.625,7,4,254,true,false,false,false,10,104,12,81,13,0,68
10176830,scikit-learn/scikit-learn,python,5635,1446237850,1446243400,1446243400,92,92,commits_in_master,false,false,false,45,1,1,0,2,0,2,0,1,0,0,4,4,3,0,0,0,0,4,4,3,0,0,17,22,17,22,18.979700840749018,0.38231338001316795,109,yanlend@users.noreply.github.com,doc/whats_new.rst|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|sklearn/utils/testing.py,102,0.013949433304272014,1,0,true,[MRG] Update joblib to 093 Bug fix release to fix joblib/joblib#263: interactively defined functions can not be used in parallel for Python 3 (because the default start method is forkserver)This needs to be merged for 017 fix for #5623 overrides PR #5624 by @lesteve,,3220,0.7422360248447205,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,324,travis,ogrisel,ogrisel,true,ogrisel,138,0.8623188405797102,1228,124,2347,true,true,false,false,53,409,71,255,91,2,92
10176715,scikit-learn/scikit-learn,python,5634,1446237577,1449626043,1449626043,56474,56474,commits_in_master,false,false,false,2,3,2,3,5,0,8,0,4,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.492901622731136,0.19121815122221925,10,peter.fischer@fau.de,doc/modules/outlier_detection.rst|sklearn/datasets/descr/breast_cancer.rst,6,0.0052310374891020054,0,3,true,Space indentation ,,3219,0.7421559490525008,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,407,travis,sieben,agramfort,false,agramfort,6,0.6666666666666666,34,136,1969,false,true,false,false,0,0,6,0,0,0,182
10176635,scikit-learn/scikit-learn,python,5633,1446237351,1446248576,1446248576,187,187,commits_in_master,false,false,false,5,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.192844925120082,0.08445763864476466,8,t3kcit@gmail.com,sklearn/neural_network/multilayer_perceptron.py,8,0.006974716652136007,0,0,true,List creation could be litteral ,,3218,0.7420758234928527,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,324,travis,sieben,amueller,false,amueller,5,0.6,34,136,1969,false,true,true,false,0,0,5,0,0,0,187
10176592,scikit-learn/scikit-learn,python,5632,1446237308,1446248624,1446248624,188,188,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.323745758859131,0.0870944104576202,7,t3kcit@gmail.com,sklearn/externals/joblib/parallel.py,7,0.006102877070619006,0,0,true,Chained compararisons can be simplified ,,3217,0.7419956481193659,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,323,travis,sieben,amueller,false,amueller,4,0.5,34,136,1969,false,true,true,false,0,0,4,0,0,0,188
10176571,scikit-learn/scikit-learn,python,5631,1446237264,,1446297173,998,,unknown,false,false,false,2,1,1,0,4,0,4,0,4,0,0,47,47,47,0,0,0,0,47,47,47,0,0,120,36,120,36,203.50718754524527,4.099301742435997,258,vighneshbirodkar@nyu.edu,sklearn/__check_build/__init__.py|sklearn/base.py|sklearn/covariance/robust_covariance.py|sklearn/datasets/species_distributions.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/discriminant_analysis.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/funcsigs.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/logger.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/odict.py|sklearn/feature_extraction/image.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/kernels.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_base.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|sklearn/mixture/tests/test_gmm.py|sklearn/model_selection/tests/test_validation.py|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/dist_metrics.pyx|sklearn/neural_network/rbm.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/tests/test_label.py|sklearn/tests/test_naive_bayes.py|sklearn/utils/estimator_checks.py|sklearn/utils/linear_assignment_.py|sklearn/utils/multiclass.py|sklearn/utils/testing.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_optimize.py|sklearn/utils/tests/test_seq_dataset.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,58,0.006102877070619006,0,2,true,Redundant parentheses ,,3216,0.742226368159204,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,325,travis,sieben,agramfort,false,,3,0.6666666666666666,34,136,1969,false,true,false,false,0,0,3,0,0,0,190
10176542,scikit-learn/scikit-learn,python,5630,1446237199,,1446299706,1041,,unknown,false,true,false,8,1,1,0,5,0,5,0,4,0,0,13,13,13,0,0,0,0,13,13,13,0,0,136,10,136,10,57.978235286137064,1.1678716796132087,106,tom.dupre-la-tour@m4x.org,sklearn/cross_validation.py|sklearn/externals/funcsigs.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/odict.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_base.py|sklearn/metrics/cluster/tests/test_bicluster.py|sklearn/mixture/dpgmm.py|sklearn/model_selection/_split.py|sklearn/preprocessing/data.py|sklearn/tests/test_base.py|sklearn/utils/estimator_checks.py,54,0.004359197907585004,0,0,true,Triple double-quoted strings should be used for docstrings ,,3215,0.7424572317262831,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,324,travis,sieben,sieben,true,,2,1.0,34,136,1969,false,true,false,false,0,0,2,0,0,0,193
10176456,scikit-learn/scikit-learn,python,5629,1446236975,1446248751,1446248751,196,196,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,3,4.735969594710168,0.09539794955597322,10,peter.fischer@fau.de,sklearn/metrics/tests/test_pairwise.py,10,0.008718395815170008,0,0,true,Dictionary creation could be rewritten as litteral ,,3214,0.7423771001866832,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,322,travis,sieben,amueller,false,amueller,1,1.0,34,136,1969,false,true,true,false,0,0,1,0,0,0,-1
10176442,scikit-learn/scikit-learn,python,5628,1446236950,1449518657,1449518657,54695,54695,commit_sha_in_comments,false,true,false,7,2,1,4,5,0,9,0,4,0,0,15,15,15,0,0,0,0,15,15,15,0,0,38,16,68,30,61.537541121070376,1.239567764586919,83,tom.dupre-la-tour@m4x.org,sklearn/datasets/california_housing.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/externals/joblib/format_stack.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/linear_model/randomized_l1.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/optimize.py,31,0.0052310374891020054,0,3,true,Assignment can be replaced with augmented assignment ,,3213,0.742296918767507,0.3225806451612903,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,398,travis,sieben,sieben,true,sieben,0,0,34,136,1969,false,true,false,false,0,0,0,0,0,0,197
10175596,scikit-learn/scikit-learn,python,5626,1446234911,1446432449,1446432449,3292,3292,commits_in_master,false,false,false,48,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.401662119808008,0.08866390133435394,18,tom.dupre-la-tour@m4x.org,sklearn/linear_model/tests/test_logistic.py,18,0.015665796344647518,0,4,true,[MRG] FIX increase tolerance of class weight check for OS X This is a fix for #5625 Although I am not 100% to understand why we need to increase the tolerance here I think this test used to pass on MondayWill do a bisect and report back,,3212,0.7422166874221668,0.3228894691035683,50704,528.7945724203219,39.50378668349637,135.7683811928053,7765,84,1900,323,travis,ogrisel,amueller,false,amueller,137,0.8613138686131386,1228,124,2347,true,true,false,true,52,402,70,255,91,2,9
10173973,scikit-learn/scikit-learn,python,5624,1446230721,,1446237883,119,,unknown,false,true,false,41,1,1,0,5,0,5,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,0,17,0,9.61368494335696,0.19365112316129912,9,t3kcit@gmail.com,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py,9,0.0078125,1,1,true,[MRG] Update joblib to 093 Bug fix release to fix https://githubcom/joblib/joblib/issues/263: interactively defined functions can not be used in parallel for Python 3 (because the default start method is forkserver)This needs to be merged for 017 I believe: ping @ogrisel,,3211,0.7424478355652445,0.3229166666666667,50704,528.7945724203219,39.50378668349637,135.7683811928053,7762,84,1900,323,travis,lesteve,ogrisel,false,,21,0.9523809523809523,4,0,1283,true,false,false,false,6,50,5,17,15,0,17
10150609,scikit-learn/scikit-learn,python,5619,1446149852,,1446233382,1392,,unknown,false,false,false,20,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,96,0,96,4.458368455054406,0.0898063555714469,18,tom.dupre-la-tour@m4x.org,sklearn/linear_model/tests/test_logistic.py,18,0.015371477369769428,0,0,false,[WIP] try to debug appveyor Debugging on appveyor is so much funLets wait a moment for the result,,3210,0.7426791277258566,0.318531169940222,50704,528.7945724203219,39.50378668349637,135.7683811928053,7747,85,1899,315,travis,TomDLT,TomDLT,true,,23,0.6521739130434783,7,4,253,true,false,false,false,8,104,10,81,12,0,-1
10143914,scikit-learn/scikit-learn,python,5617,1446130213,1446166242,1446166242,600,600,commits_in_master,false,false,false,21,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.713214506039935,0.09493980187619074,4,goix.nicolas@gmail.com,doc/modules/outlier_detection.rst,4,0.003389830508474576,1,1,false,correct doc outlier_detectionrst This corrects the doc outlier_detection where the text did not correspond to the figures as noticed by @GaelVaroquaux,,3209,0.7425989404799003,0.31864406779661014,50704,528.7945724203219,39.50378668349637,135.7683811928053,7744,86,1899,314,travis,ngoix,agramfort,false,agramfort,3,0.6666666666666666,5,1,745,true,false,false,false,1,38,1,43,14,0,14
10134726,scikit-learn/scikit-learn,python,5613,1446085701,1446123376,1446123376,627,627,commits_in_master,false,false,false,22,1,1,0,2,1,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.00784000074196,0.08073123909015488,8,t3kcit@gmail.com,sklearn/manifold/t_sne.py,8,0.006773920406435224,0,1,false,MAINT: remove unused code from TSNE removed unused code It doesnt appear that this private method is used anywhere in the package,,3207,0.7427502338634238,0.31837425910245554,50704,528.7945724203219,39.50378668349637,135.7683811928053,7733,86,1898,313,travis,jakevdp,agramfort,false,agramfort,49,0.8775510204081632,1850,0,1631,true,true,false,true,3,44,5,19,2,0,18
10133431,scikit-learn/scikit-learn,python,5612,1446081981,,1448872932,46515,,unknown,false,false,false,127,9,8,19,10,0,29,0,4,0,0,12,12,12,0,0,0,0,12,12,12,0,0,2336,0,2370,0,166.49417895259185,3.35374699729839,27,peter.fischer@fau.de,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pxd|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pyx|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pxd|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pxd|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx,25,0.008445945945945946,2,2,false,[WIP] Caching results of split between nodes This PR attempts to speed up trees by sharing precomputed statistics between nodes in the tree In essence if youve already calculated the weight on the left and right hand of a split you can pass that information to the next level where the left weight now is the total weight of that node Same for the other moments of the summary statisticsThis is currently working for depth first building but not for breadth first It shouldnt be too difficult but I have been slow to work on this and wanted to update you that I was almost done (versus vaguely saying I was going to work on it) It should actually be done by this weekend@arjoly @glouppe ,,3206,0.7429819089207735,0.31756756756756754,50704,528.7945724203219,39.50378668349637,135.7683811928053,7730,87,1898,387,travis,jmschrei,jmschrei,true,,15,0.7333333333333333,35,5,952,true,true,false,false,8,183,18,106,5,0,26
10116760,scikit-learn/scikit-learn,python,5607,1446033504,1446033604,1446033604,1,1,commits_in_master,false,false,false,19,1,1,0,1,0,1,0,1,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.985916514228297,0.20115078992375166,9,t3kcit@gmail.com,doc/modules/neural_networks_supervised.rst|doc/modules/sgd.rst,5,0.004222972972972973,0,1,false,DOC: broken links Fixes #5605 I have built the docs locally and links point to the right addresses now,,3205,0.7429017160686427,0.31841216216216217,50696,528.878017989585,39.51002051443901,135.7898059018463,7724,87,1898,313,travis,giorgiop,glouppe,false,glouppe,11,0.9090909090909091,2,7,1069,true,true,false,false,6,197,14,73,66,4,1
10095630,scikit-learn/scikit-learn,python,5603,1445964252,1445990895,1445990895,444,444,merged_in_comments,false,false,false,15,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.819744590075469,0.09708643163230539,6,shah.anish07@gmail.com,doc/modules/svm.rst,6,0.005042016806722689,0,1,false,Fix of issue #5601 in SVC documentation Adds missing ys into dual formulation of SVC,,3204,0.7428214731585518,0.31848739495798317,50698,528.8374294843978,39.508461872263204,135.78444909069393,7703,87,1897,314,travis,olologin,agramfort,false,agramfort,3,0.3333333333333333,2,2,858,true,true,false,false,5,16,5,1,6,0,444
10081261,scikit-learn/scikit-learn,python,5597,1445902516,1445903895,1445903895,22,22,commits_in_master,false,false,false,9,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.375012088190885,0.08812799736299423,3,jh1736@gmail.com,examples/linear_model/plot_robust_fit.py,3,0.002523128679562658,0,0,false,Replaced fontsize legend parameter Related to #5567 and #5479 ,,3203,0.7427411801436153,0.32043734230445753,50698,528.8374294843978,39.508461872263204,135.78444909069393,7696,86,1896,314,travis,vortex-ape,agramfort,false,agramfort,15,0.9333333333333333,8,15,906,true,false,false,false,0,5,3,2,1,0,-1
10067924,scikit-learn/scikit-learn,python,5594,1445860200,1445869710,1445869710,158,158,commits_in_master,false,false,false,113,2,2,0,1,0,1,0,2,0,0,4,4,2,0,1,0,0,4,4,2,0,1,45,0,45,0,17.58177579425349,0.35415735748626487,2,goix.nicolas@gmail.com,doc/README|doc/modules/outlier_detection.rst|examples/ensemble/plot_isolation_forest.py|sklearn/ensemble/iforest.py,2,0.0016806722689075631,1,0,false,English language changed to IsolationTree docs IsolationTrees looks very nice I was reading the documentation and example to get a feel for them and think the following small changes to the language would improve the docsThere is one more point on [ensemble/iforestpy:176+](https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/iforestpy#L176) where I dont understand what the doc string is trying to tell me Looking at the code itself I get L206 but 208 confuses me Maybe together we can improve the doc string In particular the why of doing this@ngoix could you check that the language change does not change the meaningThe second commit changes the URL mentioned in doc/README is it Ok to sneak it in here,,3201,0.7428928459856294,0.3226890756302521,50649,532.152658492764,39.783608758317044,136.19222492053152,7689,86,1896,314,travis,betatim,agramfort,false,agramfort,6,0.5,46,45,1346,true,false,false,false,0,13,1,0,7,0,148
10063455,scikit-learn/scikit-learn,python,5592,1445832036,1445854759,1445854759,378,378,commits_in_master,false,false,false,2,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.2687786587245595,0.08598786593189855,8,ragvrv@gmail.com,sklearn/feature_selection/univariate_selection.py,8,0.006734006734006734,0,0,false,corrected typo  ,,3199,0.7430447014692091,0.32323232323232326,50649,532.152658492764,39.783608758317044,136.19222492053152,7686,86,1895,315,travis,antialias,glouppe,false,glouppe,1,1.0,12,46,1831,false,false,false,false,0,0,1,0,0,0,378
10063411,scikit-learn/scikit-learn,python,5591,1445831755,1445854731,1445854731,382,382,commits_in_master,false,false,false,1,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.464948756670593,0.08993940557135856,8,ragvrv@gmail.com,sklearn/feature_selection/univariate_selection.py,8,0.006734006734006734,0,0,false,s/labe/label  ,,3198,0.7429643527204502,0.32323232323232326,50649,532.152658492764,39.783608758317044,136.19222492053152,7686,86,1895,315,travis,antialias,glouppe,false,glouppe,0,0,12,46,1831,false,false,false,false,0,0,0,0,0,0,-1
10054808,scikit-learn/scikit-learn,python,5590,1445780644,1445787102,1445787102,107,107,commits_in_master,false,false,false,35,2,2,0,4,1,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,17,15,17,16.615523230853135,0.3346934235152145,1,t3kcit@gmail.com,sklearn/model_selection/_search.py|sklearn/model_selection/tests/test_search.py|sklearn/model_selection/_search.py|sklearn/model_selection/tests/test_search.py,1,0.0008403361344537816,2,0,false,FIX: pass random_state to parameter distributions This fix a bug related to ParameterSampler as discovered in #5491 It properly pass the random_state to the parameter distributions so sampling is now fully reproducibleCC: @amueller @fabianp ,,3197,0.7428839537066,0.3235294117647059,50647,531.9959721207574,39.78517977372796,136.15811400477818,7680,86,1895,313,travis,glouppe,GaelVaroquaux,false,GaelVaroquaux,61,0.9508196721311475,165,26,1809,true,true,false,false,33,235,55,129,94,0,33
10043551,scikit-learn/scikit-learn,python,5587,1445696359,1445715541,1445715541,319,319,commits_in_master,false,false,false,43,2,2,0,2,0,2,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.46682418506563,0.19142220005278232,99,yanlend@users.noreply.github.com,doc/whats_new.rst|doc/whats_new.rst,99,0.08284518828451883,0,2,false,DOC clearer whats new for model_selection Other notes:* the deprecation warnings on grid_search and cross_validation say they will be removed in 019 This seems short for such established modules* whats new also seems to be missing some sprint merges notably MLP,,3195,0.7430359937402191,0.3238493723849372,50264,535.0748050294445,40.04854368932039,137.0961324208181,7674,87,1894,323,travis,jnothman,glouppe,false,glouppe,123,0.7073170731707317,31,1,2370,true,true,false,false,30,313,30,121,18,1,319
10043460,scikit-learn/scikit-learn,python,5586,1445695618,1445703216,1445703216,126,126,commits_in_master,false,false,false,12,1,1,0,0,0,0,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.239705059947058,0.08572818658978326,20,tom.duprelatour@orange.fr,sklearn/linear_model/ridge.py,20,0.016736401673640166,0,0,false,DOC: only sag supports sparse input in Ridge now See also https://githubcom/scikit-learn/scikit-learn/pull/5360#issuecomment-150563200,,3194,0.7429555416405761,0.3238493723849372,50264,535.0748050294445,40.04854368932039,137.0961324208181,7674,87,1894,323,travis,giorgiop,mblondel,false,mblondel,10,0.9,2,7,1065,true,true,true,false,4,185,13,73,66,4,-1
10041785,scikit-learn/scikit-learn,python,5582,1445678696,1445715443,1445715443,612,612,commits_in_master,false,false,false,18,1,1,0,2,0,2,0,2,0,2,1,3,3,0,0,0,2,1,3,3,0,0,26,18,26,18,4.960443978154248,0.10030186111400424,25,t3kcit@gmail.com,sklearn/preprocessing/_weights.py|sklearn/preprocessing/tests/test_weights.py|sklearn/tree/tests/test_tree.py,25,0.0016750418760469012,0,1,false,Fix issue #1763 Remove _balance_weights As in #1763 the functionality of the internal preprocessing_weights_balance_weights is replaced with utilscompute_sample_weight,,3192,0.7431077694235589,0.3232830820770519,50263,535.085450530211,40.04934046913236,137.09885999641884,7668,87,1894,323,travis,gclenaghan,glouppe,false,glouppe,1,1.0,2,3,388,true,true,false,false,0,9,1,0,6,0,63
10033161,scikit-learn/scikit-learn,python,5579,1445634153,1445687790,1445687790,893,893,commits_in_master,false,false,false,22,2,2,2,6,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,33,12,33,12,17.347383810594767,0.35076623751414515,70,g.louppe@gmail.com,sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py,56,0.04686192468619247,0,0,true,[MRG] FIX inspectargspect is deprecated use signature This will remove the warnings related to the deprecation of inspectgetargspec from the test output,,3190,0.7432601880877743,0.32301255230125525,50258,535.138684388555,40.05332484380597,137.11249950256678,7662,88,1893,324,travis,rvraghav93,GaelVaroquaux,false,GaelVaroquaux,22,0.6363636363636364,28,43,749,true,false,false,false,3,173,17,102,54,0,13
10033039,scikit-learn/scikit-learn,python,5578,1445633700,1449882672,1449882672,70816,70816,commits_in_master,false,false,false,48,4,3,8,8,0,16,0,3,1,0,2,3,1,0,1,1,0,2,3,1,0,1,73,0,73,0,24.314975267783538,0.491651795051712,5,prosper.burq@gmail.com,README.rst|circle.yml|continuous_integration/push_doc.sh|circle.yml|continuous_integration/push_doc.sh,3,0.0016722408026755853,0,1,true,Circle ci push This allows circleCI to push the documentation directly to githubio/dev if everything goes well (no Traceback)The script in continous_integration works on his own from the root of the project (you need to have the ssh private key setup to access the githubio repository though),,3189,0.7431796801505174,0.32274247491638797,50258,535.138684388555,40.05332484380597,137.11249950256678,7662,88,1893,410,travis,waterponey,amueller,false,amueller,3,0.3333333333333333,1,0,725,true,false,false,false,1,3,6,3,1,0,274
10032906,scikit-learn/scikit-learn,python,5577,1445633260,1446060624,1446060624,7122,7122,commits_in_master,false,true,false,22,1,1,0,4,0,4,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.739924543829779,0.095841858143745,3,peter.fischer@fau.de,README.rst,3,0.002508361204013378,0,1,true,Adding shield for circleCI in README This is just to add a shield displaying the build status of the doc on master,,3188,0.743099121706399,0.3219063545150502,50258,535.138684388555,40.05332484380597,137.11249950256678,7662,87,1893,326,travis,waterponey,ogrisel,false,ogrisel,2,0.0,1,0,725,true,false,false,false,1,3,5,3,1,0,322
10032585,scikit-learn/scikit-learn,python,5576,1445632415,1445652408,1445652408,333,333,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,25,0,25,0,17.93365187350575,0.3626206499631507,0,,examples/mixture/plot_gmm.py|examples/mixture/plot_gmm_classifier.py|examples/mixture/plot_gmm_selection.py|examples/mixture/plot_gmm_sin.py,0,0.0,0,0,true,Added colorblind capability for mixture examples Colorblind compatibility as discussed in #5435plot_gmmpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700235/a47ed170-79bc-11e5-86e4-28001284e5b5png)plot_gmm_classifierpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700253/b240480c-79bc-11e5-9428-a224bdf04b41png)plot_gmm_selectionpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700259/c14f771e-79bc-11e5-9e07-166208d5722apng)plot_gmm_sinpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700273/e1dc81fc-79bc-11e5-8980-8d2dfb628b84png),,3187,0.7430185127078758,0.32217573221757323,50258,535.138684388555,40.05332484380597,137.11249950256678,7662,87,1893,323,travis,johannah,ogrisel,false,ogrisel,12,0.75,7,9,1309,true,false,false,false,0,17,12,0,2,0,320
10032505,scikit-learn/scikit-learn,python,5575,1445632147,1445769225,1445769225,2284,2284,commits_in_master,false,false,false,33,1,1,0,5,0,5,0,3,0,0,6,6,6,0,0,0,0,6,6,6,0,0,88,0,88,0,28.89155334960184,0.5841907676124437,13,t3kcit@gmail.com,examples/covariance/plot_lw_vs_oas.py|examples/covariance/plot_robust_vs_empirical_covariance.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/manifold/plot_mds.py|examples/preprocessing/plot_function_transformer.py|examples/semi_supervised/plot_label_propagation_structure.py,7,0.0,0,0,true,Added colorblind compatibility for (unsorted) examples Colorblind compatibility as discussed in #5435covariance/plot_lw_vs_oaspy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700031/5f7b33d0-79bb-11e5-8e88-795b4ca655ddpng)covariance/plot_robust_vs_empirical_covariancepyMade lines thicker and fixed (I think) reference [figure_1](https://cloudgithubusercontentcom/assets/1568249/10700041/7004dbde-79bb-11e5-981e-c64c54f2d4f5png)gaussian_process/plot_compare_gpr_krrpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700069/9f83f8c2-79bb-11e5-8435-2532bd6113ccpng)manifold/plot_mdspy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700085/b28ab5a0-79bb-11e5-8cb5-051b8348d87apng)preprocessing/plot_function_transformerpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700110/f3ad90ac-79bb-11e5-993d-4ea5c2c7e226png)[figure_2](https://cloudgithubusercontentcom/assets/1568249/10700111/f3ae445c-79bb-11e5-9102-aed87099958epng)semi_supervised/plot_label_propagation_structurepy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10700605/2e347a26-79bf-11e5-9de6-c37dc88df096png),,3186,0.7429378531073446,0.32217573221757323,50258,535.138684388555,40.05332484380597,137.11249950256678,7662,87,1893,323,travis,johannah,glouppe,false,glouppe,11,0.7272727272727273,7,9,1309,true,false,false,false,0,17,11,0,2,0,15
10032200,scikit-learn/scikit-learn,python,5574,1445631234,1446065826,1446065826,7243,7243,commits_in_master,false,false,false,20,2,2,1,3,0,4,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,119,0,119,0,28.65952102376835,0.5853958794827888,4,peter.fischer@fau.de,examples/decomposition/plot_incremental_pca.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_sparse_coding.py|examples/decomposition/plot_incremental_pca.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_sparse_coding.py,3,0.0008382229673093043,0,0,true,Added colorblind compatibility for decomposition example Colorblind compatibility as discussed in #5435plot_incremental_pcapy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699827/fb7ab65e-79b9-11e5-83e4-13063884da14png)[figure_2](https://cloudgithubusercontentcom/assets/1568249/10699826/fb7a7752-79b9-11e5-93f6-f566a2abc2cepng)plot_pca_vs_ldapy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699839/0cded7f4-79ba-11e5-819d-7bcf4f32c252png)[figure_2](https://cloudgithubusercontentcom/assets/1568249/10699840/0cdfd7ee-79ba-11e5-9b30-8bf84e3e5bd0png)plot_sparse_codingpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699858/22063366-79ba-11e5-9c5c-0a800927afc6png),,3185,0.7428571428571429,0.32271584241408213,49518,534.795427925199,40.12682256957066,137.94983642311885,7662,87,1893,325,travis,johannah,ogrisel,false,ogrisel,10,0.7,7,9,1309,true,false,false,false,0,17,10,0,2,0,0
10032112,scikit-learn/scikit-learn,python,5573,1445630972,1445733733,1445733733,1712,1712,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,69,0,69,0,19.029294875466427,0.388690055228853,6,peter.fischer@fau.de,examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_validation_curve.py,5,0.0008382229673093043,0,1,true,Colorblind compatibility for model_selection examples Colorblind compatibility as discussed in #5435plot_precision_recallpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699253/c36ad478-79b5-11e5-9bfb-3468712efb35png)plot_rocpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699765/86f11ecc-79b9-11e5-9d40-4c22622ae4a1png)[figure_2](https://cloudgithubusercontentcom/assets/1568249/10699274/db06bde0-79b5-11e5-93f5-873311e0a062png)plot_roc_crossvalpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699292/f9ceb5f2-79b5-11e5-935b-d81d679ad2d5png)plot_validation_curvepy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10699330/34e070c2-79b6-11e5-83bd-c4826801940fpng),,3184,0.7427763819095478,0.32271584241408213,49518,534.795427925199,40.12682256957066,137.94983642311885,7662,87,1893,325,travis,johannah,arjoly,false,arjoly,9,0.6666666666666666,7,9,1309,true,false,false,false,0,17,9,0,2,0,1
10031841,scikit-learn/scikit-learn,python,5572,1445629982,1446518414,1446518414,14807,14807,commits_in_master,false,false,false,11,2,2,2,4,0,6,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,10,6,10,6,22.659736582669,0.4628450145645763,26,tom.duprelatour@orange.fr,doc/modules/multiclass.rst|sklearn/linear_model/ridge.py|doc/modules/multiclass.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,20,0.0075440067057837385,0,1,true,[WIP] DOC+ENH: Saying RidgeClassifer doesnt not support multi-label Fixes issue #5562,,3183,0.7426955702167767,0.32271584241408213,49518,534.795427925199,40.12682256957066,137.94983642311885,7662,87,1893,341,travis,dohmatob,amueller,false,amueller,6,0.3333333333333333,13,3,1703,true,false,false,false,2,24,7,5,16,0,22
10031373,scikit-learn/scikit-learn,python,5571,1445629778,,1445630177,6,,unknown,false,false,true,15,1,1,1,0,1,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.686487715514879,0.09572562624594731,1,t3kcit@gmail.com,sklearn/model_selection/tests/test_search.py,1,0.0008382229673093043,1,0,true,Less verbose tests in test_searchpy Any reason why this is set to verbose3 Ping @rvraghav93,,3182,0.742928975487115,0.32271584241408213,49518,534.795427925199,40.12682256957066,137.94983642311885,7662,87,1893,318,travis,fabianp,fabianp,true,,39,0.6923076923076923,234,26,1987,true,true,false,false,3,25,1,10,2,0,2
10030236,scikit-learn/scikit-learn,python,5566,1445625318,1446253465,1446253465,10469,10469,commits_in_master,false,false,false,10,2,2,7,16,0,23,0,5,0,0,55,55,42,0,0,0,0,55,55,42,0,0,368,10,368,10,496.88840792621534,10.149379247493712,67,tdhopper@gmail.com,doc/developers/contributing.rst|doc/model_selection.rst|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/ensemble.rst|doc/modules/grid_search.rst|doc/modules/learning_curve.rst|doc/modules/model_evaluation.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py|doc/tutorial/text_analytics/working_with_text_data.rst|examples/applications/face_recognition.py|examples/calibration/plot_calibration.py|examples/calibration/plot_calibration_curve.py|examples/classification/plot_classifier_comparison.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/covariance/plot_covariance_estimation.py|examples/decomposition/plot_pca_vs_fa_model_selection.py|examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_partial_dependence.py|examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_digits.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/feature_stacker.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/linear_model/plot_sgd_comparison.py|examples/missing_values.py|examples/mixture/plot_gmm_classifier.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_learning_curve.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_underfitting_overfitting.py|examples/model_selection/plot_validation_curve.py|examples/model_selection/randomized_search.py|examples/neighbors/plot_digits_kde_sampling.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/plot_cv_predict.py|examples/plot_digits_pipe.py|examples/plot_kernel_ridge_regression.py|examples/preprocessing/plot_function_transformer.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_anova.py|examples/svm/plot_svm_scale_c.py|doc/developers/contributing.rst|doc/model_selection.rst|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/ensemble.rst|doc/modules/grid_search.rst|doc/modules/learning_curve.rst|doc/modules/model_evaluation.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py|doc/tutorial/text_analytics/working_with_text_data.rst|examples/applications/face_recognition.py|examples/calibration/plot_calibration.py|examples/calibration/plot_calibration_curve.py|examples/classification/plot_classifier_comparison.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/covariance/plot_covariance_estimation.py|examples/decomposition/plot_pca_vs_fa_model_selection.py|examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_partial_dependence.py|examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_digits.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/feature_stacker.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/linear_model/plot_sgd_comparison.py|examples/missing_values.py|examples/mixture/plot_gmm_classifier.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_learning_curve.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_underfitting_overfitting.py|examples/model_selection/plot_validation_curve.py|examples/model_selection/randomized_search.py|examples/neighbors/plot_digits_kde_sampling.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/plot_cv_predict.py|examples/plot_digits_pipe.py|examples/plot_kernel_ridge_regression.py|examples/preprocessing/plot_function_transformer.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_anova.py|examples/svm/plot_svm_scale_c.py,27,0.0008396305625524769,1,8,true,[MRG] DOC Modify documentation/examples for the new model_selection module @amueller ,,3179,0.7433155080213903,0.3232577665827036,49518,534.795427925199,40.12682256957066,137.94983642311885,7662,87,1893,334,travis,rvraghav93,amueller,false,amueller,19,0.6842105263157895,28,43,749,true,false,false,false,3,171,14,102,53,0,4098
10028531,scikit-learn/scikit-learn,python,5561,1445620378,,1445620390,0,,unknown,false,false,false,6,3,3,0,0,0,0,0,0,0,0,3,3,3,0,0,0,0,3,3,3,0,0,25,60,25,60,21.444630440475418,0.44091452635226247,13,ragvrv@gmail.com,sklearn/tests/test_base.py|sklearn/utils/mocking.py|sklearn/base.py|sklearn/base.py|sklearn/tests/test_base.py,13,0.001693480101608806,0,0,true,Better check for DF parameters PR ,,3176,0.7437027707808564,0.32091447925486877,48020,524.3856726364014,38.83798417326114,134.33985839233654,7660,87,1893,316,travis,ogrisel,ogrisel,true,,136,0.8676470588235294,1226,124,2340,true,true,false,false,49,376,71,258,86,2,-1
10027897,scikit-learn/scikit-learn,python,5560,1445618434,1445628019,1445628019,159,159,commits_in_master,false,false,false,26,3,2,4,2,0,6,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,8,90,11,90,17.442449717280503,0.35862727860314614,35,rvraghav93@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py,22,0.018612521150592216,5,0,true,FIX serialization bug in the implementation of the pickle protocol Ping @glouppe @amueller @jmschrei @jnothman @ngoix this should solve your bug in the isolation forest pull request,,3175,0.7436220472440945,0.3206429780033841,48020,524.3856726364014,38.83798417326114,134.33985839233654,7660,87,1893,318,travis,arjoly,agramfort,false,agramfort,93,0.8064516129032258,30,26,1403,true,true,false,false,19,208,18,303,34,0,57
10026352,scikit-learn/scikit-learn,python,5556,1445613217,1445769140,1445769140,2598,2598,commits_in_master,false,false,false,16,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,0,9,0,9.78395788424434,0.20116407080813464,0,,examples/feature_selection/plot_feature_selection.py|examples/feature_selection/plot_rfe_digits.py,0,0.0,0,0,true,Added colorblind compatibility for feature examples Colorblind compatibility as discussed in #5435plot_feature_selectionpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10692200/23aa2936-7990-11e5-91c5-ab93aba5f948png)plot_rfe_digitspy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10692210/3e46103e-7990-11e5-9f64-0364310b7b8apng),,3173,0.7437756066813741,0.3211864406779661,48020,524.3856726364014,38.83798417326114,134.33985839233654,7660,87,1893,327,travis,johannah,glouppe,false,glouppe,8,0.625,7,9,1309,true,false,false,false,0,17,8,0,2,0,107
10024147,scikit-learn/scikit-learn,python,5552,1445603538,1445769074,1445769074,2758,2758,commits_in_master,false,false,false,18,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,0,15,0,9.752003221812878,0.20050726877729438,2,gael.varoquaux@normalesup.org,examples/tree/plot_tree_regression.py|examples/tree/plot_tree_regression_multioutput.py,2,0.001697792869269949,0,1,true,[MRG+1] Added colorblind compatibility for trees examples Added colorblind compatibility as discussed in #5435plot_tree_regressionpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10689025/84ca88e0-7978-11e5-98f3-60d9c060a746png)plot_tree_regression_multioutputpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10689177/87b53342-7979-11e5-93c9-a20fffbbb2fapng),,3170,0.7441640378548896,0.3225806451612903,48020,524.3856726364014,38.83798417326114,134.33985839233654,7659,87,1893,328,travis,johannah,glouppe,false,glouppe,6,0.6666666666666666,7,9,1309,true,false,false,false,0,17,6,0,2,0,1
10024045,scikit-learn/scikit-learn,python,5550,1445602689,1445649222,1445649222,775,775,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.821978183229033,0.09914287902001119,2,gael.varoquaux@normalesup.org,examples/text/document_classification_20newsgroups.py,2,0.0016963528413910093,0,1,true,Added colorblind compatibility for text examples Colorblind compatibility as discussed in #5435document_classification_20newsgroupspy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10688867/b978b5f4-7977-11e5-9f10-a129f3f32698png),,3168,0.7443181818181818,0.32230703986429177,48020,524.3856726364014,38.83798417326114,134.33985839233654,7659,87,1893,329,travis,johannah,ogrisel,false,ogrisel,5,0.6,7,9,1309,true,false,false,false,0,17,5,0,2,0,316
10023057,scikit-learn/scikit-learn,python,5546,1445598922,1445733620,1445733620,2244,2244,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,28,0,28,0,14.034182707166645,0.28855704140440075,2,gael.varoquaux@normalesup.org,examples/svm/plot_oneclass.py|examples/svm/plot_svm_regression.py|examples/svm/plot_svm_scale_c.py,2,0.0,0,0,true,Added colorblind compatibility for svm examples plot_oneclasspy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10687502/d7adb8ee-796d-11e5-83b1-34c99340b251png)plot_svm_regressionpy[figure_1](https://cloudgithubusercontentcom/assets/1568249/10687593/79640648-796e-11e5-990f-027bfddfc045png)plot_svm_scale_cpy[figure_0](https://cloudgithubusercontentcom/assets/1568249/10687651/f40d681c-796e-11e5-90bb-d4fc20c6962dpng)[figure_1](https://cloudgithubusercontentcom/assets/1568249/10687652/f40d72c6-796e-11e5-8b92-436a91a46ff6png),,3166,0.7444725205306381,0.3228279386712095,47996,524.6478873239437,38.85740478373198,134.4070339194933,7658,87,1893,333,travis,johannah,arjoly,false,arjoly,3,0.6666666666666666,7,9,1309,true,false,false,false,0,17,3,0,1,0,9
10022701,scikit-learn/scikit-learn,python,5544,1445596951,1445597605,1445597605,10,10,commits_in_master,false,false,false,26,2,2,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,8.487662609954102,0.17451498572041893,12,trev.stephens@gmail.com,sklearn/discriminant_analysis.py|sklearn/discriminant_analysis.py,12,0.010221465076660987,0,1,true,Typo in LDA This PR is the result of the discussion on this previous PR https://githubcom/scikit-learn/scikit-learn/pull/5417It fixes a typo from this[image](https://cloudgithubusercontentcom/assets/1908833/10687040/4a476124-796a-11e5-9ad0-75458e69e754png)to this[image](https://cloudgithubusercontentcom/assets/1908833/10687043/4ef36380-796a-11e5-93c7-1aaa9d84c5depng),,3165,0.744391785150079,0.3228279386712095,47996,524.6478873239437,38.85740478373198,134.4070339194933,7657,87,1893,313,travis,Mbompr,agramfort,false,agramfort,1,1.0,0,0,1210,true,false,false,false,0,8,3,1,6,0,-1
10018737,scikit-learn/scikit-learn,python,5543,1445571940,1445588042,1445588042,268,268,commits_in_master,false,false,false,2,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.258748339161879,0.08756476081661403,17,tom.duprelatour@orange.fr,sklearn/linear_model/ridge.py,17,0.014480408858603067,0,0,false,Fixed typo ,,3164,0.7443109987357776,0.3228279386712095,47983,524.5191005147657,38.84709167830273,134.4017672925828,7657,88,1892,315,travis,ltiao,glouppe,false,glouppe,3,0.6666666666666666,9,8,1475,true,false,false,false,0,0,1,0,1,0,268
10010228,scikit-learn/scikit-learn,python,5542,1445544272,,1445623042,1312,,unknown,false,false,false,12,1,1,0,5,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,12,2,12,9.133214235724186,0.18778938231308423,28,rvraghav93@gmail.com,sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,22,0.01872340425531915,3,2,false,[MRG] Raise appropriate error if y is sparse ping @rvraghav93 @glouppe @jnothman ,,3163,0.7445463167878597,0.32340425531914896,47983,524.5191005147657,38.84709167830273,134.4017672925828,7652,88,1892,325,travis,arjoly,amueller,false,,92,0.8152173913043478,30,26,1402,true,true,true,false,19,198,13,302,30,0,32
10009562,scikit-learn/scikit-learn,python,5541,1445542402,1445599230,1445599230,947,947,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.6927528670059715,0.09648839495333582,4,peter.fischer@fau.de,examples/classification/plot_lda.py,4,0.0034129692832764505,0,0,false,[MRG] Added colorblind compatibility for plot examples in classification Reviewed and edited classification for colorblind compatibility as discussed in #5435 ,,3162,0.7444655281467426,0.3225255972696246,47983,524.5191005147657,38.84709167830273,134.4017672925828,7652,88,1892,316,travis,johannah,arjoly,false,arjoly,2,0.5,7,9,1308,true,false,false,false,0,3,2,0,1,0,915
10009156,scikit-learn/scikit-learn,python,5537,1445541330,1445599623,1445599623,971,971,commits_in_master,false,false,false,20,3,2,1,14,0,15,0,3,0,0,11,11,11,0,0,0,0,11,11,11,0,0,332,0,336,0,100.8301718552365,2.0732207664811164,6,sshagunsodhani@gmail.com,examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_lasso_and_elasticnet.py|examples/linear_model/plot_multi_task_lasso_support.py|examples/linear_model/plot_polynomial_interpolation.py|examples/linear_model/plot_ransac.py|examples/linear_model/plot_robust_fit.py|examples/linear_model/plot_sgd_loss_functions.py|examples/linear_model/plot_sgd_penalties.py|examples/linear_model/plot_sparse_recovery.py|examples/linear_model/plot_theilsen.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_lasso_and_elasticnet.py|examples/linear_model/plot_multi_task_lasso_support.py|examples/linear_model/plot_polynomial_interpolation.py|examples/linear_model/plot_ransac.py|examples/linear_model/plot_robust_fit.py|examples/linear_model/plot_sgd_loss_functions.py|examples/linear_model/plot_sgd_penalties.py|examples/linear_model/plot_theilsen.py,4,0.0,0,0,false,[MRG] Added colorblind compatibility for plot examples in linear_models Reviewed and edited linear_models for colorblind compatibility as discussed in #5435 ,,3159,0.7448559670781894,0.32222222222222224,47981,524.5409641316354,38.848710948083614,134.40736958379358,7652,88,1892,315,travis,johannah,arjoly,false,arjoly,0,0,7,9,1308,true,false,false,false,0,3,0,0,1,0,30
10007085,scikit-learn/scikit-learn,python,5535,1445535889,1445990724,1445990724,7580,7580,merged_in_comments,false,false,false,31,2,2,7,8,0,15,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,76,85,76,85,18.35466213292994,0.37739973970053015,17,tom.duprelatour@orange.fr,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,17,0.014579759862778732,0,1,false,REFACTOR: fixed huge code duplication in _RidgeGCV * merged code common to _errors and _values into single helper function* merged code common to _errors_svd and _values_svd into single helper function,,3158,0.7447751741608613,0.3216123499142367,47982,524.530032095369,38.84790129631945,134.4045683798091,7651,88,1892,331,travis,dohmatob,agramfort,false,agramfort,4,0.25,13,3,1702,true,false,false,false,1,20,5,4,14,0,22
10004881,scikit-learn/scikit-learn,python,5531,1445529249,1445881909,1445881909,5877,5877,commits_in_master,false,false,false,10,3,2,27,7,0,34,0,3,0,0,10,11,8,0,0,0,0,11,11,8,0,0,636,534,636,534,82.95914965529495,1.705766156791024,53,t3kcit@gmail.com,doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/tests/test_export.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|examples/ensemble/plot_gradient_boosting_regression.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/tests/test_export.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,26,0.012048192771084338,0,2,false,[WIP]  min_samples_split and min_samples_leaf now accept a percentage superseed https://githubcom/scikit-learn/scikit-learn/pull/3359,,3156,0.7449302915082383,0.32013769363166955,47982,524.530032095369,38.84790129631945,134.4045683798091,7648,88,1892,332,travis,arjoly,arjoly,true,arjoly,91,0.8131868131868132,30,26,1402,true,true,false,false,19,196,11,302,27,0,231
10003820,scikit-learn/scikit-learn,python,5527,1445525383,1453233982,1453233982,128476,128476,commit_sha_in_comments,false,false,false,23,6,2,39,13,0,52,0,5,1,0,0,2,1,0,0,1,0,1,2,1,0,0,174,0,240,0,9.706481334411402,0.199579018761767,0,,examples/linear_model/plot_ridge_coeffs.py|examples/linear_model/plot_ridge_coeffs.py,0,0.0,0,9,false,Ridge coefficients as a function of regularisation As in Ridge path - another perspective #5470  Ive added this as a separate example ,,3153,0.7453219156359023,0.317984361424848,47971,523.7747805966104,38.856809322298886,134.20608284171686,7647,87,1892,431,travis,Kornel,TomDLT,false,TomDLT,1,0.0,0,1,1705,false,false,false,false,0,1,2,1,0,0,205
10003753,scikit-learn/scikit-learn,python,5526,1445525091,1447246919,1447246919,28697,28697,commits_in_master,false,false,false,97,2,1,21,42,0,63,0,10,0,0,2,3,2,0,0,0,0,3,3,3,0,0,8,34,8,200,8.7001875611498,0.17888819199000716,22,t3kcit@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py,22,0.019130434782608695,0,24,false,BUG fix sample_weight in LinearRegression I have noticed in #5357 that a test introduced in 017 was not correctly checking for the effect of sample_weight in LinearRegression To reproduce the issue on master: import numpy as npfrom sklearnlinear_modelbase import LinearRegressionrng  nprandomRandomState(0)y  rngrandn(6)X  rngrandn(6 5)w  10 + rngrand(6)clf  LinearRegression()clffit(X y sample_weightw)coefs1  clfcoef_scaled_y  y * npsqrt(w)scaled_X  X * npsqrt(w)[: npnewaxis]clffit(scaled_X scaled_y)coefs2  clfcoef_print(coefs1)[ 458791686 -42095038   039031788  32727146  -017386704]print(coefs2)[ 369763237 -364824351  0367363    297550307 -044881672],,3152,0.745241116751269,0.3173913043478261,47971,523.7747805966104,38.856809322298886,134.20608284171686,7647,87,1892,366,travis,giorgiop,agramfort,false,agramfort,9,0.8888888888888888,2,7,1063,true,true,false,false,3,158,12,57,64,4,47
10002471,scikit-learn/scikit-learn,python,5525,1445519653,,1446468275,15810,,unknown,false,true,false,13,2,2,5,8,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,87,12,87,16.54170579516773,0.34043966480942167,13,ragvrv@gmail.com,sklearn/base.py|sklearn/tests/test_base.py|sklearn/base.py|sklearn/tests/test_base.py,13,0.011373578302712161,0,3,false,[WIP] improving equality test inf sklearns cloning function This PR addresses issue #5522,,3151,0.7454776261504285,0.31583552055993,47870,523.7100480467934,38.87612283267182,134.1967829538333,7645,87,1892,343,travis,dohmatob,dohmatob,true,,3,0.3333333333333333,13,3,1702,true,false,false,false,1,18,4,3,11,0,18
10003350,scikit-learn/scikit-learn,python,5524,1445519174,,1445519221,0,,unknown,false,false,false,13,3,3,0,0,0,0,0,0,0,0,5,5,4,0,0,0,0,5,5,4,0,0,8,55,8,55,22.221601491535562,0.4573358187471139,110,yanlend@users.noreply.github.com,sklearn/base.py|sklearn/tests/test_base.py|doc/whats_new.rst|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,89,0.011373578302712161,0,0,false,[WIP] improving equality test inf sklearns cloning function This PR addresses issue #5522,,3150,0.7457142857142857,0.31583552055993,47870,523.7100480467934,38.87612283267182,134.1967829538333,7645,87,1892,308,travis,dohmatob,dohmatob,true,,2,0.5,13,3,1702,true,false,false,false,1,18,2,3,11,0,-1
9998717,scikit-learn/scikit-learn,python,5521,1445498915,,1445501694,46,,unknown,false,false,false,20,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,3.934321535159575,0.08097153333850653,5,peter.fischer@fau.de,sklearn/pipeline.py,5,0.00441696113074205,0,0,false,fix bug with 1D transforms in feature unions 1D transformations are cast to column vectors so that they stack correctly,,3149,0.7459510955859003,0.3136042402826855,47852,523.8443534230545,38.890746468277186,134.2472623923765,7637,86,1892,310,travis,jpopham91,jpopham91,true,,0,0,2,0,203,false,true,false,false,0,0,0,0,0,0,-1
9991252,scikit-learn/scikit-learn,python,5519,1445467278,1445510209,1445510209,715,715,commits_in_master,false,false,false,18,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.265582300673653,0.08778914900078982,2,gael.varoquaux@normalesup.org,sklearn/preprocessing/imputation.py,2,0.001763668430335097,0,0,false,(Issue #5514) check_array performed twice fixed In Imputer transform() method check_array() might be performed twice #5514 issue resolved,,3148,0.7458703939008895,0.31305114638447973,47852,523.8443534230545,38.890746468277186,134.2472623923765,7633,86,1891,309,travis,ishank08,agramfort,false,agramfort,2,0.5,0,1,321,true,false,false,false,0,14,2,6,13,0,711
9987000,scikit-learn/scikit-learn,python,5517,1445455364,1445542226,1445542226,1447,1447,commits_in_master,false,false,false,70,2,1,0,7,0,7,0,4,0,0,6,6,6,0,0,0,0,6,6,6,0,0,103,0,183,0,25.382043405118637,0.5223833609332885,11,t3kcit@gmail.com,sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/cca_.py|sklearn/cross_decomposition/pls_.py|sklearn/datasets/base.py|sklearn/linear_model/passive_aggressive.py|sklearn/utils/arpack.py,8,0.0035587188612099642,0,0,false,[MRG] Use super rather than calls to base class methods Fixes #5205Remaining non-super calls:❯ git grep \__init__(self -- git ls-files | grep -vP utils|externals|sphinxextexamples/applications/plot_out_of_core_classificationpy:        html_parserHTMLParser__init__(self)examples/svm/plot_rbf_parameterspy:        Normalize__init__(self vmin vmax clip)sklearn/covariance/outlier_detectionpy:        MinCovDet__init__(self store_precisionstore_precisionsklearn/covariance/outlier_detectionpy:        OutlierDetectionMixin__init__(self contaminationcontamination)The outlier_detection need to stay I reckon because you need to call __init__ for two of the base classesNot sure if we want to fix the examples too,,3147,0.7457896409278678,0.3122775800711744,47848,523.8881457950175,38.8939976592543,134.2584852031433,7631,85,1891,320,travis,lesteve,amueller,false,amueller,20,0.95,4,0,1274,true,false,false,false,6,48,7,18,16,0,904
9984705,scikit-learn/scikit-learn,python,5513,1445449352,,1452223265,112898,,unknown,false,false,false,7,4,4,5,2,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,105,24,105,24,17.21912479788324,0.3540705928824647,8,rvraghav93@gmail.com,sklearn/multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,8,0.007207207207207207,0,0,false,Added partial_fit method to OneVsRestClassifier from #5179 ,,3145,0.7462639109697933,0.3081081081081081,47871,523.5319922291158,38.875310730922685,134.1104217584759,7630,85,1891,432,travis,agramfort,MechCoder,false,,45,0.8666666666666667,188,187,2149,true,true,false,true,8,150,34,100,42,0,1873
9984479,scikit-learn/scikit-learn,python,5512,1445448646,1445449471,1445449471,13,13,commits_in_master,false,false,false,5,2,2,5,5,0,10,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,33,0,33,0,27.506113796104934,0.5660984363719037,95,yanlend@users.noreply.github.com,doc/whats_new.rst|sklearn/decomposition/pca.py|sklearn/utils/extmath.py|doc/whats_new.rst|sklearn/decomposition/pca.py|sklearn/utils/extmath.py,87,0.012612612612612612,0,5,false,DOC versionadded randomized_svd [#5141/comment](#https://githubcom/scikit-learn/scikit-learn/pull/5141#discussion_r42610139) addressed,,3144,0.7461832061068703,0.3081081081081081,47846,523.9100447268319,38.89562345859633,134.26409731221,7630,85,1891,315,travis,giorgiop,giorgiop,true,giorgiop,8,0.875,2,7,1062,true,true,false,false,3,147,9,54,62,4,0
9983473,scikit-learn/scikit-learn,python,5511,1445445987,1446686674,1446686674,20678,20678,commits_in_master,false,true,false,18,5,2,15,11,0,26,0,2,0,0,1,32,1,0,0,0,0,32,32,32,0,0,2,0,270,0,8.985337617478713,0.18476208219733686,22,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py,22,0.019855595667870037,0,3,false,[MRG + 1] Version Added for Robust Scaler Looks like this:[versionadded](https://cloudgithubusercontentcom/assets/11410385/10638349/d6f570f4-780a-11e5-8a3e-f309a8cf2053png)Any suggestions are welcome to proceed,,3143,0.7461024498886414,0.30866425992779783,47870,523.5429287654063,38.87612283267182,134.11322331313974,7630,85,1891,358,travis,KamalakerDadi,amueller,false,amueller,1,0.0,0,2,225,true,false,false,false,1,3,1,0,3,0,11
9980796,scikit-learn/scikit-learn,python,5508,1445436910,,1445621451,3075,,unknown,false,false,false,9,5,4,10,9,0,19,0,3,0,0,3,4,3,0,0,0,0,4,4,3,0,0,4,50,4,50,22.372502954358083,0.46003606610031883,20,t3kcit@gmail.com,sklearn/tests/test_multiclass.py|sklearn/tests/test_multiclass.py|sklearn/svm/base.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,12,0.008204193254329991,0,1,false,[WIP] FIX  fails with decision_function_shapeovr [#5495] See issue #5495,,3142,0.7463399108847868,0.3081130355515041,47861,523.8503165416519,38.88343327552705,134.1593364116922,7629,84,1891,336,travis,dohmatob,amueller,false,,1,1.0,13,3,1701,true,false,false,false,1,8,1,0,6,0,169
9976369,scikit-learn/scikit-learn,python,5506,1445432157,1445595703,1445595703,2725,2725,commits_in_master,false,false,false,23,1,1,3,7,0,10,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.975461450613947,0.10240376714905702,5,g.louppe@gmail.com,examples/gaussian_process/plot_compare_gpr_krr.py,5,0.0045871559633027525,0,2,false,Tweaking the legend of GP plot in plot_compare_gpr_krrpy Tweaking the legend for this particular issue: GP plot doesnt render nicely in docs #5500 ,,3140,0.7464968152866241,0.3073394495412844,47533,526.5604948141291,39.130709191509055,135.04302274209496,7629,83,1891,329,travis,RachitKansal,amueller,false,amueller,0,0,0,2,341,false,false,false,false,1,1,0,0,0,0,3
9969344,scikit-learn/scikit-learn,python,5499,1445414075,1445451145,1445451145,617,617,commit_sha_in_comments,false,false,false,29,2,1,4,3,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,17,0,4.490290227847602,0.09242413824966315,19,vighneshbirodkar@nyu.edu,sklearn/utils/validation.py,19,0.017823639774859287,1,0,false,Adding documentation about y nan handling in check_X_y Address #5496 I also made a comment about nan handling in multi_output and the functions description @MechCoder can you check this,,3138,0.7466539196940727,0.30863039399624764,47592,518.427466801143,38.68297192805514,132.62733232476046,7622,81,1891,323,travis,hlin117,MechCoder,false,MechCoder,8,0.5,18,20,1219,true,true,false,false,4,40,5,5,19,0,14
9970681,scikit-learn/scikit-learn,python,5498,1445385990,1453614347,1453614347,137139,137139,merged_in_comments,false,false,false,16,3,3,18,10,0,28,0,8,0,0,4,4,4,0,0,0,0,4,4,4,0,0,70,44,70,44,22.94549599932989,0.47228967100991015,52,tom.duprelatour@orange.fr,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_ridge.py,35,0.024390243902439025,1,6,false,[WIP] Storing the best attributes of (non-GridSearch) CV models Continuation of work done by @eshillts ,,3137,0.7465731590691743,0.3067542213883677,47592,518.427466801143,38.68297192805514,132.62733232476046,7620,81,1890,448,travis,MechCoder,MechCoder,true,MechCoder,76,0.868421052631579,87,41,1218,true,true,false,false,14,184,22,104,38,0,1
9969580,scikit-learn/scikit-learn,python,5497,1445382488,1452846962,1452846962,124407,124407,commits_in_master,false,false,false,85,2,1,6,27,0,33,0,5,0,0,1,3,1,0,0,0,0,3,3,2,0,0,25,0,87,55,4.331744328525953,0.08916077054417625,2,gael.varoquaux@normalesup.org,sklearn/linear_model/ransac.py,2,0.001876172607879925,0,17,false,[MRG] Supply arbitrary residual_metrics to RANSAC for 1-D targets Partly fixes https://githubcom/scikit-learn/scikit-learn/issues/4740Supply arbitrary residual metrics for 1-D targets was not possible    from sklearnlinear_model import RANSACRegressor    from sklearndatasets import make_regression    X y  make_regression()    res_met  lambda dy: dy ** 2    ransac  RANSACRegressor(min_samples5 residual_metricres_met)    ransacfit(X y)    IndexError: too many indices for arrayThe workaround was to explicitly define res_met as accepting 2-D arrays (since there is a reshape done) which is non-obvious    res_met  lambda dy: npsum(dy**2 axis1)    ransac  RANSACRegressor(min_samples5 residual_metricres_met)    ransacfit(X y),,3136,0.7464923469387755,0.3067542213883677,47592,518.427466801143,38.68297192805514,132.62733232476046,7620,81,1890,438,travis,MechCoder,MechCoder,true,MechCoder,75,0.8666666666666667,87,41,1218,true,true,false,false,14,179,21,104,38,0,2
9963545,scikit-learn/scikit-learn,python,5494,1445367047,1445421694,1445421694,910,910,commits_in_master,false,false,false,4,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.796316736953643,0.09872311558753478,5,peter.fischer@fau.de,examples/classification/plot_classifier_comparison.py,5,0.004708097928436911,0,0,false,ENH: fixes issue #5484 ,,3135,0.7464114832535885,0.3069679849340866,47592,518.427466801143,38.68297192805514,132.62733232476046,7617,81,1890,315,travis,dohmatob,glouppe,false,glouppe,0,0,13,3,1700,false,false,false,false,1,2,0,0,0,0,0
9962593,scikit-learn/scikit-learn,python,5492,1445364786,1447274479,1447274479,31828,31828,commits_in_master,false,false,false,28,5,3,75,37,0,112,0,9,1,38,3,52,41,0,1,2,41,10,53,49,0,3,1459,0,2971,0,23.058459343064115,0.474614807953996,54,ylow@graphlab.com,.gitignore|setup.py|Makefile|setup.py|sklearn/utils/cythonize.py|sklearn/__check_build/_check_build.c|sklearn/_isotonic.c|sklearn/cluster/_k_means.c|sklearn/datasets/_svmlight_format.c|sklearn/decomposition/_online_lda.c|sklearn/decomposition/cdnmf_fast.c|sklearn/ensemble/_gradient_boosting.c|sklearn/feature_extraction/_hashing.c|sklearn/linear_model/cd_fast.c|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/manifold/_barnes_hut_tsne.c|sklearn/manifold/_utils.c|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/pairwise_fast.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/svm/liblinear.c|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/tree/_criterion.c|sklearn/tree/_splitter.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/utils/_logistic_sigmoid.c|sklearn/utils/_random.c|sklearn/utils/arrayfuncs.c|sklearn/utils/graph_shortest_path.c|sklearn/utils/lgamma.c|sklearn/utils/murmurhash.c|sklearn/utils/seq_dataset.c|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/weight_vector.c,21,0.0018885741265344666,0,9,false,Removing cython files Still to do :- [ ] add cache to travis so that it does not cythonize + build at every commit in each PR,,3134,0.7463305679642629,0.307837582625118,47592,518.427466801143,38.68297192805514,132.62733232476046,7616,81,1890,376,travis,arthurmensch,MechCoder,false,MechCoder,10,0.3,1,1,927,true,true,false,false,4,46,7,30,28,0,46
9962320,scikit-learn/scikit-learn,python,5490,1445364177,,1445365546,22,,unknown,false,true,false,58,13,13,0,4,0,4,0,2,0,0,6,6,3,0,1,0,0,6,6,3,0,1,139,49,139,49,66.82589800294667,1.3754848177470307,108,yanlend@users.noreply.github.com,circle.yml|circle.yml|circle.yml|circle.yml|circle.yml|circle.yml|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|doc/related_projects.rst|doc/whats_new.rst|sklearn/gaussian_process/gaussian_process.py|circle.yml,85,0.012275731822474031,0,0,false,Send latest build documentation to githubio repository Still answering #4986 we should upload the latest doc to the dev directory of the githubio repoWe need to modify a bit my circleyml file before accepting the PR to fit the scikit-learn projectAlso we need to follow this documentation to deploy ssh keypair between github and circleCIhttps://circlecicom/docs/adding-read-write-deployment-key,,3132,0.7468071519795658,0.307837582625118,47592,518.427466801143,38.68297192805514,132.62733232476046,7616,81,1890,323,travis,waterponey,waterponey,true,,1,0.0,1,0,722,true,false,false,false,1,1,1,1,1,0,25
9960495,scikit-learn/scikit-learn,python,5487,1445359339,1445524310,1445524310,2749,2749,commits_in_master,false,false,false,31,15,4,37,24,0,61,0,6,1,0,5,10,6,0,0,1,0,9,10,9,0,0,634,92,945,145,44.63050328211425,0.9186345372232303,36,rvraghav93@gmail.com,examples/tree/plot_structure.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|examples/tree/plot_structure.py,21,0.0169971671388102,4,10,false,[MRG] Add an example and a method to analyse the decision tree stucture Ping @glouppe @pprett @ogrisel @amueller Suggestions are welcome to improve the example It should fix #1105 and #5441,,3131,0.746726285531779,0.307837582625118,47593,518.4165738659046,38.68215914105015,132.62454562645766,7616,81,1890,323,travis,arjoly,agramfort,false,agramfort,90,0.8111111111111111,30,26,1400,true,true,false,false,17,171,10,190,13,0,10
9960480,scikit-learn/scikit-learn,python,5486,1445359317,1445360014,1445360014,11,11,commits_in_master,false,false,false,19,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.22960152587666,0.08705835145532581,8,ragvrv@gmail.com,sklearn/cluster/hierarchical.py,8,0.007554296506137866,0,1,false,MAINT: use xrange from six Small change – we should use six instead of doing Python 2/3 compat manually,,3130,0.7466453674121406,0.307837582625118,47593,518.4165738659046,38.68215914105015,132.62454562645766,7616,81,1890,310,travis,jakevdp,agramfort,false,agramfort,48,0.875,1847,0,1623,true,true,false,true,2,31,3,10,2,0,-1
9958104,scikit-learn/scikit-learn,python,5480,1445351684,1445351814,1445351814,2,2,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.662630605303713,0.09597141402372954,9,g.louppe@gmail.com,examples/gaussian_process/plot_gpc.py,9,0.008514664143803218,0,0,false,[MRG] colorbar() doesnt have any label argument removing it makes documentation build Fixes https://githubcom/scikit-learn/scikit-learn/issues/5474,,3128,0.7468030690537084,0.3093661305581835,47593,518.4165738659046,38.68215914105015,132.62454562645766,7616,79,1890,308,travis,pletelli,ogrisel,false,ogrisel,1,1.0,8,6,1292,false,false,false,false,0,5,1,3,0,0,2
9956375,scikit-learn/scikit-learn,python,5479,1445345314,1445345577,1445345577,4,4,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,9.54319115133046,0.19642853433661395,1,gael.varoquaux@normalesup.org,examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py,1,0.000949667616334283,0,0,false,[WIP] fontsize legend should not be used because not available for matplotlib  13 ,,3127,0.7467220978573713,0.30959164292497626,47594,518.4056813884102,38.68134638820019,132.6217590452578,7616,79,1890,309,travis,pletelli,agramfort,false,agramfort,0,0,8,6,1292,false,false,true,false,0,4,0,3,0,0,4
9955995,scikit-learn/scikit-learn,python,5478,1445343895,1445346020,1445346020,35,35,commits_in_master,false,false,false,9,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.5679074524389565,0.09402173252526627,2,ragvrv@gmail.com,sklearn/exceptions.py,2,0.001899335232668566,1,0,false,DOC Add description for UndefinedMetricWarning @GaelVaroquaux is this ok,,3126,0.746641074856046,0.30959164292497626,47594,518.4056813884102,38.68134638820019,132.6217590452578,7616,79,1890,309,travis,rvraghav93,agramfort,false,agramfort,17,0.7058823529411765,28,43,746,true,false,false,false,3,146,12,80,41,0,31
9954256,scikit-learn/scikit-learn,python,5476,1445337514,,1446501851,19405,,unknown,false,true,false,13,3,1,13,7,0,20,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,44,0,4.334639315875467,0.08947396081990257,29,t3kcit@gmail.com,sklearn/cross_validation.py,29,0.02767175572519084,0,1,false,Update cross_validationpy documentation  Specified that output types for train_test_split depend on input types,,3125,0.74688,0.30916030534351147,47624,518.0791197715438,38.61498404165967,132.30724004703512,7616,78,1890,357,travis,adijo,amueller,false,,0,0,5,9,943,false,false,false,false,1,5,0,0,0,0,8
9954225,scikit-learn/scikit-learn,python,5475,1445337382,1445338846,1445338846,24,24,commits_in_master,false,false,false,23,1,1,0,0,4,4,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.683806196170143,0.09668132952822232,11,peter.fischer@fau.de,sklearn/gaussian_process/gaussian_process.py,11,0.01049618320610687,0,0,false,[MRG] The old GP will be deprecated in 019 not 018  also shall we remove correlation_models and regression_models from the __init__ file ,,3124,0.7467989756722151,0.30916030534351147,47624,518.0791197715438,38.61498404165967,132.30724004703512,7616,78,1890,311,travis,glouppe,agramfort,false,agramfort,59,0.9661016949152542,163,26,1804,true,true,false,true,22,183,41,130,81,0,-1
9947109,scikit-learn/scikit-learn,python,5471,1445302241,,1445302419,2,,unknown,false,false,false,34,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.36877397942623,0.09017856543607616,13,t3kcit@gmail.com,doc/modules/clustering.rst,13,0.0124282982791587,0,0,false,Fixes typo on clustering docs I found a typo [here](http://scikit-learnorg/stable/modules/clusteringhtml):This initializes the centroids to be (generally) distant from each other leading to **provably** better results than random initialization as shown in the reference,,3123,0.7470381043868075,0.30975143403441685,47624,518.0791197715438,38.61498404165967,132.30724004703512,7615,78,1889,312,travis,edublancas,GaelVaroquaux,false,,0,0,8,7,1523,false,true,true,false,0,0,0,0,0,0,2
9945237,scikit-learn/scikit-learn,python,5470,1445297061,,1445335266,636,,unknown,false,false,false,106,1,1,2,2,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,74,0,74,0,4.907214686960446,0.10129023424777148,0,,examples/linear_model/plot_ridge_path.py,0,0.0,0,3,false,Ridge path - another perspective Ive been looking at the example and found it confusing that we have to set fit_interceptFalse This was because Y  1 hence an optimal solution exists where the intercept is 1 and all other coefficients are zero Thats why Ive decided to play around with this exampleHere I propose another perspective where we set Y  bX + e an solution which is easy to find for the estimator and two plots: one where we can see how the coefficients are selected properly and another where we see how the error (estimatorcoef_ - b) changesWhat do you think,,3122,0.7472773862908392,0.31004784688995213,47588,517.9457005967892,38.6231823148693,132.36530217701943,7613,78,1889,312,travis,Kornel,Kornel,true,,0,0,0,1,1702,false,false,false,false,0,0,0,0,0,0,22
9944663,scikit-learn/scikit-learn,python,5469,1445295515,1445444910,1445444910,2489,2489,commits_in_master,false,false,false,71,2,1,0,6,0,6,0,3,0,0,1,3,1,0,0,0,0,3,3,3,0,0,19,0,64,44,4.15293587251713,0.08572114126465483,10,t3kcit@gmail.com,sklearn/svm/base.py,10,0.009578544061302681,0,1,false,[WIP] Remove deprcated stuff from SVM For issue at #5434 - [x] Remove support for gamma  00 and set gamma to 1 / n_features when auto is used- [ ] Remove support for uppercase values for loss in sv* (test in file svm/tests/test_svmpy uppercase support in _fit_liblinear function in svm/basepy Refer #4261 and #4260 also svm/classespy L192)- [ ] Change the default value of decision_function_shape to ovr (svm/basepy),,3121,0.7471964114066004,0.3103448275862069,47604,517.7716158306025,38.61020082346021,132.32081337702715,7613,78,1889,325,travis,rvraghav93,glouppe,false,glouppe,16,0.6875,28,43,745,true,false,false,false,3,138,11,80,41,0,1185
9943402,scikit-learn/scikit-learn,python,5468,1445291899,1445333967,1445333967,701,701,commits_in_master,false,false,false,61,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.754490737611456,0.09813789200488152,13,peter.fischer@fau.de,doc/related_projects.rst,13,0.012452107279693486,0,0,false,[MRG] add pomegranate to related projects pomegranate is my module for probabilistic modelling including univariate distributions general mixture models hidden Markov models discrete Bayesian networks and factor graphs It covers many forms of structures learning that dont fit into scikit-learns estimator model but maintains a scikit-learn interfaceI just pushed a new version so I figured Id update here as well ,,3120,0.7471153846153846,0.3103448275862069,47604,517.7716158306025,38.61020082346021,132.32081337702715,7612,78,1889,313,travis,jmschrei,agramfort,false,agramfort,14,0.7142857142857143,35,5,943,false,true,false,false,8,171,20,98,0,0,696
9940337,scikit-learn/scikit-learn,python,5467,1445282361,1445601507,1445601507,5319,5319,commits_in_master,false,false,false,16,9,5,6,5,0,11,0,5,0,0,4,4,3,0,1,0,0,4,4,3,0,1,145,49,186,49,40.97151691975448,0.855724643515419,5,t3kcit@gmail.com,sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/tests/test_supervised.py|.travis.yml|continuous_integration/install.sh|.travis.yml|continuous_integration/install.sh,5,0.0019138755980861245,0,2,false,(Issue #5455) Improved tavisyml - Add cache for ~/download and ~/cache/pip- Put conda in ~/download,,3119,0.7470343058672652,0.31004784688995213,46260,528.1884997838305,39.364461738002596,135.430177258971,7610,78,1889,340,travis,tomMoral,GaelVaroquaux,false,GaelVaroquaux,1,1.0,1,0,1002,true,false,false,false,1,3,1,1,1,0,881
9940228,scikit-learn/scikit-learn,python,5466,1445282046,1445282631,1445282631,9,9,commits_in_master,false,false,false,20,165,165,0,1,0,1,0,7,13,1,33,47,32,0,0,13,1,33,47,32,0,0,8347,1374,8347,1374,1614.0273355254217,33.710320489762076,87,tom.dupre-la-tour@m4x.org,sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpc_xor.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpc_xor.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpc_xor.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpc_xor.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|examples/gaussian_process/new/plot_gpr_co2.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpc_xor.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/new/plot_gpr_co2.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpr.py|examples/gaussian_process/new/plot_gpc.py|examples/gaussian_process/new/plot_gpr_co2.py|examples/gaussian_process/new/plot_gpr_noisy.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/new/plot_gpr_co2.py|examples/gaussian_process/new/plot_gpr_noisy.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/new/plot_gpr_noisy.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/new/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|examples/gaussian_process/gp_diabetes_dataset.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpc_xor.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpc.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpc_xor.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|examples/classification/plot_classifier_comparison.py|examples/gaussian_process/plot_gpc_xor.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpc_xor.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/plot_gpr_noisy_targets.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|examples/gaussian_process/plot_gpr_noisy_targets.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/tests/test_gpc.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|sklearn/gaussian_process/kernels.py|sklearn/cross_validation.py|sklearn/metrics/pairwise.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/gaussian_process/plot_gpc.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_compare_gpr_krr.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpc_xor.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_noisy_targets.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpr.py|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_gpr_co2.py|sklearn/gaussian_process/kernels.py|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_gpr_noisy_targets.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|examples/gaussian_process/plot_compare_gpr_krr.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpr_co2.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|doc/modules/gaussian_process.rst|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/tests/test_gpc.py|examples/gaussian_process/plot_gpr_co2.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|examples/classification/plot_classification_probability.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/tests/test_gpc.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_gpc_iris.py|examples/gaussian_process/plot_gpc_isoprobability.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/utils/estimator_checks.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|examples/gaussian_process/plot_gpr_co2.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpc.py|sklearn/tests/test_common.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/regression_models.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/gpc.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_gpc_xor.py|sklearn/gaussian_process/__init__.py|sklearn/utils/testing.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/regression_models.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/gpc.py|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_gpr_noisy_targets.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/kernels.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_noisy_targets.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/kernels.py|doc/modules/gaussian_process.rst|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpc.py|sklearn/tests/test_common.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/kernels.py|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/metrics/pairwise.py|doc/modules/gaussian_process.rst|sklearn/gaussian_process/tests/test_kernels.py|doc/modules/gaussian_process.rst|sklearn/gaussian_process/tests/test_kernels.py|doc/modules/gaussian_process.rst|doc/modules/gaussian_process.rst|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/gpr.py|examples/gaussian_process/plot_gpc_iris.py|examples/gaussian_process/plot_gpc_isoprobability.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_noisy_targets.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_kernels.py|doc/modules/gaussian_process.rst|examples/gaussian_process/plot_compare_gpr_krr.py|examples/gaussian_process/plot_gpc.py|examples/gaussian_process/plot_gpc_xor.py|examples/gaussian_process/plot_gpr_co2.py|examples/gaussian_process/plot_gpr_noisy.py|examples/gaussian_process/plot_gpr_prior_posterior.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/tests/test_kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/kernels.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py|sklearn/gaussian_process/tests/test_gpc.py|sklearn/gaussian_process/tests/test_gpr.py|sklearn/gaussian_process/gpc.py|sklearn/gaussian_process/gpr.py,38,0.0,1,5,false,[MRG+2] Gaussian process by @jmetzen This supersedes #4270 @jmetzen branch has been rebased Waiting for Travis and then Ill merge,,3118,0.7469531751122515,0.31004784688995213,46260,528.1884997838305,39.364461738002596,135.430177258971,7610,78,1889,316,travis,glouppe,glouppe,true,glouppe,58,0.9655172413793104,163,26,1803,true,true,false,false,17,172,36,136,79,0,9
9940029,scikit-learn/scikit-learn,python,5465,1445281537,1445449587,1445449587,2800,2800,commits_in_master,false,false,false,31,4,3,2,4,0,6,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,22,8,23,8,23.729278766425868,0.49560597556219205,20,tom.dupre-la-tour@m4x.org,sklearn/metrics/classification.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_classification.py,16,0.011494252873563218,0,0,false,partly fixed issue #3450 for hamming loss Added sample weight support to hamming_loss in sklearn/metrics/classificationpyAdded test with https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/metrics/tests/test_commonpyAdded test to compare our implementation of hamming loss with scipys one,,3117,0.7468719923002888,0.3103448275862069,46260,528.1884997838305,39.364461738002596,135.430177258971,7610,78,1889,331,travis,achab,agramfort,false,agramfort,1,1.0,3,14,-30,false,false,false,false,0,1,2,0,0,0,1396
9939096,scikit-learn/scikit-learn,python,5461,1445279136,,1445450667,2858,,unknown,false,true,false,92,2,2,5,14,0,19,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,76,0,76,8.835574373190527,0.18453841349249567,7,peter.fischer@fau.de,sklearn/tests/test_common.py|sklearn/tests/test_common.py,7,0.0067178502879078695,0,10,false,Common test for sample weight Trying to address #5444 I wrote a common test that goes through all classifiers and regressors and checks whether sample weights correspond to data augmentation If the coef_ attribute is available then it is also comparedThe estimators that fail this test are the followingpython[(AdaBoostRegressor pred) (BaggingClassifier pred) (BaggingRegressor pred) (DecisionTreeRegressor pred) (ExtraTreeClassifier pred) (ExtraTreeRegressor pred) (ExtraTreesClassifier pred) (ExtraTreesRegressor pred) (GradientBoostingRegressor pred) (LogisticRegressionCV coef_) (Perceptron coef_) (Perceptron pred) (RandomForestRegressor pred) (RidgeCV coef_) (RidgeCV pred) (RidgeClassifierCV coef_) (SGDClassifier coef_) (SGDClassifier pred) (SGDRegressor coef_) (SGDRegressor pred)],,3115,0.7473515248796148,0.30998080614203455,46260,528.1884997838305,39.364461738002596,135.430177258971,7609,78,1889,332,travis,eickenberg,eickenberg,true,,14,0.5714285714285714,13,9,1383,false,true,false,false,0,13,0,27,0,0,250
9937897,scikit-learn/scikit-learn,python,5458,1445275830,1445279010,1445279010,53,53,commits_in_master,false,false,false,46,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,27,23,27,8.670770877842136,0.1812696685457151,32,tom.dupre-la-tour@m4x.org,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,25,0.024108003857280617,4,0,false,[MRG] Remove shuffling in LabelKFold This PR removes shuffling from LabelKFold in lack of a consensus on the best way to shuffle in that case Better to back-paddle now and not ship that with release 017 See also #5396 and #5390CC @ogrisel @GaelVaroquaux @JeanKossaifi @andreasvc ,,3112,0.7477506426735219,0.3105110896817743,46266,528.422599749276,39.35935676306575,135.41261401461117,7609,77,1889,314,travis,glouppe,GaelVaroquaux,false,GaelVaroquaux,57,0.9649122807017544,163,26,1803,true,true,false,false,17,170,35,135,79,0,6
9937676,scikit-learn/scikit-learn,python,5457,1445275196,1445448466,1445448466,2887,2887,commits_in_master,false,false,false,16,2,1,0,14,0,14,0,7,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,20,0,4.449950615776577,0.09302991447138398,6,peter.fischer@fau.de,sklearn/metrics/regression.py,6,0.005791505791505791,0,7,false,[MRG] Use uniform_average as default multioutput parameter of r2_score function Partly fixed metrics deprecated code (#5434),,3111,0.7476695596271296,0.3108108108108108,46266,528.422599749276,39.35935676306575,135.41261401461117,7609,77,1889,331,travis,aabadie,agramfort,false,agramfort,2,1.0,1,0,1364,false,false,false,false,1,4,2,0,0,0,38
9937652,scikit-learn/scikit-learn,python,5456,1445275115,1445449434,1445449434,2905,2905,commits_in_master,false,false,false,14,3,1,0,6,0,6,0,4,0,1,0,2,1,0,0,0,1,1,2,1,0,0,33,0,70,0,0,0.0,1,gael.varoquaux@normalesup.org,sklearn/metrics/metrics.py,1,0.0009652509652509653,0,5,false,[MRG] removing deprecated files in metrics Partly fixed metrics deprecated code: removed metricspy file,,3110,0.747588424437299,0.3108108108108108,46266,528.422599749276,39.35935676306575,135.41261401461117,7609,77,1889,332,travis,aabadie,amueller,false,amueller,1,1.0,1,0,1364,false,false,false,false,1,4,1,0,0,0,1521
9936725,scikit-learn/scikit-learn,python,5454,1445272347,1445274090,1445274090,29,29,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.277782204260776,0.08943050653012302,2,gael.varoquaux@normalesup.org,sklearn/manifold/tests/test_spectral_embedding.py,2,0.001937984496124031,0,0,false,MAINT: remove unused import This is a trivial PR partially to test travis,,3109,0.747507237053715,0.312015503875969,46266,528.3145290277957,39.35935676306575,135.41261401461117,7609,77,1889,312,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,58,0.8103448275862069,699,3,2065,true,true,false,false,24,178,49,48,19,3,28
9935316,scikit-learn/scikit-learn,python,5452,1445267687,1445296771,1445296771,484,484,commits_in_master,false,false,false,48,2,2,0,3,1,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,26,0,26,0,8.688691164999405,0.18164448722931126,15,olivier.grisel@ensta.org,sklearn/linear_model/base.py|sklearn/linear_model/base.py,15,0.014548981571290009,0,1,false,Deprecate residues_ in LinearRegression Residues can be empty if the rank of X does not satisfy theconditions described in scipylinalglstsq documentationAs residues are not that useful (ie were not doingstat testing) this property is deprecated and will beremoved in sklearn 019see issue https://githubcom/scikit-learn/scikit-learn/issues/5313,,3108,0.7474259974259975,0.3123181377303589,46289,529.0457776145521,39.36140335716909,135.43174404286114,7609,77,1889,323,travis,MaryanMorel,GaelVaroquaux,false,GaelVaroquaux,1,0.0,9,11,791,false,true,false,false,0,0,2,0,0,0,130
9935291,scikit-learn/scikit-learn,python,5451,1445267592,1445429954,1445429954,2706,2706,commits_in_master,false,false,false,59,3,1,11,12,0,23,0,6,0,0,8,21,8,0,0,0,0,21,21,21,0,0,94,7,406,210,35.47723892115631,0.7416818885339727,55,tom.dupre-la-tour@m4x.org,sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/mean_shift_.py|sklearn/cross_validation.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tests/test_cross_validation.py,24,0.006789524733268671,0,7,false,Removed deprecation warnings I removed the following deprecated warnings:* pooling_func in transform of AgglomerationTransform in file cluster/_feature_agglomerationpy* random_state parameter in cluster/dbscan_py* n_components in cluster/hierarchicalpy* max_iterations in cluster/mean_shift_py* support for class_weightsubsample in utils/class_weightpy* allow_lists and allow_nd/ in sklearn/cross_validationpysklearnteststest_cross_validationtrain_test_split_mock_pandas is failing to return an ndarray I am not sure how to palliate this though,,3107,0.7473447055037014,0.3123181377303589,46289,529.0457776145521,39.36140335716909,135.43174404286114,7609,77,1889,331,travis,zermelozf,amueller,false,amueller,0,0,14,22,1531,false,true,false,false,1,1,0,0,0,0,186
9934759,scikit-learn/scikit-learn,python,5449,1445265290,1445431348,1445431348,2767,2767,commits_in_master,false,false,false,13,7,2,18,24,0,42,0,8,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,23,16,48,8.96576412409387,0.1874369271883287,16,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,16,0.015549076773566569,0,3,false,#5433 max abs scaler 1 row csr fix an attempt to fix #5433,,3106,0.7472633612363168,0.3119533527696793,46289,529.0457776145521,39.36140335716909,135.43174404286114,7609,77,1889,332,travis,Jeffrey04,ogrisel,false,ogrisel,0,0,1,2,2467,false,false,false,false,1,4,0,0,2,0,14
9934654,scikit-learn/scikit-learn,python,5448,1445264892,,1445267019,35,,unknown,false,false,false,49,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.336630290535014,0.09066094587803612,15,olivier.grisel@ensta.org,sklearn/linear_model/base.py,15,0.014619883040935672,0,0,false,[MRG] Deprecate residues_ in LinearRegression Residues can be empty if the rank of X does not satisfy theconditions described in scipylinalglstsq documentationAs residues are not that useful (ie were not doingstat testing) this property is deprecated and will beremoved in sklearn 019See issue https://githubcom/scikit-learn/scikit-learn/issues/5313,,3105,0.7475040257648953,0.31189083820662766,46289,529.0457776145521,39.36140335716909,135.43174404286114,7609,76,1889,313,travis,MaryanMorel,MaryanMorel,true,,0,0,9,11,791,false,true,false,false,0,0,0,0,0,0,-1
9933577,scikit-learn/scikit-learn,python,5446,1445260328,1445440287,1445440287,2999,2999,commits_in_master,false,false,false,10,3,2,0,3,0,3,0,3,0,0,2,2,1,0,1,0,0,2,2,1,0,1,8,0,23,0,9.216400645589044,0.19267669691459002,15,olivier.grisel@ensta.org,sklearn/linear_model/base.py|.gitignore,15,0.014634146341463415,0,0,false,Charles1 Updated doc in the linear_model/basepy (class LinearRegression)Issue #5313 ,,3104,0.7474226804123711,0.31121951219512195,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,334,travis,deepcharles,amueller,false,amueller,0,0,3,0,13,false,false,false,false,1,0,0,0,0,0,26
9933545,scikit-learn/scikit-learn,python,5445,1445260195,1445301092,1445301092,681,681,commits_in_master,false,false,false,37,3,1,11,3,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,97,34,133,49,8.770661887652171,0.183358148940312,2,gael.varoquaux@normalesup.org,sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/tests/test_supervised.py,2,0.001951219512195122,0,0,false,[MRG] Raise ValueError for metricsclustersupervised with too many classes Fix for #4976- Add max_n_classes param to clustersupervised metric- Add testing for the Value Error- Check that n_clustersn_classes are not too high in contingency matrix,,3103,0.7473412826297132,0.31121951219512195,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,325,travis,tomMoral,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,1002,false,false,false,false,0,1,0,0,0,0,76
9933430,scikit-learn/scikit-learn,python,5443,1445259711,1445277991,1445277991,304,304,commits_in_master,false,false,false,137,3,1,2,3,0,5,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,14,0,30,4.455450882842651,0.09314499145413938,5,olivier.grisel@ensta.org,sklearn/manifold/spectral_embedding_.py,5,0.004878048780487805,0,2,false,Optimize sklearnmanifold_graph_is_connected Fix #5024This is a naive fix where I use a temporary array to store the nodes to be explored and another one with the nodes to add in the current loop This can probably be optimized by using a single integer array but the code would be less intuitive and the memory saved would be very smallI can switch to cython if needed but the current proposition seems optimized enough I did not add doc since variable names are self explanatoryI tested it with a degenerated graph using the following code:pythonsize  2000a  npzeros((size size) dtypefloat)for i in range(size - 1):    a[i i + 1]  1_graph_connected_component(a 0)Before optimization memory consumption looks like:[before](https://cloudgithubusercontentcom/assets/1647301/10574882/15541f52-7659-11e5-8c4a-a433326b5446png)And after:[after](https://cloudgithubusercontentcom/assets/1647301/10574887/1ad61cb4-7659-11e5-8911-bdea60722f22png)It is faster and consumes less memory,,3102,0.7472598323662153,0.31121951219512195,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,319,travis,AlexandreAbraham,GaelVaroquaux,false,GaelVaroquaux,3,0.6666666666666666,25,2,1281,false,true,false,true,0,2,0,0,0,0,21
9933218,scikit-learn/scikit-learn,python,5440,1445258868,1445272794,1445272794,232,232,commits_in_master,false,false,false,17,1,1,0,2,1,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.181592171840259,0.08741974209852654,9,trev.stephens@gmail.com,sklearn/discriminant_analysis.py,9,0.00878048780487805,0,0,false,MAINT: deprecation warns from StandardScaler std_ Fixes #5430 No more DeprecationWarning for accessing std_ on my side,,3101,0.7471783295711061,0.31121951219512195,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,317,travis,giorgiop,GaelVaroquaux,false,GaelVaroquaux,7,0.8571428571428571,2,7,1060,true,true,false,false,3,131,7,50,59,4,13
9933128,scikit-learn/scikit-learn,python,5439,1445258506,1445281320,1445281320,380,380,commits_in_master,false,true,false,13,3,1,0,4,0,4,0,4,0,0,1,3,1,0,0,0,0,3,3,3,0,0,11,0,22,8,4.644125674468838,0.09708939849974463,10,t3kcit@gmail.com,sklearn/metrics/classification.py,10,0.009765625,0,0,false,fixed issue #3450 for hamming loss added sample_weight support for hamming_loss in scikit-learn/sklearn/metrics/classificationpy,,3100,0.7470967741935484,0.3115234375,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,320,travis,achab,achab,true,achab,0,0,3,14,-30,false,false,false,false,0,1,0,0,0,0,3
9933099,scikit-learn/scikit-learn,python,5438,1445258401,1445418863,1445418863,2674,2674,commits_in_master,false,false,false,55,2,1,0,5,0,5,0,3,0,0,1,7,0,0,0,0,0,7,7,0,0,0,0,0,0,0,4.250638943242961,0.08886322360047891,2,gael.varoquaux@normalesup.org,doc/datasets/boston_house_prices.rst|doc/datasets/breast_cancer.rst|doc/datasets/diabetes.rst|doc/datasets/digits.rst|doc/datasets/index.rst|doc/datasets/iris.rst|doc/datasets/linnerud.rst,2,0.0,0,2,false,DOC : referencing datasets documentation in toc Fixes #5432I moved the datasets rst files from sklearn/datasets/descr to doc/datasets and referenced those file in the indexrst The involved files were : boston_house_pricesrst breast_cancerrst diabetesrst digitsrst irisrst and linnerudrst Each of those files were slightly changed in order to use the same title levels and reference,,3099,0.7470151661826395,0.3115234375,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,330,travis,aabadie,glouppe,false,glouppe,0,0,1,0,1364,false,false,false,false,0,1,0,0,0,0,112
9932895,scikit-learn/scikit-learn,python,5437,1445257604,1445271650,1445271650,234,234,commits_in_master,false,false,false,15,1,1,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,156,56,156,56,9.059413493711544,0.18939474693813557,5,rvraghav93@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,5,0.004873294346978557,0,1,false,[WIP] MAINT remove deprecated stuff that will no longer be supported in 018 Fixes #5434 ,,3098,0.7469335054874112,0.31189083820662766,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,76,1889,316,travis,rvraghav93,agramfort,false,agramfort,15,0.6666666666666666,28,43,745,true,false,false,false,3,123,9,79,41,0,-1
9932585,scikit-learn/scikit-learn,python,5436,1445256335,1445277477,1445277477,352,352,commits_in_master,false,false,false,44,3,3,7,26,0,33,0,8,0,0,3,3,3,0,0,0,0,3,3,3,0,0,47,8,47,8,27.323222327030575,0.5712152096550658,13,tom.dupre-la-tour@m4x.org,examples/model_selection/plot_roc.py|examples/model_selection/plot_roc.py|sklearn/metrics/tests/test_ranking.py|examples/model_selection/plot_roc.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py,10,0.007804878048780488,0,9,false,The code was raising an exception while plotting 2nd curve To compute macro-average roc we needed to compute the mean of fpr computed for each class If we do not set drop_intermediateFalse this leads to an error as our fpr arrays have different shapes,,3097,0.7468517920568292,0.3121951219512195,46289,529.0457776145521,39.36140335716909,135.43174404286114,7608,75,1889,335,travis,Mbompr,Mbompr,true,Mbompr,0,0,0,0,1206,true,false,false,false,0,0,0,0,6,0,6
9914702,scikit-learn/scikit-learn,python,5431,1445131126,1445510173,1445510173,6317,6317,commits_in_master,false,false,false,15,20,2,31,20,4,55,0,4,0,0,1,6,1,0,0,0,0,6,6,6,0,0,34,0,132,5,9.214662196471576,0.19264406250762972,36,tom.dupre-la-tour@m4x.org,sklearn/utils/estimator_checks.py|sklearn/utils/estimator_checks.py,36,0.035538005923000986,0,0,false,Add check to regression models to raise error when targets are NaN Addressing issue #5322,,3096,0.7467700258397932,0.3109575518262586,46289,529.0457776145521,39.36140335716909,135.43174404286114,7586,74,1887,327,travis,hlin117,GaelVaroquaux,false,GaelVaroquaux,7,0.42857142857142855,18,20,1215,true,true,true,false,5,36,8,8,16,0,80
9899818,scikit-learn/scikit-learn,python,5429,1445035580,1445099706,1445099706,1068,1068,commits_in_master,false,false,false,11,2,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,18,0,38,4.524643221757332,0.09459300226967174,6,trev.stephens@gmail.com,sklearn/tests/test_discriminant_analysis.py,6,0.005928853754940711,0,0,true,FIX skip LDA deprecation test on python33 that has no reload ,,3095,0.7466882067851374,0.31126482213438733,46292,528.9034822431522,39.35885250151214,135.42296725136094,7576,73,1886,307,travis,amueller,ogrisel,false,ogrisel,336,0.8482142857142857,1252,40,1820,true,true,true,false,127,1107,86,429,48,11,0
9895934,scikit-learn/scikit-learn,python,5425,1445023357,1455208093,1455208093,169745,169745,commits_in_master,false,false,false,4,2,0,0,4,0,4,0,3,0,0,0,2,0,0,0,0,0,2,2,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,[MRG] DOC update doc/readme ,,3093,0.7468477206595538,0.3095944609297725,46290,528.9263339814215,39.36055303521279,135.42881831929142,7574,73,1886,447,travis,amueller,jnothman,false,jnothman,335,0.8477611940298507,1252,40,1820,true,true,false,false,125,1099,85,422,48,11,1099
9895679,scikit-learn/scikit-learn,python,5424,1445022491,,1445599940,9624,,unknown,false,false,false,8,8,1,1,4,0,5,0,2,0,0,1,12,1,0,0,0,0,12,12,13,0,0,17,0,112,4,4.613207637577698,0.09644536032447922,34,tom.dupre-la-tour@m4x.org,sklearn/utils/estimator_checks.py,34,0.033630069238377844,0,0,true,estimator_checks test for 1 sample and 1 feature ,,3092,0.7470892626131953,0.3095944609297725,46290,528.9263339814215,39.36055303521279,135.42881831929142,7574,73,1886,336,travis,sotlampr,sotlampr,true,,1,0.0,0,1,101,false,false,false,false,0,2,1,0,0,0,20
9895481,scikit-learn/scikit-learn,python,5423,1445021942,1445077445,1445077445,925,925,commits_in_master,false,false,false,13,2,2,2,6,0,8,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,34,0,34,0,28.124579960934568,0.5879824758400065,41,tom.dupre-la-tour@m4x.org,examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_random_forest_embedding.py|sklearn/utils/estimator_checks.py|examples/ensemble/plot_feature_transformation.py|examples/ensemble/plot_random_forest_embedding.py|sklearn/utils/estimator_checks.py,34,0.008902077151335312,0,4,true,[MRG] Fix broken examples using RandomTreeEmbeddings RandomTreeEmbeddings overwrites the fit_transform method of_LearntSelectorMixin,,3091,0.7470074409576188,0.3095944609297725,46290,528.9263339814215,39.36055303521279,135.42881831929142,7574,73,1886,308,travis,MechCoder,glouppe,false,glouppe,74,0.8648648648648649,86,41,1214,true,true,false,false,12,150,19,87,35,0,2
9894009,scikit-learn/scikit-learn,python,5421,1445017724,1445033900,1445033900,269,269,commits_in_master,false,false,false,18,1,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.790517875245853,0.10015216289515909,1,gael.varoquaux@normalesup.org,examples/linear_model/plot_theilsen.py,1,0.0009940357852882703,0,3,true,proposed fix for #5387 Added a fix for issue #5387 by using different line styles for each curve,,3090,0.7469255663430421,0.3081510934393638,46287,528.9606152915505,39.36310411130555,135.43759586925054,7573,73,1886,309,travis,shagunsodhani,MechCoder,false,MechCoder,0,0,66,68,1374,false,true,false,false,1,0,0,0,0,0,53
9892550,scikit-learn/scikit-learn,python,5420,1445013232,,1445268572,4255,,unknown,false,false,false,17,3,2,7,34,0,41,0,6,0,0,2,3,2,0,0,0,0,3,3,3,0,0,14,18,16,24,17.167692223182414,0.3589134938739543,15,mks542@nyu.edu,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,14,0.013916500994035786,2,5,true,[MRG] FIX+TST regression in multinomial logstc reg when class_weight is auto Fixes #5415@lesteve @MechCoder Please review,,3089,0.7471673680802848,0.3081510934393638,46287,528.9606152915505,39.36310411130555,135.43759586925054,7571,73,1886,319,travis,rvraghav93,rvraghav93,true,,14,0.7142857142857143,28,43,742,true,false,false,false,2,110,8,76,41,0,31
9891160,scikit-learn/scikit-learn,python,5417,1445008830,,1456326348,188625,,unknown,false,false,false,44,1,1,0,10,0,10,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.252603743747955,0.08890635082966906,9,trev.stephens@gmail.com,sklearn/discriminant_analysis.py,9,0.00893743793445879,0,2,true,Typo on doc for default value of solver for ldaLDA Fix a tiny tiny style issue on the documentation for the default value of the solver parameter for ldaLDA[sklearnldaLDAhtml](http://scikit-learnorg/stable/modules/generated/sklearnldaLDAhtml#sklearnldaLDA) displayed badly (cf screenshot): half the sentence is bold and half is not[bug_display_sklearn_lda_lda](https://cloudgithubusercontentcom/assets/11994719/10541397/f9729060-7410-11e5-96de-eccbc0047365png),,3088,0.7474093264248705,0.30784508440913605,46287,528.9606152915505,39.36310411130555,135.43759586925054,7571,73,1886,469,travis,Naereen,Naereen,true,,3,1.0,7,46,182,false,false,false,false,0,2,3,0,0,1,219
9888519,scikit-learn/scikit-learn,python,5416,1444997395,1445021248,1445021248,397,397,commits_in_master,false,false,false,10,2,2,10,11,0,21,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,58,38,58,38,13.138470215280442,0.27467723604442923,12,t3kcit@gmail.com,sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,12,0.011881188118811881,0,0,true,BUG: reset internal state of scaler before fitting Fixes #5408,,3087,0.7473275024295433,0.3069306930693069,46287,528.9606152915505,39.36310411130555,135.43759586925054,7570,73,1886,305,travis,giorgiop,amueller,false,amueller,6,0.8333333333333334,2,7,1057,true,true,false,false,3,125,6,48,58,4,33
9878720,scikit-learn/scikit-learn,python,5413,1444950461,1444990251,1444990251,663,663,commits_in_master,false,false,false,2,2,2,0,2,0,2,0,3,0,0,8,8,8,0,0,0,0,8,8,8,0,0,89,1,89,1,35.84025856893097,0.7492863440213768,86,vighneshbirodkar@nyu.edu,sklearn/cluster/k_means_.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/metrics/regression.py|sklearn/preprocessing/data.py|sklearn/mixture/tests/test_gmm.py|sklearn/utils/estimator_checks.py,32,0.012948207171314742,0,1,false,Cleanup tests ,,3085,0.7474878444084279,0.30378486055776893,46282,529.0393673566397,39.36735663973035,135.45222764789767,7568,73,1885,303,travis,amueller,ogrisel,false,ogrisel,334,0.8473053892215568,1252,40,1819,true,true,true,false,120,1070,83,416,44,11,102
9871671,scikit-learn/scikit-learn,python,5411,1444930715,1445018258,1445018258,1459,1459,commits_in_master,false,false,false,21,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.628690094877143,0.09677011914874166,2,gael.varoquaux@normalesup.org,examples/applications/plot_tomography_l1_reconstruction.py,2,0.0019860973187686196,0,0,false,[MRG] Fix plot_tomography_l1_reconstruction example numpy 110 raises an exception when doing inplace operationwith an int and float arrayFix #5404,,3084,0.7474059662775616,0.2999006951340616,46245,529.4626446102282,39.39885393015461,135.58222510541682,7564,73,1885,311,travis,lesteve,ogrisel,false,ogrisel,19,0.9473684210526315,4,0,1268,true,false,false,false,5,36,6,8,16,0,21
9871655,scikit-learn/scikit-learn,python,5410,1444930668,1444931156,1444931156,8,8,commits_in_master,false,false,false,28,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.2412900741145,0.08867090632632413,14,tom.duprelatour@orange.fr,sklearn/linear_model/ridge.py,14,0.013902681231380337,0,0,false,[MRG] DOC: correct docstring (trivial please review and merge) Trivial fix to correct a docstring that was wrong: None wasnt doing default 3-fold CV but LOO in RidgeCV,,3083,0.7473240350308141,0.2999006951340616,46245,529.4626446102282,39.39885393015461,135.58222510541682,7564,73,1885,306,travis,GaelVaroquaux,amueller,false,amueller,57,0.8070175438596491,698,3,2061,true,true,false,true,21,163,50,34,20,3,-1
9871673,scikit-learn/scikit-learn,python,5409,1444929593,1444947536,1444947536,299,299,commits_in_master,false,false,false,9,2,2,2,1,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,8.503744418221263,0.17778428533646742,2,olivier.grisel@ensta.org,examples/manifold/plot_lle_digits.py|examples/manifold/plot_lle_digits.py,2,0.0019860973187686196,0,0,false,[MRG] Fix missing import in plot_lle_digits example Fix #5406,,3082,0.7472420506164829,0.2999006951340616,46245,529.4626446102282,39.39885393015461,135.58222510541682,7564,73,1885,305,travis,lesteve,amueller,false,amueller,18,0.9444444444444444,4,0,1268,true,false,false,false,5,36,5,8,16,0,34
9865439,scikit-learn/scikit-learn,python,5402,1444907222,1444952834,1444952834,760,760,commits_in_master,false,false,false,9,1,1,6,1,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.7032874453314415,0.09832942573653916,3,olivier.grisel@ensta.org,sklearn/neighbors/approximate.py,3,0.003015075376884422,0,0,false,[MRG] fIX non deterministic LSHForest doctest failure Fix #5324,,3081,0.7471600129827978,0.29547738693467335,46245,529.4410206508811,39.39885393015461,135.56060114606984,7556,74,1885,307,travis,lesteve,jnothman,false,jnothman,17,0.9411764705882353,4,0,1268,true,false,false,false,5,32,4,6,10,0,300
9852918,scikit-learn/scikit-learn,python,5400,1444852794,1444854086,1444854086,21,21,github,false,false,false,22,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.4990673509403,0.09406067579563807,3,gael.varoquaux@normalesup.org,sklearn/neighbors/binary_tree.pxi,3,0.0030181086519114686,0,0,false,DOC wrong default value in docstring Default leaf_size in docstring (leaf_size20) of KDTree does not agree with function signature or code (leaf_size40),,3079,0.7473205586229296,0.2937625754527163,46209,529.6803652968036,39.40790755047718,135.60128979203185,7541,76,1884,306,travis,kwgoodman,amueller,false,amueller,2,1.0,26,3,1955,true,false,false,false,0,0,0,0,1,0,21
9848273,scikit-learn/scikit-learn,python,5399,1444838860,1444948628,1444948628,1829,1829,commits_in_master,false,false,false,57,2,0,0,9,0,9,0,4,0,0,0,6,0,0,0,0,0,6,6,5,0,0,0,0,148,0,0,0.0,0,,,0,0.0,0,2,false,[MRG] Update joblib to 091 Main change from 090b4 to 090 was to revert our attempt to make Python 2/3 compatible pickles which proved too fragile as far as long-term maintenance was concernedThis should fix #5241 tooShould I add an entry into whats_newrst  I guess there is no new feature per se since 090b4,,3078,0.7472384665367121,0.29107505070993916,46209,529.6803652968036,39.40790755047718,135.60128979203185,7541,75,1884,308,travis,lesteve,ogrisel,false,ogrisel,16,0.9375,4,0,1267,true,false,false,false,4,27,3,6,5,0,35
9829006,scikit-learn/scikit-learn,python,5398,1444830682,1444938346,1444938346,1794,1794,commits_in_master,false,false,false,25,6,2,70,39,0,109,0,7,0,0,8,10,6,0,0,0,0,10,10,7,0,1,112,8,141,27,57.960988778677105,1.2117733185670876,40,t3kcit@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/decomposition/fastica_.py|sklearn/ensemble/weight_boosting.py|sklearn/linear_model/least_angle.py|sklearn/preprocessing/data.py|doc/modules/linear_model.rst|doc/modules/preprocessing.rst|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/fastica_.py|sklearn/ensemble/weight_boosting.py|sklearn/linear_model/least_angle.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,26,0.007135575942915392,1,6,false,[MRG] FIX dtypes to conform to the stricter type cast rules of numpy 110 Explicitly specifies the dtypes to avoid failures in numpy v10@amueller ,,3077,0.7471563210919727,0.28950050968399593,46209,529.6803652968036,39.40790755047718,135.60128979203185,7538,75,1884,309,travis,rvraghav93,amueller,false,amueller,13,0.6923076923076923,28,43,740,true,false,false,false,2,87,7,52,24,0,1
9840400,scikit-learn/scikit-learn,python,5395,1444781306,1444925288,1444925288,2399,2399,commits_in_master,false,false,false,22,2,1,0,3,0,3,0,3,0,0,5,5,5,0,0,0,0,5,5,5,0,0,0,42,0,84,22.37776981946891,0.4683256543077979,23,t3kcit@gmail.com,sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sgd.py|sklearn/neighbors/tests/test_approximate.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py,13,0.008171603677221655,1,1,false,[MRG] Fixed warnings for DataDimensionalityWarning and decision function Continuation / rebase of #5277 with some fixesCloses some of #5089Thanks @hasancansaral ,,3075,0.7473170731707317,0.2900919305413687,46124,526.5805220709392,39.393807995837314,134.50698118116384,7534,74,1883,308,travis,amueller,ogrisel,false,ogrisel,332,0.8493975903614458,1251,40,1817,true,true,true,false,113,986,72,388,38,11,1318
9805320,scikit-learn/scikit-learn,python,5386,1444644285,1444644387,1444644387,1,1,github,false,false,false,28,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.933547653660902,0.10324992219162446,2,gael.varoquaux@normalesup.org,.mailmap,2,0.0020429009193054137,0,0,false,[MRG] Add me to mailmap I was under the wrong email for a long time and so my contributions arent properly mapped to me This should fix it,,3073,0.7474780344939799,0.2819203268641471,46087,526.5042202790374,39.360340226094124,134.5064768806822,7513,75,1882,308,travis,jmschrei,ogrisel,false,ogrisel,13,0.6923076923076923,35,5,936,false,true,false,false,8,168,19,92,0,0,1
9780577,scikit-learn/scikit-learn,python,5378,1444422639,1444744941,1444744941,5371,5371,commits_in_master,false,false,false,18,2,1,4,8,0,12,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,31,0,79,0,4.465005247592852,0.09354352520006459,6,t3kcit@gmail.com,sklearn/grid_search.py,6,0.005940594059405941,0,3,true,[MRG] Trying to make GridSearchCV docs more accurate (dont say classifier) Fixes #5376Help with wording appreciated ^^,,3072,0.7473958333333334,0.2712871287128713,46004,525.4108338405356,39.27919311364229,134.2926702025911,7486,75,1879,310,travis,amueller,ogrisel,false,ogrisel,331,0.8489425981873112,1250,40,1813,true,true,true,false,119,962,80,365,43,10,3103
9780940,scikit-learn/scikit-learn,python,5375,1444393137,1445256125,1445256125,14383,14383,commits_in_master,false,false,false,16,2,2,3,9,0,12,0,4,0,0,25,25,24,0,0,0,0,25,25,24,0,0,168,84,168,84,213.10301031675743,4.464587544778844,47,tom.dupre-la-tour@m4x.org,doc/modules/preprocessing.rst|sklearn/cluster/hierarchical.py|sklearn/datasets/base.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/incremental_pca.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/tests/test_image.py|sklearn/feature_selection/tests/test_chi2.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/regression_models.py|sklearn/isotonic.py|sklearn/linear_model/randomized_l1.py|sklearn/manifold/locally_linear.py|sklearn/neighbors/regression.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/svm/base.py|sklearn/tests/test_naive_bayes.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fast_dict.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_murmurhash.py|doc/modules/preprocessing.rst|sklearn/cluster/hierarchical.py|sklearn/datasets/base.py|sklearn/decomposition/factor_analysis.py|sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/tests/test_image.py|sklearn/feature_selection/tests/test_chi2.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/regression_models.py|sklearn/isotonic.py|sklearn/linear_model/randomized_l1.py|sklearn/manifold/locally_linear.py|sklearn/neighbors/regression.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/svm/base.py|sklearn/tests/test_naive_bayes.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fast_dict.py|sklearn/utils/tests/test_murmurhash.py,10,0.0029702970297029703,2,3,true,[MRG] FIX precision to float64 across the codebase Implementing https://githubcom/scikit-learn/scikit-learn/pull/5356#issuecomment-146455840@ogrisel @giorgiop Does this look okay,,3071,0.7473135786388798,0.2702970297029703,46004,525.4108338405356,39.27919311364229,134.2926702025911,7480,75,1879,319,travis,rvraghav93,glouppe,false,glouppe,12,0.6666666666666666,28,43,735,true,false,false,false,3,81,6,50,22,0,512
9770660,scikit-learn/scikit-learn,python,5372,1444354369,1450233112,1450233112,97979,97979,commits_in_master,false,false,false,69,39,12,162,115,0,277,0,8,6,1,8,20,10,0,0,7,1,13,21,12,0,0,1467,687,1911,1207,140.11361383119245,2.9354362189450796,21,tom.dupre-la-tour@m4x.org,doc/modules/feature_selection.rst|doc/modules/classes.rst|doc/modules/feature_selection.rst|examples/feature_selection/plot_mrmr.py|examples/plot_mRMR.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/tests/test_multivariate_filtering.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/tests/test_multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/tests/test_multivariate_filtering.py|sklearn/feature_selection/multivariate_filtering.py|examples/plot_mRMR.py|doc/modules/feature_selection.rst|examples/feature_selection/plot_rfe_digits.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/mutual_info.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/tests/test_mutual_info.py|sklearn/feature_selection/univariate_selection.py|doc/modules/feature_selection.rst|examples/feature_selection/plot_rfe_digits.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/mutual_info.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/tests/test_mutual_info.py|sklearn/feature_selection/univariate_selection.py,18,0.0,0,50,false,[WIP] ENH: Feature selection based on mutual information Hi This is my attempt to finish/rework #2547I tried to address code style issues and also added algorithms estimating mutual information with continuous variable involvedThere are places for trivial optimization but for now I tried to keep the code as transparent as possible It would be great if some of the core developers can start seriously reviewing with PR,,3070,0.7472312703583062,0.2662013958125623,46004,525.4108338405356,39.27919311364229,134.2926702025911,7476,75,1878,474,travis,nmayorov,MechCoder,false,MechCoder,6,0.3333333333333333,7,0,561,true,false,false,false,1,1,0,0,1,0,77
9770100,scikit-learn/scikit-learn,python,5371,1444352432,1444482398,1444482398,2166,2166,commits_in_master,false,false,false,70,1,1,0,10,0,10,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,0,8,0,4.370710905980221,0.0915681406334291,2,amueller@nyu.edu,sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pyx,2,0.0019940179461615153,0,4,false,Increase length of array indexing type in ArrayDataset int (32-bit on all platforms) will loop around once the number of elements exceed 2^31-1 While ints are used elsewhere for row index this is particularly significant here since this int is used as an index into a dense matrix ie as long as #rows * #cols is greater than 2^31 this will fail This is probably the root cause of #2393,,3069,0.747148908439231,0.2662013958125623,46004,525.4108338405356,39.27919311364229,134.2926702025911,7476,75,1878,311,travis,ylow,jnothman,false,jnothman,0,0,33,0,1065,false,false,false,false,0,0,0,0,0,0,80
9769202,scikit-learn/scikit-learn,python,5370,1444349676,,1444531844,3036,,unknown,false,false,false,39,7,7,0,7,3,10,3,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,469,11,469,11,27.4604578598721,0.5753075692402019,12,t3kcit@gmail.com,sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py|sklearn/utils/validation.py,12,0.0019940179461615153,0,0,false,Validating sample_weight Added a check_weights() function to util/validationpy in response to reviewers comments in PR #4974 Maybe a core contributor can guide us on when to deploy such a check whether theres a faster way to implement it etc,,3068,0.7473924380704041,0.2662013958125623,46004,525.4108338405356,39.27919311364229,134.2926702025911,7476,75,1878,311,travis,Fenugreek,Fenugreek,true,,1,0.0,0,0,991,false,false,false,false,0,7,1,0,0,0,123
9763205,scikit-learn/scikit-learn,python,5369,1444330887,1444356932,1444356932,434,434,commits_in_master,false,false,false,41,1,1,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.450946412085805,0.0932486667618067,5,larsmans@users.noreply.github.com,sklearn/feature_extraction/hashing.py,5,0.004975124378109453,0,0,false,correct optional arguments for FeatureHasher I noticed that the optional argumentsin the docstring for the FeatureHasher were incorrect (the default for non_negative was npfloat64)  This is just a quick change to correct that as well as document the other default arguments,,3067,0.7473100749918488,0.2656716417910448,45998,524.9576068524718,39.28431670942214,134.31018739945216,7474,76,1878,311,travis,joshloyal,jnothman,false,jnothman,2,0.5,2,1,1484,false,false,false,false,0,0,0,0,0,0,433
9756404,scikit-learn/scikit-learn,python,5365,1444306740,1444308053,1444308053,21,21,commits_in_master,false,false,false,20,1,1,0,0,3,3,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.998867338703673,0.1047278891323033,9,t3kcit@gmail.com,doc/modules/clustering.rst,9,0.00897308075772682,0,1,false,Fix few typos on links and doi - Add https for **Wikipedia entries**- Add http://dxdoiorg/ link for **doi entries**,,3065,0.7474714518760196,0.26321036889332006,45998,524.9576068524718,39.28431670942214,134.31018739945216,7469,76,1878,307,travis,Naereen,agramfort,false,agramfort,2,1.0,7,46,174,false,false,false,false,0,2,2,0,0,1,-1
9748152,scikit-learn/scikit-learn,python,5363,1444264504,1444294422,1444294422,498,498,commits_in_master,false,false,false,27,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.448504186027737,0.09319731792954486,7,t3kcit@gmail.com,doc/modules/clustering.rst,7,0.006958250497017893,0,0,false,Missing link for inertia (about K-Means) Fixing the rST link tag issue so the display issue get fixed but the link still does not existAny suggestions,,3064,0.7473890339425587,0.26043737574552683,45838,526.4845761158864,39.39962476547842,134.7353723984467,7466,77,1877,310,travis,Naereen,agramfort,false,agramfort,1,1.0,7,46,173,false,false,false,false,0,1,1,0,0,1,0
9747309,scikit-learn/scikit-learn,python,5362,1444261877,1444660294,1444660294,6640,6640,commits_in_master,false,false,false,34,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,13,1,13,8.326756119390037,0.1744477030769142,17,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,17,0.016898608349900597,0,0,false,[MRG] Lasso and ElasticNet should handle non-integer dtypes for fit_interceptFalse Quick fix for https://githubcom/scikit-learn/scikit-learn/issues/5351The reason that it was not failing for fit_interceptTrue was that y was being centered and hence coaxed to npfloat64,,3063,0.7473065621939275,0.26043737574552683,45838,526.4845761158864,39.39962476547842,134.7353723984467,7466,77,1877,313,travis,MechCoder,ogrisel,false,ogrisel,73,0.863013698630137,86,41,1205,true,true,true,false,8,116,15,66,32,0,6492
9741561,scikit-learn/scikit-learn,python,5360,1444245648,1444751776,1444751776,8435,8435,commits_in_master,false,false,false,35,3,2,15,5,0,20,0,6,0,0,3,3,2,0,0,0,0,3,3,2,0,0,95,43,109,43,22.91523599730831,0.4800801450023358,71,trev.stephens@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,65,0.008955223880597015,0,2,false,[WIP] temporary fix for sparse ridge with intercept fitting Temporary fix for #4710 using sag solver that can directly fit the interceptIf fit_interceptTrue and X is sparse then the solver is switch to sag,,3062,0.7472240365774004,0.25870646766169153,45838,526.4845761158864,39.39962476547842,134.7353723984467,7462,77,1877,312,travis,TomDLT,TomDLT,true,TomDLT,21,0.6666666666666666,6,4,231,true,false,false,false,6,102,9,70,29,0,155
9739558,scikit-learn/scikit-learn,python,5358,1444240064,1446447625,1446447625,36792,36792,commit_sha_in_comments,false,true,false,79,4,2,51,14,0,65,0,6,0,0,6,6,5,0,0,0,0,6,6,5,0,0,126,168,143,168,41.526282217391234,0.8699863964165517,85,trev.stephens@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/utils/estimator_checks.py|sklearn/utils/extmath.py|doc/whats_new.rst|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/externals/joblib/format_stack.py|sklearn/utils/estimator_checks.py,65,0.005005005005005005,0,3,false,[WIP] Fix fit_transform stability issue and scale issue in PLS This PR fix stability issue and sign indeterminacy in PLS and CCA (see bug #2821)The signs of x_loadings_ x_score_ x_weights_ x_rotations_ can differ columnwise from R implementation as there is a sign indeterminacy that we seek to raise (cf SVD with svd_flip)- [ ] Since test_pls is based on R output I still need to change it so that it does not fail because of sign differences,,3060,0.7473856209150327,0.25425425425425424,45838,526.4845761158864,39.39962476547842,134.7353723984467,7461,77,1877,365,travis,arthurmensch,amueller,false,amueller,8,0.25,1,1,914,true,true,false,false,2,32,5,16,27,0,23
9739040,scikit-learn/scikit-learn,python,5357,1444238699,1445265159,1445265159,17107,17107,merged_in_comments,false,false,false,81,3,1,40,39,0,79,0,7,0,0,2,12,2,0,0,0,0,12,12,11,0,0,73,51,646,354,8.64288612464011,0.18107071444583056,13,olivier.grisel@ensta.org,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py,13,0.013026052104208416,0,18,false,[WIP]: BUG fix standardization in center_data for linear models Fixes #2601TO DO:- [x] deprecate normalize and introduce standardize behaviour- [x] fix tests in linear_model/test_basepy + deprecation test- [ ] introduce the same behaviour for every class calling center_data internally + related tests- [ ] change center_data behaviour wrt fit_intercept Currently [input data is not touched](https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/basepy#L119) if we not are fitting the intercept- [ ] make sense of [this old test](https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/tests/test_basepy#L63) on sample_weights with unused variables,,3059,0.7473030402092187,0.2535070140280561,45838,526.4845761158864,39.39962476547842,134.7353723984467,7461,77,1877,481,travis,giorgiop,giorgiop,true,giorgiop,5,0.8,2,7,1048,true,true,false,false,2,75,5,35,47,2,32
9738748,scikit-learn/scikit-learn,python,5356,1444237841,1444302903,1444302903,1084,1084,commits_in_master,false,true,false,14,2,2,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,8.579731174925955,0.17974760180722585,9,t3kcit@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py,9,0.009054325955734407,1,0,false,FIX Use float64 instead of float Attempting to fix  #4914 (#5235)Thanks to @giorgiop,,3058,0.7472204054937868,0.2535211267605634,45838,526.4845761158864,39.39962476547842,134.7353723984467,7460,77,1877,309,travis,rvraghav93,ogrisel,false,ogrisel,11,0.6363636363636364,28,43,733,true,false,true,true,2,81,5,51,19,0,49
9734116,scikit-learn/scikit-learn,python,5355,1444219204,,1444234078,247,,unknown,false,false,false,40,15,14,3,9,0,12,0,4,0,0,7,8,7,0,0,0,0,8,8,7,0,0,161,34,161,34,67.00124118425094,1.4036912932269279,14,tom.dupre-la-tour@m4x.org,sklearn/datasets/olivetti_faces.py|sklearn/datasets/california_housing.py|sklearn/datasets/base.py|sklearn/datasets/rcv1.py|sklearn/datasets/species_distributions.py|sklearn/datasets/covtype.py|sklearn/datasets/twenty_newsgroups.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/base.py|sklearn/datasets/rcv1.py|sklearn/datasets/species_distributions.py|sklearn/datasets/covtype.py|sklearn/datasets/twenty_newsgroups.py|sklearn/datasets/california_housing.py,11,0.003009027081243731,0,1,false,[MRG] use distinct filenames for pickled datasets under Python 2 and 3 This is a fix for #3596Note that I changed the covtype loader to be consistent with the others even though that might re-trigger a useless download once,,3057,0.7474648348053647,0.25175526579739216,45822,526.515647505565,39.39155863995461,134.73877176901925,7458,79,1877,306,travis,ogrisel,ogrisel,true,,135,0.8740740740740741,1220,124,2324,true,true,false,false,22,336,67,211,65,2,14
9729408,scikit-learn/scikit-learn,python,5349,1444190039,1444441849,1444441849,4196,4196,commits_in_master,false,false,false,18,6,2,1,12,0,13,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,8,10,12,14,8.924031022089903,0.18696048647437666,3,olivier.grisel@ensta.org,sklearn/tests/test_naive_bayes.py|sklearn/naive_bayes.py,3,0.0029940119760479044,0,1,false,Naive bayes scale (Fixes #5314) This modifies the epsilon parameter in GaussianNB so that classifications will be scale-invariant,,3056,0.7473821989528796,0.25249500998003993,45822,526.515647505565,39.39155863995461,134.73877176901925,7453,80,1876,316,travis,jakevdp,amueller,false,amueller,47,0.8723404255319149,1843,0,1609,true,true,false,true,2,14,2,3,1,0,15
9722286,scikit-learn/scikit-learn,python,5348,1444165382,,1444745006,9660,,unknown,false,true,false,23,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,15,0,15,0,4.44064512782544,0.09303256847833019,6,t3kcit@gmail.com,sklearn/grid_search.py,6,0.005988023952095809,0,0,false,DOC: fix outdated information in doc string of GridSearchCV The old docstring implied that the wrapped object had to be a supervised estimator,,3055,0.7476268412438625,0.25149700598802394,45813,526.5535983236199,39.37746927727938,134.74341344159953,7449,78,1876,312,travis,jakevdp,ogrisel,false,,46,0.8913043478260869,1843,0,1609,false,true,false,false,2,10,0,1,0,0,8751
9716889,scikit-learn/scikit-learn,python,5347,1444149680,1444153336,1444153336,60,60,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.333011901260825,0.09077762586359377,6,t3kcit@gmail.com,sklearn/calibration.py,6,0.006097560975609756,0,0,false,DOC: Fix typo in CalibratedClassifierCV ,,3054,0.7475442043222004,0.24898373983739838,45813,526.5535983236199,39.37746927727938,134.74341344159953,7444,78,1876,305,travis,ariddell,glouppe,false,glouppe,1,1.0,68,167,2616,false,true,false,false,0,0,0,0,0,0,60
9710945,scikit-learn/scikit-learn,python,5346,1444123756,1444125329,1444125329,26,26,commits_in_master,false,false,false,38,1,1,0,1,0,1,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.440324591805019,0.09302583674983744,2,gael.varoquaux@normalesup.org,doc/modules/naive_bayes.rst,2,0.002026342451874367,0,1,false,Fixed typo in the final  note:: of naive_bayesrst The [rST **note** admonition](http://docutilssourceforgenet/docs/ref/rst/directiveshtml#specific-admonitions) syntax is  note:: the two   were missing (the last remark displayed badly on http://scikit-learnorg/stable/modules/naive_bayeshtml)Demo of the *old* page (now fixed):[github_scikit-learn_naive_bayes_pull-request_06-10-15_08-26-28](https://cloudgithubusercontentcom/assets/11994719/10301570/ff567d80-6c03-11e5-959e-9e3f45e9c25epng),,3053,0.7474615132656404,0.24924012158054712,45813,526.5535983236199,39.37746927727938,134.74341344159953,7444,78,1876,305,travis,Naereen,GaelVaroquaux,false,GaelVaroquaux,0,0,7,46,172,false,false,true,false,0,0,0,0,0,0,25
9708100,scikit-learn/scikit-learn,python,5345,1444104020,1445604913,1445604913,25014,25014,merged_in_comments,false,false,false,18,15,12,16,10,0,26,0,5,0,0,5,5,3,0,0,0,0,5,5,3,0,0,118,0,118,0,63.44446369172037,1.3291763248478599,73,trev.stephens@gmail.com,doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/modules/linear_model.rst|doc/whats_new.rst|sklearn/decomposition/cdnmf_fast.c|sklearn/decomposition/cdnmf_fast.pyx|sklearn/decomposition/nmf.py|sklearn/decomposition/nmf.py|doc/modules/linear_model.rst|doc/modules/linear_model.rst,63,0.0070921985815602835,0,4,false,Add MultiTaskElasticNet and MultiTaskElasticNetCV to Narrative This pull request is intended to cover ticket #4803  - documentation only ,,3052,0.7473787680209698,0.24924012158054712,45813,526.5535983236199,39.37746927727938,134.74341344159953,7441,78,1875,347,travis,JohnBoersma,agramfort,false,agramfort,0,0,0,4,361,true,false,true,false,0,2,0,0,14,0,247
9659293,scikit-learn/scikit-learn,python,5337,1443815863,1444045838,1444045838,3832,3832,commits_in_master,false,false,false,113,3,2,5,7,0,12,0,5,0,0,7,8,7,0,0,0,0,8,8,8,0,0,217,26,220,26,53.303588457596646,1.1167182122788195,24,t3kcit@gmail.com,sklearn/covariance/tests/test_covariance.py|sklearn/decomposition/dict_learning.py|sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/covariance/tests/test_covariance.py|sklearn/decomposition/dict_learning.py|sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,15,0.007106598984771574,1,2,true,[MRG] FIX consistency of memory layout for linear CD solver This should fix #5013This PR is fixing several related consistency issues in the handling of the memory layout in models using the linear coordinate descent solvers (Gram or not)Also I made the expectation wrt memory layout explicit in the Cython prototypes directly which should prevent re-introducing regressions in the future@arthurmensch I would appreciate a review on this as I touched a lot of code you changed when introducing the ability to skip input checks Especially if you have your benchmark scripts at end it would be great if you could check that I do not re-introduce unwanted redundant input checks,,3050,0.7475409836065574,0.24873096446700507,45797,526.5410398061008,39.347555516736904,134.72498198571958,7408,82,1872,305,travis,ogrisel,ogrisel,true,ogrisel,134,0.8731343283582089,1220,124,2319,true,true,false,false,21,319,64,209,44,2,2
9661131,scikit-learn/scikit-learn,python,5336,1443803148,1444232375,1444232375,7153,7153,commits_in_master,false,false,false,29,1,1,3,2,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,10,2,10,8.950712181650466,0.18751574717545588,2,gael.varoquaux@normalesup.org,sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py,2,0.002036659877800407,0,6,true,remove numpys RuntimeWarning from corner case of PCAfit When n_samples  n_features and n_components  n_samples we are calculating a mean over an empty array That should be 0,,3049,0.7474581830108232,0.24643584521384929,45800,526.5065502183406,39.34497816593886,134.71615720524017,7405,82,1872,310,travis,giorgiop,agramfort,false,agramfort,4,0.75,2,7,1043,true,true,false,false,1,68,4,32,43,2,317
9641898,scikit-learn/scikit-learn,python,5335,1443737616,1443979408,1443979408,4029,4029,commits_in_master,false,false,false,21,2,2,1,7,0,8,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,6,15,6,13.31614376721564,0.27945571844531736,4,olivier.grisel@ensta.org,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,3,0.003061224489795918,0,6,false,fixes scikit-learn/scikit-learn#5329 Fixed by: * Adding to the docstring (in alpha and learning_rate) * Raise an error if alpha0 with learning_rateoptimal,,3048,0.7473753280839895,0.24693877551020407,45768,526.8528229330537,39.328788673308864,134.7229505331236,7394,82,1871,307,travis,rrohan,agramfort,false,agramfort,0,0,0,0,1253,true,false,false,false,1,2,0,0,1,0,21
9634205,scikit-learn/scikit-learn,python,5334,1443714275,1443722158,1443722158,131,131,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,3,0,0,8,8,8,0,0,0,0,8,8,8,0,0,50,2,50,2,33.87346703823889,0.710875000433591,20,trev.stephens@gmail.com,sklearn/cluster/hierarchical.py|sklearn/covariance/graph_lasso_.py|sklearn/discriminant_analysis.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/manifold/spectral_embedding_.py|sklearn/mixture/gmm.py|sklearn/utils/validation.py,10,0.005107252298263534,0,0,false,[MRG] ENH better error message for estimators with ensure_min_* checks As discussed in #5234,,3047,0.7472924187725631,0.24616956077630234,45767,526.8643345642057,39.329647999650405,134.72589420324687,7391,81,1871,305,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,133,0.8721804511278195,1220,124,2318,true,true,true,true,21,322,63,217,44,2,108
9631292,scikit-learn/scikit-learn,python,5332,1443701261,1443805961,1443805961,1745,1745,commits_in_master,false,false,false,72,3,2,0,10,0,10,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,153,0,207,0,26.835319163343456,0.5631710949558926,66,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/decomposition/cdnmf_fast.c|sklearn/decomposition/cdnmf_fast.pyx|sklearn/decomposition/nmf.py|doc/whats_new.rst|sklearn/decomposition/cdnmf_fast.c|sklearn/decomposition/cdnmf_fast.pyx|sklearn/decomposition/nmf.py,61,0.011270491803278689,2,5,false,MAINT remove shuffling from NMF CD solver Unless Im very mistaken the iterations of _update_cdnmf_fast are independent of each other so shuffling only serves to thrash the cache and add complexityPing @TomDLT @mblondel (Also thanks for this solver it works miracles)Also changed the integer types inside the pyx file removed the cimport numpy and used Cythons own min and max to reduce the amount of generated C code by 3kLOC,,3045,0.7474548440065681,0.24385245901639344,45767,526.8643345642057,39.329647999650405,134.72589420324687,7391,81,1871,306,travis,larsmans,GaelVaroquaux,false,GaelVaroquaux,137,0.7372262773722628,155,38,1901,true,true,true,false,14,85,23,15,29,1,22
9456781,scikit-learn/scikit-learn,python,5325,1443444323,1443743302,1443743302,4982,4982,commit_sha_in_comments,false,false,false,31,10,6,20,16,0,36,0,4,2,0,3,5,3,0,1,2,0,3,5,3,0,1,317,36,317,36,71.22176903884383,1.4945376794603686,5,tom.dupre-la-tour@m4x.org,sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/breast_cancer.csv|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/tests/test_base.py|sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/breast_cancer.csv|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/tests/test_base.py|sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/breast_cancer.csv|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/tests/test_base.py|sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/breast_cancer.csv|sklearn/datasets/descr/breast_cancer.rst|sklearn/datasets/tests/test_base.py,5,0.0020964360587002098,0,1,false,add Wisconsin Breast Cancer Dataset Add a (classical) binary classification task #5105 issue resolvedI have successfully added Wisconsin Breast cancer (diagnostic) dataset and its documentation file to sklearn datasets module ,,3043,0.7476174827472889,0.23375262054507337,45687,526.8238229693349,39.37662792479261,134.98369339199334,7359,81,1868,311,travis,ishank08,MechCoder,false,MechCoder,0,0,0,1,298,true,false,false,false,0,3,0,0,7,0,15
9556941,scikit-learn/scikit-learn,python,5319,1443306009,1445425366,1445425366,35322,35322,commits_in_master,false,false,false,48,1,1,0,3,0,3,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.612792217761934,0.09679613227813585,11,michigraber@gmail.com,doc/related_projects.rst,11,0.011652542372881356,0,3,false,Add k-modes clustering project to related projects The kmodes project implements k-modes for clustering categorical data and k-prototypes for mixed numerical and categorical data It follows the sklearn APII previously discussed the project with amueller on gitter and he suggested including it in the related projects page,,3040,0.7480263157894737,0.2341101694915254,45687,526.8238229693349,39.37662792479261,134.98369339199334,7338,81,1866,347,travis,nicodv,ogrisel,false,ogrisel,0,0,7,0,941,false,true,false,false,0,0,0,0,0,0,2480
9556410,scikit-learn/scikit-learn,python,5317,1443302644,1443555045,1443555045,4206,4206,commits_in_master,false,false,false,101,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,30,0,4.384118211643277,0.09199757246882967,5,olivier.grisel@ensta.org,sklearn/decomposition/dict_learning.py,5,0.005296610169491525,0,1,false,[MRG+1] Verbosity in sparse_encode Addresses #5309 A couple of notes:* I did not pass in max(0 verbose-1) into Lars nor LassoLars (lines 106 and 132 respectively) because they already provide code that sets verbose equal to max(0 verbose-1) See Larsfit in least_anglepy* I did not add in a verbose option for Lasso because the original issue in #5309 did not specify this Same thing goes for orthogonal_mp_gram* My choice for the description of the parameter verbose in sparse_encode and _sparse_encode were motivated by a hrefhttp://scikit-learnorg/stable/modules/generated/sklearngrid_searchGridSearchCVhtml#sklearngrid_searchGridSearchCVGridSearchCV/as description of its verbose parameterPlease let me know what you think Thanks,,3039,0.7479434024350116,0.2341101694915254,45687,526.8238229693349,39.37662792479261,134.98369339199334,7338,81,1866,307,travis,hlin117,GaelVaroquaux,false,GaelVaroquaux,5,0.4,18,20,1194,true,true,true,false,3,25,6,8,14,0,35
9545461,scikit-learn/scikit-learn,python,5311,1443212035,1443265761,1443265761,895,895,github,false,false,false,10,1,1,0,1,2,3,0,1,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.81426330607692,0.10119263564148451,4,t3kcit@gmail.com,doc/testimonials/images/dataiku_logo.png|doc/testimonials/testimonials.rst,4,0.00423728813559322,0,0,true,Dataiku testimonial Hi allPlease find Dataikus testimonialRegards Jeremy,,3036,0.7483530961791831,0.23516949152542374,45687,526.8238229693349,39.37662792479261,134.98369339199334,7326,80,1865,302,travis,jereze,GaelVaroquaux,false,GaelVaroquaux,0,0,6,1,869,true,false,false,false,0,0,0,0,1,0,858
9544560,scikit-learn/scikit-learn,python,5310,1443208786,1443208982,1443208982,3,3,github,false,false,false,16,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.785667540359897,0.10059157677295269,58,trev.stephens@gmail.com,doc/whats_new.rst,58,0.06157112526539278,0,0,true,DOC: minor update to whats new Minor changes added link to github repository in the doc,,3035,0.7482701812191104,0.2346072186836518,45687,526.8238229693349,39.37662792479261,134.98369339199334,7326,79,1865,301,travis,JeanKossaifi,glouppe,false,glouppe,9,0.8888888888888888,7,13,1572,false,true,true,false,0,7,0,0,0,0,3
9396991,scikit-learn/scikit-learn,python,5305,1443036595,1443044178,1443044178,126,126,commits_in_master,false,false,false,48,1,1,4,7,0,11,0,6,0,0,5,5,5,0,0,0,0,5,5,5,0,0,1098,0,1098,0,19.621613937391004,0.41243434256435446,19,olivier.grisel@ensta.org,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_tree.c,19,0.005319148936170213,4,2,false,[MRG] Hotfix for _criterionpyx I accidentally included a few variable name errors in the last round of review for #5278 because I did not recompile it before running unit tests This is a patch to fix solely those issues as noticed also in #5304@agramfort @GaelVaroquaux @glouppe @arjoly ,,3034,0.7481872116018458,0.22659574468085106,45685,526.2558826748386,39.334573711283795,134.94582466892854,7285,80,1863,303,travis,jmschrei,glouppe,false,glouppe,12,0.6666666666666666,35,5,917,false,true,false,true,8,157,19,69,0,0,5
9505737,scikit-learn/scikit-learn,python,5303,1443030730,1443450710,1443450710,6999,6999,commits_in_master,false,false,false,73,1,1,0,2,0,2,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,36,5,36,5,13.329036663324176,0.2806776119629985,24,t3kcit@gmail.com,sklearn/decomposition/_online_lda.c|sklearn/decomposition/_online_lda.pyx|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py,17,0.01276595744680851,0,0,false,[MRG] optimize LDA ~15% faster on one core This is not the prettiest solution ever but it gives a very decent speedup on 20news I also tried the obviouspydoc_topic_d  (exp_doc_topic_d *               npdot(cnts / norm_phi exp_topic_word_dT))doc_topic_d + doc_topic_priorbut that didnt help quite as much Multiplying in the exp_doc_topic_d might make things even fasterThis could be optimized further by also multiplying in the exp_doc_topic_d in the Cython code,,3033,0.7481041872733267,0.225531914893617,45195,531.9615001659475,39.76103551277796,136.40889478924657,7284,80,1863,310,travis,larsmans,ogrisel,false,ogrisel,136,0.7352941176470589,155,38,1893,true,true,true,true,14,83,23,17,27,1,1798
9504613,scikit-learn/scikit-learn,python,5302,1443026939,1443545004,1443545004,8634,8634,commits_in_master,false,false,false,31,4,1,6,7,0,13,0,3,0,0,3,4,2,0,1,0,0,4,4,2,0,1,251,0,251,0,15.217728317954624,0.32044780338418294,6,olivier.grisel@ensta.org,appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/run_with_env.cmd,6,0.0,2,1,false,[MRG] MAINT build and test with Python 35 on appveyor Bump up the Python 3 version on appveyorThanks to @nedbat and @ionelmc who contributed most of the work on python-appveyor-demo,,3032,0.7480211081794196,0.22470713525026625,45184,532.0910056657224,39.77071529745042,136.44210339943345,7284,80,1863,310,travis,ogrisel,ogrisel,true,ogrisel,132,0.8712121212121212,1216,124,2310,true,true,false,false,21,342,59,236,38,2,244
9394096,scikit-learn/scikit-learn,python,5301,1443024072,1443033393,1443033393,155,155,github,false,false,false,21,1,1,2,13,0,15,0,3,2,0,5,7,7,0,0,2,0,5,7,7,0,0,1190,0,1190,0,30.826551564920734,0.6491311008117546,40,tom.dupre-la-tour@m4x.org,sklearn/base.py|sklearn/calibration.py|sklearn/externals/funcsigs.py|sklearn/externals/odict.py|sklearn/utils/estimator_checks.py|sklearn/utils/fixes.py|sklearn/utils/validation.py,25,0.007462686567164179,0,1,false,[MRG+2] MAINT use inspectsignature for introspection This  is a fix for the deprecation warnings on Python 35 as reported in #5281,,3031,0.7479379742659188,0.22494669509594883,45184,532.0910056657224,39.77071529745042,136.44210339943345,7283,80,1863,304,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,131,0.8702290076335878,1216,124,2310,true,true,true,true,21,336,58,235,37,2,1
9503377,scikit-learn/scikit-learn,python,5300,1443022073,1443461347,1443461347,7321,7321,commits_in_master,false,false,false,15,1,1,0,5,0,5,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,27,26,27,8.780859020635088,0.18490322117716046,29,tom.dupre-la-tour@m4x.org,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,22,0.023454157782515993,1,4,false,[MRG] Ensure correct LabelKFold folds when shuffleTrue This is meant to fix #5292 CC: @andreasvc ,,3030,0.7478547854785479,0.22494669509594883,45184,532.0910056657224,39.77071529745042,136.44210339943345,7283,80,1863,309,travis,glouppe,glouppe,true,glouppe,56,0.9642857142857143,162,26,1777,true,true,false,false,12,141,27,121,78,0,1049
9502029,scikit-learn/scikit-learn,python,5299,1443015244,1457669721,1457669721,244241,244241,commits_in_master,false,true,false,61,2,2,96,83,0,179,0,11,0,0,15,15,12,0,0,0,0,15,15,12,0,0,1721,495,1721,495,101.5048123549407,2.1374408500700772,81,trev.stephens@gmail.com,doc/modules/pipeline.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/manifold/t_sne.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|doc/modules/pipeline.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/whats_new.rst|examples/decomposition/plot_pca_vs_fa_model_selection.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_factor_analysis.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/ensemble/tests/test_forest.py|sklearn/manifold/t_sne.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,58,0.004273504273504274,0,15,false,[WIP] Collapsing PCA and RandomizedPCA Fixes #5243  and #3930to do- [x] collapse the two classes- [x] old tests passing- [ ] integrate svd_solverarpack- [ ] benchmark the 3 solvers and establish the best auto policy- [ ] fix docstrings- [ ] discuss whether to uniform with IncrementalPCA and to inherit from _BasePCA see  #3285,,3029,0.7477715417629581,0.22435897435897437,45184,532.0910056657224,39.77071529745042,136.44210339943345,7282,80,1863,493,travis,giorgiop,MechCoder,false,MechCoder,2,1.0,2,7,1034,true,true,false,false,1,57,2,31,36,2,155
9343275,scikit-learn/scikit-learn,python,5297,1442964819,1444341277,1444341277,22940,22940,merged_in_comments,false,false,false,7,12,2,16,12,0,28,0,6,0,0,1,10,1,0,0,0,0,10,10,10,0,0,0,16,165,95,8.297657937976874,0.1747281988416654,4,m.lyra@sussex.ac.uk,sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py,4,0.004287245444801715,0,7,false,[WIP] Fix warnings during tests Addresses #5089 ,,3028,0.7476882430647291,0.22293676312968919,45184,532.0910056657224,39.77071529745042,136.44210339943345,7276,80,1862,318,travis,vighneshbirodkar,MechCoder,false,MechCoder,4,0.75,14,1,926,true,false,false,false,1,8,4,26,29,0,2612
9491166,scikit-learn/scikit-learn,python,5294,1442902967,1442949863,1442949864,781,781,github,false,false,false,34,1,1,0,7,0,7,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,84,2,84,2,13.757661343975197,0.28970084578025374,60,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py,58,0.004319654427645789,1,2,false,[MRG] Deprecate fit params in qda and lda @amueller  I believe this fixes #4107 and supercedes #4112 as LinearDiscriminantAnalysis and QuadraticDiscriminantAnalysis have moved to discriminant_analysispy and that PR appears to have gone stale,,3025,0.748099173553719,0.21706263498920086,45164,532.3266318306615,39.78832698609512,136.50252413426622,7264,80,1862,302,travis,trevorstephens,amueller,false,amueller,17,0.8235294117647058,137,51,771,true,true,true,false,2,13,0,3,1,0,16
9484152,scikit-learn/scikit-learn,python,5293,1442893337,1442923843,1442923843,508,508,github,false,false,false,13,1,1,0,3,0,3,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,21,3,21,8.815352965599827,0.18562752781511094,2,gael.varoquaux@normalesup.org,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,2,0.002162162162162162,0,3,false,[MRG + 1] add future warning to pipelineinverse_transform with 1d X Replaces #5065,,3024,0.748015873015873,0.2172972972972973,45161,532.1848497597484,39.76882708531698,136.46730586125196,7262,79,1861,301,travis,amueller,ogrisel,false,ogrisel,330,0.8484848484848485,1236,40,1795,true,true,true,false,121,1007,80,383,40,11,0
9465485,scikit-learn/scikit-learn,python,5291,1442806159,1456461841,1456461841,227594,227594,commits_in_master,false,false,false,8,5,2,130,108,0,238,0,8,1,0,2,10,2,0,0,3,0,9,12,7,0,0,66,0,497,238,13.50512455189537,0.2849356415067829,3,gael.varoquaux@normalesup.org,sklearn/linear_model/huber.py|sklearn/linear_model/__init__.py|sklearn/linear_model/huber.py,3,0.003260869565217391,0,27,false,[WIP] Add Huber Estimator to sklearn linear models ,,3023,0.7479325173668541,0.21304347826086956,44959,533.2858826931204,40.01423519206388,136.88026868925022,7242,80,1860,484,travis,MechCoder,MechCoder,true,MechCoder,71,0.8732394366197183,86,41,1188,true,true,false,false,7,66,13,46,18,0,12
9428510,scikit-learn/scikit-learn,python,5289,1442588868,1442589119,1442589119,4,4,github,false,false,false,35,1,1,0,1,0,1,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,4,0,4,0,9.592905772640357,0.20320737097291336,9,olivier.grisel@ensta.org,doc/modules/metrics.rst|sklearn/metrics/pairwise.py,9,0.009635974304068522,0,0,true,Updated dead link to publication  Updated link: Original link to ZhangIJCV06 paper Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study  was dead Replaced all occurances of http://eprintspascal-networkorg/archive/00002309/01/Zhang06-IJCVpdf with http://researchmicrosoftcom/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06pdf,,3022,0.7478491065519524,0.20877944325481798,44827,516.3852142681866,39.10589600017846,132.2194213308943,7210,79,1858,305,travis,carrillo,GaelVaroquaux,false,GaelVaroquaux,0,0,5,5,1165,false,false,false,false,0,0,0,0,0,0,4
9371639,scikit-learn/scikit-learn,python,5288,1442553355,1442763107,1442763107,3495,3495,github,false,false,false,14,3,2,2,6,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,0,13,0,8.81807964042056,0.18679416052123193,12,olivier.grisel@ensta.org,sklearn/neighbors/base.py|examples/bicluster/bicluster_newsgroups.py,12,0.01282051282051282,0,0,true,[MRG] Bicluster example speed ENH avoid slow CSR fancy indexing variant …See scipy/scipy#5265,,3021,0.7477656405163853,0.2094017094017094,44827,516.3852142681866,39.10589600017846,132.2194213308943,7204,79,1858,304,travis,jnothman,jnothman,true,jnothman,122,0.7049180327868853,30,1,2334,true,true,false,false,30,265,32,111,21,1,264
9399167,scikit-learn/scikit-learn,python,5284,1442436409,1442440535,1442440535,68,68,github,false,false,false,36,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.547748122653649,0.09633524826849078,4,t3kcit@gmail.com,doc/developers/contributing.rst,4,0.004287245444801715,0,0,false,Go to easy _open_ issues page Currently the link to the easy open issues page lists open and closed issues When trying to get people involved itd be more valuable to just show the open issues,,3019,0.7479297780722094,0.20793140407288319,44821,516.1866089556236,39.08881997278062,132.19249905178376,7189,79,1856,306,travis,tdhopper,MechCoder,false,MechCoder,0,0,137,102,1679,false,true,false,false,0,0,0,0,0,0,-1
9355164,scikit-learn/scikit-learn,python,5283,1442431963,1442655482,1442655482,3725,3725,github,false,false,false,103,2,1,11,5,0,16,0,3,0,0,7,8,7,0,0,0,0,8,8,8,0,0,14,54,44,54,31.535362018587666,0.6680156524422242,32,t3kcit@gmail.com,sklearn/decomposition/tests/test_kernel_pca.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/utils/multiclass.py,14,0.00857449088960343,0,1,false,[MRG] Remove some warnings from test suit I removed some warnings from the test suit- (1 left) DeprecationWarning: elementwise comparison failed this will raise the error in the future- (0 left) Warning: The least populated class in y has only 2 members which is too few The minimum number of labels for any class cannot be less than 3- (0 left) DeprecationWarning: Default multioutput behavior now corresponds to variance_weighted value it will be changed to uniform_average in 018- (some left) UndefinedMetricWarning: Precision is ill-defined and being set to 00 due to no predicted samplesIssue #5089 Complementary work #5277,,3018,0.747846255798542,0.20793140407288319,44821,516.1866089556236,39.08881997278062,132.19249905178376,7189,79,1856,307,travis,TomDLT,agramfort,false,agramfort,17,0.7647058823529411,6,3,210,true,false,true,true,4,79,6,91,27,0,194
9396962,scikit-learn/scikit-learn,python,5282,1442429267,1444221729,1444221729,29874,29874,commit_sha_in_comments,false,false,false,25,2,1,0,4,0,4,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,2,13,2,13,8.697263777444089,0.18423471191885932,4,olivier.grisel@ensta.org,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,3,0.003215434083601286,0,1,false,[MRG+2] Fix SGD partial_fit multiclass w/ average See https://githubcom/scikit-learn/scikit-learn/issues/5246#issuecomment-140062688_fit_multiclass was setting selfintercept_ to a single element of the intercept instead of the entire intercept,,3017,0.7477626781571097,0.20793140407288319,44821,516.1866089556236,39.08881997278062,132.19249905178376,7189,79,1856,323,travis,andylamb,ogrisel,false,ogrisel,3,0.3333333333333333,2,1,886,true,false,false,false,3,12,3,3,5,0,2761
9394902,scikit-learn/scikit-learn,python,5280,1442424400,1442449437,1442449437,417,417,github,false,false,false,23,1,1,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,20,9,20,8.927557953303813,0.18911304862702524,2,olivier.grisel@ensta.org,sklearn/discriminant_analysis.py|sklearn/tests/test_discriminant_analysis.py,2,0.002145922746781116,0,1,false,[MRG+2] Adding checks for the input LDA prior This is rebased version of #4924Can be merged as soon as CI is green,,3016,0.7476790450928382,0.20708154506437768,44821,516.1866089556236,39.08881997278062,132.19249905178376,7187,79,1856,306,travis,ogrisel,ogrisel,true,ogrisel,130,0.8692307692307693,1213,124,2303,true,true,false,false,21,340,55,244,34,2,-1
9350931,scikit-learn/scikit-learn,python,5278,1442410419,1442917785,1442917785,8456,8456,commits_in_master,false,false,false,148,1,1,63,33,0,96,0,4,0,0,7,7,7,0,0,0,0,7,7,7,0,0,605,0,605,0,12.967976713168879,0.2747015055605519,17,olivier.grisel@ensta.org,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pxd|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c,17,0.003243243243243243,3,11,false,[MRG] _criterionpyx cleanup This pull request focuses on cleaning up criterion code without adding any new functionality This is the first step to adding caching between splits That branch had too many changes so I broke it up to be easier to review This PR addresses the following:(1) Eliminates nearly 200 lines of unneeded code (mostly by calling class variables selfn_outputs directly instead of unpacking them)(2) Standardizes label_count for classification criteria and sum_total for regression into the node_sum (repeat for left and right versions)(3) Adds yw_sq_sum as the last sufficient statistic needed for regression criteria between splits(4) Previously regression child_impurity recalculated sufficient statistics by rescanning the sequence despite them already being present I fixed this However the speed increase is minimal given how rarely this function is calledAll unit tests pass and speed is basically the same as before  ping @arjoly @glouppe @ogrisel ,,3014,0.747843397478434,0.20540540540540542,44821,516.1866089556236,39.08881997278062,132.19249905178376,7184,79,1856,309,travis,jmschrei,glouppe,false,glouppe,11,0.6363636363636364,35,5,910,false,true,false,true,7,123,17,34,0,0,1
9358091,scikit-learn/scikit-learn,python,5277,1442401549,1444931197,1444931197,42160,42160,merged_in_comments,false,true,false,19,1,1,2,7,0,9,0,4,0,0,9,9,9,0,0,0,0,9,9,9,0,0,0,70,0,70,40.014936117618696,0.8476390295532152,26,t3kcit@gmail.com,sklearn/ensemble/tests/test_bagging.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sgd.py|sklearn/neighbors/tests/test_approximate.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_calibration.py|sklearn/tests/test_common.py|sklearn/tests/test_pipeline.py,12,0.004333694474539545,1,4,false,[issue #5089] Fixed warnings for DataDimensionalityWarning decision_function and decision_function_shape @amueller [issue #5089] Fixed warnings for DataDimensionalityWarning decision_function and decision_function_shape,,3013,0.7477597079322934,0.2047670639219935,44821,516.1866089556236,39.08881997278062,132.19249905178376,7182,79,1856,331,travis,hasancansaral,amueller,false,amueller,1,0.0,21,4,1629,false,true,false,false,0,3,2,0,0,0,86
9357616,scikit-learn/scikit-learn,python,5276,1442364033,,1442451466,1457,,unknown,false,false,false,20,5,2,6,8,0,14,0,4,4,1,2,8,4,0,0,4,1,3,8,5,0,0,349,34,362,48,28.93462725554235,0.6129241165169075,0,,sklearn/metric_learning/NCA.py|sklearn/metric_learning/demo.py|sklearn/tests/test_nca.py|sklearn/metric_learning/NCA.py|sklearn/metric_learning/__init__.py|sklearn/metric_learning/demo.py|sklearn/tests/test_nca.py,0,0.0,0,5,false,New Feature: Added NCA as first algorithm in metric_learning module See #3213 Ive integrated NCA with scikit-learn in this PR ,,3012,0.74800796812749,0.20217391304347826,44821,516.1866089556236,39.08881997278062,132.19249905178376,7176,79,1855,306,travis,terrytangyuan,terrytangyuan,true,,0,0,168,51,871,true,true,false,false,0,0,0,0,2,0,69
9381052,scikit-learn/scikit-learn,python,5274,1442361310,1445599415,1445599415,53968,53968,commits_in_master,false,false,false,8,4,2,18,20,0,38,0,8,0,0,12,12,11,0,0,0,0,12,12,11,0,0,240,110,306,131,102.203340390332,2.1649800967047854,85,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h|doc/whats_new.rst|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h,54,0.009793253536452665,0,16,false,[MRG] Patch liblinear for sample_weights in LogisticRegression(and CV) ,,3011,0.7479242776486217,0.20239390642002175,44821,516.1866089556236,39.08881997278062,132.19249905178376,7174,79,1855,356,travis,MechCoder,agramfort,false,agramfort,70,0.8714285714285714,86,41,1183,true,true,true,false,7,56,10,18,17,0,68
9352622,scikit-learn/scikit-learn,python,5271,1442258681,1442319346,1442319346,1011,1011,github,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.505469987440752,0.09543970799838523,4,olivier.grisel@ensta.org,doc/modules/neighbors.rst,4,0.004352557127312296,0,0,false,Fixed LDA typo in doc ,,3009,0.7480890661349285,0.20348204570184983,44802,516.405517610821,39.10539708048748,132.24856033212802,7152,78,1854,302,travis,vortex-ape,larsmans,false,larsmans,14,0.9285714285714286,8,15,864,true,false,false,false,0,6,2,2,1,0,-1
9335978,scikit-learn/scikit-learn,python,5268,1442249929,,1442412302,2706,,unknown,false,false,false,149,1,1,3,6,0,9,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,26,0,26,0,4.406582166088488,0.09334495987648786,8,olivier.grisel@ensta.org,sklearn/ensemble/gradient_boosting.py,8,0.0087527352297593,4,1,false,Patch gradient boosting least squares loss Currently the negative gradient of the least squares returns (y - pred) when it should return 2*(y-pred) This difference is causing master branch to produce less accurate models here are just a few toy examples:MASTER               mse   timeboston       2674   0008regression   3393   174diabetes    41811   0006BRANCHboston       1991   0008regression   3334   177diabetes    35067   0007This pull request patches the negative gradient and also removes taking in arbitrary keywords as arguments hardcoding sample_weights I know there is a unit test failing and wanted to discuss the best way to deal with it That suite of tests is there to test the underlying loss functions (by setting learning_rate to 10) and I think we need a more comprehensive suite of tests So it may be worth removing that temporarily in order to add in better tests @arjoly @glouppe @pprett @ogrisel ,,3007,0.7485866311938809,0.20350109409190373,44802,516.405517610821,39.10539708048748,132.24856033212802,7152,78,1854,305,travis,jmschrei,jmschrei,true,,10,0.7,35,5,908,false,true,false,false,7,119,16,33,0,0,12
9331096,scikit-learn/scikit-learn,python,5263,1442110446,1442116517,1442116517,101,101,github,false,false,false,37,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.366058886387597,0.09247435777076975,6,olivier.grisel@ensta.org,sklearn/utils/validation.py,6,0.0066298342541436465,0,0,false,check_X_y: Added mention that function returns y array I noticed that the documentation for sklearnutilscheck_X_y didnt specify that the function returned a y array This is a quick fix to add that[screenshot 2015-09-12 18 13 10](https://cloudgithubusercontentcom/assets/1865885/9834294/05fe47fe-597a-11e5-98ea-450e957306capng),,3005,0.7487520798668885,0.20662983425414364,44735,516.620096121605,39.141611713423494,132.37956857047052,7132,79,1852,304,travis,hlin117,amueller,false,amueller,4,0.25,18,20,1180,true,true,true,false,2,20,5,8,13,0,101
9163712,scikit-learn/scikit-learn,python,5262,1442089239,1442214562,1442214562,2088,2088,github,false,false,false,31,3,1,3,3,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,17,2,41,8.530522174506935,0.18067886395126562,2,gael.varoquaux@normalesup.org,sklearn/utils/class_weight.py|sklearn/utils/tests/test_class_weight.py,2,0.002207505518763797,0,0,false,[MRG + 1] Fix check in compute_class_weight In https://githubcom/scikit-learn/scikit-learn/issues/4921 if a key in class_weights a user-specified dictionary of classes is not present in classes an error should be raisedAdded unittest,,3004,0.7486684420772304,0.206401766004415,44735,516.620096121605,39.141611713423494,132.37956857047052,7130,79,1852,304,travis,andylamb,MechCoder,false,MechCoder,1,0.0,2,1,882,true,false,false,false,2,3,1,2,4,0,10
9163323,scikit-learn/scikit-learn,python,5261,1442074969,1442220343,1442220343,2422,2422,github,false,false,false,56,5,1,22,9,0,31,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,170,0,238,9.085786355078598,0.19243951579486948,15,tw991@nyu.edu,sklearn/ensemble/tests/test_forest.py|sklearn/tree/tests/test_tree.py,13,0.014412416851441241,1,2,false,[MRG+1] Stronger tests for variable importances This PR adds stronger tests for variable importances in forests including:- checks for all forests and all criteria (as proposed earlier by @arjoly)- checks for invariance with respect to sample weight scaling- checks for the convergence of variable importances of totally randomized trees towards their true values,,3003,0.7485847485847485,0.2073170731707317,44735,516.620096121605,39.141611713423494,132.37956857047052,7128,79,1852,303,travis,glouppe,glouppe,true,glouppe,55,0.9636363636363636,162,26,1766,true,true,false,false,10,121,23,83,70,0,104
9163169,scikit-learn/scikit-learn,python,5260,1442074941,1444856332,1444856332,46356,46356,commit_sha_in_comments,false,false,false,26,5,2,9,8,0,17,0,5,0,0,6,6,3,0,0,0,0,6,6,3,0,0,57,40,64,54,41.46369345509289,0.878212713763062,66,tom.dupre-la-tour@m4x.org,sklearn/kernel_ridge.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|doc/modules/classes.rst|doc/modules/metrics.rst|doc/whats_new.rst|sklearn/kernel_ridge.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,51,0.009977827050997782,0,1,false,Added laplacian kernel Almost identical to the rbf_kernel but uses l1 norm for the distance Tried to mimick rbf_kernel updated the references/comments and added a test,,3002,0.7485009993337775,0.2073170731707317,44735,516.620096121605,39.141611713423494,132.37956857047052,7128,79,1852,332,travis,Clyde-fare,amueller,false,amueller,0,0,5,11,1031,true,false,false,false,0,0,0,0,2,0,28
9308459,scikit-learn/scikit-learn,python,5258,1442018197,1442160586,1442160586,2373,2373,github,false,false,false,7,1,1,0,0,0,0,0,1,0,0,8,8,5,0,0,0,0,8,8,5,0,0,43,0,43,0,36.47400158933928,0.7725296337207384,44,tom.dupre-la-tour@m4x.org,doc/modules/linear_model.rst|doc/preface.rst|doc/tutorial/basic/tutorial.rst|sklearn/datasets/rcv1.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/sag.py,22,0.0066815144766146995,0,1,true,[MRG] minor fixes to the doc build ,,3001,0.7484171942685771,0.20601336302895323,44735,516.620096121605,39.141611713423494,132.37956857047052,7126,79,1851,303,travis,amueller,agramfort,false,agramfort,329,0.8480243161094225,1230,40,1785,true,true,true,false,128,1016,75,377,40,11,-1
9307654,scikit-learn/scikit-learn,python,5257,1442015108,1446754624,1446754624,78991,78991,commits_in_master,false,false,false,10,2,1,35,11,0,46,0,5,1,0,2,4,0,0,0,1,0,3,4,0,0,0,0,0,0,0,13.932051217204055,0.29508477147599377,2,gael.varoquaux@normalesup.org,doc/developers/advanced_installation.rst|doc/developers/index.rst|doc/install.rst,2,0.002229654403567447,0,3,true,[MRG] split installation into simple and advanced part Fixes #4742,,3000,0.7483333333333333,0.20624303232998886,44735,516.620096121605,39.141611713423494,132.37956857047052,7126,79,1851,384,travis,amueller,amueller,true,amueller,328,0.8475609756097561,1230,40,1785,true,true,false,false,128,1014,74,377,40,11,3455
9303705,scikit-learn/scikit-learn,python,5255,1442000281,1442241085,1442241085,4013,4013,merged_in_comments,false,false,false,24,1,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.533549885141139,0.09602186433107031,6,olivier.grisel@ensta.org,appveyor.yml,6,0.0067039106145251395,0,0,true,MAINT tentative fix for weird multiprocessing + test failure under windows This is a tentative fix for #5254Lets wait for appveyor to catch-up,,2999,0.7482494164721574,0.20670391061452514,44735,516.620096121605,39.141611713423494,132.37956857047052,7123,78,1851,301,travis,ogrisel,ogrisel,true,ogrisel,129,0.8682170542635659,1212,124,2298,true,true,false,false,19,333,51,231,32,2,-1
9301905,scikit-learn/scikit-learn,python,5253,1441994008,1443030211,1443030211,17270,17270,commits_in_master,false,true,false,54,1,1,0,16,0,16,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,78,0,78,0,4.110486314473841,0.08706067517679164,16,olivier.grisel@ensta.org,sklearn/decomposition/online_lda.py,16,0.017977528089887642,0,2,true,[MRG] OPTIM make (Online) LDA reuse a joblibParallel instance While running the tests under Python 3 I noticed that online LDA would seriously benefit from using the new context manager APIUnder Python 2 this is less noticeable because it uses forks that hide some overhead We should still get a good speedup anyway,,2998,0.7481654436290861,0.20674157303370785,44701,517.0130422138208,39.17138319053265,132.4802577123554,7121,78,1851,317,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,128,0.8671875,1212,124,2298,true,true,true,true,18,320,47,231,30,2,15
9158494,scikit-learn/scikit-learn,python,5252,1441991152,1443528709,1443528709,25625,25625,commits_in_master,false,false,false,251,2,2,76,27,0,103,0,4,0,0,17,17,15,0,0,0,0,17,17,15,0,0,1622,818,1622,818,115.39008205092759,2.4440191869020245,84,tw991@nyu.edu,doc/modules/tree.rst|doc/whats_new.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_criterion.c|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tree.py|doc/modules/tree.rst|doc/whats_new.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_criterion.c|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,51,0.007882882882882882,3,15,true,[WIP] Merge PresortBestSplitter and BestSplitter This pull request merges PresortBestSplitter into BestSplitter without loss of functionality All tree based estimators (Decision Trees Random Forests Extra Trees and Gradient Boosting) have an optional parameter presort which is default false for all except Gradient BoostingIn addition to allowing other estimators to turn on presorting on smaller data sets it allows Gradient Boosting to turn off presorting on large datasets A good default for this switch needs to be found as it currently is done manually Gradient Boosting can now also work on sparse data by turning off the presorting optionHere is a checklist of what needs to be done (much more manageable than my last one):- [x] Merge PresortBestSplitter into BestSplitter- [x] Write unit tests to test sparse data presorting options for RF and DT and non-presorting for GBTs - [x] Find a good default switch for GBT to switch from presorting to non-presortingAs a side note Im getting some pretty high variance running this benchmark with sometimes my branch being faster and sometimes slower by up to 50s Can someone else run the benchmarks a few times and see what they get@arjoly @pprett @glouppe MASTERClassification performance:Classifier   train-time test-time error-rate--------------------------------------------RandomForest  321754s   04057s     00330  ExtraTrees    395981s   05889s     00372  CART          121820s   00209s     00424  GBRT         5847982s   04412s     01777BRANCHClassification performance:Classifier   train-time test-time error-rate--------------------------------------------RandomForest  360459s   03898s     00330  ExtraTrees    352664s   05764s     00372  CART          153547s   02031s     00424  GBRT         5570054s   03247s     01777  ,,2997,0.7480814147480814,0.2072072072072072,44797,516.3515413978614,39.109761814407214,132.28564412795498,7121,78,1851,321,travis,jmschrei,arjoly,false,arjoly,9,0.6666666666666666,35,5,905,false,true,false,false,6,106,15,18,0,0,208
9156992,scikit-learn/scikit-learn,python,5251,1441984911,1449272589,1449272589,121461,121461,commits_in_master,false,false,false,106,3,2,51,24,0,75,0,6,1,0,16,17,15,0,0,1,0,16,17,15,0,0,2474,381,2493,381,125.8417847679368,2.665391435904262,71,tom.dupre-la-tour@m4x.org,doc/modules/linear_model.rst|examples/linear_model/plot_logistic_multinomial.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sag.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/utils/tests/test_seq_dataset.py|doc/modules/linear_model.rst|doc/whats_new.rst|examples/linear_model/plot_logistic_multinomial.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sag.py|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/utils/tests/test_seq_dataset.py,51,0.0022471910112359553,0,4,true,[MRG] add multinomial SAG solver for LogisticRegression I added multinomial sag solver in LogisticRegressionThe results are identical to lbfgs and newton-cg multinomial solvers___The benchmark [(gist)](https://gistgithubcom/TomDLT/da7c44fa277003e7549e) on RCV1 dataset is very promising I only took 4 classes of the dataset and removed samples that have more than one classepythonn_samples_train  129073n_samples_test  23149n_features  47236n_classes  4[dloss](https://cloudgithubusercontentcom/assets/11065596/9814439/73b5f374-588e-11e5-94f4-adc82287026bpng)[train_score](https://cloudgithubusercontentcom/assets/11065596/9814441/76b4dd60-588e-11e5-8183-7a6fb733fc13png)[test_score](https://cloudgithubusercontentcom/assets/11065596/9814440/7578b106-588e-11e5-874c-c56f4ac607f4png)___I also added a plot_logistic_multinomialpy to show the difference between OvR and Multinomial[ovr](https://cloudgithubusercontentcom/assets/11065596/9814355/8e521902-588d-11e5-8c2b-a962200e1e66png)[multinomial](https://cloudgithubusercontentcom/assets/11065596/9814356/8f41eea0-588d-11e5-8f16-47bce1e89dcepng)___In the same spirit I could probably add a multinomial LogisticRegression in SGD (previously proposed in #849) Do we want it,,2996,0.7479973297730307,0.20674157303370785,44797,516.3515413978614,39.109761814407214,132.28564412795498,7121,78,1851,438,travis,TomDLT,MechCoder,false,MechCoder,16,0.75,6,3,205,true,false,false,false,4,72,6,91,24,0,89
9288679,scikit-learn/scikit-learn,python,5249,1441923343,1441994135,1441994135,1179,1179,github,false,false,false,15,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,12,3,12,9.01141686010152,0.19085748913278228,5,t3kcit@gmail.com,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,4,0.004545454545454545,0,0,false,[MRG + 1] [BUG] _init_centroids has an optional x_squared_norms parameter which is not exactly optional ,,2994,0.7481629926519706,0.2034090909090909,44698,516.3989440243412,39.151639894402436,132.42203230569598,7116,77,1850,303,travis,MechCoder,ogrisel,false,ogrisel,69,0.8695652173913043,86,41,1178,true,true,true,false,5,41,6,18,11,0,135
9147791,scikit-learn/scikit-learn,python,5245,1441902356,1442241306,1442241306,5649,5649,github,false,false,false,49,1,1,8,11,0,19,0,4,1,1,15,17,13,0,0,1,1,15,17,13,0,0,1543,324,1543,324,73.80021966458527,1.563056217490864,89,you@example.com,doc/modules/classes.rst|doc/modules/lda_qda.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/whats_new.rst|examples/classification/plot_classifier_comparison.py|examples/classification/plot_lda.py|examples/classification/plot_lda_qda.py|examples/decomposition/plot_pca_vs_lda.py|examples/manifold/plot_lle_digits.py|sklearn/__init__.py|sklearn/discriminant_analysis.py|sklearn/lda.py|sklearn/qda.py|sklearn/tests/test_discriminant_analysis.py|sklearn/tests/test_qda.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_testing.py,51,0.0022598870056497176,0,2,false,[MRG+1] Deprecate LDA/QDA in favor of expanded names The LDA accronym for Linear Discriminant Analysis is ambiguousbecause of the newly introduced Latent Dirichlet Allocation modelWe therefore deprecate the sklearnldaLDA and sklearnldaQDAin favor of explicit namesThis is a squash and rebase of the content of #4421,,2992,0.7483288770053476,0.20451977401129945,44699,516.5887380030873,39.17313586433701,132.46381350813215,7114,76,1850,308,travis,ogrisel,ogrisel,true,ogrisel,127,0.8661417322834646,1211,124,2297,true,true,false,false,17,304,45,233,28,2,20
9281977,scikit-learn/scikit-learn,python,5242,1441899459,1441966568,1441966568,1118,1118,github,false,false,false,25,1,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,11,0,11,5.073676499650556,0.10818647859459636,11,tw991@nyu.edu,sklearn/ensemble/tests/test_forest.py,11,0.012415349887133182,0,3,false,FIX unstable test due to bootstrap and unset random state superseed of https://githubcom/scikit-learn/scikit-learn/pull/5240 with only the required fixFIX unstable test  as descibe in #5239,,2991,0.7482447342026078,0.20428893905191872,44306,511.4205750914097,39.227192705276934,132.2619961179073,7114,76,1850,304,travis,arjoly,glouppe,false,glouppe,89,0.8089887640449438,29,26,1360,true,true,true,true,15,152,12,99,11,0,0
9146737,scikit-learn/scikit-learn,python,5240,1441881024,,1441899266,304,,unknown,false,false,false,20,4,2,11,9,0,20,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,73,0,95,9.217691723078056,0.19655285030488784,11,tw991@nyu.edu,sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py,11,0.012429378531073447,2,0,false,FIX unstable test #5239 + improve coverage of importance tests FIX #5239 + improve coverage given #5233Ping @glouppe @jmschrei,,2990,0.748494983277592,0.20451977401129945,43985,515.016482891895,39.51347050130726,133.1590314880073,7113,76,1850,301,travis,arjoly,arjoly,true,,88,0.8181818181818182,29,26,1360,true,true,false,false,14,146,10,94,11,0,5
9293913,scikit-learn/scikit-learn,python,5238,1441867905,1442176167,1442176167,5137,5137,github,false,false,false,20,1,1,0,5,0,5,0,4,0,0,10,10,10,0,0,0,0,10,10,10,0,0,363,0,363,0,43.73897976795259,0.9326652920377888,61,t3kcit@gmail.com,sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py,22,0.007927519818799546,3,0,false,[MRG+1] DOC Make cv documentation consistent across our codebase @jnothman @amueller @GaelVaroquaux Please take a look :)Ref - https://githubcom/scikit-learn/scikit-learn/pull/5184#issuecomment-139091293,,2989,0.7484108397457344,0.20498301245753114,43985,515.016482891895,39.51347050130726,133.1590314880073,7111,76,1850,312,travis,rvraghav93,amueller,false,amueller,10,0.6,28,43,706,true,false,false,false,3,166,14,150,54,0,671
9279639,scikit-learn/scikit-learn,python,5237,1441860009,1444275767,1444275767,40262,40262,commits_in_master,false,false,false,293,6,4,1,17,0,18,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,62,55,62,55,27.627435685681405,0.5851572840216372,54,tom.dupre-la-tour@m4x.org,sklearn/metrics/ranking.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py|doc/whats_new.rst|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py,51,0.005681818181818182,0,4,false,In roc_curve: drop some suboptimal thresholds Currently the roc_curve function returns a value for every distinct possible threshold For many classification routines this is close to the number of data points and the results of this can get very unwieldy For instance with a large test set the plotted ROC curve is unsuitable for a vector graphics format since the image file will be large and slow to load Moreover many of the thresholds are suboptimal: one could choose a different threshold for a higher TPR at the same FPR or a lower FPR at the same TPRIve added an option keep_all_thresholds to the roc_curve function which drops thresholds which are invisible on a plotted ROC curve Basically in the sorted list of scores if two of the same class appear in a row it will drop threshold corresponding to the firstHere are some tests:In [21]:t1 t2import sklearndatasetsimport sklearnmetrics as metimport timeprobas_pred y_true  sklearndatasetsmake_classification(n_samples1000000n_features1 n_informative1n_redundant0 n_clusters_per_class1random_state1031)​t0  timetime()fpr tpr thr  metroc_curve(y_true probas_pred)t1  timetime() - t0​t0  timetime()fpr2 tpr2 thr2  metroc_curve(y_true probas_pred keep_all_thresholdsFalse)t2  timetime() - t0​t1 t2Out[21]:(02661151885986328 024276185035705566)In [18]:fprshape fpr2shapeOut[18]:((990748) (9843))In [19]:metauc(fpr tpr) - metauc(fpr2 tpr2)Out[19]:-22459999415858078e-09With this option roc_curve is faster and returns many fewer results (how many fewer depends on how good the classifier is results may vary)Ive also changed some tests to use the new optionIve been doing something similar to this with the results of roc_curve but it would be quicker and easier to have it directly in the function Please let me know if there is anything I forgot to do,,2988,0.7483266398929049,0.2056818181818182,44797,516.2399267808112,39.109761814407214,132.28564412795498,7111,76,1849,334,travis,gclenaghan,jnothman,false,jnothman,0,0,2,3,343,true,false,false,false,0,0,0,0,3,0,10701
9274196,scikit-learn/scikit-learn,python,5236,1441845300,1441908365,1441908365,1051,1051,commit_sha_in_comments,false,false,false,36,1,1,0,4,0,4,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.0284654579676875,0.10722415634151629,46,tom.dupre-la-tour@m4x.org,doc/whats_new.rst,46,0.05227272727272727,0,0,false,[MRG+1] Updated whatsnew for sprint and LatentDirichletAllocation Fixes #5191I didnt add whatsnew entries for minor example and doc changesI added links to the pull requests when appropriate I think that will be helpful wdyt,,2987,0.7482423836625377,0.2056818181818182,43985,515.016482891895,39.51347050130726,133.1590314880073,7110,76,1849,304,travis,amueller,amueller,true,amueller,327,0.8470948012232415,1227,40,1783,true,true,false,false,111,972,73,361,44,12,0
9146344,scikit-learn/scikit-learn,python,5234,1441825084,1444660434,1444660434,47255,47255,commits_in_master,false,true,false,25,4,2,10,23,0,33,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,18,17,18,31,17.21714080509109,0.3671285177692254,5,olivier.grisel@ensta.org,sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/utils/validation.py,5,0.002277904328018223,0,18,false,[MRG + 1]Deprecating 1D inputs in fast_mcd Addresses #4512I am just waiting to make sure that Travis does not throw the 1D deprecation warning,,2986,0.7481580709979906,0.20501138952164008,43985,515.016482891895,39.51347050130726,133.1590314880073,7106,76,1849,333,travis,vighneshbirodkar,ogrisel,false,ogrisel,2,1.0,14,1,913,true,false,false,false,1,4,2,26,25,0,144
9140343,scikit-learn/scikit-learn,python,5233,1441820010,,1441902149,1368,,unknown,false,false,false,328,1,1,3,6,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,86,0,86,0,4.334471691178046,0.09242592268935115,2,jmschreiber91@gmail.com,sklearn/tree/_criterion.c|sklearn/tree/_criterion.pyx,2,0.0022753128555176336,3,0,false,[MRG] Proxy improvement methods added to entropy/gini This pull request handles implementing explicit proxy improvement methods for entropy and gini criterion to compliment the one added in #5203 Entropy is changed by refactoring the equation-weighted_n_left / weighted_n_total * -sum( count_left_i / weighted_n_left * log( count_left_i / weighted_n_left ) - weighted_n_right / weighted_n_total * -sum( count_i / weighted_n_right * log(count_right_i / weighted_n_right )into sum( count_left_i * log( count_left_i ) ) + sum( count_right_i * log( count_right_i ) ) - weighted_n_left * log(weighted_n_left) - weighted_n_right * log(weighted_n_right)which collapses some terms and caches the calculation involving weighted_n_left and weighted_n_right The gini calculation time is collapsed into sum( count_left_i ** 20 ) / weighted_n_left + sum( count_right_i ** 20 ) / weighted_n_rightHere are time tests Entropy seems to take basically the same amount of time whereas gini seems to run slightly faster I was unsure if this was worth merging so figured Id see what you had to say@glouppe @arjoly @ogrisel ENTROPYBRANCHRandomForestClassifierspambase   094 0081Gaussian   0922 7972mnist      0946 9669covtypes   0948 19574ExtraTreesClassifierspambase   0943 0047Gaussian   0924 0846mnist      0952 4678covtypes   0943 12225DecisionTreeClassifierspambase   0911 0065Gaussian   0739 17631mnist      0887 30058covtypes   0948 11025MASTERRandomForestClassifierspambase   094 0081Gaussian   0922 8333mnist      0946 9324covtypes   0948 17701ExtraTreesClassifierspambase   0943 0047Gaussian   0924 079mnist      0952 4542covtypes   0943 1181DecisionTreeClassifierspambase   0911 006Gaussian   0739 1675mnist      0887 2983covtypes   0948 10561GINIBRANCHRandomForestClassifierGaussian   0914 3931spambase   0941 0045mnist      0948 5016covtypes   0944 13238ExtraTreesClassifierGaussian   0918 0652spambase   0946 0041mnist      0947 4641covtypes   0942 11275DecisionTreeClassifierGaussian   0727 8971spambase   0902 0056mnist      0873 21287covtypes   0944 8989MASTERRandomForestClassifierGaussian   0914 4513spambase   0939 0052mnist      0949 527covtypes   0944 13767ExtraTreesClassifierGaussian   0917 0679spambase   0947 0042mnist      0952 5131covtypes   094 11591DecisionTreeClassifierGaussian   0741 1002spambase   0898 0052mnist      0872 24078covtypes   0943 9836,,2985,0.7484087102177555,0.20477815699658702,43973,514.9978395833807,39.5015122916335,133.172628658495,7106,76,1849,306,travis,jmschrei,jmschrei,true,,8,0.75,35,5,903,false,true,false,false,5,93,13,12,0,0,4
9135276,scikit-learn/scikit-learn,python,5231,1441754079,1444180456,1444180456,40439,40439,commits_in_master,false,false,false,11,6,2,17,8,0,25,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,22,52,35,17.72631038155202,0.38024032614598147,2,gael.varoquaux@normalesup.org,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,2,0.002285714285714286,0,1,false,[MRG + 1] Making predict in kmeans be same as labels_ ,,2983,0.7485752598055648,0.2022857142857143,43942,514.8832552000364,39.529379636793955,133.26657867188567,7096,76,1848,338,travis,vighneshbirodkar,jakevdp,false,jakevdp,1,1.0,14,1,912,true,false,false,false,0,4,1,24,23,0,15
9134659,scikit-learn/scikit-learn,python,5230,1441750686,1441807442,1441807442,945,945,github,false,false,false,164,1,1,12,17,0,29,0,3,6,0,13,19,19,0,0,6,0,13,19,19,0,0,3381,2,3381,2,57.84350373162303,1.2408362957468733,29,tw991@nyu.edu,sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_criterion.c|sklearn/tree/_criterion.pxd|sklearn/tree/_criterion.pyx|sklearn/tree/_splitter.c|sklearn/tree/_splitter.pxd|sklearn/tree/_splitter.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/export.py|sklearn/tree/setup.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,16,0.0,3,3,false,[MRG+1] split tree module into several packages This pull request handles splitting _treepyx into _treepyx with Tree and TreeBuilders _splitterpyx with all splitters and split records _criterionpyx with all criteria and moves some functions into _utilspyx No code is changed simply moved in this PR I almost have this working with all unit tests passing except those which trigger a specific segfault The segfault relates to the following function:cdef inline npndarray sizet_ptr_to_ndarray(SIZE_t* data SIZE_t size):    Encapsulate data into a 1D numpy array of intps    cdef npnpy_intp shape[1]    shape[0]  npnpy_intp size    return npPyArray_SimpleNewFromData(1 shape npNPY_INTP data)specifically this call:npPyArray_SimpleNewFromData(1 shape npNPY_INTP data)In the test case shape  1 data  [ 2 ] and it triggers a segfault I have been reading about PyArray_SimpleNewFromData but figured that it might be easier to ask So (1) do you agree with this layout and (2) do you know why this segfault is being triggered Thankscc @glouppe @arjoly @ogrisel ,,2982,0.7484909456740443,0.2016036655211913,43949,515.1425515938929,39.52308357414275,133.24535256774897,7096,76,1848,307,travis,jmschrei,arjoly,false,arjoly,7,0.7142857142857143,35,5,902,false,true,false,false,5,82,12,7,0,0,4
9130897,scikit-learn/scikit-learn,python,5228,1441718145,1441885045,1441885045,2781,2781,github,false,true,false,21,1,1,21,13,0,34,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,103,10,103,10,18.28661745276657,0.38993365600977725,56,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|examples/ensemble/plot_feature_transformation.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,50,0.004592422502870264,4,5,false,[MRG+1] Apply method added to GradientBoosting Fixed the issues in #5222 cc @glouppe @ogrisel @arjoly @amueller Apologies about convoluting the PR,,2981,0.7484065749748406,0.19747416762342135,43987,514.9930661331757,39.51167390365335,133.15297701593653,7089,76,1848,314,travis,jmschrei,ogrisel,false,ogrisel,6,0.6666666666666666,35,5,902,false,true,false,false,5,80,11,5,0,0,24
9255698,scikit-learn/scikit-learn,python,5225,1441689172,1441824455,1441824455,2254,2254,github,false,false,false,10,2,1,0,15,0,15,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,42,0,42,9,8.991875455981804,0.1928915106235392,7,mathieu@mblondel.org,sklearn/linear_model/logistic.py|sklearn/utils/extmath.py,7,0.008045977011494253,0,1,false,[MRG + 1] Add numerically stable softmax function to utilsextmath ,,2980,0.7483221476510067,0.19310344827586207,43943,514.8715381289397,39.528480076462685,133.2635459572628,7084,76,1848,312,travis,MechCoder,MechCoder,true,MechCoder,68,0.8676470588235294,86,41,1176,true,true,false,false,4,24,3,10,8,0,801
9136853,scikit-learn/scikit-learn,python,5223,1441653589,1441742752,1441742752,1486,1486,github,false,false,false,42,2,1,1,2,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,33,4,33,8.354876346482822,0.17921772785038625,4,joel.nothman@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py,4,0.004597701149425287,0,0,false,[MRG + 1] BaggingClassifier decision_function should accept sparse matrix Added the option of passing in a sparse X matrix into decision function plus tests for sparse for all prediction functionsThis PR is still open and waiting for test results from Travis,,2978,0.7484889187374076,0.19310344827586207,43905,514.3605511900695,39.47158637968341,132.99168659605968,7077,76,1847,309,travis,mattilyra,glouppe,false,glouppe,5,0.8,6,1,1452,false,true,false,false,1,4,0,0,0,0,829
9126623,scikit-learn/scikit-learn,python,5222,1441642425,,1441714421,1199,,unknown,false,false,false,68,10,4,6,4,0,10,0,7,0,0,3,3,2,0,0,0,0,3,3,2,0,0,461,60,803,66,39.928518479941985,0.8564913826672604,45,tom.dupre-la-tour@m4x.org,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|doc/whats_new.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|doc/whats_new.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,45,0.0023094688221709007,4,4,false,[MRG] Expose an apply method for gradient boosters In response to #5209 I have added an apply method for gradient boosters This returns a matrix of dimensions (n_samples n_estimators n_classes) where each index is the terminal leaf which the sample ends up This mostly wraps the DecisionTree apply method and mirrors the RandomForest one A simple unit test has been added as wellcc @ogrisel @pprett @glouppe @arjoly ,,2977,0.7487403426268056,0.19399538106235567,43942,514.8832552000364,39.529379636793955,133.26657867188567,7075,76,1847,309,travis,jmschrei,jmschrei,true,,5,0.8,35,5,901,false,true,false,false,5,73,7,4,0,0,74
9125595,scikit-learn/scikit-learn,python,5220,1441629513,1441702551,1441702551,1217,1217,github,false,false,false,45,1,1,4,15,0,19,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,150,0,150,0,13.079912806561492,0.2805729440460174,53,tom.dupre-la-tour@m4x.org,doc/whats_new.rst|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,42,0.016241299303944315,0,3,false,[MRG+1] Optimize criterion update by performing reverse update The speed up idea is to update the left and the right node statistics from end to new_pos instead of from pos to new_pos if its beneficial (end - new_pos  new_pos - pos)Benchmarks are coming,,2975,0.7489075630252101,0.1937354988399072,43905,514.3605511900695,39.47158637968341,132.99168659605968,7072,76,1847,308,travis,arjoly,glouppe,false,glouppe,87,0.8160919540229885,29,26,1357,true,true,true,true,13,133,7,89,8,0,23
9244143,scikit-learn/scikit-learn,python,5219,1441620333,1452740267,1452740267,185332,185332,commits_in_master,false,false,true,130,3,1,0,22,0,22,0,9,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.953183403854169,0.10624931253711668,0,,AUTHORS.rst,0,0.0,0,10,false,[MRG+1] DOC update AUTHORSrst to better reflect team This removes a couple of names that seem to have been misplaced (I dont know if there are other names added here incorrectly and am not brave enough to suggest so) while adding some members of the [owners/reviewers teams](https://githubcom/orgs/scikit-learn/teams) that were absent from AUTHORSrst Members of that team that are not added by this PR are:* David Warde-Farley: very few contribs limited to datasets* Shiqiao Du: relatively few contribs to HMM which is now external~~Perhaps a statement the following people have been core contributors to scikit-learns development and maintenance is also due~~ Now included~~Name order is intended to roughly match chronology (which can be estimated with grep -n ^ *name doc/whats_newrst | tail -n1)~~ Now alphabetical by surname,,2974,0.7488231338264963,0.1930232558139535,43905,514.3605511900695,39.47158637968341,132.99168659605968,7070,76,1847,465,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,120,0.7083333333333334,30,1,2323,true,true,false,false,29,278,40,147,28,3,2870
9240402,scikit-learn/scikit-learn,python,5218,1441604059,1441756474,1441756474,2540,2540,commits_in_master,false,false,false,23,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.920828447860427,0.10555527568218642,0,,doc/modules/dp-derivation.rst,0,0.0,0,0,false,Correct expectation notation in DP-GMM doc Unify expectation notation to E_q[\log P(X_i|z_ik)]which is thoroughly used in the other part of the document,,2973,0.7487386478304743,0.19324796274738068,43905,514.3605511900695,39.47158637968341,132.99168659605968,7067,76,1847,309,travis,mooz,amueller,false,amueller,0,0,302,35,2459,false,true,false,false,0,0,0,0,0,0,2540
9123707,scikit-learn/scikit-learn,python,5216,1441479208,1441975360,1441975360,8269,8269,commits_in_master,false,false,false,74,5,1,3,11,0,14,0,7,0,0,1,2,1,0,0,0,0,2,2,2,0,0,1,0,11,17,4.546560566977214,0.09752696301812833,6,michigraber@gmail.com,sklearn/lda.py,6,0.006920415224913495,0,7,false,Creation of the attribute LDAexplained_variance_ratio_ for the eige… Creation of the attribute LDAexplained_variance_ratio_ for the eigen solver It is an analogy to PCAexplained_variance_ratio and works in the same wayOnly one line of code has been added there was not much to change However the attribute is not available for the default solver as Im not good enough to change this part of the codeThis is a PR to solve the issue #5210 ,,2972,0.7486541049798116,0.1914648212226067,43905,514.3605511900695,39.47158637968341,132.99168659605968,7060,77,1845,313,travis,rififi,agramfort,false,agramfort,0,0,0,0,1078,false,false,false,false,0,0,0,0,0,0,970
9134402,scikit-learn/scikit-learn,python,5214,1441400647,1445631742,1445631742,70518,70518,commits_in_master,false,false,false,205,58,36,154,62,0,216,0,9,13,1,18,34,17,0,1,13,1,20,34,17,0,1,6500,2023,6926,2067,423.89756224009807,9.09290468411173,29,tom.dupre-la-tour@m4x.org,benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/estimator_checks.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|examples/classification/plot_classifier_comparison.py|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|examples/neural_networks/plot_mlp_training_curves.py|examples/neural_networks/plot_mnist_filters.py|sklearn/neural_network/__init__.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/estimator_checks.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/base.py|sklearn/neural_network/sgd_optimizer.py|sklearn/neural_network/sgd_optimizer.py|sklearn/neural_network/stochastic_optimizer.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/stochastic_optimizer.py|sklearn/neural_network/stochastic_optimizer.py|sklearn/neural_network/tests/test_stochastic_optimizer.py|sklearn/neural_network/stochastic_optimizer.py|sklearn/neural_network/tests/test_stochastic_optimizer.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/estimator_checks.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|examples/classification/plot_classifier_comparison.py|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|examples/neural_networks/plot_mlp_training_curves.py|examples/neural_networks/plot_mnist_filters.py|sklearn/neural_network/__init__.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/estimator_checks.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/modules/neural_networks_supervised.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|examples/neural_networks/plot_mlp_training_curves.py|examples/neural_networks/plot_mnist_filters.py|sklearn/neural_network/_base.py|sklearn/neural_network/_stochastic_optimizers.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/tests/test_stochastic_optimizers.py|sklearn/utils/estimator_checks.py,20,0.0,0,32,true,Mlp with adam nesterovs momentum early stopping Based on #3204 and #3939Code and results for timing and performance benchmarks on MNIST 20 New Groups and RCV1 can be found at https://gistgithubcom/glennq/44ca8b66770430ee10f9Basically my observation is that adam performs consistently well Early stopping cannot help to improve performance for adam but can reduce training time significantly In other updating schemes adaptive learning rate (divide by 5 if not improving) with nesterovs momentum also lines just after adam followed by constant learning rate with nesterovs momentum The problem with adaptive learning rate though is that training time is significantly longerAlso there are cases when early stopping can increase training time which I think is due to the fact that the heuristic we use (stop if loss not improving over best loss by tol for more than two consecutive iterations) can be affected if training loss has large variation but validation score is more smoothly improving This affects update schemes using momentum but it seems to be not a problem for adamThe following is a visualized classifier comparison modified from http://scikit-learnorg/dev/auto_examples/classification/plot_classifier_comparisonhtml[figure_1](https://cloudgithubusercontentcom/assets/5950453/9690764/60b1c5c8-530c-11e5-83f7-1095741a170cpng)Ill still work on benchmarking small datasets such as digits and iris and probably plot training/validation loss for each iterations for more insights,,2971,0.7485695052170986,0.18949771689497716,43905,514.3605511900695,39.47158637968341,132.99168659605968,7055,76,1844,378,travis,glennq,amueller,false,amueller,0,0,14,4,658,true,true,true,false,0,0,0,0,26,0,71
9186146,scikit-learn/scikit-learn,python,5208,1441252469,,1449331634,134652,,unknown,false,false,false,21,1,1,0,3,0,3,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,36,0,36,0,14.035524717926814,0.301072003757634,18,thomas.unterthiner@gmx.net,sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/preprocessing/label.py,10,0.010368663594470046,0,2,false,fixed doctests on i686 related to #5193 #5177 #5197-- Reviewable:start --[img srchttps://reviewableio/review_buttonpng height40 altReview on Reviewable/](https://reviewableio/reviews/scikit-learn/scikit-learn/5208)-- Reviewable:end --,,2969,0.7490737622094982,0.18663594470046083,43905,514.3605511900695,39.47158637968341,132.99168659605968,7039,77,1842,449,travis,garbas,garbas,true,,1,0.0,119,16,2579,false,false,false,false,1,5,1,0,0,0,8336
9125338,scikit-learn/scikit-learn,python,5207,1441246865,1441792094,1441792094,9087,9087,commits_in_master,false,false,false,36,1,1,4,21,0,25,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,24,10,24,8.995350127119533,0.1929566682900189,9,rvraghav93@gmail.com,sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py,9,0.010392609699769052,0,10,false,[MRG+1] [BUG] AdaBoostRegressor should not raise errors if the base_estimator does not support sample_weights AdaBoostRegressor should work without sample_weights in the base estimatorThe random weighted sampling is done internally in the _boost method in AdaBoostRegressor,,2968,0.748989218328841,0.18475750577367206,43905,514.3605511900695,39.47158637968341,132.99168659605968,7039,77,1842,310,travis,MechCoder,glouppe,false,glouppe,67,0.8656716417910447,86,41,1170,true,true,false,false,4,11,2,5,7,0,15
9157454,scikit-learn/scikit-learn,python,5206,1441243204,1441243229,1441243229,0,0,commits_in_master,false,false,false,81,5,1,1,10,0,11,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,143,0,4.556024677792427,0.09772997493711139,2,gael.varoquaux@normalesup.org,setup.py,2,0.0023148148148148147,0,0,false,[MRG+1] Added check to ensure that NumPy and SciPy meet minimum version requirements Added a check to the build process to ensure that NumPy and SciPy meet minimum version requirements (NumPy  161 and SciPy  09) as suggested in issue #5202  The check is performed by the [parse_version](http://pythonhostedorg/setuptools/pkg_resourceshtml#parsing-utilities) function defined in setuptools which is the function used by pip and easy_install to handle version comparison  Using this utility versions are parsed according to the sorting algorithm defined in [PEP 0440](https://wwwpythonorg/dev/peps/pep-0440/)  ,,2967,0.7489046174587125,0.1840277777777778,43905,514.3605511900695,39.47158637968341,132.99168659605968,7039,77,1842,316,travis,acganesh,acganesh,true,acganesh,0,0,1,2,1013,true,false,false,false,0,0,0,0,1,0,26
9105380,scikit-learn/scikit-learn,python,5204,1441237918,1442346119,1442346119,18470,18470,merged_in_comments,false,true,false,218,3,1,44,23,0,67,0,4,0,0,3,4,3,0,0,0,0,4,4,3,0,0,108,79,135,249,13.694857581850574,0.2937644510060189,12,ragvrv@gmail.com,sklearn/ensemble/bagging.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,9,0.008130081300813009,0,5,false,Enabling sample weights for 2 of the 3 logistic regression solvers I observed that logistic regression could not be trained with sample weights By digging deeper I found that two of the three solvers (lbfgs and newton-cg) could handle sample weights while the default solver (liblinear) could not and as a result sample weights were hidden from the user completely To remedy this I have:* added a ValueError when the liblinear solver is used with sample weights* allowed for sample_weight to be a parameter in both LogisticRegressionfit() and LogisticRegressionCVfit()* documented the parameter everywhere it can be passed* updated the documentation for the class_weight parameter to note that it will be multiplied by sample_weight if sample_weight is provided in fit()* handled sample weights appropriately throughout the fitting by passing them to logistic_regression_path and _log_reg_scoring_path* added a new test function in test_logisticpy with four different tests to make sure:   * that a ValueError is thrown when liblinear is used with sample weights   * that passing sample weights as npones is the same as not passing them (default None)   * passing sample weights to both lbfgs and newton-cg solvers yields the same results   * passing class weights to scale classes is the same as passing sample weights that scale the training examples of those classes,,2966,0.74881995954147,0.18234610917537747,43905,514.3605511900695,39.47158637968341,132.99168659605968,7038,76,1842,318,travis,vstolbunov,MechCoder,false,MechCoder,0,0,1,6,930,true,false,true,false,0,0,0,0,1,0,101
9108134,scikit-learn/scikit-learn,python,5203,1441212336,1441627674,1441627674,6922,6922,commits_in_master,false,false,false,346,1,1,9,37,0,46,0,6,0,0,4,4,3,0,0,0,0,4,4,3,0,0,413,0,413,0,12.960285500839646,0.2780073565774864,57,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,46,0.01522248243559719,1,18,false,[MRG+1] Optimize MSE criterion by avoiding computing constant terms during split optimization In the pull request #5041 @jmschrei proposes to approximate the impurityimprovement during the optimisation by avoiding to compute all the constantterms Since this could be done simply by adding only one method I have decidedto implement this optimisationThe benchmarks show that:- GBRT is 35% faster on covertype- random forest and decision tree are 10% faster on mnist regression- no impact on extra trees / random forest / decision tree classifier- trees between master and this pr are different due to differences in numerical errors and in tie breakingPS : I havent been able to benchmark against #5041 since I had segmentation faultsHere are the raw benchmark results± make in && python benchmarks/bench_mnistpy --classifiers RandomForest ExtraTrees CARTClassifier                          train-time test-time error-ratemaster ExtraTrees                   4473s       050s       00288master RandomForest                 4224s       043s       00304master CART                         2087s       002s       01214thispr ExtraTrees                   4692s       050s       00294thispr RandomForest                 4175s       042s       00318thispr CART                         2056s       018s       01219± python benchmarks/bench_covertypepy --classifiers CART ExtraTrees GBRT RandomForestClassifier                          train-time test-time error-ratemaster RandomForest  621709s   03572s     00334master ExtraTrees    409040s   05348s     00366master CART          152244s   00225s     00425master GBRT         6820862s   02816s     01777thispr RandomForest  660639s   03583s     00330thispr ExtraTrees    436342s   05493s     00372thispr CART          148608s   00683s     00424thispr GBRT         5070393s   03322s     01777  # mnist in regressionfrom sklearndatasets import fetch_mldatafrom sklearnensemble import RandomForestRegressorfrom sklearnensemble import ExtraTreesRegressorfrom sklearntree import DecisionTreeRegressortry:    from time import process_time as bench_timeexcept ImportError:    from time import time as bench_timedata  fetch_mldata(MNIST original)for name EstimatorClass in  [(random forest RandomForestRegressor)                              (extra trees ExtraTreesRegressor)                              (cart DecisionTreeRegressor)]:    est  EstimatorClass(random_state0)    time_start  bench_time()    estfit(data[data] data[target])    chrono  bench_time() - time_start    print(%s  %s % (name chrono))# ± make in && python bench_mnist_regressionpy## master random forest  155378248# master extra trees    13687747000000002# master cart  25426823## thispr random forest  13921123# thispr extra trees    134557126# thispr cart           23304993,,2965,0.7487352445193929,0.18266978922716628,43905,514.3605511900695,39.47158637968341,132.99168659605968,7034,75,1842,305,travis,arjoly,arjoly,true,arjoly,86,0.813953488372093,29,26,1352,true,true,false,false,13,114,5,75,5,0,6
9158965,scikit-learn/scikit-learn,python,5201,1441142169,1441181771,1441181771,660,660,github,false,false,false,22,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.451687579185334,0.09549188155391568,15,ragvrv@gmail.com,sklearn/cross_validation.py,15,0.017584994138335287,0,0,false,DOC Clarify random_state parameter in KFold() doc Without paying attention we could think KFold(30 random_state123)and KFold(30 random_state124) generate two different versions,,2964,0.7486504723346828,0.18053927315357562,43905,514.3605511900695,39.47158637968341,132.99168659605968,7031,75,1841,296,travis,christophebourguignat,GaelVaroquaux,false,GaelVaroquaux,3,1.0,13,0,497,false,false,false,false,2,4,3,0,0,0,180
9103579,scikit-learn/scikit-learn,python,5200,1441130320,1441839317,1441839317,11816,11816,commits_in_master,false,false,false,39,2,1,3,2,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,5,0,4.3602887266491255,0.0935313108163464,6,larsmans@users.noreply.github.com,sklearn/datasets/svmlight_format.py,6,0.007001166861143524,0,0,false,added comments for y in dump_svmlight_file() method Hey guys I added the comments below the y variableThis is my first time contribution:) so if anything doesnt look right please let me know and thank you for your help,,2963,0.7485656429294634,0.17969661610268378,43905,514.3605511900695,39.47158637968341,132.99168659605968,7029,75,1841,309,travis,jackzhang84,amueller,false,amueller,0,0,0,0,506,false,false,false,false,1,1,0,0,0,0,1632
9104148,scikit-learn/scikit-learn,python,5199,1441128604,1441760654,1441760654,10534,10534,commits_in_master,false,false,false,31,4,3,4,5,0,9,0,4,0,0,7,8,6,0,0,0,0,8,8,6,0,0,20,57,20,57,33.01411992313845,0.7081763907877825,20,t3kcit@gmail.com,sklearn/cluster/tests/test_k_means.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/tests/test_sparse_pca.py|sklearn/utils/testing.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|doc/faq.rst,9,0.008168028004667444,0,2,false,[MRG+1] MAINT bump joblib to 090b4 to use forkserver for Py3 & POSIX This upgrades joblib to use the safer forkserver mode of multiprocessing under Python 34 and later by default,,2962,0.7484807562457799,0.17969661610268378,43905,514.3605511900695,39.47158637968341,132.99168659605968,7029,75,1841,309,travis,ogrisel,amueller,false,amueller,126,0.8650793650793651,1209,124,2288,true,true,false,true,17,262,41,177,27,3,122
9150676,scikit-learn/scikit-learn,python,5197,1441120166,,1441792751,11209,,unknown,false,true,false,20,1,1,0,10,0,10,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,36,0,36,0,14.023140910165763,0.30838707738103793,0,,sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/preprocessing/label.py,0,0.0,0,1,false,[MRG] fixed doctests on i686 rel #5193 #5177-- Reviewable:start --[img srchttps://reviewableio/review_buttonpng height40 altReview on Reviewable/](https://reviewableio/reviews/scikit-learn/scikit-learn/5197)-- Reviewable:end --,,2961,0.7487335359675785,0.17777777777777778,42724,524.6231626252223,40.258402771276096,136.57429079674188,7028,75,1841,308,travis,garbas,glouppe,false,,0,0,119,16,2578,false,false,false,false,1,0,0,0,0,0,18
9093666,scikit-learn/scikit-learn,python,5195,1441112532,1441754571,1441754571,10700,10700,commits_in_master,false,false,false,123,5,5,1,6,0,7,0,3,0,0,7,7,2,1,0,0,0,7,7,2,1,0,19,0,19,0,33.158118843223384,0.7112653574191724,57,trev.stephens@gmail.com,doc/Makefile|doc/themes/scikit-learn/layout.html|doc/whats_new.rst|doc/sphinxext/gen_rst.py|doc/about.rst|doc/testimonials/testimonials.rst|doc/index.rst,47,0.005834305717619603,0,0,false,[MRG+1] PDF documentation There has been some interest apparently in PDF documentation I can see it provides a few benefits even if its certainly not how the documentation is intendedWeve fixed up a few glitches with it previously This PR combines a few further fixes:* a patch to example gallery generation that has been accepted to Sphinx-Gallery (https://githubcom/sphinx-gallery/sphinx-gallery/pull/59)* image sizes (Sphinx image sizes in px are ignored when generating latex)* some other HTML-specific content that was showing up strangely* making whats new substantially more readable in PDFThe last change is likely more controversial: Ive added a Makefile command dist to compile the PDF and copy into into the html directory structure where it is linked from layouthtml,,2959,0.748901655964853,0.17736289381563594,43905,514.3605511900695,39.47158637968341,132.99168659605968,7026,75,1841,311,travis,jnothman,amueller,false,amueller,119,0.7058823529411765,30,1,2317,true,true,false,false,29,307,39,173,31,3,151
9167657,scikit-learn/scikit-learn,python,5194,1441066979,1441104718,1441104719,629,628,github,false,false,false,47,2,2,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,52,0,52,0,17.375830369769933,0.3727233093138519,14,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/learning_curve.py|sklearn/cross_validation.py|sklearn/learning_curve.py,14,0.016412661195779603,0,0,false,[MRG+1] DOC Updated documentation for cv parameter Updated cv parameter documentation in : def cross_val_predict(estimator X yNone cvNone n_jobs1def cross_val_score(estimator X yNone scoringNone cvNone n_jobs1def permutation_test_score(estimator X y cvNonein cross_validationpyanddef validation_curve(estimator X y param_name param_range cvNonein validation_curvepyFix for issue #4533,,2958,0.748816768086545,0.17819460726846426,43876,513.9484000364664,39.47488376333303,132.92004740632694,7019,75,1840,295,travis,christophebourguignat,GaelVaroquaux,false,GaelVaroquaux,2,1.0,13,0,496,false,false,false,false,2,3,2,0,0,0,604
9220659,scikit-learn/scikit-learn,python,5193,1441059583,,1441752697,11551,,unknown,false,true,false,10,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.750628996920194,0.10190420140424544,12,thomas.unterthiner@gmx.net,sklearn/preprocessing/data.py,12,0.01411764705882353,0,0,false,[MRG+1] doc test one hot encoder ellipsis fix Fixes #5177,,2957,0.7490700033818058,0.1776470588235294,43877,513.9366866467625,39.47398409189325,132.91701802766826,7019,75,1840,312,travis,amueller,amueller,true,,326,0.8496932515337423,1225,40,1774,true,true,false,false,122,1060,79,365,61,12,580
9109141,scikit-learn/scikit-learn,python,5190,1440964697,1441703457,1441703457,12312,12312,commits_in_master,false,false,false,6,4,2,14,12,0,26,0,4,0,0,4,5,2,0,0,0,0,5,5,2,0,0,308,200,344,200,37.39066427074302,0.8028739598282418,32,tvas@sics.se,doc/modules/classes.rst|doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,15,0.016706443914081145,0,3,false,[MRG+1] Label K-Fold This supersedes #4444,,2956,0.7489851150202977,0.1766109785202864,43817,514.6404363603167,39.5280370632403,133.09902549238882,7018,75,1839,315,travis,glouppe,glouppe,true,glouppe,54,0.9629629629629629,162,26,1753,true,true,false,false,9,67,12,59,62,0,9
9139954,scikit-learn/scikit-learn,python,5189,1440957904,1440971364,1440971364,224,224,github,false,false,false,17,6,6,0,1,0,1,0,1,0,0,3,3,2,0,0,0,0,3,3,2,0,0,330,22,330,22,41.07767082089168,0.8820914885072494,41,trev.stephens@gmail.com,sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py|doc/whats_new.rst|sklearn/cluster/mean_shift_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py,41,0.004790419161676647,0,0,false,Pr 4779: Implemented parallelised version of mean_shift and test function Rebased version of #4779 with minor change,,2955,0.7489001692047378,0.17604790419161676,43784,494.2673122601864,38.68993239539558,129.77343321761376,7018,75,1839,301,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,56,0.8035714285714286,682,3,2015,true,true,false,false,15,115,24,52,10,1,224
9138733,scikit-learn/scikit-learn,python,5188,1440949871,1440958307,1440958307,140,140,github,false,false,false,21,10,10,0,0,0,0,0,1,0,0,9,9,8,0,0,0,0,9,9,8,0,0,140,204,140,204,106.2310751298099,2.281176257973752,46,tw991@nyu.edu,doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/regression.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/neighbors/tests/test_neighbors.py,42,0.006009615384615385,0,0,false,Support metricprecomputed in nearest neighbors [rebased version of #4090] Rebased version of #4090 Waiting for travis to be green and merging,,2954,0.7488151658767772,0.17668269230769232,43792,493.94866642309097,38.66002922908294,129.68122031421265,7018,75,1839,303,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,55,0.8,682,3,2015,true,true,false,false,14,109,21,50,9,0,-1
9138035,scikit-learn/scikit-learn,python,5187,1440943139,1440946565,1440946565,57,57,github,false,false,false,29,3,3,0,1,0,1,0,1,0,0,14,14,13,0,0,0,0,14,14,13,0,0,251,25,251,25,49.0653345918687,1.0536031519084657,10,tw991@nyu.edu,doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/base.py|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/classification.py|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pxd|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/kd_tree.c|sklearn/neighbors/kd_tree.pyx|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py,5,0.001201923076923077,1,0,false,Parallel queries in nearest neighbor search This is a version of  #4009 with merge conflicts fixedIll merge if travis passes @fredericov: do you want to have a look,,2953,0.7487301049779885,0.17668269230769232,43768,494.2195211113142,38.65838055200147,129.72948272710656,7016,75,1839,306,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,54,0.7962962962962963,682,3,2015,true,true,false,false,14,103,16,49,9,0,56
9138608,scikit-learn/scikit-learn,python,5186,1440939377,1440948346,1440948347,149,149,github,false,false,false,9,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.375928493942303,0.09396616034780791,3,t3kcit@gmail.com,sklearn/manifold/t_sne.py,3,0.003605769230769231,0,0,false,[MRG+1] ENH exposing extra parameters in t-sne closes #5165,,2952,0.7486449864498645,0.17668269230769232,43737,493.9982166129364,38.6629169810458,129.66138509728606,7015,75,1839,307,travis,sdvillal,jnothman,false,jnothman,0,0,12,2,1644,false,true,false,false,1,0,0,0,0,0,18
9132603,scikit-learn/scikit-learn,python,5184,1440888113,1440958392,1440958392,1171,1171,github,false,true,false,8,1,1,0,3,0,3,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,4.258912488210082,0.09145445739678683,10,t3kcit@gmail.com,sklearn/grid_search.py,10,0.012062726176115802,0,6,false,DOC Updated documentation for cv parameter (issue #4533) ,,2950,0.7488135593220339,0.17370325693606756,43729,494.08859109515424,38.669990166708594,129.68510599373414,7006,75,1838,307,travis,christophebourguignat,GaelVaroquaux,false,GaelVaroquaux,1,1.0,12,0,494,false,false,false,false,1,0,1,0,0,0,288
9089049,scikit-learn/scikit-learn,python,5182,1440806895,1440978251,1440978251,2855,2855,github,false,false,false,15,3,1,2,13,0,15,0,7,0,0,3,3,3,0,0,0,0,3,3,3,0,0,40,12,93,36,13.289032251047527,0.28535023194429116,15,t3kcit@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,10,0.00834326579261025,0,2,true,[MRG + 2] predict_proba should use the softmax function in the multinomial case Fixes https://githubcom/scikit-learn/scikit-learn/issues/5176,,2949,0.7487283825025433,0.17044100119189512,43807,514.7350879996347,39.53706028716872,133.12940854201383,6997,75,1837,307,travis,MechCoder,GaelVaroquaux,false,GaelVaroquaux,66,0.8636363636363636,86,41,1165,true,true,false,false,4,3,1,1,4,0,1131
9110833,scikit-learn/scikit-learn,python,5179,1440775677,,1445449374,77894,,unknown,false,false,false,68,2,1,1,5,0,6,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,62,0,62,24,4.2834939119219415,0.09198161086254868,4,t3kcit@gmail.com,sklearn/multiclass.py,4,0.004733727810650888,0,1,true,Added partial_fit method to OneVsRestClassifier I noticed that the OneVsRestClassifier currently does not support incremental updates via partial_fit even if the underlying classifiers do This is inconvenient when not all training data will fit into RAMIve fixed this but I havent written test cases yet since Im not totally familiar with how this is done for sklearn Id appreciate some pointers on what Id need to add,,2947,0.7492365117068205,0.1680473372781065,43709,493.0334713674529,38.61904870850397,129.584296140383,6997,75,1837,373,travis,phdowling,agramfort,false,,0,0,18,21,1524,false,false,false,false,0,0,0,0,1,0,217
9084727,scikit-learn/scikit-learn,python,5174,1440687930,1441081856,1441081856,6565,6565,github,false,false,false,44,1,1,9,13,0,22,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,50,17,50,17,9.003520795113186,0.19313160837878868,6,t3kcit@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py,6,0.007125890736342043,0,2,false,[MRG+2] ENH sparse precomputed distance matrix in DBSCAN Arguably fixes #5163 Allowing the user to provide an incomplete (sparse) precomputed distance matrix puts them in control of how (efficiently) radius neighbors are computed and allows this data to be reused with other parameters changing,,2946,0.7491513917175832,0.16508313539192399,43876,513.9484000364664,39.47488376333303,132.92004740632694,6989,75,1836,311,travis,jnothman,jnothman,true,jnothman,118,0.7033898305084746,30,1,2312,true,true,false,false,25,277,33,169,29,3,79
9079282,scikit-learn/scikit-learn,python,5167,1440629359,1440686309,1440686309,949,949,github,false,false,false,17,1,1,0,4,0,4,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.589356491380756,0.0985496871458306,5,t3kcit@gmail.com,doc/modules/clustering.rst,5,0.005988023952095809,0,0,false,fixes the wrap issue  This is goofy but I had to reopen a pull to close #5139,,2944,0.7493206521739131,0.1592814371257485,43708,491.4889722705226,38.50553674384552,129.44998627253594,6983,75,1835,308,travis,NoonienSoong,ogrisel,false,ogrisel,4,0.25,9,39,419,true,true,true,false,1,1,7,0,8,0,424
9069458,scikit-learn/scikit-learn,python,5162,1440597574,1440943982,1440943982,5773,5773,github,false,false,false,28,1,1,2,5,0,7,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.729788652839241,0.10156501906259123,3,t3kcit@gmail.com,doc/faq.rst,3,0.0035714285714285713,0,1,false,[MRG] DOC explain fork related issues in FAQ + stopgap for Python 34 Here is a new FAQ entry to factor the wisdom of the discussion in #636,,2943,0.7492354740061162,0.15833333333333333,43709,492.75892836715553,38.59617012514585,129.5614175570249,6978,75,1835,316,travis,ogrisel,glouppe,false,glouppe,125,0.864,1208,124,2282,true,true,false,true,16,250,36,190,28,3,1010
9072275,scikit-learn/scikit-learn,python,5161,1440565906,1440692543,1440692543,2110,2110,github,false,false,false,22,6,1,10,8,4,22,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,18,0,44,39,4.0762271553976674,0.08753098863734601,4,t3kcit@gmail.com,sklearn/cross_validation.py,4,0.0047789725209080045,0,4,false,[MRG+1] Add check for sparse prediction in cross_val_predict (fixes #5132) This pull request is for #5132cross_val_predict should work for sparse y,,2942,0.7491502379333786,0.15890083632019117,43708,491.4889722705226,38.50553674384552,129.44998627253594,6973,75,1835,309,travis,beepee14,ogrisel,false,ogrisel,1,1.0,9,4,580,true,false,false,false,1,7,2,1,2,0,7
9045769,scikit-learn/scikit-learn,python,5159,1440543449,1444057225,1444057225,58562,58562,commit_sha_in_comments,false,false,false,30,2,2,4,7,0,11,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,45,28,45,28,18.190044842358116,0.3906045481146563,7,t3kcit@gmail.com,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py,5,0.005980861244019139,0,3,false,[MRG] Fix sparse_encode input checks (issue #5158) Following recent PR sparse_encode did no longer check that dictionary was dictionary was C-ordered yielding wrong results and bad performanceShould fix #5158 ,,2940,0.7493197278911564,0.1590909090909091,43699,491.59019657200395,38.51346712739422,129.47664706286184,6968,73,1834,347,travis,arthurmensch,ogrisel,false,ogrisel,6,0.16666666666666666,1,1,871,true,true,false,false,0,36,9,18,47,0,670
9043848,scikit-learn/scikit-learn,python,5157,1440535696,1440550833,1440550833,252,252,github,false,false,false,14,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,4.495576829590601,0.09653592232757317,10,trev.stephens@gmail.com,sklearn/linear_model/ridge.py,10,0.011933174224343675,0,0,false,[MRG] DOC mention intercept_ attribute in ridge docstring Add undocumented intercept_ attributeVia #5145,,2939,0.7492344334807758,0.15871121718377088,43699,491.59019657200395,38.51346712739422,129.47664706286184,6967,73,1834,308,travis,amueller,agramfort,false,agramfort,325,0.8492307692307692,1219,40,1768,true,true,true,false,119,1065,81,354,63,13,252
9041785,scikit-learn/scikit-learn,python,5154,1440502561,1440522777,1440522777,336,336,commit_sha_in_comments,false,false,false,7,1,0,0,3,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,[MRG+1] DOC rearrange related projects supersedes #5151,,2938,0.7491490810074881,0.15827338129496402,43699,491.59019657200395,38.51346712739422,129.47664706286184,6959,73,1834,309,travis,jnothman,jnothman,true,jnothman,117,0.7008547008547008,30,1,2310,true,true,false,false,24,259,29,153,26,3,142
8940564,scikit-learn/scikit-learn,python,5152,1440471351,1441817492,1441817492,22435,22435,commit_sha_in_comments,false,false,false,39,3,1,84,17,0,101,0,6,0,0,49,49,46,0,0,0,0,49,49,46,0,0,253,198,271,228,221.9748293084092,4.761518218003113,95,tw991@nyu.edu,doc/modules/model_evaluation.rst|doc/modules/model_persistence.rst|doc/tutorial/basic/tutorial.rst|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_covariance.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/feature_selection/base.py|sklearn/feature_selection/tests/test_base.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/tests/test_variance_threshold.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/lda.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/mixture/gmm.py|sklearn/naive_bayes.py|sklearn/neighbors/approximate.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py|sklearn/neural_network/tests/test_rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_dummy.py|sklearn/tests/test_naive_bayes.py|sklearn/tests/test_random_projection.py|sklearn/tree/tests/test_tree.py|sklearn/utils/__init__.py|sklearn/utils/estimator_checks.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,26,0.006053268765133172,1,16,false,[MRG + 1] Warn on 1D arrays addresses #4511 Just to reiterate the discussion I had with @amueller in personI have put a check in check_array and issued a warning I will proceed to fix the tests now ,,2937,0.7490636704119851,0.15980629539951574,43905,514.3605511900695,39.47158637968341,132.99168659605968,6952,73,1833,329,travis,vighneshbirodkar,ogrisel,false,ogrisel,0,0,13,1,897,true,false,false,false,0,0,0,0,2,0,77
9032550,scikit-learn/scikit-learn,python,5151,1440466043,,1440522784,945,,unknown,false,true,false,7,1,1,0,1,0,1,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.678946213302467,0.1004735979416507,7,larsmans@users.noreply.github.com,doc/related_projects.rst,7,0.00849514563106796,0,0,false,DOC at PMML project to related projects ,,2936,0.7493188010899182,0.15898058252427186,43699,491.59019657200395,38.51346712739422,129.47664706286184,6951,72,1833,310,travis,amueller,amueller,true,,324,0.8518518518518519,1216,40,1767,true,true,false,false,116,1040,76,318,62,13,921
8922385,scikit-learn/scikit-learn,python,5144,1440114116,1440678675,1440678675,9409,9409,commit_sha_in_comments,false,false,false,11,1,0,10,3,0,13,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,update the faq with detailed bunch object notes #5137 and #5140,,2935,0.7492333901192504,0.14988290398126464,43708,491.4889722705226,38.50553674384552,129.44998627253594,6913,71,1829,317,travis,NoonienSoong,NoonienSoong,true,NoonienSoong,3,0.0,9,39,413,true,true,false,false,1,1,4,0,6,0,304
8980776,scikit-learn/scikit-learn,python,5143,1440090077,1440353124,1440353124,4384,4384,github,false,false,false,32,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.219812171550939,0.09061369026752963,6,larsmans@gmail.com,sklearn/base.py,6,0.007017543859649123,0,1,false,[MRG+1] FIX: for changing mode Due to the changes in r2_score I was getting a lot of DeprecationWarnings This should preserve the old behaviorAlternatively an argument could be added to score,,2934,0.7491479209270621,0.14853801169590644,43674,491.23048037734117,38.4897192837844,129.45917479507258,6911,71,1829,312,travis,Eric89GXL,agramfort,false,agramfort,0,0,26,1,1067,false,false,false,true,0,0,0,0,0,0,1503
8915757,scikit-learn/scikit-learn,python,5141,1440023663,1445436107,1445436107,90207,90207,commits_in_master,false,true,false,199,9,2,15,86,0,101,0,11,1,0,6,10,6,0,0,1,0,9,10,8,0,0,771,145,1996,254,41.55855699431744,0.8924074380327015,43,trev.stephens@gmail.com,benchmarks/bench_plot_randomized_svd.py|sklearn/utils/tests/test_extmath.py|benchmarks/bench_plot_randomized_svd.py|doc/whats_new.rst|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,39,0.0023148148148148147,0,18,false,[WIP] ENH: optimizing power iterations phase for randomized_svd I am writing some benchmarks to see if we can set a default number of power iterations for utilsextmathrandomized_SVD The function implements the work of *Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions* Halko et al 2009 http://arxivorg/abs/arXiv:09094061See the code for mode detailsStill to do:- [x] delmake faster benchmarks with scipysparselinalgsvds/del- [x] review unit tests for randomized_svd- [x] benchmark if we can find an *absolute number* regardless of input matrix rank  eg is 1 *always* better than 0 In other words can we find a small integer such that we always improve approximation quality while not making the algorithm significantly slower- [x] extract at least 5 components in every experiment- [ ] extend randomized_svd with *normalized power iterations* with QR at each step and another version with LU- [ ] sample the random matrix from U[-11] instead of from a Gaussian- [ ] docsAnd finally- [x] set a default value for n_iter in randomized_svd and a default strategy for normalizing matrices between power iterations- [ ] review all use of randomized_svd (in TruncatedSVD and RandomizedPCA),,2933,0.7490623934538015,0.14351851851851852,43698,491.60144629044805,38.51434848276809,129.47961005080325,6896,71,1828,381,travis,giorgiop,ogrisel,false,ogrisel,1,1.0,2,7,999,true,false,true,false,0,6,1,2,14,0,24
8916719,scikit-learn/scikit-learn,python,5140,1440023212,,1440116346,1552,,unknown,false,false,false,4,2,1,1,0,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.475935077936483,0.09611351840350785,2,t3kcit@gmail.com,doc/faq.rst,2,0.0023148148148148147,0,0,false,update faqrst closes #5137,,2932,0.7493178717598908,0.14351851851851852,43674,491.23048037734117,38.4897192837844,129.45917479507258,6896,71,1828,312,travis,NoonienSoong,NoonienSoong,true,,2,0.0,9,39,412,true,true,false,false,1,1,3,0,4,0,1150
8970126,scikit-learn/scikit-learn,python,5139,1440022654,,1440629354,10111,,unknown,false,false,false,6,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.601344316775444,0.09880648042713831,5,t3kcit@gmail.com,doc/modules/clustering.rst,5,0.005793742757821553,0,0,false,exapmle added to user_guide/clusteringrst closes #5130,,2931,0.7495735243944046,0.1436848203939745,43674,491.23048037734117,38.4897192837844,129.45917479507258,6896,71,1828,315,travis,NoonienSoong,NoonienSoong,true,,1,0.0,9,39,412,true,true,false,false,1,1,2,0,3,0,7261
8913412,scikit-learn/scikit-learn,python,5138,1440012018,,1440021724,161,,unknown,false,false,false,2,2,1,2,2,0,4,0,1,0,0,1,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,4.626278181734908,0.09934189513868433,2,t3kcit@gmail.com,doc/faq.rst,2,0.0023121387283236996,0,0,false,fix NoonienSoong/faqrst#5137 ,,2930,0.7498293515358362,0.14219653179190753,43674,491.23048037734117,38.4897192837844,129.45917479507258,6891,71,1828,308,travis,NoonienSoong,NoonienSoong,true,,0,0,9,39,412,true,false,false,false,0,1,0,0,1,0,42
8902259,scikit-learn/scikit-learn,python,5133,1439908485,1440090770,1440090770,3038,3038,commits_in_master,false,false,false,125,1,0,21,12,0,33,0,4,0,0,0,5,0,0,0,0,0,5,5,4,0,0,0,0,109,38,0,0.0,0,,,0,0.0,0,1,false,[WIP] Removing repeated input checking in Lasso and DictLearning I use :- a check_input flag in coordinate_descentenet_path that is set to False when called from ElasticNetfit - a check_input flag in coordinate_descentElasticNetfit that is set to False when called from sparse_encodeI changed the condition for overriding provided Gram Matrix in linear_modelbase_pre_fit in order not to do a pass on data when fit_intercept and normalize are set to False (in master we override Gram matrix if we find that X was changed by centering and rescaling even when these are disabled which wastes computation)I also avoid computing cov in sparse_encode when using coordiante_descent as cov computation is done within the Lasso classOn provided  plot_online_sparse_pca example we gain a factor 2 in performance,,2929,0.7497439399112324,0.13356562137049943,43674,490.9557173604433,38.4897192837844,129.3904840408481,6876,70,1827,312,travis,arthurmensch,ogrisel,false,ogrisel,5,0.0,1,1,864,true,true,false,false,1,33,8,15,38,0,56
8937541,scikit-learn/scikit-learn,python,5131,1439884206,1440801789,1440801789,15293,15293,commits_in_master,false,false,false,40,25,11,7,14,0,21,0,5,0,0,1,3,1,0,0,0,0,3,3,2,0,0,290,0,397,369,38.618895882122324,0.829298679309053,7,t3kcit@gmail.com,sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py,7,0.008158508158508158,0,9,false,Nonnegative lars I extended the lars_path Lars and LarsLasso estimators in the scikit-learn least_anglepy module with the possibility to restrict coefficients to be  0 using the method described in the original paper by Efron et al 2004 chapter 34,,2928,0.7496584699453552,0.13286713286713286,43674,490.9557173604433,38.4897192837844,129.3904840408481,6871,70,1827,316,travis,michigraber,agramfort,false,agramfort,0,0,0,3,1241,true,false,false,false,0,0,0,0,11,0,37
8932853,scikit-learn/scikit-learn,python,5128,1439837619,,1439908472,1180,,unknown,false,false,false,255,11,1,0,2,0,2,0,2,1,0,3,7,4,0,0,1,0,6,7,6,0,0,298,0,597,22,17.76753373798811,0.38153841343541883,13,t3kcit@gmail.com,examples/decomposition/plot_online_sparse_pca.py|sklearn/decomposition/dict_learning.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py,10,0.005889281507656066,1,0,false,[WIP] PERF: Bypassing checks in Lasso estimator in dictionary learning module This work intend to improve dictionary learning performance using coordinate descent backendI initially wanted to take advantage of multiprocessing using @ogrisel context manager for Parallel pool in joblib However this did not show improvement at all It turned out that a lot of computation was spent on checking input way too often due to multiple calls to Lassofit in _sparse_encodeI added a bypass flag to allow us to bypass these checks performing them initially in dictionary learning module We gain a 2x to 3x performance using lasso_cd algorithm in dictionary learning functions with this tweak It complexifies the code a little and add a strange flag (bypass_checks) to the Lasso API which should not be used by the end user but I believe the performance gain is worth itI then added the context manager improvement to the dictionary learning module I do not have precise benchmark yet but it turns out that we gain a little performance with high dimensional samples and relatively large batches : I obtained a 10% performance gain with n_features  128000 and batch_size  100 and I believe that this gain imrpove with sample dimensionality (more precision to come)Also this is quite disappointing this was somehow expected as our dictionary learning algorithm is only partly parallelizable : the block coordinate descent update of the dictionary is sequential which causes our pool to spend considerable time waiting for all workers to finish at each sparse_encode step,,2927,0.7499145883156816,0.13427561837455831,43674,490.9557173604433,38.4897192837844,129.3904840408481,6870,70,1826,308,travis,arthurmensch,arthurmensch,true,,4,0.0,1,1,863,true,true,false,false,1,32,6,15,29,0,995
8923002,scikit-learn/scikit-learn,python,5126,1439775352,1439804739,1439804739,489,489,github,false,false,false,10,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.640890977614855,0.09965932703084428,3,jmschrei@is146139.intra.cea.fr,examples/svm/plot_rbf_parameters.py,3,0.003579952267303103,0,0,false,small typo fix: radius in rbf kernel should be radial ,,2925,0.7500854700854701,0.1360381861575179,43672,490.65762960249134,38.46858398974171,129.3506136655065,6859,70,1825,308,travis,rasbt,agramfort,false,agramfort,5,1.0,1373,43,680,true,true,false,false,2,6,4,0,2,0,-1
8903355,scikit-learn/scikit-learn,python,5121,1439610889,,1441850477,37326,,unknown,false,false,false,26,1,1,2,1,0,3,0,1,0,0,3,3,0,0,0,0,0,3,3,0,0,0,0,0,0,0,13.622757157731431,0.29253754845460767,51,trev.stephens@gmail.com,doc/modules/classes.rst|doc/modules/feature_extraction.rst|doc/whats_new.rst,39,0.015402843601895734,4,0,true,[MRG] DOC/MAINT remove _ref tags _ref tags were added in 6709ed5fd99e59e0b0d26d05b116d1789b957a66 and later removedRefer Andys [comment](https://githubcom/rvraghav93/scikit-learn/pull/4#discussion_r37102100) which pointed this out@glouppe @GaelVaroquaux @vene @amueller ping,,2922,0.7508555783709788,0.13507109004739337,43672,490.65762960249134,38.46858398974171,129.3506136655065,6834,70,1823,331,travis,rvraghav93,amueller,false,,9,0.6666666666666666,25,42,679,true,false,false,false,3,186,17,179,79,0,5144
8898928,scikit-learn/scikit-learn,python,5120,1439586938,1439921582,1439921582,5577,5577,github,false,false,false,7,1,1,0,8,0,8,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,693,20,693,20,14.445396118521344,0.3102030461263582,7,larsmans@users.noreply.github.com,sklearn/decomposition/_online_lda.c|sklearn/decomposition/_online_lda.pyx|sklearn/decomposition/tests/test_online_lda.py,5,0.003468208092485549,0,0,true,[MRG+1] ENH: faster LatentDirichletAllocation (~15% on 20news) ,,2921,0.750770284149264,0.12947976878612716,43672,490.65762960249134,38.46858398974171,129.3506136655065,6830,69,1823,310,travis,larsmans,amueller,false,amueller,135,0.7333333333333333,155,38,1853,true,true,true,true,12,64,22,18,31,1,32
8863389,scikit-learn/scikit-learn,python,5117,1439487083,1439495111,1439495111,133,133,github,false,false,false,31,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.426324180424168,0.09505185033429639,4,t3kcit@gmail.com,doc/modules/feature_extraction.rst,4,0.004592422502870264,0,0,false,fixing a typo in feature extraction documentation I was reading the online docs and saw a small typo This is my first PR so I hope Im doing it right :),,2920,0.7506849315068493,0.1285878300803674,43669,490.64553802468566,38.44832718862351,129.35949987405255,6807,68,1822,310,travis,kylerbrown,amueller,false,amueller,0,0,6,13,1192,false,false,false,false,0,0,0,0,0,0,133
8862759,scikit-learn/scikit-learn,python,5116,1439484584,,1439511240,444,,unknown,false,false,false,44,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.570812650536907,0.09815462723819489,0,,sklearn/feature_extraction/stop_words.py,0,0.0,0,0,false,add Japanese stopwords [ Purpose of this pull request ]Add Japanese stopwords[ Where I changed ]scikit-learn/sklearn/feature_extraction/stop_wordspyAlthough data analyst has to optimize stop_words according to the subject of data this kinds of module would be useful for beginners in any language ,,2919,0.750942103460089,0.128,43669,490.64553802468566,38.44832718862351,129.35949987405255,6806,68,1822,309,travis,kyu999,kyu999,true,,0,0,2,1,843,false,false,false,false,0,0,0,0,0,0,176
8850148,scikit-learn/scikit-learn,python,5114,1439409036,,1439468851,996,,unknown,false,false,false,51,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,12,13,12,9.105646919746349,0.19554441906746764,19,larsmans@users.noreply.github.com,sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py,18,0.020454545454545454,0,0,false,ENH: make LatentDirichletAllocation ~35% faster One of the bottlenecks in training LDA is calling npdot on small arrays many times in a loop scipylinalgblasddot offers a more direct interface to the BLAS dot product function and brings thetime needed to fit on all of 20newsgroups down from 62s to 39s,,2918,0.7511994516792323,0.12840909090909092,43733,492.2369835135938,38.414926943040726,129.87903871218532,6795,68,1821,310,travis,larsmans,larsmans,true,,134,0.7388059701492538,155,38,1851,true,true,false,false,9,50,17,16,25,1,116
8321868,scikit-learn/scikit-learn,python,5113,1439389410,1439831799,1439831799,7373,7373,commits_in_master,false,false,false,64,9,1,8,11,0,19,0,5,0,0,1,4,1,0,0,1,1,3,5,3,0,0,2,0,10,165,3.9349678726901023,0.08450371659433611,2,t3kcit@gmail.com,sklearn/tree/export.py,2,0.0022727272727272726,0,3,false,[MRG + 1] added a string for FriedmanMSE (instead impurity) when exporting a do… This pull request fixes a minor issue when exporting a dot file via treeexport_graphvizWhen having tree_treeFriedmanMSE as criterion for a Decision Tree (that is always the case for the weak learners of ensembleGradientBoostingClassifier eg) in each node we found impurity   Now it should be friedman_mse  ,,2917,0.7511141583818992,0.12840909090909092,43733,492.2369835135938,38.414926943040726,129.87903871218532,6789,68,1821,314,travis,fzalkow,ogrisel,false,ogrisel,0,0,8,11,691,true,false,false,false,0,0,0,0,1,0,1850
8319385,scikit-learn/scikit-learn,python,5112,1439372068,1439511082,1439511082,2316,2316,commit_sha_in_comments,false,false,false,33,3,1,3,2,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,25,0,4.417667117577277,0.09487002605798427,17,larsmans@users.noreply.github.com,sklearn/decomposition/online_lda.py,17,0.0193621867881549,0,0,false,Rename n_iter variable to weight_update_counter(Issue 5107) This resolves #5107   We need to rename n_iter_ variable as it is the number of iterations of the EM step This will help avoid unnecessary confusion,,2916,0.7510288065843621,0.12870159453302962,43740,492.15820759030635,38.408779149519894,129.85825331504344,6787,67,1821,310,travis,beepee14,beepee14,true,beepee14,0,0,9,4,566,false,false,false,false,1,2,0,0,1,0,1145
8320726,scikit-learn/scikit-learn,python,5111,1439363712,1439581617,1439581617,3631,3631,commits_in_master,false,false,false,23,1,1,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.332584130698028,0.09303882435045023,19,larsmans@users.noreply.github.com,sklearn/decomposition/online_lda.py,19,0.02161547212741752,0,0,false,Remove dirichlet_component_ variable in LatentDirichletAllocation (issue #5101) For issue #5101 dirichlet_component_ variable can be removed since it is only used in perplexity calculation,,2915,0.7509433962264151,0.1285551763367463,43671,490.62306793982276,38.446566371276134,129.35357559936799,6785,67,1821,308,travis,chyikwei,larsmans,false,larsmans,5,0.6,17,1,1051,true,true,false,false,0,15,0,1,5,0,2281
8833542,scikit-learn/scikit-learn,python,5108,1439321304,,1439525162,3397,,unknown,false,false,false,67,1,1,0,9,0,9,0,4,1,0,3,4,4,0,0,1,0,3,4,4,0,0,974,0,974,0,18.337133511871386,0.3937925352451693,9,tom.dupre-la-tour@m4x.org,sklearn/ensemble/base.py|sklearn/linear_model/theil_sen.py|sklearn/utils/__init__.py|sklearn/utils/base.py,5,0.004509582863585118,3,0,false,[RFC+WIP] Reorganize utils__init__ (and demarcate what is public and what is private) @GaelVaroquaux @amueller @vene ping- [x] Split the long __init__py into basepy- [ ] Demarcate the public and private parts of utilThis will move stuff from __init__py to basepy to organize things a bit better**Also I feel we could use this PR to decide what all stays public and what stays private**,,2914,0.7512010981468772,0.1262683201803833,43740,492.15820759030635,38.408779149519894,129.85825331504344,6781,68,1820,309,travis,rvraghav93,rvraghav93,true,,8,0.75,25,42,676,true,false,false,false,3,165,15,176,77,0,11
8320770,scikit-learn/scikit-learn,python,5104,1439220686,1444784136,1444784136,92724,92724,commits_in_master,false,true,false,189,10,2,119,35,0,154,0,8,1,0,12,16,11,0,0,1,0,15,16,13,0,0,978,1066,1826,2007,75.69381817112759,1.625530310888387,77,trev.stephens@gmail.com,sklearn/decomposition/incremental_pca.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/extmath.py|sklearn/utils/testing.py|sklearn/utils/tests/test_extmath.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/decomposition/incremental_pca.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/extmath.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsefuncs_fast.pyx|sklearn/utils/testing.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_sparsefuncs.py,39,0.0022624434389140274,1,11,false,[WIP] partial_fit for StandardScaler I am fixing #5028 This is only about StandardScaler I moved the call to _handle_zeros_in_scale into trasform and inverse_trasform This is necessary since otherwise we lose the true value of std (and var) that is needed for incremental computation —or otherwise we would need an additional variable only for that which is not very elegant Incidentally this addresses #4609 I have also slightly extended utilsextmath_batch_mean_variance_update and did some refactoring in decompositionincremental_PCA that calls the former A more conceptual change is the following partial_fit must interpret an 1d array as a row matrix ie must be able to process batches of size 1 This clashes with the behaviour of fit which by default assumes that 1d input arrays are column matrices I think this is a design issue that should be tackled more generally hence out of scope of this PR See for example #4511 @GaelVaroquaux may want to comment hereI have not written tests on the interplay of subsequent calls to fit and partial_fit as the semantics does not seem defined yet See #3896Related open discussions:- scalers #2514- partial_fit #3907 #3896,,2913,0.7511156882938551,0.12330316742081449,43740,492.15820759030635,38.408779149519894,129.85825331504344,6765,68,1819,356,travis,giorgiop,amueller,false,amueller,0,0,2,7,990,true,false,false,false,0,1,0,0,6,0,19
8757430,scikit-learn/scikit-learn,python,5102,1439158618,,1442332427,52896,,unknown,false,false,false,18,3,2,3,5,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,9.123074219642941,0.19591868319270928,12,t3kcit@gmail.com,sklearn/svm/tests/test_svm.py|sklearn/svm/tests/test_svm.py,12,0.01348314606741573,0,3,false,[issue #5089] decision_function DeprecationWarning suppressed for svm unit tests [issue #5089] decision_function DeprecationWarning suppressed for svm unit tests,,2912,0.7513736263736264,0.12022471910112359,43740,492.15820759030635,38.408779149519894,129.85825331504344,6759,68,1818,339,travis,hasancansaral,hasancansaral,true,,0,0,21,4,1591,false,true,false,false,0,0,0,0,0,0,924
8769519,scikit-learn/scikit-learn,python,5099,1439075938,,1439111664,595,,unknown,false,false,false,10,5,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,28,0,88,0,3.98405514397397,0.0861371769567251,2,t3kcit@gmail.com,sklearn/neighbors/approximate.py,2,0.002229654403567447,0,2,false,[WIP] Modified LSHForest to check candidates only in relevant sections ,,2911,0.7516317416695294,0.11928651059085842,43440,490.79189686924497,38.236648250460405,130.11049723756906,6748,68,1817,306,travis,maheshakya,jnothman,false,,15,0.5333333333333333,3,0,1297,false,false,false,false,0,1,0,0,0,0,592
8320745,scikit-learn/scikit-learn,python,5098,1439071873,1440675633,1440675633,26729,26729,commits_in_master,false,false,false,5,4,1,4,3,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,39,0,56,4.569773026504231,0.09880067760393327,2,larsmans@gmail.com,sklearn/svm/tests/test_sparse.py,2,0.0022271714922048997,0,1,false,OneClassSVM sparsity regression test added ,,2910,0.7515463917525773,0.11915367483296214,43440,490.79189686924497,38.236648250460405,130.11049723756906,6748,68,1817,320,travis,olologin,ogrisel,false,ogrisel,1,0.0,2,2,778,false,false,false,false,1,2,1,0,0,0,7156
8790567,scikit-learn/scikit-learn,python,5096,1438978156,,1439328655,5841,,unknown,false,false,false,11,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.462016539151876,0.0964709423390935,13,t3kcit@gmail.com,sklearn/svm/classes.py,13,0.014270032930845226,0,4,true,According to description of epsilon above Default value must be 01,,2909,0.7518047438982468,0.11855104281009879,43437,490.8257936782006,38.23928908534199,130.11948338973687,6739,71,1816,314,travis,olologin,olologin,true,,0,0,2,2,777,false,false,false,false,1,1,0,0,0,0,4138
8781858,scikit-learn/scikit-learn,python,5094,1438904853,1440337703,1440337703,23880,23880,commits_in_master,false,false,false,22,1,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.502845483533511,0.09735368374192017,2,t3kcit@gmail.com,sklearn/utils/tests/test_shortest_path.py,2,0.002197802197802198,0,1,false,[MRG + 1] TST: fix undefined behavior in test Trivial unimportant bugfix in a testDiscovered using https://gistgithubcom/pv/d1ecc8784c9d6a2a92bf when working on https://githubcom/numpy/numpy/pull/6166#issuecomment-128485865,,2908,0.7517193947730398,0.11758241758241758,43437,490.8257936782006,38.23928908534199,130.11948338973687,6723,71,1815,317,travis,pv,jnothman,false,jnothman,0,0,43,0,2453,false,false,false,false,0,0,0,0,0,0,1091
8778865,scikit-learn/scikit-learn,python,5093,1438892059,1439059097,1439059097,2783,2783,commits_in_master,false,false,false,39,1,1,0,3,0,3,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,[MRG] Fix memory access error in OneClassSVM Fixes #4942This is a lazy fix in that I dont touch the C code but allocate a useless array y This seems just soo much easierValgrind is happy after this,,2907,0.7516339869281046,0.11836283185840708,43436,490.6529146330233,38.217147066949074,130.0764342941339,6719,71,1815,307,travis,amueller,amueller,true,amueller,323,0.8513931888544891,1208,40,1749,true,true,false,false,127,1181,95,423,168,12,28
8301440,scikit-learn/scikit-learn,python,5092,1438863489,1438904049,1438904049,676,676,github,false,false,false,58,4,2,4,4,0,8,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,2,14,5,14,8.851488825782408,0.19137345374763473,2,t3kcit@gmail.com,sklearn/lda.py|sklearn/tests/test_lda.py,2,0.0022148394241417496,1,2,false,[MRG] Fix for a bug in shrinkage LDA covariance cc @cle1109 I found a bug in our LDA code where the shrunk covariance matrix could become unsymmetric This is caused by incorrectly broadcasting a vector to the columns of the matrix instead of the rowsThis PR contains a test case that reveals the bug and a fix,,2906,0.7515485203028217,0.1184939091915836,43436,490.6529146330233,38.217147066949074,130.0764342941339,6711,71,1815,305,travis,kazemakase,agramfort,false,agramfort,1,1.0,3,0,1049,true,true,false,true,0,0,0,0,2,0,8
8296817,scikit-learn/scikit-learn,python,5091,1438816054,1439524636,1439524636,11809,11809,commit_sha_in_comments,false,false,false,23,2,1,4,12,0,16,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,5,4,5,4,9.0140106413595,0.1948872537160344,4,t3kcit@gmail.com,sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py,3,0.0033222591362126247,0,5,false,precision_recall_curve threshold fix (potential) Fix for #4996Included smallest threshold value when full recall is attained in sklearnmetricsprecision_recall_curve  Modified associated tests to agree,,2905,0.7514629948364888,0.1184939091915836,43436,490.6529146330233,38.217147066949074,130.0764342941339,6705,71,1814,315,travis,jayflo,jnothman,false,jnothman,0,0,2,2,733,false,true,false,false,0,4,0,0,0,0,1226
8291416,scikit-learn/scikit-learn,python,5084,1438717440,1444859224,1444859224,102363,102363,commit_sha_in_comments,false,false,false,10,14,4,22,16,0,38,0,7,0,0,13,16,13,0,0,0,0,16,16,16,0,0,185,0,503,99,113.41484500739327,2.4520836011335088,76,tw991@nyu.edu,sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/logistic.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/utils/multiclass.py|sklearn/utils/estimator_checks.py|sklearn/utils/estimator_checks.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/logistic.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/utils/multiclass.py,28,0.004530011325028313,0,5,false,Reject regression type targets for classifiers This PR solves https://githubcom/scikit-learn/scikit-learn/issues/5060,,2903,0.7516362383740958,0.11551528878822197,43436,490.6529146330233,38.217147066949074,130.0764342941339,6689,70,1813,355,travis,vermouthmjl,amueller,false,amueller,0,0,0,2,949,true,false,false,false,0,1,0,0,5,0,36
8751423,scikit-learn/scikit-learn,python,5083,1438703085,1445267171,1445267171,109401,109401,commit_sha_in_comments,false,false,false,42,4,2,0,9,0,9,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,22,0,8.372644240989839,0.1810206030799386,1,olivier.grisel@ensta.org,sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/dist_metrics.pyx,1,0.0011376564277588168,1,3,false,[MRG + 1] Edit the docs to clarify inputs for Haversine Distance Metric The haversine distance metric requires units of radians and this simple PR edits the DistanceMetric documentation to make this more clearcc @jakevdp because he wrote it (thanks dude),,2902,0.7515506547208821,0.1149032992036405,43436,490.6529146330233,38.217147066949074,130.0764342941339,6688,69,1813,366,travis,andrewgiessel,glouppe,false,glouppe,0,0,23,31,1374,true,false,false,false,0,0,0,0,3,0,188
8743756,scikit-learn/scikit-learn,python,5081,1438646032,1440410888,1440410888,29414,29414,commits_in_master,false,false,false,11,1,0,0,6,0,6,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,3,0,0,0.0,0,,,0,0.0,0,0,false,[MRG + 1] Add common test that transformers dont change n_samples ,,2901,0.7514650120648052,0.11649365628604383,43436,490.6529146330233,38.217147066949074,130.0764342941339,6684,68,1812,315,travis,amueller,ogrisel,false,ogrisel,322,0.8509316770186336,1208,40,1746,true,true,true,false,128,1201,96,435,168,12,3212
8728473,scikit-learn/scikit-learn,python,5077,1438552853,1438565562,1438565562,211,211,github,false,false,false,7,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.506643728966516,0.09772048232788014,8,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py,8,0.009650180940892641,0,0,false,[MRG] FIX import column_or_1d from utilsvalidation directly ,,2899,0.7516384960331148,0.11821471652593486,43372,489.90131882320395,38.22742783362538,130.08392511297612,6673,68,1811,304,travis,rvraghav93,amueller,false,amueller,7,0.7142857142857143,25,42,667,true,false,false,false,3,143,14,149,69,0,-1
8721305,scikit-learn/scikit-learn,python,5076,1438479849,1438644814,1438644814,2749,2749,github,false,false,false,48,3,3,0,5,0,5,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,14.295872831109254,0.30998669350647784,2,trev.stephens@gmail.com,doc/related_projects.rst|doc/related_projects.rst|doc/related_projects.rst,2,0.0024630541871921183,0,3,false,[MRG + 1] Add more extensions and related packages to related_projectsrst (issue #5068) Hi guysI just updated some documentation based on the issue 5068I also fixed some lines in that file that exceeded the 80 characters and a small typo I foundHope that helpsProkopis,,2898,0.7515527950310559,0.11822660098522167,43372,489.90131882320395,38.22742783362538,130.08392511297612,6664,68,1810,305,travis,PGryllos,larsmans,false,larsmans,0,0,7,15,1011,true,true,false,false,0,1,0,0,3,0,1277
8719919,scikit-learn/scikit-learn,python,5074,1438405289,1439508629,1439508629,18389,18389,commits_in_master,false,false,false,96,2,1,0,12,0,12,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.765353087102441,0.10302923006007822,6,trev.stephens@gmail.com,doc/related_projects.rst,6,0.007380073800738007,0,0,false,[MRG + 1] Added several related projects Not sure how extensive you want this list to be but these are all great additions- Seaborn is the best statistical visualization package Ive seen by far  I use it all the time on machine learning projects- sparkit-learn is a really cool project that provides a scikit-learn-like API on Spark- Keras is a popular deep learning library that has a scikit-learn compatible classification API (I know because I wrote it)That said let me know if any of them dont belong and Ill modify the PR,,2896,0.7517265193370166,0.11808118081180811,43436,490.6529146330233,38.217147066949074,130.0764342941339,6659,67,1810,318,travis,jdwittenauer,larsmans,false,larsmans,0,0,10,22,1219,true,true,false,false,0,0,0,0,1,0,7
8757828,scikit-learn/scikit-learn,python,5071,1438386954,1440417854,1440417854,33848,33848,commits_in_master,false,false,false,23,1,1,2,10,0,12,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.320722475158922,0.0934161019723258,5,t3kcit@gmail.com,sklearn/datasets/lfw.py,5,0.006172839506172839,0,1,true,[MRG + 1] Fixes #3594 Fix for issue #3594 Added basic error handling in case there is any problem while reading/opening the image,,2895,0.7516407599309154,0.11851851851851852,43436,490.6529146330233,38.217147066949074,130.0764342941339,6659,67,1809,318,travis,mth4saurabh,jnothman,false,jnothman,0,0,9,6,739,true,false,false,false,1,0,0,0,1,0,34
8740492,scikit-learn/scikit-learn,python,5069,1438370557,1438631233,1438631233,4344,4344,commit_sha_in_comments,false,false,false,9,1,1,0,4,0,4,0,3,0,0,2,2,1,0,0,0,0,2,2,1,0,0,0,0,0,0,4.563684699125941,0.09895734532673975,42,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/utils/sparsefuncs_fast.c,41,0.05067985166872682,0,1,true,added whatsnew for densify_rows added cython file Fixes #5066,,2894,0.7515549412577747,0.11866501854140915,43371,489.7973300131424,38.228309238892344,130.0869244425999,6659,65,1809,308,travis,amueller,larsmans,false,larsmans,321,0.8504672897196262,1207,40,1743,true,true,true,true,130,1200,96,521,169,11,27
8821821,scikit-learn/scikit-learn,python,5065,1438341740,,1443046937,78419,,unknown,false,true,false,42,1,1,0,14,0,14,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,22,2,22,8.488832283686758,0.18407163350616962,7,t3kcit@gmail.com,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,5,0.006195786864931847,0,9,true,[MRG + 1] Remove special-case in Pipelineinverse_transform for 1D arrays This special-case resulted in pipelines that do not produce the sameresults as calling their component transforms manually andprevented pipelines from containing transforms that processednon-array inputs or outputs (closes #5029),,2893,0.7518147251987556,0.11771995043370508,43352,489.61985606200403,38.17586270529618,130.00553607676693,6659,65,1809,355,travis,joewreschnig,ogrisel,false,,0,0,3,0,1294,true,false,false,false,1,1,0,0,1,0,409
8696453,scikit-learn/scikit-learn,python,5063,1438300099,1440653749,1440653749,39227,39227,commits_in_master,false,false,false,21,2,0,8,6,0,14,0,4,0,0,0,5,0,0,0,0,0,5,5,5,0,0,0,0,136,0,0,0.0,0,,,0,0.0,0,2,false,[MRG+1] test for accepted sparse matrix types Fixes #5033Checks that all estimators correctly convert / pass through sparse matrix types,,2892,0.7517289073305671,0.12025316455696203,43373,489.8900237474927,38.226546469001455,130.08092592165633,6657,64,1808,329,travis,amueller,jnothman,false,jnothman,320,0.85,1206,40,1742,true,true,false,false,125,1148,87,484,164,9,665
8695940,scikit-learn/scikit-learn,python,5061,1438297842,,1438372038,1236,,unknown,false,false,false,23,1,1,3,2,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.178412586468911,0.09060612932583909,6,t3kcit@gmail.com,sklearn/decomposition/nmf.py,6,0.0076045627376425855,0,0,false,[MRG] ENH better random init for NMF Fixes #4930Probably needs a whatsnewThis is a backward-incompatible changeMight be interesting for #4852,,2891,0.7519889311656867,0.11913814955640051,43352,489.61985606200403,38.17586270529618,130.00553607676693,6657,64,1808,309,travis,amueller,amueller,true,,319,0.8526645768025078,1206,40,1742,true,true,false,false,124,1143,86,501,163,9,1
8692517,scikit-learn/scikit-learn,python,5059,1438285506,1438643059,1438643059,5959,5959,github,false,false,false,14,5,3,3,2,0,5,0,3,3,0,6,9,7,0,0,3,0,6,9,8,0,0,134,81,192,129,40.482121241780845,0.8778001206957622,45,tom.dupre-la-tour@m4x.org,sklearn/preprocessing/function_transformer.py|doc/modules/classes.rst|doc/modules/preprocessing.rst|examples/preprocessing/plot_callable_transformer.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/callable_transformer.py|sklearn/preprocessing/tests/test_callable_transformer.py|sklearn/utils/estimator_checks.py|sklearn/preprocessing/tests/test_function_transformer.py,25,0.0,0,2,false,[MRG + 1] Function transformer rebase Rebase of #4798 Should be good to go,,2890,0.7519031141868512,0.11772151898734177,43373,489.8900237474927,38.226546469001455,130.08092592165633,6655,64,1808,312,travis,amueller,larsmans,false,larsmans,318,0.8522012578616353,1206,40,1742,true,true,true,true,124,1141,89,505,163,6,4
8690880,scikit-learn/scikit-learn,python,5057,1438279753,1438364218,1438364218,1407,1407,github,false,false,false,36,3,0,2,3,0,5,0,3,0,0,0,6,0,0,0,0,0,6,6,5,0,0,0,0,76,0,0,0.0,0,,,0,0.0,0,0,false,[MRG + 2] FIX make sure we handle yshape  (n_samples 1) consistently in regressors Fixes #5056This is the same behavior as in classifiers: if we dont support multi-output / multi-task we flatten and warn,,2889,0.7518172377985463,0.11689961880559085,43352,489.66599003506184,38.17586270529618,130.00553607676693,6654,64,1808,308,travis,amueller,amueller,true,amueller,317,0.8517350157728707,1206,40,1742,true,true,false,false,122,1138,88,505,161,6,64
8690688,scikit-learn/scikit-learn,python,5056,1438279075,,1438364218,1419,,unknown,false,true,false,93,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.206100243658658,0.09120651796534004,13,t3kcit@gmail.com,sklearn/svm/base.py,13,0.016476552598225603,0,0,false,Fixes Buffer has wrong number of dimensions ---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)ipython-input-5-1a154f38b9fb in module()---- 1 clffit(X y)/usr/local/lib/python27/dist-packages/sklearn/svm/basepyc in fit(self X y sample_weight)    176     177         seed  rndrandint(npiinfo(i)max)-- 178         fit(X y sample_weight solver_type kernel random_seedseed)    179         # see comment on the other call to npiinfo in this file    180 /usr/local/lib/python27/dist-packages/sklearn/svm/basepyc in _dense_fit(self X y sample_weight solver_type kernel random_seed)    234                 cache_sizeselfcache_size coef0selfcoef0    235                 gammaself_gamma epsilonselfepsilon-- 236                 max_iterselfmax_iter random_seedrandom_seed)    237     238         self_warn_from_fit_status()sklearn/svm/libsvmpyx in sklearnsvmlibsvmfit (sklearn/svm/libsvmc:1756)()ValueError: Buffer has wrong number of dimensions (expected 1 got 2),,2888,0.7520775623268698,0.11660329531051965,43352,489.61985606200403,38.17586270529618,130.00553607676693,6654,64,1808,307,travis,abhishekkrthakur,abhishekkrthakur,true,,4,0.75,124,48,1359,false,true,false,false,0,0,0,0,0,0,9
8689675,scikit-learn/scikit-learn,python,5054,1438275609,,1440522516,37448,,unknown,false,false,false,286,9,8,0,4,0,4,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,128,28,151,28,35.00817464572622,0.7591292467584643,6,trev.stephens@gmail.com,sklearn/tree/tests/test_export.py|sklearn/tree/export.py|sklearn/tree/__init__.py|sklearn/tree/export.py|sklearn/tree/export.py|sklearn/tree/export.py|sklearn/tree/export.py|sklearn/tree/export.py,4,0.005076142131979695,0,1,false,[WIP] Decision tree export to javascript This PR provide decision tree convert to javascriptsupporting Regressor and single output ClassifierHow to use----python clf  sklearntreeDecisionTreeClassifier() iris  sklearndatasetsload_iris() clf  clffit(irisdata iristarget) sklearntreeexport_javascript(clf)javascriptif(feature[2]  24500000476837158) {  return 0} else {  if(feature[3]  17500000000000000) {    if(feature[2]  49499998092651367) {      if(feature[3]  16500000953674316) {        return 1      } else {        return 2      }    } else {      if(feature[3]  15499999523162842) {        return 2      } else {        if(feature[2]  54499998092651367) {          return 1        } else {          return 2        }      }    }  } else {    if(feature[2]  48500003814697266) {      if(feature[1]  30999999046325684) {        return 2      } else {        return 1      }    } else {      return 2    }  }}and write a function interface you can use trained decision tree in your javascript codejavascriptfunction iris_decision_tree(feature){  if(feature[2]  24500000476837158) {    return 0  } else {    if(feature[3]  17500000000000000) {      if(feature[2]  49499998092651367) {        if(feature[3]  16500000953674316) {          return 1        } else {          return 2        }      } else {        if(feature[3]  15499999523162842) {          return 2        } else {          if(feature[2]  54499998092651367) {            return 1          } else {            return 2          }        }      }    } else {      if(feature[2]  48500003814697266) {        if(feature[1]  30999999046325684) {          return 2        } else {          return 1        }      } else {        return 2      }    }  }}consolelog(iris_decision_tree([ 51  35  14  02]))// output is 0Context----This code is exported from our companys libraryWe wanted to predict in production code running on nodejsbut our companys analysis-team is using python and scikit-learnso there is difference of languages python and javascriptthis PR is bridge the gap----this code is still WIP,,2887,0.7523380671977832,0.116751269035533,43352,489.61985606200403,38.17586270529618,130.00553607676693,6654,64,1808,323,travis,tokoroten,amueller,false,,1,1.0,26,23,1455,true,false,false,false,1,5,1,2,7,0,58
8608113,scikit-learn/scikit-learn,python,5052,1438202923,1438370182,1438370182,2787,2787,github,false,false,false,73,1,1,8,6,0,14,0,4,0,0,4,4,3,0,0,0,0,4,4,3,0,0,52,23,52,23,18.273248334260046,0.3962433741317801,8,t3kcit@gmail.com,doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py,8,0.003865979381443299,0,1,false,[MRG + 1] add Cohens kappa score to metrics This was recently requested on the ML and I happened to need an implementation myselfI wasnt sure what the API should be:pycohen_kappa(y1 y2)orpycohen_kappa(confusion_matrix(y1 y2))but I chose the former to save users a call and an import The code is simple enough to copy-paste if it needs to be applied to a confusion matrixNeeds tests,,2886,0.7522522522522522,0.1172680412371134,43352,489.61985606200403,38.17586270529618,130.00553607676693,6651,62,1807,307,travis,larsmans,amueller,false,amueller,133,0.7368421052631579,154,38,1837,true,true,true,true,6,25,10,16,16,0,1
8321243,scikit-learn/scikit-learn,python,5050,1438192420,1439581405,1439581405,23149,23149,commits_in_master,false,false,false,8,1,1,3,10,0,13,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,9,0,9,0,13.174722908705917,0.28567606227070474,14,t3kcit@gmail.com,doc/modules/svm.rst|examples/svm/plot_custom_kernel.py|sklearn/svm/classes.py,13,0.009055627425614488,0,1,false,[MRG + 1] Fix documentation of callable kernel ,,2885,0.7521663778162911,0.11642949547218628,43373,489.8900237474927,38.226546469001455,130.08092592165633,6651,62,1807,320,travis,AnishShah,larsmans,false,larsmans,0,0,10,14,937,true,false,false,false,1,0,0,0,2,0,4
8682710,scikit-learn/scikit-learn,python,5049,1438150307,1438383621,1438383621,3888,3888,github,false,true,false,32,1,1,0,9,0,9,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,3.5748348507228545,0.07760249095374448,2,t3kcit@gmail.com,sklearn/gaussian_process/gaussian_process.py,2,0.0026109660574412533,2,4,false,[MRG + 1] fix shape of thetaL thetaU theta0 in gp Hopefully fixes the second error in master in #5045 @jakevdp or possibly @jmetzen or anyone who knows about the GP code,,2884,0.7520804438280166,0.11357702349869452,43391,489.1797838261391,38.141550091032705,129.88868659399412,6647,61,1807,309,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,316,0.8512658227848101,1206,40,1741,true,true,true,false,119,1102,84,460,160,7,190
8674555,scikit-learn/scikit-learn,python,5048,1438150197,1438193245,1438193245,717,717,github,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.443381432431362,0.09645689432186404,3,t3kcit@gmail.com,sklearn/datasets/lfw.py,3,0.0039164490861618795,0,0,false,Update lfwpy Fixed colloquial loggerwarn() to now be loggerwarning(),,2883,0.7519944502254596,0.11357702349869452,43391,489.1797838261391,38.141550091032705,129.88868659399412,6647,61,1807,304,travis,marktab,amueller,false,amueller,0,0,2,16,85,false,true,false,false,0,0,0,0,0,0,717
8669951,scikit-learn/scikit-learn,python,5047,1438144965,1438391192,1438391192,4103,4103,github,false,true,false,23,1,1,0,10,0,10,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.311468696076558,0.09348822264978446,2,t3kcit@gmail.com,sklearn/kernel_ridge.py,2,0.00261437908496732,0,5,false,[MRG+1] force y to be numeric in kernel ridge fixes one of the errors in master in #5045 Missed ynumeric in a regressor,,2882,0.7519083969465649,0.11372549019607843,43370,489.9239105372377,38.22919068480517,130.08992391053724,6644,61,1806,308,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,315,0.8507936507936508,1204,40,1740,true,true,true,false,119,1100,83,460,159,7,265
8661027,scikit-learn/scikit-learn,python,5042,1438117640,1438127044,1438127044,156,156,github,false,false,false,18,1,1,0,2,0,2,0,3,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.03687974602637,0.19617276352384508,1,joel.nothman@gmail.com,doc/developers/contributing.rst|doc/faq.rst,1,0.001314060446780552,0,0,false,[MRG] DOC: adding algorithms is not the best way to contribute minor additions to faq and contributor guide,,2881,0.7518222839291913,0.11300919842312747,43380,489.23467035500227,38.151221761180274,129.89857076994008,6640,61,1806,303,travis,amueller,amueller,true,amueller,314,0.8503184713375797,1203,40,1740,true,true,false,false,118,1093,80,452,161,7,23
8655386,scikit-learn/scikit-learn,python,5041,1438095700,1441316817,1441316817,53685,53685,commits_in_master,false,true,false,798,56,15,3,88,0,91,0,7,0,0,8,12,8,0,0,0,0,12,12,12,0,0,3701,0,12402,120,139.2630455013459,3.023124934771738,14,tw991@nyu.edu,sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,9,0.003937007874015748,3,19,false,[WIP] _treepyx rewrite The following tasks must be completed:- [x] Propose new API to simplify Criterion Splitter and TreeBuilder methods- [x] FriedmanMSE rewritten- [x] MSE rewritten- [x] DecisionTreeRegressor works with new API- [x] GradientBoostingClassifier works with new API- [x] Initial multi-threading proposal  - [x] GradientBoostingRegressor works with new API- [x] RandomForestRegressor works with new API- [x] Entropy rewritten- [x] DecisionTreeClassifier works with new API - [x] Gini rewritten- [x] CHECKPOINT: All criterion rewritten- [x] RandomForestClassifier works with new API- [x] Random splitting method added to DenseSplitter (for ExtraTrees estimators)- [x] ExtraTreesClassifier/Regressor works with new API- [x] Refactor Random Forest internals  - [x] Fix segfault issue with Ensemble classifiers on more than one tree (recent)  - [x] Cache presort outside splitter objects to share among ensembles- [x] CHECKPOINT: All classifiers run on dense single output data- [ ] Utilize caching more thoroughly in criterion - [ ] Prediction differences debugged or understood  - [x] Debug error in same tree different prediction anomaly  - [ ] Use counts of training labels in each leaf instead of hot-encoding of prediction  - [ ] Carry this vector around until the end instead of a single value- [ ] Support for sparse data through SparseSplitter- [ ] Multi-output predictions- [ ] Resolve multi-threading support issue- [ ] CHECKPOINT: All code written- [ ] Add proper documentation to all objects mostly from #5010 ---I attempted to get my changes proposed in #5007 to include min_sample and min_weight constraints but the API imposed by the Criterion class/splitters was a bit difficult for me to efficiently do so without a large number of hacks Instead I chose to change the API in _treepyx to be simpler to use and easier for me to understand The API is as follows:cdef class Criterion:    def __cinit__( self n_output ): # Store the number of targets same as before    cdef void __init__( self X X_sample_stride X_feature_stride y y_stride w size min_leaf_samples min_leaf_weight ): # Store constants about this dataset and the constraints the user imposed    cdef SplitRecord best_split(self samples start end feature) nogil: # Find the best split for a specific feature using this criterion looking at start end the sample mapping and return a SplitRecord of the best splitcdef class Splitter:    cdef void init( self X y sample_weight ) # Same as before    cdef SplitRecord best_split(self start end n_constant_features) # Find the best split across all features TreeBuilder is currently has the same API but is changed internally to handle the new API This makes it clearer that Splitter handles shuffling the samples around based on previous splits and determining the best global split while Criterion handles splitting a single feature as coordinated by the SplitterTime tests show that this is slightly slower than the embedded criterion/splitter object I proposed in #5007 with many small trees and even slower with fewer deep trees with time tests below It is also slightly more accurate but I dont know a specific reason for that 25 estimators max depth 2          accuracy   train timemadelon    master     068833     2487branch     070833     0882xgboost    07         0663gaussian (50000 points 2 classes 200 dimensions each dimension z05 apart)master     091712     30815branch     09192      10757xgboost    092068     7232spambasemaster     092122     0444branch     092122     0166xgboost    092122     0110 mnistmaster     083786     877432branch     083871     359564xgboost    081014     200688covtypemaster     071034     462218branch     071034     249624xgboost    068720     1033725 estimators max depth 6          accuracy   train timemadelon    master     0835        2460branch     0832        1218xgboost    0818        05404gaussian (50000 points 2 classes 200 dimensions each dimension z05 apart)master     083908     219226branch     084588      94118xgboost    084288      48209spambasemaster     091471      0341branch     091862      0160xgboost    092252      0070mnistmaster     089471     720519branch     089542     463294xgboost    090257     118749covtypemaster     075713     290271branch     075730     241988xgboost    075476      57540Deep trees seem a lot slower than before (though still faster than master) Ill need to track down where this slowdown comes from Another change I am considering is removing the caching of constant features for two reasons (1) it would make multithreading easier to handle and (2) in none of these examples do constant features arise and they can cause a speed decrease in larger examples I have modification (not in this pull request) which will check if a feature is constant before checking for the best split but wont cache the result Here are time tests I get on covtypes between this branch and without cachingThoughts @glouppe @pprett @ogrisel 25 trees depth 2covtypesbranch       24962sno_cache     23504smnistbranch       359564no_cache     333525 trees depth 6covtypesbranch       241988no_cache     22025mnist         branch       463294no_cache     4281,,2880,0.7517361111111112,0.11286089238845144,43380,489.23467035500227,38.151221761180274,129.89857076994008,6635,62,1806,335,travis,jmschrei,jmschrei,true,jmschrei,3,1.0,35,5,860,false,true,false,false,3,7,3,1,0,0,20
8663487,scikit-learn/scikit-learn,python,5038,1438065105,1438125692,1438125693,1009,1009,github,false,false,false,4,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,24,0,24,0,4.4502144407705195,0.09660517852432174,3,t3kcit@gmail.com,sklearn/feature_extraction/text.py,3,0.003952569169960474,0,0,false,DOC tweaks for feature_extractiontext ,,2879,0.7516498784300104,0.11198945981554677,43380,489.23467035500227,38.151221761180274,129.89857076994008,6631,62,1806,305,travis,jnothman,amueller,false,amueller,116,0.6982758620689655,30,1,2282,true,true,false,false,11,187,19,117,21,2,1009
8620972,scikit-learn/scikit-learn,python,5026,1437723433,1439050510,1439050510,22117,22117,commit_sha_in_comments,false,false,true,24,3,1,0,7,0,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,4,2,4.21109280742629,0.09141413404792906,3,t3kcit@gmail.com,sklearn/feature_extraction/text.py,3,0.004087193460490463,0,0,true,[MRG+1] ENH O(1) stop-word lookup when list provided The docstring says stop_words can be a list but it should be accessed as a set,,2875,0.7523478260869565,0.10217983651226158,43397,489.04302140700975,38.13627670115446,129.8476853238703,6570,63,1802,319,travis,jnothman,jnothman,true,jnothman,115,0.6956521739130435,30,1,2278,true,true,false,false,11,189,18,117,20,2,72
8600202,scikit-learn/scikit-learn,python,5025,1437693764,1437694065,1437694065,5,5,github,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.2360589022973185,0.09195610181590651,0,,doc/modules/calibration.rst,0,0.0,0,0,false,DOC: Remove extra not ,,2874,0.7522616562282533,0.10259917920656635,43397,489.04302140700975,38.13627670115446,129.8476853238703,6561,63,1801,299,travis,jakirkham,agramfort,false,agramfort,0,0,9,5,954,false,false,false,false,0,0,0,0,0,0,-1
8562774,scikit-learn/scikit-learn,python,5021,1437660358,1437670740,1437670740,173,173,github,false,false,false,19,1,0,2,1,0,3,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,19,0,0,0.0,0,,,0,0.0,0,0,false,[MRG] Make copy_joblibsh Python 3 compatible Bumped into that recently when trying to update joblib to the latest beta,,2873,0.7521754263835712,0.10263522884882108,43397,489.11215060948916,38.13627670115446,129.87072839136346,6553,63,1801,303,travis,lesteve,ogrisel,false,ogrisel,15,0.9333333333333333,4,0,1184,true,false,false,false,2,25,5,3,2,0,59
8605549,scikit-learn/scikit-learn,python,5020,1437644836,1437646541,1437646541,28,28,github,false,false,false,7,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,2,15,2,8.896375140717513,0.19313398362859086,6,t3kcit@gmail.com,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,4,0.005594405594405594,0,0,false,[MRG+1] FIX seed the initialization of NMF ,,2872,0.7520891364902507,0.1020979020979021,43396,489.12342151350356,38.137155498202596,129.8737210802839,6547,63,1801,303,travis,ogrisel,ogrisel,true,ogrisel,124,0.8629032258064516,1193,124,2248,true,true,false,false,24,259,40,179,39,3,2
8598529,scikit-learn/scikit-learn,python,5019,1437644194,1437690504,1437690505,771,771,github,false,false,false,31,1,1,0,4,0,4,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,8,3,8,8.819518757526508,0.19146548615426254,5,t3kcit@gmail.com,sklearn/ensemble/tests/test_voting_classifier.py|sklearn/ensemble/voting_classifier.py,5,0.006993006993006993,1,0,false,[MRG+1] FIX: ensure that get_params returns VotingClassifier own params This is a fix to ensure that VotingClassifierget_params returns VotingClassifier own params and not only the params from the sub-estimators CC: @rasbt ,,2871,0.7520027864855451,0.1020979020979021,43396,489.12342151350356,38.137155498202596,129.8737210802839,6546,63,1801,303,travis,glouppe,ogrisel,false,ogrisel,53,0.9622641509433962,159,26,1715,true,true,true,false,8,55,9,30,28,0,1
8604557,scikit-learn/scikit-learn,python,5018,1437637750,1437638977,1437638977,20,20,github,false,false,false,31,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.066217754670797,0.0882746921556512,4,t3kcit@gmail.com,doc/modules/pipeline.rst,4,0.0056022408963585435,0,0,false,Filled in missing negation Since make_union is a convenience factory method that returns a FeatureUnion class with names automatically filled in and obviously does *not* require manual naming of the components,,2870,0.7519163763066202,0.10224089635854341,43396,489.12342151350356,38.137155498202596,129.8737210802839,6541,63,1801,302,travis,ltiao,GaelVaroquaux,false,GaelVaroquaux,2,0.5,9,8,1384,true,false,false,false,0,0,1,0,1,0,20
8571647,scikit-learn/scikit-learn,python,5017,1437587018,1437663936,1437663936,1281,1281,github,false,false,false,53,1,1,0,4,0,4,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.656390648207662,0.10108692544928957,4,loic.esteve@ymail.com,README.rst,4,0.005633802816901409,1,0,false,[MRG+1] Fix READMErst coveralls badge It looks like the shieldsio link to the image doesnt work right now I switched it to the badge provided by coveralls I have to admit I dont really know whether shieldsio badges are supposed to have any advantages ping @amueller who added the coveralls badge in 0d3fa51,,2869,0.7518299058905542,0.09859154929577464,43396,489.12342151350356,38.137155498202596,129.8737210802839,6494,62,1800,303,travis,lesteve,ogrisel,false,ogrisel,14,0.9285714285714286,4,0,1183,true,false,false,false,2,24,4,3,2,0,1070
8571270,scikit-learn/scikit-learn,python,5016,1437586022,1438267122,1438267122,11351,11351,github,false,false,false,31,1,1,0,3,0,3,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,284,0,284,0,8.875942877406253,0.19269039986709127,2,loic.esteve@ymail.com,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py,2,0.0028208744710860366,0,0,false,[MRG+1] Update joblib to 090b3 This is to get some early testing of Parallel context manager to re-use the same pool of workers across consecutive parallel callsCorresponding joblib PR: https://githubcom/joblib/joblib/pull/221,,2868,0.7517433751743375,0.09873060648801128,43396,489.12342151350356,38.137155498202596,129.8737210802839,6493,62,1800,309,travis,lesteve,GaelVaroquaux,false,GaelVaroquaux,13,0.9230769230769231,4,0,1183,true,false,false,false,2,24,3,3,1,0,9
8556807,scikit-learn/scikit-learn,python,5012,1437519852,1445596004,1445596004,134602,134602,commits_in_master,false,true,false,74,6,2,10,35,0,45,0,5,0,0,6,7,5,0,0,0,0,7,7,5,0,0,58,62,66,76,52.736053707313346,1.1447803373973835,53,yanlend@users.noreply.github.com,doc/whats_new.rst|sklearn/cluster/bicluster.py|sklearn/decomposition/kernel_pca.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/tests/test_utils.py|doc/whats_new.rst|sklearn/cluster/bicluster.py|sklearn/decomposition/kernel_pca.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/tests/test_utils.py,41,0.008534850640113799,0,9,false,Initialize ARPACK eigsh v0  random_staterand(Mshape[0]) leads to an initial residual vector in ARPACK which is all positive However this is not the absolute or squared residual but a true difference Thus it is better to initialize with randn to have an equally distributed signThe effect of the previous initialization is that eigsh frequently does not converge to the correct eigenvalues eg negative eigenvalues for spd matrix which leads to an incorrect null-space,,2867,0.7516567840948727,0.0953058321479374,43397,489.11215060948916,38.13627670115446,129.87072839136346,6472,61,1799,385,travis,yanlend,GaelVaroquaux,false,GaelVaroquaux,3,0.6666666666666666,0,1,875,true,false,false,false,0,1,2,0,5,0,682
8549236,scikit-learn/scikit-learn,python,5011,1437498407,1437517664,1437517664,320,320,github,false,false,false,30,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,4,7,4,9.371439797633037,0.20344838290574954,4,tom.dupre-la-tour@m4x.org,sklearn/datasets/rcv1.py|sklearn/datasets/tests/test_rcv1.py,4,0.005722460658082976,0,1,false,[MRG+1] Reorder target_names in rcv1 I re-ordered the categories to be in lexicographic orderNow the ordering is equivalent to the one obtained loading rcv1v2 (topics full sets) [from LIBSVM](http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/multilabelhtml),,2866,0.7515701325889742,0.09585121602288985,43391,489.0645525569818,38.141550091032705,129.84259408633125,6468,61,1799,300,travis,TomDLT,agramfort,false,agramfort,14,0.7857142857142857,6,3,153,true,false,true,true,3,100,16,83,66,0,6
8548102,scikit-learn/scikit-learn,python,5010,1437476346,1438337738,1438337738,14356,14356,github,false,false,false,41,8,3,83,17,0,100,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,885,0,1415,0,9.436057150738359,0.20461469441635996,3,olivier.grisel@ensta.org,sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,3,0.004297994269340974,0,4,false,[MRG] _treepyx documentation additions This is the documentation aspect of #5007 alone No functional changes to the code are made and it passes all nosetests This documentation covers all criterion and the dense splitters and includes both docstrings and inline comments,,2865,0.7514834205933683,0.09455587392550144,43352,489.61985606200403,38.17586270529618,130.00553607676693,6466,61,1799,315,travis,jmschrei,glouppe,false,glouppe,2,1.0,35,5,853,false,true,false,true,2,3,2,0,0,0,55
8529630,scikit-learn/scikit-learn,python,5008,1437411740,1445452568,1445452568,134013,134013,commits_in_master,false,true,false,50,2,2,25,9,0,34,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,73,121,73,121,22.146253586872486,0.4807821130969593,20,tom.dupre-la-tour@m4x.org,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/tests/test_common.py,16,0.017191977077363897,0,4,false,[MRG] fix multinomial logistic regression class weights Multinomial LogisticRegression does not gives the same results with class_weightbalanced or with precomputed class weights with compute_class_weight(balanced npunique(y) y)(see added test that fails on master)I also updated some remaining auto (deprecated) into balanced and renamed the binary version of y: y_bin,,2864,0.7513966480446927,0.09312320916905444,43391,489.0645525569818,38.141550091032705,129.84259408633125,6456,61,1798,381,travis,TomDLT,TomDLT,true,TomDLT,13,0.7692307692307693,6,3,152,true,false,false,false,3,99,15,82,65,0,1275
8523215,scikit-learn/scikit-learn,python,5007,1437408961,1438361548,1438361548,15876,15876,commits_in_master,false,true,false,419,4,3,3,6,0,9,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,1410,0,1410,0,21.46103897423428,0.4659063888867977,7,t3kcit@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c,4,0.004316546762589928,0,2,false,[WIP] Gradient Boosting speed improvement Through more stringent caching of intermediate results and hardcoding the FriedmanMSE criterion into a PresortBestSplitter I propose the FriedmanMSESplitter which builds the same trees but 2-3x faster than before This effect scales with number of estimators more than with depth and so building many small trees gives a better speed improvement (~3x) than building a few large trees (~2x)Instead of making any calls to a Criterion class this method will simply store the cumulative weight weight times residual and weight times residual squared values in a vector from which it can calculate improvement and impurity Previously this required multiple scans of the full sample space which was less efficientIn addition to being faster this also simplifies the code base by removing the (in my opinion) verbose criterion class The next step for me is to add in multithreaded training and so making the code as compact as possible is beneficial moving forward While I hardcoded FriedmanMSE into this splitter I believe all tree-based methods can benefit from the caching done hereLastly Ive added some more documentation to some Criterion and Splitting objects which I found helpful in learning the purpose of these objects  Here are time tests and accuracies on master and branch (mine) on a variety of datasets in two settings 25 estimators each of depth 2 and 5 estimators each of depth 625 estimators max depth 2          accuracy   train timemadelon    master     068833     2487branch     068833     0860gaussian (50000 points 2 classes 200 dimensions each dimension z05 apart)master     091712     30815branch     091712     10336 spambasemaster     092122     0444branch     092122     0165 mnistmaster     083786     877432branch     083786     340564covtypemaster     071034     462218branch     071034     2496245 estimators max depth 6          accuracy   train timemadelon    master     0835        2460branch     0835        1197gaussian (50000 points 2 classes 200 dimensions each dimension z05 apart)master     083908     219226branch     083908       94118spambasemaster     091471     0341branch     091536     0160mnistmaster     089471     720519branch     089457     373040covtypemaster     075713     290271branch     075713     178403Of note the two versions differ slightly (1%) in the spambase and mnist accuracies for 5 trees of depth 6 This seems to happen when the dimensionality is very high (several hundred)I am also currently getting segmentation faults when subsample is less than 1 which is odd because I didnt touch the allocation and deallocation steps However I thought I would open this WIP so that others could comment while I track down this issue,,2863,0.751309814879497,0.09064748201438849,43391,489.0645525569818,38.141550091032705,129.84259408633125,6455,61,1798,318,travis,jmschrei,jmschrei,true,jmschrei,1,1.0,35,5,852,false,true,false,false,2,3,1,0,0,0,13
8528872,scikit-learn/scikit-learn,python,5006,1437405751,1437409169,1437409170,56,56,github,false,false,false,6,2,2,0,4,0,4,0,2,0,0,2,2,0,1,0,0,0,2,2,0,1,0,0,0,0,0,13.835537030999696,0.3042578515892648,0,,doc/themes/scikit-learn/layout.html|doc/documentation.rst|doc/themes/scikit-learn/layout.html,0,0.0,0,0,false,Fixes to 016x docs See #4993,,2862,0.7512229210342418,0.0893371757925072,42724,524.6231626252223,40.258402771276096,136.57429079674188,6454,61,1798,301,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,313,0.8498402555910544,1180,40,1732,true,true,true,false,118,1115,82,468,167,7,47
8527530,scikit-learn/scikit-learn,python,5005,1437404054,1437684809,1437684809,4679,4679,github,false,false,false,8,1,1,0,3,0,3,0,2,0,0,2,2,0,1,0,0,0,2,2,0,1,0,0,0,0,0,8.550820500773192,0.18563327349919978,9,t3kcit@gmail.com,doc/documentation.rst|doc/themes/scikit-learn/layout.html,8,0.011527377521613832,0,0,false,[MRG+1] remove links to old docs see #4993,,2861,0.7511359664452989,0.0893371757925072,43391,489.0645525569818,38.141550091032705,129.84259408633125,6453,61,1798,304,travis,amueller,ogrisel,false,ogrisel,312,0.8493589743589743,1180,40,1732,true,true,true,false,118,1112,81,468,167,7,9
8411752,scikit-learn/scikit-learn,python,5002,1437304318,,1437437287,2216,,unknown,false,false,false,27,8,1,20,3,0,23,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,42,0,114,23,4.413626338866005,0.095703517313416,32,tom.dupre-la-tour@m4x.org,sklearn/preprocessing/data.py,32,0.046242774566473986,0,5,false,Allow MinMaxScaler to scale specified columns Addresses #5000 This should allow users to specify which columns to scale This allows categorical data to exist in the dataset,,2860,0.7513986013986014,0.0838150289017341,43370,489.9239105372377,38.22919068480517,130.08992391053724,6430,62,1797,302,travis,hlin117,hlin117,true,,2,0.5,18,20,1125,true,true,false,false,0,13,2,4,3,0,546
8515534,scikit-learn/scikit-learn,python,5001,1437298025,1437313922,1437313923,264,264,github,false,false,false,46,3,3,0,0,0,0,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,26,0,26,0,12.994802161330469,0.2821094513781985,12,t3kcit@gmail.com,sklearn/svm/classes.py|sklearn/svm/classes.py|sklearn/svm/classes.py,12,0.017391304347826087,0,0,false,Svm documentation Fixing issue #4998Before:img width638 altscreenshot 2015-07-18 15 23 05 srchttps://cloudgithubusercontentcom/assets/1865885/8764809/44e415f4-2da4-11e5-941f-ba6a7fa7e0b2pngimg width958 altscreenshot 2015-07-18 15 28 48 srchttps://cloudgithubusercontentcom/assets/1865885/8764807/44e3e99e-2da4-11e5-851f-978620f40829pngAfter:img width638 altscreenshot 2015-07-18 23 24 20 srchttps://cloudgithubusercontentcom/assets/1865885/8764810/44e56512-2da4-11e5-8ae3-5afc99abc872pngimg width635 altscreenshot 2015-07-18 23 23 40 srchttps://cloudgithubusercontentcom/assets/1865885/8764808/44e3e994-2da4-11e5-9d23-b62815f6cbf0pngI also fixed this issue for NuSVC,,2859,0.751311647429171,0.08260869565217391,43390,489.07582392256285,38.142429131136204,129.84558654067757,6429,62,1797,300,travis,hlin117,agramfort,false,agramfort,1,0.0,18,20,1125,true,true,false,false,0,13,1,4,1,0,-1
8503759,scikit-learn/scikit-learn,python,4995,1437182150,1445273350,1445273350,134853,134853,commits_in_master,false,false,false,38,14,2,0,9,2,11,0,5,0,0,1,5,1,0,0,1,2,3,6,2,0,1,0,34,0,114,8.937621496873023,0.19323568360201893,6,yanlend@users.noreply.github.com,sklearn/manifold/spectral_embedding_.py|sklearn/manifold/spectral_embedding_.py,6,0.00872093023255814,0,3,true,[MRG + 1] Bug fix for unnormalized laplacian So far _set_diag() always set the diagonal of the Laplacian to 1 This is only valid for the normalized Laplacian For the unnormalized Laplacian the diagonal should not be changed,,2858,0.7512246326102169,0.08139534883720931,43436,490.6529146330233,38.217147066949074,130.0764342941339,6415,61,1795,370,travis,yanlend,GaelVaroquaux,false,GaelVaroquaux,2,0.5,0,1,871,true,false,false,false,0,0,1,0,1,0,4276
8507389,scikit-learn/scikit-learn,python,4992,1437137161,1438306694,1438306694,19492,19492,commit_sha_in_comments,false,false,false,17,2,1,7,1,0,8,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.91465876556785,0.10669432166458426,0,,doc/modules/lda_qda.rst,0,0.0,0,0,true,[MRG+1] Rewrite of the documentation for LDA/QDA Small rewrite of the documentation for LDA/QDA following issue #4970,,2857,0.7511375568778439,0.07894736842105263,43390,489.07582392256285,38.142429131136204,129.84558654067757,6405,60,1795,317,travis,SPL361,SPL361,true,SPL361,1,1.0,0,0,722,false,false,false,false,0,4,2,0,1,0,9223
8483042,scikit-learn/scikit-learn,python,4987,1437072796,1437135837,1437135837,1050,1050,commits_in_master,false,false,false,25,250,250,0,3,10,13,4,2,5,1,240,246,202,1,3,5,1,240,246,202,1,3,4205,1482,4205,1482,2126.2105776101316,46.75758237071374,0,,sklearn/utils/estimator_checks.py|sklearn/utils/_random.c|sklearn/utils/_random.pyx|sklearn/decomposition/dict_learning.py|sklearn/utils/estimator_checks.py|sklearn/metrics/ranking.py|doc/modules/decomposition.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/tests/test_dummy.py|sklearn/utils/random.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|examples/preprocessing/README.txt|examples/preprocessing/plot_robust_scaling.py|sklearn/manifold/mds.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_base.py|sklearn/base.py|doc/modules/ensemble.rst|doc/modules/decomposition.rst|sklearn/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|doc/developers/utilities.rst|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/naive_bayes.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/__init__.py|sklearn/utils/graph.py|sklearn/utils/random.py|sklearn/utils/testing.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/mocking.py|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|sklearn/utils/estimator_checks.py|sklearn/metrics/regression.py|sklearn/linear_model/omp.py|.travis.yml|continuous_integration/install.sh|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/utils/tests/test_estimator_checks.py|sklearn/utils/tests/test_testing.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py|examples/preprocessing/plot_robust_scaling.py|doc/whats_new.rst|sklearn/utils/optimize.py|sklearn/preprocessing/imputation.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_least_angle.py|doc/modules/preprocessing.rst|examples/preprocessing/plot_robust_scaling.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/datasets/twenty_newsgroups.py|sklearn/metrics/tests/test_ranking.py|sklearn/manifold/locally_linear.py|sklearn/feature_extraction/text.py|sklearn/grid_search.py|examples/text/document_clustering.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/tune_toc.rst|doc/themes/scikit-learn/static/sidebar.js|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|examples/preprocessing/plot_robust_scaling.py|doc/modules/classes.rst|doc/whats_new.rst|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/whats_new.rst|doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|doc/themes/scikit-learn/static/nature.css_t|sklearn/decomposition/dict_learning.py|doc/themes/scikit-learn/static/nature.css_t|sklearn/cluster/tests/test_hierarchical.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/tests/test_pca.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/feature_extraction/tests/test_image.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_ransac.py|sklearn/manifold/tests/test_isomap.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_dist_metrics.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/neighbors/tests/test_kde.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_dummy.py|sklearn/tests/test_isotonic.py|sklearn/tests/test_multiclass.py|sklearn/tree/tests/test_export.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_random.py|sklearn/utils/tests/test_shortest_path.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/ensemble/forest.py|README.rst|doc/modules/svm.rst|doc/whats_new.rst|examples/applications/face_recognition.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/linear_model/logistic.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sgd.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/utils/class_weight.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_class_weight.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|doc/whats_new.rst|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_class_weight.py|sklearn/preprocessing/data.py|sklearn/utils/estimator_checks.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/mixture/gmm.py|doc/modules/clustering.rst|doc/modules/covariance.rst|doc/modules/decomposition.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/grid_search.rst|doc/modules/kernel_approximation.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/metrics.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/naive_bayes.rst|doc/modules/neighbors.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/calibration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/bicluster.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/cca_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/datasets/california_housing.py|sklearn/datasets/covtype.py|sklearn/datasets/lfw.py|sklearn/datasets/mlcomp.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/twenty_newsgroups.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/partial_dependence.py|sklearn/ensemble/voting_classifier.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/isotonic.py|sklearn/kernel_approximation.py|sklearn/kernel_ridge.py|sklearn/lda.py|sklearn/learning_curve.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/theil_sen.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/mds.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/t_sne.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/bicluster.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/pairwise.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/metrics/scorer.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/approximate.py|sklearn/neighbors/classification.py|sklearn/neighbors/graph.py|sklearn/neighbors/kde.py|sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/regression.py|sklearn/neighbors/unsupervised.py|sklearn/neural_network/rbm.py|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/classes.py|sklearn/tree/export.py|sklearn/tree/tree.py|doc/about.rst|doc/about.rst|doc/about.rst|doc/tutorial/basic/tutorial.rst|examples/preprocessing/README.txt|sklearn/linear_model/stochastic_gradient.py|doc/modules/model_persistence.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/whats_new.rst|sklearn/base.py|sklearn/grid_search.py|sklearn/multiclass.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py|sklearn/ensemble/tests/test_forest.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py|doc/modules/model_persistence.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/modules/svm.rst|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/tests/test_cross_validation.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|doc/developers/index.rst|doc/developers/index.rst|doc/developers/utilities.rst|doc/developers/index.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/decomposition/pca.py|doc/developers/index.rst|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|doc/index.rst|doc/themes/scikit-learn/layout.html|sklearn/base.py|sklearn/tests/test_pipeline.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|examples/decomposition/plot_ica_vs_pca.py|doc/modules/model_evaluation.rst|sklearn/tests/test_pipeline.py|sklearn/calibration.py|sklearn/dummy.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/theil_sen.py|sklearn/preprocessing/data.py|doc/index.rst|doc/conf.py|doc/developers/contributing.rst|doc/developers/index.rst|doc/documentation.rst|doc/index.rst|doc/modules/classes.rst|doc/preface.rst|doc/tutorial/index.rst|doc/user_guide.rst|doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/grid_search.py|examples/linear_model/plot_ols.py|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|sklearn/metrics/cluster/unsupervised.py|examples/ensemble/plot_adaboost_regression.py|examples/tree/plot_tree_regression.py|examples/tree/plot_tree_regression_multioutput.py|sklearn/tests/test_common.py|sklearn/kernel_approximation.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/cross_decomposition/pls_.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/tree/tree.py|doc/whats_new.rst|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/cross_decomposition/pls_.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_memory_helpers.py|sklearn/externals/joblib/format_stack.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|appveyor.yml|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|examples/model_selection/randomized_search.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|examples/applications/face_recognition.py|sklearn/ensemble/forest.py|sklearn/cross_decomposition/pls_.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/feature_extraction/hashing.py|sklearn/linear_model/ransac.py|sklearn/svm/base.py|sklearn/linear_model/tests/test_ransac.py|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|sklearn/linear_model/ransac.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/hashing.py|doc/whats_new.rst|sklearn/datasets/svmlight_format.py|doc/whats_new.rst|examples/cluster/plot_kmeans_digits.py|CONTRIBUTING.md|sklearn/decomposition/nmf.py|sklearn/svm/base.py|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|sklearn/tree/tree.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_svmlight_format.py|doc/modules/multiclass.rst|doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|doc/modules/lda_qda.rst,0,0.0,0,0,false,Ldq qda doc This is a small reorganisation/clean-up of the documentation for LDA/QDA answering issue #4970  First pull-request ever hope Im doing this right,,2856,0.7510504201680672,0.07894736842105263,42724,524.6231626252223,40.258402771276096,136.57429079674188,6397,60,1794,298,travis,SPL361,SPL361,true,SPL361,0,0,0,0,721,false,false,false,false,0,1,0,0,0,0,977
8480102,scikit-learn/scikit-learn,python,4984,1437061771,1437079370,1437079370,293,293,github,false,false,false,25,1,1,0,4,0,4,0,2,0,0,5,5,3,0,0,0,0,5,5,3,0,0,53,32,53,32,23.396731275366104,0.5079398146374926,10,tom.dupre-la-tour@m4x.org,doc/datasets/rcv1.rst|doc/datasets/rcv1_fixture.py|doc/modules/classes.rst|sklearn/datasets/rcv1.py|sklearn/datasets/tests/test_rcv1.py,8,0.00291970802919708,0,1,false,[MRG+1] implement official LYRL2004 train/test split of RCV1 dataset A new parameter subset is added to choose the desired subsetTest and doc are updated,,2855,0.7509632224168126,0.07883211678832117,43371,489.3361923866178,38.15913859491366,129.9485831546425,6395,60,1794,298,travis,TomDLT,ogrisel,false,ogrisel,12,0.75,6,3,148,true,false,true,false,3,98,14,82,60,0,11
8478484,scikit-learn/scikit-learn,python,4983,1437051027,1437053790,1437053791,46,46,github,false,false,false,38,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,5.161976773113161,0.11253312253202508,1,olivier.grisel@ensta.org,appveyor.yml,1,0.0014641288433382138,0,0,false,[MRG] MAINT fix numpy / scipy install on appveyor Make it possible to use plain HTTP to download pre-built numpy & scipy from rackspace containerNew versions of pip refuse to download packages over non-secure HTTP by default,,2854,0.7508759635599159,0.07906295754026355,43264,489.4600591715976,38.23039940828402,129.8539201183432,6393,60,1794,299,travis,ogrisel,ogrisel,true,ogrisel,123,0.8617886178861789,1178,124,2241,true,true,false,false,22,212,35,162,36,3,46
8390648,scikit-learn/scikit-learn,python,4981,1437003246,,1458049428,350769,,unknown,false,false,false,112,1,1,2,5,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,14,2,14,8.053339585658977,0.17556597127836757,10,t3kcit@gmail.com,sklearn/base.py|sklearn/ensemble/tests/test_weight_boosting.py,8,0.011661807580174927,0,0,false,Test case that reproduces behaviour in issue #4949 Grid searching different base_estimators for AdaBoostClassifier and setting their parameters fails You end up with the default parameters in best_estimator_ This adds a test that reproduces the example I posted in #4949 as well as a potential fix Unsure if this is the correct fixI think there are two (potential) problems here:1 setting a property to a new object so that valid_params now doesnt contain a reference to the correct object2 there is no guarantee about the order in which the parameters from the parameter grid get looped over so we could end up setting base_estimator__max_depth first and then setting base_estimator,,2853,0.7511391517700666,0.07871720116618076,43264,489.4600591715976,38.23039940828402,129.8539201183432,6386,59,1793,524,travis,betatim,betatim,true,,4,0.75,43,44,1243,true,false,false,false,2,17,2,5,11,0,0
8476890,scikit-learn/scikit-learn,python,4979,1436963270,1436963399,1436963399,2,2,github,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.378290260010194,0.09544852539009767,0,,CONTRIBUTING.md,0,0.0,0,0,false,Fix #4978: Typo in CONTRIBUTINGmd ,,2851,0.7513153279551035,0.07894736842105263,43264,489.4600591715976,38.23039940828402,129.8539201183432,6379,58,1793,297,travis,donnemartin,glouppe,false,glouppe,0,0,429,9,669,true,true,false,false,1,0,0,0,1,0,2
8462176,scikit-learn/scikit-learn,python,4977,1436935139,1436999570,1436999570,1073,1073,github,false,false,false,6,1,1,0,0,0,0,0,1,0,0,5,5,5,0,0,0,0,5,5,5,0,0,26,0,26,0,22.33666092984917,0.4869483796827456,0,,examples/svm/plot_iris.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_separating_hyperplane.py|examples/svm/plot_svm_nonlinear.py|examples/svm/plot_weighted_samples.py,0,0.0,0,0,false,Fixed small typos in SVM examples ,,2850,0.7512280701754386,0.07906295754026355,43264,489.4600591715976,38.23039940828402,129.8539201183432,6375,57,1792,297,travis,edsonduarte1990,agramfort,false,agramfort,0,0,8,13,702,true,true,false,false,0,1,0,0,1,0,-1
8440302,scikit-learn/scikit-learn,python,4974,1436828821,,1444349821,125350,,unknown,false,true,false,63,6,2,1,12,3,16,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,340,0,444,11,9.026379893409219,0.19677863477428098,7,t3kcit@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py,7,0.010014306151645207,0,6,false,Weighted pls -- adding support for sample_weight option Added support for sample_weight option refactoring some code along the way See commit message for more detailsI have tested that results do not change when no sample_weight option is specified and when sample_weight  ones(len(X)) Also in my weighted real-world use case results look reasonable and are an improvement over using no weightsDeepak,,2848,0.7517556179775281,0.08297567954220315,43263,489.47137276656724,38.231283082541665,129.85692161893536,6361,58,1791,362,travis,Fenugreek,Fenugreek,true,,0,0,0,0,904,false,false,false,false,0,0,0,0,0,0,24299
8431774,scikit-learn/scikit-learn,python,4972,1436793382,1437246392,1437246392,7550,7550,github,false,false,false,9,1,1,0,9,0,9,0,6,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.814452007123484,0.10451889247846598,40,trev.stephens@gmail.com,doc/whats_new.rst,40,0.056338028169014086,0,5,false,[MRG] DOC updated whatsnew for joblib 090b2 for #4905,,2847,0.7516684229012996,0.08169014084507042,43390,489.07582392256285,38.142429131136204,129.84558654067757,6359,58,1791,299,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,122,0.860655737704918,1175,124,2238,true,true,true,true,23,239,39,186,42,3,124
8428115,scikit-learn/scikit-learn,python,4971,1436742979,1436746371,1436746371,56,56,github,false,false,false,34,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.7913416160532645,0.10445331774042262,2,t3kcit@gmail.com,doc/modules/multiclass.rst,2,0.002828854314002829,0,0,false,[MRG+2] Added list of multi-label supporting classifiers Closes #4724 The multi-class page now includes a list of estimatorsthat support multi-labels Also adjusted the reference to LDA to includeQDA as a multiclass-supporting classifier,,2846,0.7515811665495432,0.08203677510608204,43240,489.7086031452359,38.25161887141536,129.92599444958373,6349,58,1790,294,travis,dotsdl,amueller,false,amueller,1,1.0,19,34,1276,false,true,true,false,2,2,1,0,0,0,15
8425644,scikit-learn/scikit-learn,python,4967,1436724335,1437689914,1437689914,16092,16092,github,false,false,false,91,2,1,2,16,0,18,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,5,32,7,32,9.287850256106324,0.2016207622206394,4,t3kcit@gmail.com,sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py,2,0.002844950213371266,0,6,false,[MRG+2] BUG Use epsilon threshold in _samme_proba The ensembleweight_boosting_samme_proba function should preserve ordering of class probabilities within an example but thresholding logic (to avoid taking the log of values  0) was disrupting this Instead of thresholding proba  0 probabilities to 1e-5 threshold proba  epsilon to epsilon This avoids the issue of eg probability values of 0 becoming larger than values of 1e-7Add a unit test for _samme_proba which checks that probability ordering is unchanged This test fails on the un-modified version of _samme_proba Resolves issue #4944 ,,2844,0.7517580872011251,0.07823613086770982,43397,489.11215060948916,38.13627670115446,129.87072839136346,6347,57,1790,307,travis,stephen-hoover,ogrisel,false,ogrisel,2,0.5,5,1,342,false,true,false,false,2,6,2,0,0,0,87
8424307,scikit-learn/scikit-learn,python,4966,1436691175,1436710439,1436710440,321,321,github,false,false,false,26,1,1,0,0,0,0,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,33,14,33,14,9.286400636130402,0.20245021966726148,1,alexandre.gramfort@m4x.org,sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_svmlight_format.py,1,0.0014245014245014246,0,0,false,Add multilabel support for dump_svmlight_file We already have multilabel support for load_svmlight_file So I think it make sense to add support for dumping as wellThanks,,2843,0.7516707703130496,0.07834757834757834,43219,489.50692982253173,38.20079131863301,129.75774543603507,6343,57,1790,298,travis,lazywei,larsmans,false,larsmans,3,0.0,83,73,964,false,false,false,false,0,3,0,1,0,0,-1
8367363,scikit-learn/scikit-learn,python,4965,1436665932,1438194027,1438194027,25468,25468,github,false,false,false,44,3,1,3,11,0,14,0,4,1,0,0,2,1,0,0,1,0,1,2,1,0,0,63,0,63,0,5.051859929487126,0.10966583456117251,0,,examples/cluster/plot_kmeans_assumptions.py,0,0.0,0,1,false,[MRG+1] New k-means example Fix for #4922 Here is the beginning to a new k-means example showing the possible pitfalls people can run into when running it I still need to write a nice description of whats going on in the examplesComments welcome,,2842,0.7515833919774807,0.07857142857142857,43397,489.11215060948916,38.13627670115446,129.87072839136346,6341,57,1789,317,travis,mrphilroth,amueller,false,amueller,2,1.0,18,17,1011,false,true,false,false,0,3,2,0,0,0,1051
8420977,scikit-learn/scikit-learn,python,4962,1436662909,1436666127,1436666128,53,53,github,false,false,false,30,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,3.954999645559617,0.08622180970097054,2,gael.varoquaux@normalesup.org,examples/applications/face_recognition.py,2,0.0028653295128939827,0,0,false,Added random seed for facial recognition example & updated the docstring Random_state was not set so the expected output/results were different from the examples docstring Also updated the new docstring,,2839,0.7520253610426206,0.07879656160458452,43219,489.50692982253173,38.20079131863301,129.75774543603507,6341,57,1789,298,travis,vinc456,amueller,false,amueller,0,0,6,65,1232,false,false,false,false,0,0,0,0,0,0,53
8367332,scikit-learn/scikit-learn,python,4961,1436662796,1437565737,1437565737,15049,15049,github,false,false,false,17,4,1,1,4,0,5,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,7,0,7,13,4.36037869306335,0.095059361716903,2,t3kcit@gmail.com,sklearn/datasets/samples_generator.py,2,0.0028653295128939827,0,1,false,[MRG] Allowing for different cluster stds Fixed #4959 This will be helpful for my WIP k-means example,,2838,0.751937984496124,0.07879656160458452,43219,489.50692982253173,38.20079131863301,129.75774543603507,6341,57,1789,309,travis,mrphilroth,ogrisel,false,ogrisel,1,1.0,18,17,1011,false,true,false,false,0,3,1,0,0,0,7
8420783,scikit-learn/scikit-learn,python,4960,1436661205,1438125776,1438125776,24409,24409,github,false,false,false,24,2,1,2,5,0,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,3,0,3,5,4.204397118727921,0.09165885227961225,2,t3kcit@gmail.com,sklearn/preprocessing/label.py,2,0.0028776978417266188,0,1,false,[MRG + 1] Dont allow unseen values for inverse transform Currently negative labels are allowed for LabelEncoder inverse_transform instead raise error similar to transform,,2837,0.7518505463517801,0.07913669064748201,43219,489.50692982253173,38.20079131863301,129.75774543603507,6341,57,1789,313,travis,mkrump,agramfort,false,agramfort,0,0,0,2,1402,false,true,false,false,0,0,0,0,0,0,9
8420208,scikit-learn/scikit-learn,python,4958,1436655952,1436703022,1436703022,784,784,github,false,false,false,37,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.293944271304697,0.09361104399659685,14,tw991@nyu.edu,sklearn/tree/tree.py,14,0.02014388489208633,0,4,false,[MRG + 1] Added a note about variance reduction in DecisionTreeRegressor HiI just added a short note to clarify that using mse as criterion is the same as the variance reduction which was introduced in CART,,2836,0.7517630465444288,0.07625899280575539,43219,489.50692982253173,38.20079131863301,129.75774543603507,6341,57,1789,297,travis,rasbt,glouppe,false,glouppe,4,1.0,1326,43,644,true,true,false,false,2,16,3,2,5,0,41
8419738,scikit-learn/scikit-learn,python,4957,1436654228,1437743457,1437743457,18153,18153,github,false,false,false,40,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,105,0,105,0,4.678386648105397,0.10199216167652234,20,tom.dupre-la-tour@m4x.org,sklearn/utils/estimator_checks.py,20,0.02877697841726619,0,2,false,[MRG+2] Closes #4614 : Added pickle tests for all estimators  Moved pickle tests for estimators from individual tests for classifiersregressors and transformers to tests for all estimators This meansthat all estimators are now tested for pickleabilityCloses #4614 ,,2835,0.7516754850088183,0.07625899280575539,43219,489.50692982253173,38.20079131863301,129.75774543603507,6341,57,1789,307,travis,dotsdl,ogrisel,false,ogrisel,0,0,19,34,1275,false,true,false,false,1,1,0,0,0,0,73
8365207,scikit-learn/scikit-learn,python,4956,1436647524,1436714283,1436714283,1112,1112,github,false,false,false,16,5,2,3,1,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,13,20,25,12.8786932659278,0.28076484351964687,6,t3kcit@gmail.com,sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|sklearn/linear_model/ransac.py,4,0.005780346820809248,0,0,false,Better error message for ransac fit when number of inliers equals 0 Fix for issue #4907 ,,2834,0.7515878616796048,0.07658959537572255,43213,489.5748964432,38.20609538796196,129.77576192349525,6341,57,1789,296,travis,nealchau,larsmans,false,larsmans,0,0,0,1,922,true,false,false,false,0,1,0,0,2,0,3
8365190,scikit-learn/scikit-learn,python,4955,1436647137,1443138079,1443138079,108182,108182,commits_in_master,false,false,false,17,5,2,15,14,0,29,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,62,6,90,17.64278459838284,0.3846203283243894,3,t3kcit@gmail.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,3,0.004341534008683068,0,16,false,vocabulary of type set now coerced to list to preserve iteration ordering after serialization Fix for https://githubcom/scikit-learn/scikit-learn/issues/4895,,2833,0.7515001764913519,0.07670043415340087,43240,489.7086031452359,38.25161887141536,129.92599444958373,6341,57,1789,351,travis,copyconstructor,amueller,false,amueller,0,0,1,0,812,true,false,false,false,0,1,0,0,2,0,4
8418910,scikit-learn/scikit-learn,python,4954,1436646473,1437337585,1437337585,11518,11518,github,false,false,false,16,1,0,0,9,0,9,0,5,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,4,6,0,0.0,0,,,0,0.0,0,3,false,[MRG + 2] Fixes bug in OOB score with csc matrices Fixes #4744 Existing PR: #4745 ,,2832,0.751412429378531,0.07547169811320754,43390,489.07582392256285,38.142429131136204,129.84558654067757,6341,56,1789,304,travis,ankurankan,glouppe,false,glouppe,0,0,29,30,1235,false,false,false,false,1,0,0,0,0,0,18
8418640,scikit-learn/scikit-learn,python,4953,1436644027,1436644667,1436644667,10,10,github,false,false,false,18,2,2,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,7.936795251601481,0.17302789031425592,10,t3kcit@gmail.com,sklearn/svm/base.py|sklearn/svm/base.py,10,0.014534883720930232,0,0,false,Fix issue #4940 with better error messages Penalty and loss were switched between the message and the arguments,,2831,0.751324620275521,0.0755813953488372,43213,489.5748964432,38.20609538796196,129.77576192349525,6341,56,1789,288,travis,mrphilroth,amueller,false,amueller,0,0,18,17,1011,false,true,false,false,0,1,0,0,0,0,9
8418236,scikit-learn/scikit-learn,python,4952,1436640693,1436640868,1436640868,2,2,github,false,false,false,31,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.492699169126875,0.09794409879748138,2,t3kcit@gmail.com,sklearn/decomposition/nmf.py,2,0.0029282576866764276,0,1,false,DOC Fix doc for NMF init default Docs had claimed that the init argument defaulted to nndsvdar when n_components  n_features but it actually defaults to nndsvdResolves issue #4929 ,,2830,0.7512367491166078,0.07613469985358712,43213,489.5748964432,38.20609538796196,129.77576192349525,6341,55,1789,289,travis,stephen-hoover,GaelVaroquaux,false,GaelVaroquaux,0,0,5,1,341,false,true,false,false,1,1,0,0,0,0,2
8400825,scikit-learn/scikit-learn,python,4947,1436541245,1436643423,1436643423,1702,1702,github,false,false,false,15,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.745846437589277,0.1034628927650313,0,,examples/cluster/plot_kmeans_digits.py,0,0.0,0,0,true,Updated graph ranges The minimum and maximum values for the graph were not correctly defined,,2827,0.7516802263883976,0.07692307692307693,43213,489.5748964432,38.20609538796196,129.77576192349525,6333,55,1788,288,travis,zyrikby,amueller,false,amueller,0,0,8,4,1392,false,false,false,false,0,0,0,0,0,0,1700
8366070,scikit-learn/scikit-learn,python,4941,1436394665,,1436728297,5560,,unknown,false,false,false,113,4,2,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,28,0,32,0,8.778559722231257,0.19137896582132086,3,t3kcit@gmail.com,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,3,0.004379562043795621,0,1,false,Tfidf mem fix previously line 760 in textpyvalues  npones(len(j_indices))would cause memory issues as len(j_indices) is equal to the entire corpus word count I had issues with a dataset of 200000 documents with ~4000 words each when many gigabytes would be allocated temporarily Ive eliminated the need for this line and the Xsum_duplicates calculation without a perceptible performance hitAdditionally for matrices with greater than 2 billion nnz npintc and arrayarray(str(i)) were insufficient for storing values of indptr that had to index into more than the (2^31 - 1)th position in the values or j_indices arrays Ive changed npintc - npint64 and  arrayarray(str(i)) - arrayarray(str(l)) to accommodate this possibility,,2826,0.7519462137296532,0.07737226277372262,43213,489.5748964432,38.20609538796196,129.77576192349525,6313,54,1786,297,travis,aupiff,aupiff,true,,0,0,36,58,1507,true,true,false,false,0,0,0,0,2,0,1169
8357604,scikit-learn/scikit-learn,python,4939,1436371364,,1441127657,79271,,unknown,false,false,false,538,5,1,0,11,0,11,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,20,0,20,218,4.1155430691540165,0.08972182240478145,2,t3kcit@gmail.com,sklearn/linear_model/passive_aggressive.py,2,0.00291970802919708,0,3,false,[MRG] Added average option to passive aggressive classifier/regressor Differently from the SGDClassifier (and SGDRegressor) the PassiveAggressiveClassifier (and PassivieAggressiveRegressor) does not expose the average option of the BaseSGD classThe average option helps smoothing out the impact of rarely observed variables For example when used in combination with the HashingVectorizer on text it helps to compensate for the lack of idf information lowering the impact low-idf (ie rare) featuresThe following is an example of using averaging both on SGDClassifier and PassiveAggressiveClassifierThe data is the Bo Pang movie review dataset ( https://wwwcscornelledu/people/pabo/movie-review-data/review_polaritytargz )The results show that averaging in BaseSGD works perfectly for both classifiers to remove noise features[averagedemo](https://cloudgithubusercontentcom/assets/6543521/8570575/8e7df796-2581-11e5-8fa0-61000a12b73fpng)The following is the code to replicate the experiment that generated the above plot:import argparseimport codecsimport sysimport matplotlibpyplot as pltimport numpy as npfrom sklearncross_validation import ShuffleSplitfrom sklearn import cross_validationfrom sklearndatasets import load_filesfrom sklearnfeature_extractiontext import TfidfVectorizer HashingVectorizerfrom sklearnlinear_model import PassiveAggressiveClassifier SGDClassifierfrom sklearnmetrics import SCORERSfrom sklearnpipeline import Pipelinefrom sklearnsvm import LinearSVCdef randomized_train_test(estimator data target test_size n_iter scorer random_state n_jobs):    cv  ShuffleSplit(len(data) n_itern_iter test_sizetest_size random_staterandom_state)    scores  cross_validationcross_val_score(estimator data target scorer cv n_jobs)    score_mean  scoresmean()    score_std  scoresstd()    return score_mean score_std scoresdef plot(scorer_name results grid_wNone grid_hNone):    all_scores  list()    for _ (_ _ scores) in results:        all_scoresextend(scores)    count  len(results)    bins  nphistogram(all_scores bins10 + (len(all_scores) /                                               (count * npmax((                                                   1 nplog2(len(all_scores) / count))))))[1]    if grid_w is None:        grid_w  int(npceil(npsqrt(count)))    if grid_h is None:        grid_h  int(npceil(count / grid_w))    subplots  list()    for i (name (score_mean score_std scores)) in enumerate(results):        if len(subplots)  0:            subplot  pltsubplot2grid((grid_h grid_w)                                       (int(i / grid_w) int(i % grid_w))                                       sharexsubplots[0]                                       shareysubplots[0])        else:            subplot  pltsubplot2grid((grid_h grid_w)                                       (int(i / grid_w) int(i % grid_w)))        subplotset_title(%s mean %s%03f std%03f % (name scorer_name score_mean score_std))        subplothist(scores alpha05 binsbins)        subplotsappend(subplot)    ylim  subplots[0]get_ylim()    for (name (score_mean score_std _)) subplot in zip(results subplots):        subplotplot([score_mean for _ in ylim] ylim --k linewidth2)        subplotplot([score_mean - score_std for _ in ylim] ylim :k linewidth2)        subplotplot([score_mean + score_std for _ in ylim] ylim :k linewidth2)# download demo data from:# https://wwwcscornelledu/people/pabo/movie-review-data/review_polaritytargzdef main():    sysstdout  codecsgetwriter(utf8)(sysstdoutbuffer)    parser  argparseArgumentParser(description)    parseradd_argument(-p --path helpInput path requiredTrue)    parseradd_argument(-r --random helpRandomization seed (default: 0) typeint default0)    parseradd_argument(-s --scorer                        helpEvaluation function (default: f1 values: %s) %  join(SCORERSkeys()) typestr                        defaultf1)    parseradd_argument(-i --iter helpNumber of randomized runs (default: 100) typeint default100)    parseradd_argument(-t --test helpPortions of examples to be used as test set (default: 02) typefloat                        default02)    args  parserparse_args()    test_size  argstest    random_state  argsrandom    n_iter  argsiter    n_jobs  -1    scorer  SCORERS[argsscorer]    examples  load_files(argspath shuffleFalse)    data  examplesdata    target  examplestarget    print(Total %i Train/test split %02f/%02f %i/%i % (len(data)                                                           (1 - argstest) argstest                                                           int(len(data) * (1 - argstest))                                                           int(len(data) * argstest)))    experiments  list()    experimentsappend((pa-tfidf Pipeline([        (vec TfidfVectorizer())        (clf PassiveAggressiveClassifier(random_staterandom_state))    ])))    experimentsappend((pa-hash-no-average Pipeline([        (vec HashingVectorizer())        (clf PassiveAggressiveClassifier(random_staterandom_state))    ])))    experimentsappend((pa-hash-average-100 Pipeline([        (vec HashingVectorizer())        (clf PassiveAggressiveClassifier(random_staterandom_state average100))    ])))    experimentsappend((sgdc-tfidf Pipeline([        (vec TfidfVectorizer())        (clf SGDClassifier(random_staterandom_state))    ])))    experimentsappend((sgdc-hash-no-average Pipeline([        (vec HashingVectorizer())        (clf SGDClassifier(random_staterandom_state))    ])))    experimentsappend((sgdc-hash-average-100 Pipeline([        (vec HashingVectorizer())        (clf SGDClassifier(random_staterandom_state average100))    ])))    results  list()    for name estimator in experiments:        print(name)        resultsappend((name list(randomized_train_test(estimator data target test_size n_iter                                                         scorer                                                         random_state n_jobs))))    print(results)    pltfigure()    plot(argsscorer results grid_w3 grid_h2)    pltshow()if __name__  __main__:    main(),,2825,0.7522123893805309,0.07737226277372262,43213,489.5748964432,38.20609538796196,129.77576192349525,6309,53,1786,326,travis,aesuli,aesuli,true,,0,0,4,0,524,false,false,false,false,0,0,0,0,0,0,4
8373445,scikit-learn/scikit-learn,python,4937,1436346528,1436710620,1436710620,6068,6068,github,false,false,false,10,3,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,18,0,4.682642450751391,0.10208500003324449,2,t3kcit@gmail.com,sklearn/feature_extraction/hashing.py,2,0.0029282576866764276,0,0,false,Issue  #4882 Improve FeatureHasher documentation  add an example for FeatureHasher,,2824,0.7521246458923513,0.0746705710102489,43213,489.5748964432,38.20609538796196,129.77576192349525,6307,53,1786,296,travis,pianomania,larsmans,false,larsmans,1,0.0,0,0,232,true,false,false,false,0,9,1,3,1,0,95
8331206,scikit-learn/scikit-learn,python,4934,1436276831,1436285032,1436285032,136,136,github,false,false,false,30,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.433107093241791,0.09664499323580467,0,,doc/modules/cross_validation.rst,0,0.0,0,0,false,[docs] [trivial] Fix small error in cross-validation docs In the cross-validation docs in the user guide the StratifiedKFold example mentions 2-fold CV but actually uses 3 This fixes the text,,2822,0.7523033309709426,0.07782672540381791,43219,489.3218260487286,38.20079131863301,129.64205557740806,6293,51,1785,284,travis,thvasilo,jnothman,false,jnothman,0,0,1,0,273,true,true,false,false,0,0,0,0,1,0,136
8317759,scikit-learn/scikit-learn,python,4933,1436272646,1437438602,1437438602,19432,19432,github,false,false,false,15,1,0,4,6,0,10,0,6,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,10,0,0,0.0,0,,,0,0.0,0,2,false,[MRG+1] extend R^2 description To make the docs more explicit for R^2 score in regression,,2821,0.7522155264090747,0.07669616519174041,43219,489.50692982253173,38.20079131863301,129.75774543603507,6290,50,1785,306,travis,banilo,agramfort,false,agramfort,6,0.5,5,0,840,true,false,false,false,0,0,0,0,1,0,9
8348698,scikit-learn/scikit-learn,python,4932,1436252163,1436273428,1436273428,354,354,github,false,false,false,72,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.484070963109477,0.09775606229518152,17,tw991@nyu.edu,sklearn/ensemble/forest.py,17,0.025373134328358207,0,0,false,RandomForest Docstring Update HiI added some lines to the RandomForest docstring I think that using the original sample size for the bootstrap sub-sample size may be the typical implementation of random forests but maybe it is useful to mention it explicitly for clarity Also auto and sqrt are the same for the max_features parameter in the rf classifier and although this may be intended someone might think that its a typo,,2820,0.752127659574468,0.07164179104477612,43219,489.3218260487286,38.20079131863301,129.64205557740806,6285,49,1785,284,travis,rasbt,arjoly,false,arjoly,3,1.0,1319,43,640,true,true,false,false,2,16,2,2,5,0,354
8331816,scikit-learn/scikit-learn,python,4928,1436197273,1436279065,1436279065,1363,1363,github,false,false,false,9,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.44856324719959,0.09698196783570413,5,t3kcit@gmail.com,sklearn/cross_decomposition/pls_.py,5,0.00744047619047619,0,0,false,[MRG] MAINT PLS Remove deprecated coefs attribute and pep8 ,,2819,0.7520397304008514,0.06994047619047619,43219,489.3218260487286,38.20079131863301,129.64205557740806,6284,50,1784,284,travis,rvraghav93,ogrisel,false,ogrisel,6,0.6666666666666666,25,42,640,true,false,true,true,3,121,13,122,50,0,7
8331875,scikit-learn/scikit-learn,python,4927,1436160838,1436279328,1436279330,1974,1974,github,false,false,false,16,1,0,0,2,0,2,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,9,21,0,0.0,0,,,0,0.0,0,0,false,[MRG+1] FIX #4902: SV*predict_proba visible before fit Fixes #4902 and provides an alternative fix to #4791,,2818,0.7519517388218595,0.06576980568011959,43219,489.3218260487286,38.20079131863301,129.64205557740806,6284,50,1784,284,travis,jnothman,ogrisel,false,ogrisel,114,0.6929824561403509,30,1,2260,true,true,false,false,14,228,22,117,23,2,248
8270700,scikit-learn/scikit-learn,python,4924,1435945567,,1442424428,107981,,unknown,false,true,false,107,1,1,4,16,0,20,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,19,9,19,8.94179992672786,0.19493784952549092,2,t3kcit@gmail.com,sklearn/lda.py|sklearn/tests/test_lda.py,2,0.002976190476190476,0,2,true,Re-adding checks for prior in LDA After version 0152 the checks to selfpriors and the conversion of priors to a numpy array seem to have been removed Im not sure if this is intentional but it causes issues if priors is passed as a list to init instead of as a numpy array eg if  priors is passed as a list then the following error results when multiplying n_samples* selfpriors_ in _solve_svd (line 379 of ldapy):    X  npdot(((npsqrt((n_samples * selfpriors_) * fac)) *    TypeError: cant multiply sequence by non-int of type floatThis is my first pull request to scikit-learn so hopefully Im following protocol :),,2817,0.7522186723464679,0.05654761904761905,43219,489.3218260487286,38.20079131863301,129.64205557740806,6284,47,1781,341,travis,benjaminirving,ogrisel,false,,0,0,5,17,1114,true,true,false,false,0,0,0,0,1,0,32
8262443,scikit-learn/scikit-learn,python,4919,1435849490,1439480074,1439480074,60509,60509,commits_in_master,false,false,false,9,19,2,62,15,0,77,0,5,0,0,19,22,18,0,0,0,0,22,22,21,0,0,132,745,379,1058,85.86946541645027,1.8720187576506742,34,tw991@nyu.edu,sklearn/preprocessing/label.py|benchmarks/bench_multilabel_metrics.py|doc/modules/multiclass.rst|examples/datasets/plot_random_multilabel_dataset.py|examples/plot_multilabel.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_voting_classifier.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_score_objects.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_multiclass.py|sklearn/tree/tests/test_tree.py|sklearn/utils/tests/test_multiclass.py,9,0.002824858757062147,0,19,false,[WIP] MAINT Remove deprecated sequence of sequence support etc ,,2816,0.7521306818181818,0.05790960451977401,43219,489.3218260487286,38.20079131863301,129.64205557740806,6281,52,1780,321,travis,rvraghav93,jnothman,false,jnothman,5,0.6,25,42,636,true,false,false,false,4,111,12,107,36,0,98
8260281,scikit-learn/scikit-learn,python,4918,1435839729,1435858231,1435858231,308,308,commits_in_master,false,false,false,23,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.522713421227286,0.09859854511676,0,,examples/model_selection/randomized_search.py,0,0.0,0,0,false,Change name of variable to be consistent with dataset Example is loading digits dataset onto iris variable Changed to digits variable for consistency,,2815,0.7520426287744227,0.05718270571827057,43219,489.3218260487286,38.20079131863301,129.64205557740806,6280,52,1780,280,travis,ejcaropr,agramfort,false,agramfort,0,0,0,2,441,false,false,false,false,0,0,0,0,0,0,-1
8253014,scikit-learn/scikit-learn,python,4915,1435796404,1436744233,1436744233,15797,15797,commits_in_master,false,false,false,31,4,2,2,4,0,6,0,4,0,0,5,6,3,0,0,0,0,6,6,3,0,0,87,28,99,28,45.47774753110489,0.9914490095057746,72,trev.stephens@gmail.com,doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,39,0.015151515151515152,0,0,false,[MRG] ENH add minmax_scale This PR adds minmax_scale which is a function that provides the same functionality as MinMaxScaler but as a function Exactly like how scale/robust_scale/maxabs_scale provide function-interfaces for StandardScaler/RobustScaler/MaxAbsScaler,,2814,0.751954513148543,0.05647382920110193,43219,489.3218260487286,38.20079131863301,129.64205557740806,6275,52,1779,297,travis,untom,amueller,false,amueller,13,0.6923076923076923,10,0,863,true,false,false,false,0,15,2,19,2,0,452
8234321,scikit-learn/scikit-learn,python,4912,1435713234,,1456275274,342700,,unknown,false,false,false,9,15,4,15,12,0,27,0,3,0,0,3,4,3,0,0,0,0,4,4,4,0,0,171,9,419,33,13.172389703106884,0.2871674321903711,7,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|examples/linear_model/plot_lasso_adaptive_lasso_and_elasticnet.py|examples/linear_model/plot_lasso_adaptive_lasso_and_elasticnet.py,5,0.0026702269692923898,0,3,false,Adaptive lasso Implement adaptive Lasso and resolves issue #555 ,,2812,0.7524893314366998,0.056074766355140186,43219,489.3218260487286,38.20079131863301,129.64205557740806,6257,52,1778,498,travis,henridwyer,henridwyer,true,,0,0,8,0,868,false,false,false,false,0,0,0,0,0,0,1372
8222488,scikit-learn/scikit-learn,python,4909,1435672900,,1435722585,828,,unknown,false,false,false,61,2,2,0,9,0,9,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,0,13,0,8.728701830079572,0.19029188684784576,23,trev.stephens@gmail.com,sklearn/svm/base.py|sklearn/grid_search.py,16,0.02127659574468085,2,2,false,[Correct delegation of predict proba Fixes #4902 The decorator (used for ducktyping) was set to estimator (the unfitted est) rather than the final best_estimator_ at [94f37f43fac](https://githubcom/scikit-learn/scikit-learn/commit/94f37f43faccf0ba165e14377a79d3a18964f486)And so predict_proba_ checked for the attr probA_ in the estimator instead of best_estimator_Also we need to make sure NotFittedError gets raised first since estimator is not fitted@agramfort @adammenges Please take a look,,2811,0.7527570259694059,0.05585106382978723,43219,489.3218260487286,38.20079131863301,129.64205557740806,6251,52,1778,281,travis,rvraghav93,jnothman,false,,4,0.75,25,42,634,true,false,false,false,3,94,9,82,35,0,1
8222089,scikit-learn/scikit-learn,python,4908,1435671107,1437053857,1437053857,23045,23045,commits_in_master,false,false,false,5,2,2,25,20,0,45,0,7,4,0,5,9,6,0,0,4,0,5,9,6,0,0,481,71,481,71,60.013217265583876,1.3083306740898908,42,trev.stephens@gmail.com,doc/datasets/index.rst|doc/datasets/rcv1.rst|sklearn/datasets/__init__.py|sklearn/datasets/rcv1.py|doc/datasets/index.rst|doc/datasets/rcv1.rst|doc/datasets/rcv1_fixture.py|doc/whats_new.rst|sklearn/datasets/__init__.py|sklearn/datasets/covtype.py|sklearn/datasets/rcv1.py|sklearn/datasets/tests/test_rcv1.py|sklearn/utils/fixes.py,39,0.0,0,2,false,[WIP] add RCV1 dataset loader ,,2810,0.7526690391459074,0.05585106382978723,43219,489.3218260487286,38.20079131863301,129.64205557740806,6251,52,1778,302,travis,TomDLT,ogrisel,false,ogrisel,11,0.7272727272727273,6,3,132,true,false,true,false,3,94,13,77,59,0,52
8196599,scikit-learn/scikit-learn,python,4904,1435548852,,1435593865,750,,unknown,false,false,false,30,1,1,2,3,0,5,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,7,8,7,8,8.638912593332233,0.18851619298646466,0,,sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/_hashing.pyx|sklearn/feature_extraction/tests/test_feature_hasher.py,0,0.0,0,3,false,Fix #4883 improved compatibility of FeatureHasher with dict-vectorizer Sorry i was supposed to update the tests but i messed up with the pull request Here is the original PRhttps://githubcom/scikit-learn/scikit-learn/pull/4903,,2808,0.7532051282051282,0.054474708171206226,43147,493.0354369944608,38.264537511298585,129.8583910816511,6236,52,1776,279,travis,ryadzenine,ryadzenine,true,,2,0.0,6,17,1910,false,false,false,false,0,2,4,0,0,0,11
8195730,scikit-learn/scikit-learn,python,4903,1435540641,,1435548540,131,,unknown,false,false,false,26,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,0,7,0,3.825345546684072,0.08347573511453332,0,,sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/_hashing.pyx,0,0.0,0,0,false,Fix #4883 Improve FeatureHasher compatibility with DictVectorizera This fix extend FeatureHasher to support string valued featuresThis is done by hashing every string value using murmurhash_bytes_s32,,2807,0.75347345920912,0.054333764553686936,43147,493.0354369944608,38.264537511298585,129.8583910816511,6236,52,1776,278,travis,ryadzenine,ryadzenine,true,,1,0.0,6,17,1910,false,false,false,false,0,1,2,0,0,0,11
8166608,scikit-learn/scikit-learn,python,4900,1435321951,1435603561,1435603561,4693,4693,commits_in_master,false,false,false,24,2,2,0,6,0,6,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,0,8,0,7.375167960802807,0.16093907324394016,1,olivier.grisel@ensta.org,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,1,0.0012453300124533001,0,3,true,FIX raises memory error in depth first builder When tree_add_node it might raise a memory error with node_id  SIZE_t(-1) This wasnt catch before,,2805,0.7536541889483066,0.06102117061021171,43147,493.0354369944608,38.264537511298585,129.8583910816511,6219,52,1774,278,travis,arjoly,larsmans,false,larsmans,84,0.8214285714285714,29,26,1284,true,true,true,true,16,125,7,85,23,0,4286
8149929,scikit-learn/scikit-learn,python,4897,1435238707,,1435299308,1010,,unknown,false,false,false,26,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,0,5,0,3.7202913246789238,0.0811832770242321,0,,sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/_hashing.pyx,0,0.0,0,0,false,Fix #4883 Improve FeatureHasher compatibility with DictVectorizer This fix extend FeatureHasher to support string valued featuresThis is done by hashing every string value using murmurhash_bytes_s32,,2804,0.753922967189729,0.06180469715698393,43146,493.04686413572523,38.26542437305891,129.86140082510545,6209,53,1773,277,travis,ryadzenine,ryadzenine,true,,0,0,6,17,1907,false,false,false,false,0,1,0,0,0,0,-1
8136705,scikit-learn/scikit-learn,python,4894,1435170870,1435192391,1435192391,358,358,commits_in_master,false,false,false,89,1,1,0,1,0,1,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,8,20,8,20,18.782400418673564,0.4098636967243937,29,t3kcit@gmail.com,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/tree/tree.py,20,0.018656716417910446,1,2,false,fix dtype transform problem in KNN and RandomForest Fix #4879I add a int array to store the indices before the loop @amueller I also checked LabelEncoder but it can only accommodate 1d array so a loop is still needed and slicing problem will still be there For MultiLabelBinarizer it can accommodate 2d array but it will return a 2d binary array to store the indices which is very different from the original implementation in RandomForest and KNN I am wondering is there any better way to do so ,,2803,0.7538351765965038,0.05970149253731343,43136,492.8366097922849,38.22793026706232,129.68286350148367,6205,53,1772,279,travis,tw991,agramfort,false,agramfort,4,0.5,11,29,282,false,false,false,false,2,4,6,5,0,0,54
8134180,scikit-learn/scikit-learn,python,4893,1435161881,1435279633,1435279633,1962,1962,commits_in_master,false,false,false,259,2,1,1,3,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,11,0,4.623223789430476,0.10088655075398262,3,t3kcit@gmail.com,sklearn/cross_decomposition/pls_.py,3,0.0037359900373599006,0,1,false,[MRG] CCA Stability This pull request should address #4888On a Windows machine with 64bit python the unit tests for CCA would fail because of the presence of NaN This issue was traced to the function _nipals_twoblocks_inner_loop Conceptually the function takes in a data matrix X and a multivariate response Y and tries to find a lower dimensional representation of both one component at a time However if earlier components can reconstruct the data well later components may have their weights shrunk to quantities below machine precision In the case of the unit test the values are ~5e-16 which can be represented on Ubuntu machines but not Windows machines where they would be rounded to 0 Practically this caused an issue because on line 47 and 60 of pls_py where the weights are normalized by dividing by some function of themselves Instead of explicitly raising an error numpy would fill in nan in those positions which would cause math issues for the remainder of the function but return successfully  It also wouldnt be caught in lines 300-302 because the numbers were not small they were nan It isnt until line 336-338 where the pseudoinverse is calculated that the array is checked to make sure no nan elements are present and since they exist the error above is raisedThis PR fixes the issue by adding an epsilon close to machine precision to the weights before they are divided so that there is never a divide by zero issue Unit tests run successfully on both Ubuntu and Windows 64bit machines ,,2802,0.7537473233404711,0.05977584059775841,43136,492.8366097922849,38.22793026706232,129.68286350148367,6204,53,1772,279,travis,jmschrei,agramfort,false,agramfort,0,0,35,5,826,false,false,false,false,1,0,0,0,0,0,4
8130509,scikit-learn/scikit-learn,python,4891,1435142679,1435154883,1435154883,203,203,commits_in_master,false,false,false,16,2,2,2,2,0,4,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,19,2,19,17.321269401316357,0.3779796892237632,16,vmehta94@gmail.com,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,16,0.019925280199252802,0,2,false,[MRG] [BUG] Pass penaly to the final logistic regression fit Fixes https://githubcom/scikit-learn/scikit-learn/issues/4880Soory about this :/,,2801,0.7536594073545162,0.05977584059775841,43135,492.8480352382056,38.22881650631737,129.6858699432016,6202,53,1772,278,travis,MechCoder,mblondel,false,mblondel,65,0.8615384615384616,85,41,1100,true,true,false,false,6,9,0,6,1,2,0
8107024,scikit-learn/scikit-learn,python,4887,1435013724,1442730792,1442730792,128617,128617,commits_in_master,false,false,false,8,6,4,0,14,0,14,0,3,6,0,6,17,11,0,0,6,2,9,17,11,0,0,3984,618,4348,643,101.08384841826428,2.2058222596418866,3,t3kcit@gmail.com,sklearn/manifold/setup.py|sklearn/src/cblas/ATL_srefnrm2.c|sklearn/src/cblas/cblas_snrm2.c|doc/modules/manifold.rst|examples/manifold/plot_tsne.py|examples/manifold/plot_tsne_faces.py|sklearn/manifold/_barnes_hut_tsne.c|sklearn/manifold/_barnes_hut_tsne.pyx|sklearn/manifold/_utils.c|sklearn/manifold/_utils.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/setup.py|sklearn/src/cblas/ATL_srefnrm2.c|sklearn/src/cblas/cblas_snrm2.c|doc/modules/manifold.rst|examples/manifold/plot_tsne.py|examples/manifold/plot_tsne_faces.py|sklearn/manifold/_barnes_hut_tsne.c|sklearn/manifold/_barnes_hut_tsne.pyx|sklearn/manifold/_utils.c|sklearn/manifold/_utils.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py,3,0.0,0,1,false,Rebase of barnes-hut TSNE Trying to rebase #4025,,2800,0.7535714285714286,0.06056860321384425,43135,492.8480352382056,38.22881650631737,129.6858699432016,6183,52,1770,343,travis,amueller,amueller,true,amueller,311,0.8488745980707395,1166,40,1704,true,true,false,false,145,1537,106,580,228,9,60538
8076292,scikit-learn/scikit-learn,python,4881,1434911041,1440959701,1440959701,100811,100811,commits_in_master,false,false,false,8,2,2,23,27,0,50,0,7,0,0,4,4,4,0,0,0,0,4,4,4,0,0,62,140,62,140,27.61874108342633,0.6026873674116567,16,zgstewart@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/linear_model/base.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_base.py,10,0.0049504950495049506,0,4,false,[WIP] add sample_weight into LinearRegression Working on #4735 ,,2799,0.7534833869239014,0.06064356435643564,43135,492.8480352382056,38.22881650631737,129.6858699432016,6165,53,1769,326,travis,SonnyHu,GaelVaroquaux,false,GaelVaroquaux,2,0.5,2,14,727,true,true,true,false,0,10,2,6,3,0,3
8053204,scikit-learn/scikit-learn,python,4872,1434644477,1434645540,1434645540,17,17,github,false,false,false,22,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.194466354272395,0.09153021759622568,11,t3kcit@gmail.com,sklearn/tests/test_common.py,11,0.0125,0,0,false,[MRG] remove referrence to HMM in common tests This remaining condition in test_common should now be useless Will merge if travis agrees,,2795,0.7542039355992844,0.06704545454545455,43135,492.8480352382056,38.22881650631737,129.6858699432016,6134,55,1766,276,travis,ogrisel,ogrisel,true,ogrisel,121,0.859504132231405,1165,124,2213,true,true,false,false,33,264,55,184,79,2,-1
8040908,scikit-learn/scikit-learn,python,4870,1434579849,1434583180,1434583180,55,55,commits_in_master,false,false,false,40,1,1,0,1,0,1,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,42,0,42,0,14.908768095986446,0.32533016634090156,0,,examples/ensemble/plot_adaboost_regression.py|examples/tree/plot_tree_regression.py|examples/tree/plot_tree_regression_multioutput.py,0,0.0,0,0,false,Change clf to regr in decision tree regression examples I just replaced the variables that used clf with regr in the Decision tree regression examples Its not really important but the wording classifier in the regression examples bugged me ),,2794,0.7541159627773801,0.06787330316742081,43135,492.8480352382056,38.22881650631737,129.6858699432016,6130,55,1765,276,travis,rasbt,amueller,false,amueller,2,1.0,1307,43,620,true,true,true,false,3,22,1,3,67,0,55
8023918,scikit-learn/scikit-learn,python,4868,1434497427,1434502119,1434502119,78,78,commits_in_master,false,false,false,19,2,1,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,20,0,4.337336956989864,0.09464668989755652,4,t3kcit@gmail.com,sklearn/metrics/cluster/unsupervised.py,4,0.004479283314669653,0,0,false,silhouette_score doscstring update Just a minor update of the silhouette_score docstring to clarify the purpose of the random_state parameter,,2793,0.7540279269602578,0.0683090705487122,43134,492.85946121389156,38.22970278666482,129.68887652431957,6123,56,1764,276,travis,rasbt,amueller,false,amueller,1,1.0,1306,43,619,true,true,true,false,3,21,0,3,67,0,1
7994275,scikit-learn/scikit-learn,python,4862,1434361794,1434610383,1434610383,4143,4143,commits_in_master,false,false,false,17,1,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,DOC links should not always point to dev I assume this was introduced in error in ca7b6df4,,2792,0.7539398280802292,0.06892778993435449,43134,492.85946121389156,38.22970278666482,129.68887652431957,6105,56,1763,277,travis,jnothman,jnothman,true,jnothman,113,0.6902654867256637,30,1,2239,true,true,false,false,12,215,19,84,19,2,498
7994025,scikit-learn/scikit-learn,python,4861,1434359890,1434361241,1434361241,22,22,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.509954274034798,0.09841338868692333,0,,examples/linear_model/plot_ols.py,0,0.0,0,0,false,Update plot_olspy (trivial) Minor/trivial NumPy slicing refactor,,2791,0.7538516660695092,0.06900328587075576,43135,492.8480352382056,38.22881650631737,129.6858699432016,6105,57,1763,279,travis,ltiao,agramfort,false,agramfort,1,0.0,9,8,1346,false,false,false,false,0,0,0,0,0,0,-1
7989196,scikit-learn/scikit-learn,python,4860,1434319009,1434941951,1434941951,10382,10382,commits_in_master,false,false,false,19,2,2,6,2,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,9.988810154131585,0.21796953948692147,2,t3kcit@gmail.com,sklearn/kernel_approximation.py|sklearn/kernel_approximation.py,2,0.002171552660152009,2,3,false,Correct the reference link for additive chi squared sampler A minor PRPlease take a look @GaelVaroquaux @agramfort :),,2790,0.7537634408602151,0.06840390879478828,43135,492.8480352382056,38.22881650631737,129.6858699432016,6103,57,1762,282,travis,rvraghav93,jnothman,false,jnothman,3,0.6666666666666666,25,42,618,true,false,false,false,2,45,5,45,30,0,146
7979066,scikit-learn/scikit-learn,python,4859,1434221769,1434361463,1434361463,2328,2328,commits_in_master,false,false,false,15,4,3,0,5,0,5,0,3,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,13.578623628098187,0.29630419373408423,6,najera.oscar@gmail.com,doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t,6,0.006329113924050633,0,0,false,Home page centering fixes the problem evidenced in the homepage centering as started in #4663 ,,2789,0.7536751523843671,0.06751054852320675,43135,492.8480352382056,38.22881650631737,129.6858699432016,6098,57,1761,278,travis,Titan-C,jnothman,false,jnothman,6,0.8333333333333334,7,3,1529,true,false,false,false,0,14,2,0,15,1,26
7973001,scikit-learn/scikit-learn,python,4857,1434158346,1434158554,1434158554,3,3,commits_in_master,false,false,false,6,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.240866440989064,0.09254151576566716,13,trev.stephens@gmail.com,sklearn/grid_search.py,13,0.013429752066115703,0,1,true,DOC document missing attributes in RandomizedSearchCV ,,2788,0.753586800573888,0.07024793388429752,43132,492.8823147547065,38.23147547064824,129.6948901047946,6093,58,1760,277,travis,christophebourguignat,agramfort,false,agramfort,0,0,12,0,416,false,false,false,false,0,0,0,0,0,0,3
7950408,scikit-learn/scikit-learn,python,4854,1434056954,1434057115,1434057115,2,2,commits_in_master,false,false,false,10,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.600698109807436,0.10059274712282003,8,t3kcit@gmail.com,doc/modules/model_evaluation.rst,8,0.008188331627430911,0,0,false,Fix typo in model evaluation documentation added a missing parenthesis,,2786,0.7537688442211056,0.07267144319344933,43078,492.7341102186731,38.25618645248154,129.7413993221598,6075,59,1759,279,travis,opahk,amueller,false,amueller,0,0,7,3,2133,false,false,false,false,0,0,0,0,0,0,2
7949691,scikit-learn/scikit-learn,python,4853,1434054394,1434360383,1434360383,5099,5099,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.599753397806105,0.10057209130642313,0,,examples/decomposition/plot_ica_vs_pca.py,0,0.0,0,0,false,[MRG] DOC remove linewidth in pca/ica example http://scikit-learnorg/016/auto_examples/decomposition/plot_ica_vs_pcahtmlvshttp://scikit-learnorg/dev/auto_examples/decomposition/plot_ica_vs_pcahtmlMaybe it was a change in matplotlib Im not sure,,2785,0.7536804308797127,0.07267144319344933,43078,492.7341102186731,38.25618645248154,129.7413993221598,6075,59,1759,282,travis,amueller,jnothman,false,jnothman,309,0.8511326860841424,1159,40,1693,true,true,false,false,146,1666,110,620,276,14,961
7947260,scikit-learn/scikit-learn,python,4852,1434046112,1442893171,1442893171,147450,147450,commits_in_master,false,true,false,106,2,2,88,45,0,133,0,5,3,0,9,12,8,0,1,3,0,9,12,8,0,1,1925,353,1925,353,67.8212602102151,1.4828894909087385,76,trev.stephens@gmail.com,benchmarks/bench_plot_nmf.py|sklearn/decomposition/cdnmf_fast.c|sklearn/decomposition/cdnmf_fast.pyx|sklearn/decomposition/nmf.py|sklearn/decomposition/setup.py|sklearn/decomposition/tests/test_nmf.py|.gitattributes|benchmarks/bench_plot_nmf.py|doc/modules/decomposition.rst|doc/whats_new.rst|examples/decomposition/plot_faces_decomposition.py|sklearn/decomposition/cdnmf_fast.c|sklearn/decomposition/cdnmf_fast.pyx|sklearn/decomposition/nmf.py|sklearn/decomposition/setup.py|sklearn/decomposition/tests/test_nmf.py|sklearn/utils/estimator_checks.py,43,0.0020304568527918783,1,21,false,[WIP] refactor NMF and add CD solver This PR is a first part of what is discussed in #4811It includes:- refactor ProjectedGradientNMF into NMF with a proj-grad solver- include random initialization into _initialize_nmf()- add ElasticNet-like regularization (with parameters alpha and l1_ratio)- add a solver with coordinate descent (from @mblondel) with Cython for critical part- update some tests and bench_plot_nmf (which looks terrible btw)In some future PR I will:- include MM (cf [comment](https://githubcom/scikit-learn/scikit-learn/issues/4811#issuecomment-110391396)) in order to include more loss functions (I-divergence and IS-divergence)- investigate more on initialization of NMF- Please tell me what you think :),,2784,0.7535919540229885,0.07208121827411168,43078,492.7341102186731,38.25618645248154,129.7413993221598,6073,59,1759,347,travis,TomDLT,amueller,false,amueller,10,0.7,6,3,113,true,false,false,false,3,77,12,30,57,0,58
7947028,scikit-learn/scikit-learn,python,4851,1434045470,1434998289,1434998289,15880,15880,commits_in_master,false,false,false,12,2,2,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,40,0,40,0,8.531001460170549,0.18652753388838697,12,trev.stephens@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py,12,0.012182741116751269,1,5,false,Ridge - dont set sample_weight if no class_weight provided @mblondel fixes #4846,,2783,0.753503413582465,0.07208121827411168,43078,492.7341102186731,38.25618645248154,129.7413993221598,6073,59,1759,282,travis,trevorstephens,mblondel,false,mblondel,16,0.8125,128,51,668,true,true,true,false,2,67,8,13,6,0,16
7944964,scikit-learn/scikit-learn,python,4849,1434038107,1434393773,1434393773,5927,5927,commits_in_master,false,false,false,51,2,0,3,3,0,6,0,4,0,0,0,2,0,0,0,0,0,2,2,0,1,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,DOC make contributors guide etc more prominent on front page I find it strange that some parts of the documentation need you to click the Documentation menu item despite it having a drop-down This is most noticeable for contributors guide but Ive played with some other front page links as well,,2782,0.7534148094895758,0.0708502024291498,43074,492.4780610112829,38.236523192645215,129.70701583321724,6072,59,1759,281,travis,jnothman,amueller,false,amueller,112,0.6875,30,1,2235,true,true,false,false,11,193,13,71,13,0,148
7936528,scikit-learn/scikit-learn,python,4845,1433984803,,1441821625,130613,,unknown,false,true,false,58,1,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.31942637585279,0.09444194608529774,13,xuewei4d@gmail.com,sklearn/mixture/gmm.py,13,0.013078470824949699,0,3,false,Added error messages in case user provides one dimensional data When trying to learn a GMM on one dimensional data both weights and the data can be represented as 1-d vectors  However this breaks GMMfit() and it used to die with cryptic error messages  This addition specifically checks for one dimensional input and issues an error message accordingly,,2779,0.7542281396185678,0.07243460764587525,43074,492.24590240052004,38.21330733156893,129.66058411106468,6061,59,1758,338,travis,Carldeboer,amueller,false,,0,0,0,1,370,false,false,false,false,0,0,0,0,0,0,7
7907823,scikit-learn/scikit-learn,python,4840,1433844322,1435194530,1435194530,22503,22503,commits_in_master,false,false,false,50,3,0,12,11,0,23,0,5,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,116,26,0,0.0,0,,,0,0.0,0,3,false,[MRG] FIX avoid memory cost when sampling from large parameter grids Solve the issue reported at https://githubcom/scikit-learn/scikit-learn/pull/3850#issuecomment-110238740 by making ParameterGrid able to dynamically look up a param setting by index ie list of all possible settings is never realisedNeeds:* [x] test* [x] docstring* [x] clearer code,,2775,0.7549549549549549,0.06896551724137931,43072,492.75631500742946,38.19186478454681,129.50408618127787,6034,62,1757,288,travis,jnothman,jnothman,true,jnothman,111,0.6846846846846847,30,1,2233,true,true,false,false,11,181,11,54,10,0,134
7905240,scikit-learn/scikit-learn,python,4838,1433835485,1433874170,1433874170,644,644,commits_in_master,false,false,false,43,1,1,0,5,0,5,0,7,0,0,3,3,2,0,0,0,0,3,3,2,0,0,16,29,16,29,13.52671586936939,0.2957528668406369,46,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,40,0.007670182166826462,0,1,false,[MRG] Add sample_weight support to RidgeClassifier Added sample_weight support to RidgeClassifier as it was in RidgeClassifierCV but not the non-CV implementation Also added some tests to both of the above to check it reacts as expected when compared to class_weight from the constructor,,2774,0.7548666186012978,0.06903163950143816,43072,492.3384101040119,38.16864784546806,129.45765230312037,6033,61,1757,274,travis,trevorstephens,amueller,false,amueller,15,0.8,128,51,666,true,true,true,false,1,71,7,9,6,0,224
7901670,scikit-learn/scikit-learn,python,4836,1433810903,1435793595,1435793595,33044,33044,commits_in_master,false,false,false,12,2,1,3,2,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,16,0,4.302897558960683,0.0940800635626516,2,t3kcit@gmail.com,sklearn/decomposition/pca.py,2,0.0019065776930409914,0,1,false,[MRG] Doc Better docstring for PCA Closes #3322 I didnt include loadings,,2773,0.7547782185358817,0.06863679694947569,43072,492.3384101040119,38.16864784546806,129.45765230312037,6027,61,1756,290,travis,amueller,amueller,true,amueller,308,0.8506493506493507,1157,40,1690,true,true,false,false,153,1721,120,622,291,12,590
7900809,scikit-learn/scikit-learn,python,4835,1433807412,1433952698,1433952698,2421,2421,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,10,6,10,8.89219418929947,0.19442205701564544,23,vmehta94@gmail.com,sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,17,0.01623686723973257,0,1,false,fix SVM can be tricked into running proba() #4791 ,,2772,0.7546897546897547,0.06876790830945559,43072,492.3384101040119,38.16864784546806,129.45765230312037,6027,61,1756,275,travis,tw991,agramfort,false,agramfort,1,1.0,11,29,266,false,false,false,false,1,0,3,0,0,0,13
7897831,scikit-learn/scikit-learn,python,4833,1433795682,1433795699,1433795699,0,0,commits_in_master,false,false,false,20,2,1,2,4,0,6,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.666855939101004,0.10203777742141047,6,thomas.unterthiner@gmx.net,doc/modules/preprocessing.rst,6,0.005714285714285714,0,1,false,[MRG] add polynomial features to user guide But still trying to fix my LaTeX to render the right html file,,2771,0.754601226993865,0.06857142857142857,43072,492.3384101040119,38.16864784546806,129.45765230312037,6026,61,1756,276,travis,tw991,tw991,true,tw991,0,0,11,29,266,false,false,false,false,0,0,0,0,0,0,1
7894776,scikit-learn/scikit-learn,python,4832,1433784216,1433793354,1433793354,152,152,commits_in_master,false,false,false,92,7,5,5,5,0,10,0,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,27.254929337654694,0.5959095765261709,13,vmehta94@gmail.com,doc/developers/index.rst|doc/developers/utilities.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/utilities.rst,11,0.010446343779677113,0,0,false,Fixed typos and formatting added links to doc/developers/index and /utilities I was reading through the developer guides and saw a few formatting/grammar errors I figured it would be easiest to fix them right away to avoid having to remember their location I also added a link to the networkx package referenced in the utilities pageI built the pages and checked them in my browser they seem to be working correctly I ran make on the whole package and everything passed nicelyFirst PR let me know if you have any feedback ,,2770,0.7545126353790613,0.06742640075973409,43063,507.62835845157093,38.94294405870469,134.45417179481225,6022,61,1756,273,travis,kronosapiens,amueller,false,amueller,0,0,20,27,1083,true,false,true,false,0,0,0,0,1,0,44
7889997,scikit-learn/scikit-learn,python,4831,1433760881,1433774002,1433774002,218,218,commit_sha_in_comments,false,false,false,24,4,1,1,4,0,5,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,4,14,4.534095037634935,0.09913525750694166,9,t3kcit@gmail.com,sklearn/ensemble/gradient_boosting.py,9,0.008514664143803218,0,2,false,Fix overflow warning in GradientBoosting Use expit function to compute the probability in ExponentialLoss class in GB This PR may solve issues like #4152,,2769,0.7544239797760924,0.06717123935666983,43046,507.48036983691867,38.95832365376574,134.3446545555917,6015,61,1756,276,travis,pjknkda,pjknkda,true,pjknkda,0,0,4,1,697,false,false,false,false,0,0,0,0,1,0,12
7882057,scikit-learn/scikit-learn,python,4829,1433689796,1433774703,1433774703,1415,1415,commits_in_master,false,false,false,20,2,2,0,11,0,11,0,4,0,0,11,11,11,0,0,0,0,11,11,11,0,0,74,24,74,24,99.59469127182916,2.1775779474413604,71,xuewei4d@gmail.com,sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/tests/test_cross_validation.py|sklearn/calibration.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/tests/test_cross_validation.py,18,0.005612722170252572,0,5,false,MAINT merge _check_cv into check_cv as indices argument is removed in 017 As a follow up to #4370 Ref: https://githubcom/scikit-learn/scikit-learn/pull/4370/files#diff-d1a79b4a1b5f91f6b6c1829b212fca53L1108,,2768,0.7543352601156069,0.06828811973807297,43046,507.48036983691867,38.95832365376574,134.3446545555917,6008,60,1755,275,travis,rvraghav93,agramfort,false,agramfort,1,1.0,24,42,611,true,false,false,false,2,15,3,30,25,0,602
7880759,scikit-learn/scikit-learn,python,4828,1433674526,1434061027,1434061027,6441,6441,commits_in_master,false,false,false,59,6,4,25,6,0,31,0,6,0,0,5,5,3,0,0,0,0,5,5,3,0,0,709,214,730,227,78.0180583153611,1.7058178615769124,71,trev.stephens@gmail.com,doc/modules/preprocessing.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,40,0.014084507042253521,0,2,false,ENH Add MaxAbsScaler This PR adds the MaxAbsScaler and maxabs_scale to sklearnpreprocessing This scaler scales its inputs by the maximum absolute value of each feature This scaler is especially useful for sparse data but is probably also always a better alternative to MinMaxScaler when the data is already centeredThe scaler itself was previously discussed in #1799 and #2514,,2767,0.7542464763281532,0.06854460093896714,43046,507.48036983691867,38.95832365376574,134.3446545555917,6007,60,1755,285,travis,untom,jnothman,false,jnothman,12,0.6666666666666666,10,0,839,true,false,false,false,0,11,1,14,2,0,1
7867092,scikit-learn/scikit-learn,python,4827,1433544740,,1433766793,3700,,unknown,false,false,false,21,9,4,9,5,0,14,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1397,798,2785,1533,36.43717320769558,0.7966763347806823,6,t3kcit@gmail.com,sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py,4,0.0036496350364963502,0,1,true,Issue 4441 Im only getting single lines in the compare view now for     https://githubcom/scikit-learn/scikit-learn/compare/masterlbillingham:issue_4441 hopefully line endings are now actually posix-y,,2766,0.7545191612436731,0.07208029197080291,43046,507.48036983691867,38.95832365376574,134.3446545555917,5986,60,1753,276,travis,lbillingham,lbillingham,true,,1,0.0,0,0,135,false,false,false,false,0,4,1,0,0,0,16
7866377,scikit-learn/scikit-learn,python,4826,1433541698,1445341725,1445341725,196667,196667,commits_in_master,false,true,false,62,9,4,58,58,0,116,0,7,1,0,53,56,51,0,0,3,0,55,58,53,0,0,619,53,874,58,311.3189280698952,6.80679648634743,161,zgstewart@gmail.com,sklearn/cross_validation.py|sklearn/exceptions.py|sklearn/grid_search.py|sklearn/utils/validation.py|sklearn/__init__.py|sklearn/cross_validation.py|sklearn/exceptions.py|sklearn/grid_search.py|sklearn/utils/validation.py|doc/developers/performance.rst|sklearn/__init__.py|sklearn/cross_validation.py|sklearn/exceptions.py|sklearn/grid_search.py|sklearn/utils/extmath.py|sklearn/utils/validation.py|doc/developers/performance.rst|doc/developers/utilities.rst|doc/modules/classes.rst|examples/linear_model/plot_sparse_recovery.py|sklearn/__init__.py|sklearn/base.py|sklearn/cluster/birch.py|sklearn/cluster/tests/test_k_means.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/cross_validation.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_factor_analysis.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/exceptions.py|sklearn/feature_selection/from_model.py|sklearn/grid_search.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/sag.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_theil_sen.py|sklearn/linear_model/theil_sen.py|sklearn/metrics/base.py|sklearn/metrics/classification.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_ranking.py|sklearn/neighbors/base.py|sklearn/preprocessing/tests/test_data.py|sklearn/random_projection.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_random_projection.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/utils/__init__.py|sklearn/utils/estimator_checks.py|sklearn/utils/extmath.py|sklearn/utils/validation.py,24,0.0036496350364963502,6,36,true,MAINT move custom error/warning classes into sklearnexceptions Hijacks into @larsmans #4309 (@larsmans apologies for proceeding without ur reply I thought this would be nice to have while fixing #2904 since 2 / 5 classes were in cross_validationpy and grid_searchpy)BTW I left out the utilsarpackArpackErrorall of most of those classes or warnings and utilsteststest_estimator_checksCorrectNotFittedError untouchedPlease review @amueller @larsmans @ogrisel @agramfort :),,2765,0.7544303797468355,0.07208029197080291,43046,507.48036983691867,38.95832365376574,134.3446545555917,5984,60,1753,371,travis,rvraghav93,GaelVaroquaux,false,GaelVaroquaux,0,0,24,42,609,true,false,false,false,2,10,2,17,19,0,1245
7865384,scikit-learn/scikit-learn,python,4824,1433537540,1433863822,1433863822,5438,5438,commits_in_master,false,false,false,42,1,1,0,2,6,8,0,2,0,0,6,6,6,0,0,0,0,6,6,6,0,0,8,40,8,40,27.512913662799875,0.6015529001406581,36,vmehta94@gmail.com,sklearn/ensemble/tests/test_forest.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py,24,0.006398537477148081,0,0,true,[MRG] minor fixes to the tests dont raise as many warnings in the test suite Also changes more docstrings in tests to comments so that tests are easier to find and avoids taking mean of an empty sequence in the clustering metrics,,2764,0.7543415340086831,0.07221206581352833,43046,507.48036983691867,38.95832365376574,134.3446545555917,5983,60,1753,277,travis,amueller,jnothman,false,jnothman,307,0.8501628664495114,1156,40,1687,true,true,false,false,155,1729,120,612,299,12,2990
7856790,scikit-learn/scikit-learn,python,4821,1433498139,,1433545970,797,,unknown,false,false,false,44,2,2,0,6,0,6,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1389,762,1389,762,18.53349807445831,0.4052235608678174,6,t3kcit@gmail.com,sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py,4,0.0036429872495446266,0,0,true,Issue 4441 Protection from tiny eigenvalues in PCA(mle) Ive run into log(0) errors on data from the wild I havent been able to construct a synthetic pathological X Since  _assess_dimension_ and _infer_dimension_ are imported in the tests I just test using a pathological spectrum,,2761,0.7551611734878667,0.07285974499089254,43044,507.50394944707745,38.9601338165598,134.35089675680697,5977,60,1753,274,travis,lbillingham,amueller,false,,0,0,0,0,135,false,false,false,false,0,2,0,0,0,0,617
7851313,scikit-learn/scikit-learn,python,4818,1433461942,1433499956,1433499956,633,633,commits_in_master,false,false,false,30,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.270123220587028,0.09336362352314334,10,tom.dupre-la-tour@m4x.org,sklearn/linear_model/stochastic_gradient.py,10,0.009025270758122744,0,0,false,[MRG] Fix deprecation of decision function in SGD Fixes some left-overs from #4418The predict in SGDRegressor and PassiveAgressiveRegressor raise deprecation warnings as they were using the deprecated public decision_function,,2759,0.7553461399057629,0.07490974729241877,43044,507.50394944707745,38.9601338165598,134.35089675680697,5970,62,1752,269,travis,amueller,agramfort,false,agramfort,306,0.8496732026143791,1156,40,1686,true,true,true,false,155,1719,122,608,299,12,-1
7827468,scikit-learn/scikit-learn,python,4810,1433352509,1433458970,1433458970,1774,1774,commits_in_master,false,false,false,6,3,2,7,3,0,10,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.005937311728635,0.19690729130921727,1,olivier.grisel@ensta.org,doc/about.rst|doc/about.rst,1,0.0008873114463176575,0,0,false,BibTeX entry for the API paper ,,2758,0.7552574329224075,0.07542147293700088,43044,507.50394944707745,38.9601338165598,134.35089675680697,5948,62,1751,267,travis,mblondel,agramfort,false,agramfort,34,0.6470588235294118,471,32,1891,true,true,true,true,5,61,2,23,5,4,106
7796901,scikit-learn/scikit-learn,python,4798,1433200181,1438643394,1438643394,90720,90720,merged_in_comments,false,true,false,21,6,2,33,21,0,54,0,10,3,0,4,9,5,0,0,3,0,6,9,8,0,0,234,143,310,193,58.84622924791842,1.2867899372450453,29,vmehta94@gmail.com,doc/modules/classes.rst|doc/modules/preprocessing.rst|examples/preprocessing/plot_callable_transformer.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/callable_transformer.py|sklearn/preprocessing/tests/test_callable_transformer.py|doc/modules/classes.rst|doc/modules/preprocessing.rst|examples/preprocessing/plot_callable_transformer.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/callable_transformer.py|sklearn/preprocessing/tests/test_callable_transformer.py|sklearn/utils/estimator_checks.py,19,0.0017226528854435831,0,8,false,ENH: Adds CallableTransformer CallableTransformer allows a user to convert a standard python callableinto a transformer for use in a Pipeline,,2754,0.7559912854030502,0.07321274763135228,42881,507.5208134138662,39.08490940043376,133.88213894265525,5921,62,1749,320,travis,llllllllll,amueller,false,amueller,0,0,69,42,896,false,false,true,false,0,0,0,0,1,0,78
7795855,scikit-learn/scikit-learn,python,4797,1433196320,1434039921,1434039921,14060,14060,commits_in_master,false,false,false,10,17,2,7,4,0,11,0,3,0,0,2,3,2,0,0,0,0,3,3,3,0,0,20,44,20,191,16.593669743255624,0.3628533560783892,16,t3kcit@gmail.com,sklearn/base.py|sklearn/tests/test_pipeline.py|sklearn/base.py|sklearn/tests/test_pipeline.py,9,0.007778738115816767,0,1,false,Document how to get valid parameters in set_params Issue #4690,,2753,0.7559026516527425,0.07346585998271392,42881,507.3575709521699,39.061589048762855,133.81217788764255,5920,62,1749,283,travis,joshloyal,jnothman,false,jnothman,1,0.0,1,0,1355,true,false,false,false,1,0,1,0,1,0,2
7777660,scikit-learn/scikit-learn,python,4794,1433084791,,1436733938,60819,,unknown,false,false,false,11,1,1,0,5,0,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,58,14,58,14,8.73041752533677,0.19090652133060876,21,vmehta94@gmail.com,sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,14,0.011864406779661017,0,0,false,Change undocumented attributes to be private in svc Fix issue #4687,,2751,0.7564521992002908,0.07203389830508475,42878,508.83903167125334,38.994356080041044,133.70493026726993,5898,61,1748,312,travis,SonnyHu,SonnyHu,true,,1,1.0,2,14,706,true,true,false,false,0,4,1,6,1,0,436
7771655,scikit-learn/scikit-learn,python,4792,1433019914,1433023018,1433023018,51,51,commits_in_master,false,false,false,32,2,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.708147569790325,0.10295227300314558,4,loic.esteve@ymail.com,README.rst,4,0.0033984706881903144,0,0,false,[MRG] DOC Add coveralls badge to Readme Apart from showing off our coverage this creates a link to coveralls which gives a nice entry point to people who want to improve coverage,,2750,0.7563636363636363,0.07221750212404418,42878,508.83903167125334,38.994356080041044,133.70493026726993,5887,61,1747,267,travis,amueller,agramfort,false,agramfort,305,0.8491803278688524,1153,40,1681,true,true,true,false,156,1720,130,610,317,13,4
7746807,scikit-learn/scikit-learn,python,4786,1432855090,1433199419,1433199419,5738,5738,commits_in_master,false,false,false,31,2,1,0,4,0,4,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,22,8,43,8.812826552327085,0.19270859965524711,15,xuewei4d@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,13,0.011216566005176877,0,0,false,[MRG] Fix RFE / RFECV estimator tags Pretty bad regression from introducing _estimator_tags I forgot to define them for RFE which made accuracies on iris be 0 using default cross-validation ouch,,2745,0.7573770491803279,0.07333908541846419,42878,508.83903167125334,38.994356080041044,133.70493026726993,5865,60,1745,269,travis,amueller,amueller,true,amueller,304,0.8486842105263158,1153,40,1679,true,true,false,false,156,1713,129,610,316,13,350
7745636,scikit-learn/scikit-learn,python,4785,1432850076,1433189103,1433189103,5650,5650,commits_in_master,false,false,false,11,1,1,0,1,0,1,0,2,0,0,32,32,32,0,0,0,0,32,32,32,0,0,0,171,0,171,144.62519633775352,3.162494903948185,69,xuewei4d@gmail.com,sklearn/cluster/tests/test_hierarchical.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/tests/test_pca.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/feature_extraction/tests/test_image.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_ransac.py|sklearn/manifold/tests/test_isomap.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_dist_metrics.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/neighbors/tests/test_kde.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_dummy.py|sklearn/tests/test_isotonic.py|sklearn/tests/test_multiclass.py|sklearn/tree/tests/test_export.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_random.py|sklearn/utils/tests/test_shortest_path.py,12,0.001725625539257981,1,1,false,[MRG] TST/COSMIT remove nose call boilerplate slight cleanupping @ogrisel ),,2744,0.7572886297376094,0.07333908541846419,42878,508.83903167125334,38.994356080041044,133.70493026726993,5865,60,1745,268,travis,amueller,amueller,true,amueller,303,0.8481848184818482,1153,40,1679,true,true,false,false,156,1711,128,610,316,13,5641
7734792,scikit-learn/scikit-learn,python,4783,1432806834,1436647930,1436647930,64018,64018,commits_in_master,false,false,false,37,5,1,15,21,0,36,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,99,0,4.269702735593057,0.09326519342830987,11,t3kcit@gmail.com,sklearn/ensemble/forest.py,11,0.009490940465918895,0,4,false,RandomForest decrease allocated memory when using bootstrap option (Fix issue#4774) Fix issue https://githubcom/scikit-learn/scikit-learn/issues/4774RandomForest was allocating a lot of memory for calculate oob_score Remove member-valuable what sample is using for bootstrap and generate there when it referenced,,2743,0.7572001458257382,0.0724762726488352,42877,508.85089908342474,38.9952655269725,133.70804860414677,5857,60,1745,306,travis,tokoroten,amueller,false,amueller,0,0,26,23,1392,false,false,false,false,1,2,0,0,0,0,577
7724985,scikit-learn/scikit-learn,python,4779,1432756335,1432757013,1432757013,11,11,commits_in_master,false,true,false,76,5,1,6,36,0,42,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,128,11,330,15,9.244560454666926,0.20193348809466838,2,ragvrv@gmail.com,sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py,2,0.0017094017094017094,0,13,false,Implemented parallelised version of mean_shift and test function The main iterative loop which is run for every seed was put in a separate external function (this is basically required by multiprocessingPoolmap()) A method was added to the main class fit_parallel which allows for parallel execution of the algorithm on the many seeds but everything else is the same It is fully back compatible and the changes are null unless the user calls explicitly the fit_parallel method,,2742,0.7571115973741794,0.07264957264957266,42874,508.88650464150766,38.997994122311894,133.71740448756822,5849,60,1744,333,travis,martinosorb,martinosorb,true,martinosorb,0,0,1,0,85,false,false,false,false,0,0,0,0,0,0,2
7717211,scikit-learn/scikit-learn,python,4773,1432724836,,1432752356,458,,unknown,false,true,false,45,2,1,8,6,0,14,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,9,0,4.3558347861665325,0.09514683065475867,4,t3kcit@gmail.com,sklearn/decomposition/dict_learning.py,4,0.003430531732418525,0,6,false,Multiprocessing fix for dictionary elarning main loop copying design matrix to avoid lasso_cd error on large dataset Setting n_jobs  1 during online dictionary learning processCopying X before Lassofit call for lasso_cd algorithm to avoid buffer source-array read-only errorFixing issues : #4772 #4769,,2738,0.758217677136596,0.07204116638078903,42883,508.8030221766201,38.98980948161276,133.7126600284495,5844,59,1744,262,travis,arthurmensch,arthurmensch,true,,0,0,1,1,781,false,true,false,false,1,3,0,0,0,0,2
7703813,scikit-learn/scikit-learn,python,4770,1432638361,1432743127,1432743127,1746,1746,commits_in_master,false,false,false,15,2,0,2,6,0,8,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,152,110,0,0.0,0,,,0,0.0,0,1,false,[MRG+1] improve parameter check in LogisticRegression I factorized solver parameters checking and added some tests,,2737,0.7581293386919985,0.07257383966244725,42756,509.6828515296099,39.03545701188138,133.94611282627,5832,60,1743,263,travis,TomDLT,ogrisel,false,ogrisel,7,0.8571428571428571,6,3,97,true,false,true,false,2,43,9,18,32,0,210
7684993,scikit-learn/scikit-learn,python,4767,1432503337,1440959153,1440959153,140930,140930,commits_in_master,false,false,false,59,5,4,18,14,0,32,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,116,288,132,304,45.19008923778643,0.989148880724818,53,trev.stephens@gmail.com,sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/tests/test_passive_aggressive.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/tests/test_passive_aggressive.py|doc/whats_new.rst|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/tests/test_passive_aggressive.py|doc/whats_new.rst|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/tests/test_passive_aggressive.py,51,0.0032921810699588477,0,5,false,[MRG] Add class_weight to PA Classifier remove from PA Regressor Was browsing Landscapeio and noticed this strange one class_weight is a zombie param for the PassiveAggressiveRegressor not present for the PassiveAggressiveClassifier O_oRemoved from regressor didnt think deprecation is necessary since it didnt go anywhere and makes no sense either and implemented it for the classifier with some tests,,2736,0.7580409356725146,0.07078189300411522,42751,509.7424621646277,39.04002245561507,133.96177867184394,5812,62,1741,336,travis,trevorstephens,GaelVaroquaux,false,GaelVaroquaux,14,0.7857142857142857,127,51,650,true,true,true,false,2,71,6,5,5,0,4216
7683458,scikit-learn/scikit-learn,python,4765,1432490975,1432651586,1432651586,2676,2676,commits_in_master,false,false,false,302,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,4.66554386353561,0.10212233630045485,0,,examples/text/document_clustering.py,0,0.0,0,0,false,Printing top terms when using LSI in examples/text/document_clusteringpy Now when LSI is used it doesnt print the top 10 terms per cluster However we can bring the centroids from the semantic space back to the original space via svdinverse_transform and then find the top terms there For example here are the top terms for 20 clusters (for all categories in 20 newsgroups)    Cluster 0:  space  shuttle  alaska  edu  nasa  moon  launch  orbit  henry  sci    Cluster 1:  edu  game  team  games  year  ca  university  players  hockey  baseball    Cluster 2:  sale  00  edu  10  offer  new  distribution  subject  lines  shipping    Cluster 3:  israel  israeli  jews  arab  jewish  arabs  edu  jake  peace  israelis    Cluster 4:  cmu  andrew  org  com  stratus  edu  mellon  carnegie  pittsburgh  pa    Cluster 5:  god  jesus  christian  bible  church  christ  christians  people  edu  believe    Cluster 6:  drive  scsi  card  edu  mac  disk  ide  bus  pc  apple    Cluster 7:  com  ca  hp  subject  edu  lines  organization  writes  article  like    Cluster 8:  car  cars  com  edu  engine  ford  new  dealer  just  oil    Cluster 9:  sun  monitor  com  video  edu  vga  east  card  monitors  microsystems    Cluster 10:  nasa  gov  jpl  larc  gsfc  jsc  center  fnal  article  writes    Cluster 11:  windows  dos  file  edu  ms  files  program  os  com  use    Cluster 12:  netcom  com  edu  cramer  fbi  sandvik  408  writes  article  people    Cluster 13:  armenian  turkish  armenians  armenia  serdar  argic  turks  turkey  genocide  soviet    Cluster 14:  uiuc  cso  edu  illinois  urbana  uxa  university  writes  news  cobb    Cluster 15:  edu  cs  university  posting  host  nntp  state  subject  organization  lines    Cluster 16:  uk  ac  window  mit  server  lines  subject  university  com  edu    Cluster 17:  caltech  edu  keith  gatech  technology  institute  prism  morality  sgi  livesey    Cluster 18:  key  clipper  chip  encryption  com  keys  escrow  government  algorithm  des    Cluster 19:  people  edu  gun  com  government  don  like  think  just  access,,2735,0.7579524680073126,0.07031888798037612,42751,509.7424621646277,39.04002245561507,133.96177867184394,5810,62,1741,261,travis,alexeygrigorev,ogrisel,false,ogrisel,0,0,7,1,1429,false,false,false,false,0,0,0,0,0,0,1228
7677804,scikit-learn/scikit-learn,python,4763,1432425294,1433850890,1433850890,23759,23759,commits_in_master,false,false,false,3,5,2,12,13,0,25,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,28,10,56,12.986076025100616,0.2842473378984827,7,ragvrv@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,5,0.004061738424045491,0,2,false,Fix issue #4755 ,,2734,0.7578639356254572,0.06986190089358245,42751,509.7424621646277,39.04002245561507,133.96177867184394,5796,62,1740,285,travis,SonnyHu,agramfort,false,agramfort,0,0,2,14,698,true,true,false,false,0,2,0,0,1,0,4424
7677085,scikit-learn/scikit-learn,python,4762,1432419709,1432420559,1432420559,14,14,commits_in_master,false,false,false,24,1,1,0,1,1,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.448598906602953,0.09737371406263862,5,t3kcit@gmail.com,sklearn/grid_search.py,5,0.0040683482506102524,0,0,false,Unused params in GridSearchCV Looks like the zombie parameters loss_func & score_func should have been removed in #3411  Deprecated for 015 it seems,,2733,0.7577753384559093,0.06997558991049634,42751,509.7424621646277,39.04002245561507,133.96177867184394,5796,61,1740,258,travis,trevorstephens,agramfort,false,agramfort,13,0.7692307692307693,127,51,649,true,true,true,false,2,70,5,5,4,0,13
7673966,scikit-learn/scikit-learn,python,4761,1432391851,1432756217,1432756217,6072,6072,commits_in_master,false,false,false,16,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.1168680528644535,0.09011256345702137,2,tom.dupre-la-tour@m4x.org,sklearn/manifold/locally_linear.py,2,0.0016299918500407497,0,0,false,Added the regularization term in the method _fit_transform Added the regularization term in the method _fit_transform,,2732,0.7576866764275256,0.07008964955175224,42752,509.7305389221557,39.03910928143713,133.95864520958085,5791,61,1740,263,travis,tiagofrepereira2012,amueller,false,amueller,0,0,3,0,1101,false,false,false,false,0,1,0,0,0,0,138
7642409,scikit-learn/scikit-learn,python,4754,1432216505,1432220497,1432220497,66,66,commits_in_master,false,false,false,20,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,3.8806069123394495,0.08494102624317258,0,,sklearn/preprocessing/imputation.py,0,0.0,0,0,false,[MRG] use astype to avoid unnecessary copy use astype to avoid unnecessary copyAdditional to PR #4645 for issue #4573,,2731,0.757597949469059,0.06930693069306931,42778,508.7428117256534,38.945252232455935,133.57333208658656,5766,60,1738,265,travis,TomDLT,ogrisel,false,ogrisel,6,0.8333333333333334,6,3,92,true,false,true,false,2,38,8,18,28,0,32
7642303,scikit-learn/scikit-learn,python,4753,1432216020,1432216737,1432216737,11,11,commits_in_master,false,false,false,12,1,1,0,0,0,0,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,9.545513398874661,0.2089378600909538,56,vmehta94@gmail.com,doc/whats_new.rst|sklearn/utils/optimize.py,52,0.0429042904290429,0,0,false,[MRG] DOC update whats_new for PR #4679 update whats_new for PR #4679,,2730,0.7575091575091575,0.06930693069306931,42778,508.7428117256534,38.945252232455935,133.57333208658656,5766,60,1738,265,travis,TomDLT,agramfort,false,agramfort,5,0.8,6,3,92,true,false,true,true,2,38,7,18,28,0,-1
7631845,scikit-learn/scikit-learn,python,4751,1432161018,1432306519,1432306519,2425,2425,commits_in_master,false,false,false,16,1,1,0,3,0,3,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,18,31,18,31,17.770278407746282,0.38897052003222604,15,tom.dupre-la-tour@m4x.org,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py,9,0.006661115736885929,0,0,false,[MRG] FIX n_jobs slicing bug in dict learning Also add input validation to gen_even_slicesFixes #4746,,2729,0.757420300476365,0.06994171523730225,42772,508.7674179369681,38.950715421303656,133.59206957822875,5753,61,1737,265,travis,amueller,ogrisel,false,ogrisel,302,0.847682119205298,1152,40,1671,true,true,true,false,171,1779,137,609,331,12,703
7626195,scikit-learn/scikit-learn,python,4749,1432141921,1432157323,1432157323,256,256,commits_in_master,false,false,false,47,1,1,0,1,0,1,0,2,0,0,2,2,1,0,1,0,0,2,2,1,0,1,6,0,6,0,9.887744290414116,0.21642678438573512,0,,.travis.yml|continuous_integration/install.sh,0,0.0,0,0,false,[MRG] use the new container-based travis workers This is an attempt to make it possible for the scikit-learn project to use the new container-based travis workers Those workers run on EC2 and should be provisioned elastically meaning that queue time should be reducedMore details here:http://blogtravis-cicom/2015-03-31-docker-default-on-the-way/,,2728,0.7573313782991202,0.06982543640897755,42769,508.42900231476074,38.93006616942178,133.62482171666392,5750,61,1737,267,travis,ogrisel,amueller,false,amueller,120,0.8583333333333333,1154,124,2184,true,true,false,true,43,385,78,213,142,5,8
7625066,scikit-learn/scikit-learn,python,4748,1432137715,1432218399,1432218399,1344,1344,commits_in_master,false,false,false,8,2,2,5,4,0,9,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,16,24,16,24,17.509679143025423,0.38325865245370877,13,vmehta94@gmail.com,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,7,0.005838198498748957,0,0,false,FIX check parameter in LogisticRegression Solve issue #4747,,2727,0.7572423909057573,0.07005838198498748,42769,508.42900231476074,38.93006616942178,133.62482171666392,5750,61,1737,268,travis,TomDLT,ogrisel,false,ogrisel,4,0.75,6,3,91,true,false,true,false,1,32,6,17,23,0,32
7622749,scikit-learn/scikit-learn,python,4745,1432127389,,1451763211,327263,,unknown,false,false,false,9,1,1,0,7,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.5982833475104496,0.07876098404891023,12,t3kcit@gmail.com,sklearn/ensemble/forest.py,12,0.01014370245139476,0,1,false,fix problem with csr matrix value error fixes #4744,,2726,0.7575201760821717,0.07100591715976332,42762,507.6937467845283,38.88966839717506,133.55315467003413,5749,61,1737,480,travis,y0ast,y0ast,true,,1,0.0,76,18,1645,false,false,false,false,0,2,0,0,0,0,63
7622273,scikit-learn/scikit-learn,python,4743,1432125588,1432143413,1432143413,297,297,commits_in_master,false,false,false,23,1,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.438976743110744,0.09716249185865801,3,olivier.grisel@ensta.org,sklearn/linear_model/omp.py,3,0.00253592561284869,0,1,false,[MRG] FIX Gram OMP check_finite on old scipy This should fix the heisen failure observed on travis with scipy 090 such as:https://travis-ciorg/scikit-learn/scikit-learn/jobs/62722617,,2725,0.7574311926605505,0.07100591715976332,42762,507.6937467845283,38.88966839717506,133.55315467003413,5748,61,1737,270,travis,ogrisel,amueller,false,amueller,119,0.8571428571428571,1154,124,2184,true,true,false,true,41,376,74,210,139,5,139
7613339,scikit-learn/scikit-learn,python,4741,1432075305,1432202277,1432202277,2116,2116,commits_in_master,false,false,false,34,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.486089506700945,0.09819371904764389,2,amueller@nyu.edu,sklearn/metrics/regression.py,2,0.0016792611251049538,0,0,false,[MRG] dont warn about multi-output deprecation if not using multi-output Fixes part of #4728I feel that we dont need to warn about multi-output stuff if it doesnt have any consequence for the computation,,2724,0.7573421439060205,0.07052896725440806,42762,507.6937467845283,38.88966839717506,133.55315467003413,5736,61,1736,269,travis,amueller,ogrisel,false,ogrisel,301,0.8471760797342193,1152,40,1670,true,true,true,false,169,1758,132,606,324,12,637
7613127,scikit-learn/scikit-learn,python,4739,1432074581,1432400782,1432400782,5436,5436,commits_in_master,false,false,false,59,1,1,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,13,4,13,4,12.851250263267508,0.28129444494527156,27,t3kcit@gmail.com,sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|sklearn/utils/estimator_checks.py,27,0.0,1,0,false,[MRG] FIX ransac output shape add test for regressor output shapes Adds a tests for regressor output shapesIm not sure if it is strict enough Im not sure what happens with yshape  (n_samples 1) currentlyThis is to make sure that we dont return (n_samples 1) if the training data was (n_samples)Inspired by @naught101s comment [here](https://githubcom/scikit-learn/scikit-learn/pull/3204#issuecomment-98048384),,2723,0.757253029746603,0.07052896725440806,42762,507.6937467845283,38.88966839717506,133.55315467003413,5736,61,1736,267,travis,amueller,larsmans,false,larsmans,300,0.8466666666666667,1152,40,1670,true,true,true,true,168,1756,131,606,324,12,652
7606690,scikit-learn/scikit-learn,python,4738,1432051761,,1441916935,164419,,unknown,false,true,false,32,90,16,103,112,0,215,0,10,5,0,24,38,21,0,0,13,0,33,46,30,0,6,9924,7669,21431,13077,463.6748046750794,10.149061311164418,95,vmehta94@gmail.com,sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/utils/estimator_checks.py|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|benchmarks/bench_covertype.py|benchmarks/bench_mnist.py|doc/modules/linear_model.rst|doc/modules/sgd.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/whats_new.rst|examples/linear_model/plot_sgd_comparison.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sag.py|sklearn/linear_model/sag_fast.c|sklearn/linear_model/sag_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sag.py|sklearn/tests/test_common.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx,53,0.0,1,19,false,[WIP] Adding Implementation of SAG - next episode I took over the great work of @dsullivan7 in #3814I removed the merges with master squashed all the commits and rebased on master,,2722,0.7575312270389419,0.0713678844519966,42752,507.78910928143716,38.89876497005988,133.58439371257487,5732,61,1736,351,travis,TomDLT,amueller,false,,3,1.0,6,3,90,true,false,false,false,1,23,5,14,10,0,9
7579443,scikit-learn/scikit-learn,python,4734,1431899786,1431902224,1431902224,40,40,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.01507288709575,0.08788329999448297,0,,sklearn/manifold/mds.py,0,0.0,0,0,false,typo: precomputed should be in quotes ,,2721,0.7574421168687983,0.0694560669456067,42750,507.8128654970761,38.900584795321635,133.5906432748538,5717,61,1734,267,travis,ajschumacher,agramfort,false,agramfort,6,1.0,191,357,1333,false,false,false,false,0,1,0,0,0,0,-1
7563072,scikit-learn/scikit-learn,python,4733,1431737652,1431745373,1431745373,128,128,commits_in_master,false,false,false,9,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,6,3,6,8.77524275251058,0.19207551786051272,2,sseg@users.noreply.github.com,sklearn/tests/test_dummy.py|sklearn/utils/random.py,2,0.0016051364365971107,0,0,true,[MRG] make random_choice_csc and therefor DummyClassifier deterministic Fixes #4731,,2720,0.7573529411764706,0.0666131621187801,42749,507.8247444384664,38.90149477180753,133.5937682752813,5706,62,1732,267,travis,amueller,mblondel,false,mblondel,299,0.8461538461538461,1148,40,1666,true,true,true,false,167,1716,128,582,316,12,128
7560058,scikit-learn/scikit-learn,python,4730,1431725158,1432135218,1432135218,6834,6834,commits_in_master,false,false,false,10,2,1,2,2,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,15,10,32,8.761551185509283,0.1917758321542291,10,t3kcit@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,7,0.0056406124093473006,0,0,true,[MRG] support for sparse coef_ in ovr classifier Fixes #4729,,2719,0.7572636998896654,0.06688154713940371,42749,507.8247444384664,38.90149477180753,133.5937682752813,5706,62,1732,271,travis,amueller,ogrisel,false,ogrisel,298,0.8456375838926175,1148,40,1666,true,true,true,false,165,1704,127,569,309,12,341
7552766,scikit-learn/scikit-learn,python,4727,1431693217,1431715577,1431715577,372,372,commits_in_master,false,false,false,12,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.54336262952423,0.09944667063214625,2,t3kcit@gmail.com,doc/modules/decomposition.rst,2,0.0015987210231814548,0,0,true,Fix mistake in TruncatedSVD explanation in docs Fix for docs Issue #4726 ,,2718,0.7571743929359823,0.06634692246203037,42749,507.8247444384664,38.90149477180753,133.5937682752813,5703,62,1732,266,travis,Spikhalskiy,amueller,false,amueller,0,0,7,2,1606,false,false,false,false,0,0,0,0,0,0,372
7547134,scikit-learn/scikit-learn,python,4725,1431654470,1432055559,1432055559,6684,6684,commits_in_master,false,false,false,16,5,4,0,7,0,7,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,25,0,25,8,15.76036748416979,0.3449682981620039,4,t3kcit@gmail.com,sklearn/base.py|sklearn/base.py|sklearn/base.py|sklearn/base.py,4,0.0031923383878691143,0,0,false,ENH: Avoid equality comparison for nans Its possible that a default parameter could be a NaN,,2717,0.757085020242915,0.06624102154828412,42749,507.8247444384664,38.90149477180753,133.5937682752813,5698,62,1731,267,travis,jseabold,ogrisel,false,ogrisel,2,0.5,173,21,1805,true,false,true,true,0,1,1,1,14,0,1023
7543481,scikit-learn/scikit-learn,python,4723,1431640417,1433351011,1433351011,28509,28509,commits_in_master,false,false,false,150,5,1,2,24,0,26,0,4,0,0,1,121,1,0,0,0,0,121,121,103,0,0,1,0,2856,36,4.490119535622393,0.09828126763563332,8,t3kcit@gmail.com,sklearn/svm/classes.py,8,0.006530612244897959,0,3,false,RFC website: adding backlinks to docstrings We discussed previously that doclinks from the API back to the user-guide would be helpful and I have heard many people wishing for theseI dont think there is another PR for that though The main questions are:* manual or automatics* just on the website or also in the docstringIm happy to do this manually if we can agree on how to do it Doing it directly in the docstring is substantially easier than using sphinx and/or JS to insert them into the page later The downside is that the the printed docstring looks a bit uglyI would argue that people mostly look at the docs on the website and that it isnt *that bad*[backlinks1](https://cloudgithubusercontentcom/assets/449558/7639306/e0b7da68-fa48-11e4-801d-62a5d2e86686png)[backlinks2](https://cloudgithubusercontentcom/assets/449558/7639307/e0bada38-fa48-11e4-9655-1eefd6d9e39epng)[backlinks3](https://cloudgithubusercontentcom/assets/449558/7639308/e0bfa5b8-fa48-11e4-9ad2-272008d2aa4fpng)The placement directly after the one-line summary is slightly violating [pep 0257][https://wwwpythonorg/dev/peps/pep-0257/#one-line-docstrings] but makes for a nice rendering on the websiteWdyt,,2716,0.7569955817378498,0.06775510204081632,42749,507.8247444384664,38.90149477180753,133.5937682752813,5698,62,1731,284,travis,amueller,amueller,true,amueller,297,0.8451178451178452,1148,40,1665,true,true,false,false,165,1695,126,572,282,12,5949
7523021,scikit-learn/scikit-learn,python,4718,1431536110,,1432313097,12949,,unknown,false,false,false,66,2,1,0,6,1,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,42,0,4.505624977704277,0.09862112562606727,0,,sklearn/decomposition/dict_learning.py,0,0.0,0,0,false, minor cosmetic changes to follow the work of Mairal 2009 In the paper from Mairal et al referenced in the script the set of possible dictionary vectors is not the ball but the convex set of dictionaries whose norm is inferior or equal to oneThis is simply implemented in the normalization step in the function updating the script + corresponding changes in the documentation,,2715,0.7572744014732965,0.06887966804979254,42711,508.2765563906254,38.93610545292782,133.7126267237948,5681,62,1730,268,travis,meduz,meduz,true,,1,0.0,16,5,1716,true,false,false,false,0,2,1,0,1,0,92
7519314,scikit-learn/scikit-learn,python,4716,1431517581,1432231843,1432231843,11904,11904,commits_in_master,false,false,false,20,5,2,13,12,0,25,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,70,133,133,229,26.5503365519969,0.5811455878964434,34,vmehta94@gmail.com,sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_estimator_checks.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/utils/tests/test_estimator_checks.py|sklearn/utils/tests/test_testing.py,25,0.008298755186721992,0,3,false,[WIP] check that unfitted estimators raise ValueError Issue : check_estimator requires raising NotFittedError #4661Please tell me what you think,,2714,0.7571849668386146,0.06887966804979254,42711,508.2765563906254,38.93610545292782,133.7126267237948,5677,62,1730,269,travis,TomDLT,amueller,false,amueller,2,1.0,6,3,84,true,false,false,false,1,16,4,5,4,0,230
7515706,scikit-learn/scikit-learn,python,4715,1431491051,,1455754783,404395,,unknown,false,false,false,5,1,1,2,2,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.354302251897462,0.09575563603814033,1,amueller@nyu.edu,sklearn/feature_extraction/text.py,1,0.0008298755186721991,0,3,false,Issue w/ tf-idf computation #4391 ,,2713,0.7574640619240693,0.06887966804979254,42724,524.6231626252223,40.258402771276096,136.57429079674188,5673,62,1729,495,travis,ermakovpetr,ermakovpetr,true,,0,0,16,31,1659,false,false,false,false,0,1,0,0,0,0,8419
7514363,scikit-learn/scikit-learn,python,4714,1431482996,1433766774,1433766774,38062,38062,commits_in_master,false,false,false,51,10,7,9,8,0,17,0,6,0,0,11,13,6,0,0,0,0,13,13,7,0,0,429,76,1067,298,109.16033581889921,2.3893496118714364,46,vmehta94@gmail.com,sklearn/multiclass.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/grid_search.py|sklearn/multiclass.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/base.py|sklearn/grid_search.py|sklearn/multiclass.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/base.py|doc/modules/model_persistence.rst|doc/modules/pipeline.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/multiclass.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_svm.py,19,0.004159733777038269,1,1,false,[MRG] Svm decision function shape This adds an option to change the shape of SVCdecision_function to a scikit-learn compatible shapeThis will allow people to use it with CalibratedClassifierCV in a multi-class setting and also just be much more api-friendlyI reused the function from the OvO multiclass classifierping @mblondel,,2712,0.7573746312684366,0.06905158069883527,42711,508.2765563906254,38.93610545292782,133.7126267237948,5672,62,1729,293,travis,amueller,larsmans,false,larsmans,296,0.8445945945945946,1146,40,1663,true,true,true,true,161,1673,125,570,260,12,969
7512337,scikit-learn/scikit-learn,python,4713,1431473814,1431484064,1431484064,170,170,commits_in_master,false,false,false,16,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.70947206659409,0.10308316013709466,4,olivier.grisel@ensta.org,sklearn/linear_model/ridge.py,4,0.0033444816053511705,0,0,false,[MRG] dont use mutable defaults in RidgeCV Fixes some of #4373Replaces #4376 which seems fubar,,2711,0.7572851346366655,0.06939799331103678,42712,508.26465630267836,38.93519385652744,133.70949616032965,5672,62,1729,263,travis,amueller,mblondel,false,mblondel,295,0.8440677966101695,1146,40,1663,true,true,true,false,161,1671,124,570,254,12,170
7510825,scikit-learn/scikit-learn,python,4709,1431428806,,1431506424,1293,,unknown,false,false,false,125,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,80,23,80,23,9.265753224658564,0.20281319966601957,18,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,14,0.011764705882352941,0,1,false,LabelSegmentedKFold cross-validation iterator This PR implements a variant of KFold that ensures that the train and test split is disjoint wrt a third-party label So far as I have seen all other Leave*Out methods either return all or a sample of all combinations of the third-party labels However if each party should be incorporated as test instance and the number of possible labels is huge this k-fold variant is more efficient *Example*: Assume that we have 1000000 observations from 10000 different users and we would like to estimate the generalization performance for new users So the train and test split should be disjoint  each user should occur once in the test set and leave one user out would be intractableThis is related to https://githubcom/scikit-learn/scikit-learn/pull/4583,,2709,0.757844222960502,0.0680672268907563,42704,508.359872611465,38.942487823154735,133.73454477332334,5657,62,1729,263,travis,BigCrunsh,BigCrunsh,true,,0,0,10,6,1162,false,false,false,false,0,1,0,0,0,0,487
7496848,scikit-learn/scikit-learn,python,4707,1431395119,1444265470,1444265470,214505,214505,commits_in_master,false,false,false,78,2,1,2,3,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,37,50,75,9.140483913104648,0.20007124558196338,8,xin_soft@foxmail.com,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,6,0.005076142131979695,0,0,false,[MGR] Raise error when init shape doesnt match n_clusters in KMeans This reverts part of 244abfbd17bc96cf5cf3654f0ac975a79320fdbaI feel that overwriting n_clusters is not a good idea In particular doing it in __init__ seems to break our APIIn hindsight maybe the use-case is grid-searching over different init methods which might change the number of clusters You might be  better served with searching over callables thoughThe current behavior is not documented and I would rather raise an error,,2708,0.757754800590842,0.06852791878172589,42704,508.359872611465,38.942487823154735,133.73454477332334,5655,62,1728,366,travis,amueller,MechCoder,false,MechCoder,293,0.8464163822525598,1146,40,1662,true,true,false,false,161,1658,122,569,249,12,2248
7491675,scikit-learn/scikit-learn,python,4705,1431369676,1432132944,1432132944,12721,12721,commits_in_master,false,false,false,21,3,1,7,9,0,16,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,11,52,40,156,13.762513933004978,0.3012404798310467,18,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/mocking.py,14,0.00937766410912191,0,2,false,[MRG] make cross_val_predict work on lists Also test compatibility with various input types and that they are passed throughFixes #4700,,2706,0.7579453067257945,0.06905370843989769,42704,508.359872611465,38.942487823154735,133.73454477332334,5655,61,1728,271,travis,amueller,ogrisel,false,ogrisel,292,0.8458904109589042,1146,40,1662,true,true,true,false,159,1651,121,571,248,11,920
7486812,scikit-learn/scikit-learn,python,4702,1431342082,1431623401,1431623401,4688,4688,commits_in_master,false,false,false,40,4,1,0,7,0,7,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,14,6,44,12,13.59071026654749,0.2974799591026156,27,t3kcit@gmail.com,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/utils/estimator_checks.py,27,0.0,0,1,false,Ensure dict_learning_online has at least one batch If there are less samples than the batch_size set the number of batches to one This came up during #4694 Is this a good fix Are there more tests which special case this,,2704,0.7581360946745562,0.06899488926746167,42704,508.359872611465,38.942487823154735,133.73454477332334,5651,60,1728,264,travis,betatim,amueller,false,amueller,3,0.6666666666666666,42,44,1178,true,false,false,false,0,10,5,9,21,0,26
7482491,scikit-learn/scikit-learn,python,4698,1431300838,1431340080,1431340080,654,654,github,false,false,false,37,1,1,0,6,0,6,0,4,0,0,3,3,0,0,2,0,0,3,3,0,0,2,0,0,0,0,9.87231875290088,0.21608733899043828,3,t3kcit@gmail.com,doc/images/iris.pdf|doc/images/iris.svg|doc/modules/tree.rst,3,0.0,0,1,false,[MRG] update export_graphviz docs In light of the #3735 merge I have updated the DecisionTreeClassifier docs Also added how to plot inline in IPython notebooksLocally the changes render like this:[screenshot from 2015-05-10 13 24 29](https://cloudgithubusercontentcom/assets/5210848/7556113/f9f2441a-f718-11e4-8af0-bd1771d0644apng),,2703,0.758046614872364,0.06633906633906633,42697,508.4432161510176,38.9488722861091,133.75647000960257,5644,60,1727,256,travis,trevorstephens,trevorstephens,true,trevorstephens,12,0.75,126,51,636,true,true,false,false,2,63,2,5,4,0,12
7475429,scikit-learn/scikit-learn,python,4697,1431221379,1432135056,1432135056,15227,15227,commits_in_master,false,false,false,50,2,1,2,2,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,8,0,15,4.434357537496311,0.09705821576950345,6,t3kcit@gmail.com,sklearn/utils/tests/test_utils.py,6,0.00494641384995878,1,0,false,[MRG]  dont check that warning was raised when slicing pd dataframe Removes a silly test I added in #4678Whether a copy needs to be made or not depends on the pandas version Version 012 and 013 apparently dont use cython here and 017 will have a fix by @ogrisel,,2702,0.7579570688378978,0.06677658697444352,42540,507.4518100611189,39.092618711800654,134.109073812882,5636,59,1726,273,travis,amueller,ogrisel,false,ogrisel,291,0.845360824742268,1146,40,1660,true,true,true,false,160,1669,122,576,251,11,4878
7458941,scikit-learn/scikit-learn,python,4695,1431120429,1431331680,1431331680,3520,3520,commits_in_master,false,false,false,31,4,3,5,5,0,10,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,16,49,21,49,13.41611742426356,0.2936479762335193,7,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,6,0.004979253112033195,0,0,true,ENH: preprocess: adding a max-normalization option Ive found that max-normalization is about as common as l1 or l2 normalization in my work so I added that option to the preprocessingnormalize() function,,2700,0.7581481481481481,0.07136929460580912,42537,507.2525095798952,39.09537579048828,134.0480052660037,5630,58,1725,260,travis,perimosocordiae,GaelVaroquaux,false,GaelVaroquaux,6,0.6666666666666666,47,48,2190,true,true,false,false,0,0,0,0,1,0,35
7458902,scikit-learn/scikit-learn,python,4692,1431100181,1431100440,1431100440,4,4,github,false,false,false,19,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,4.738154794682789,0.10370683846207948,5,hui.kian.ho@googlemail.com,examples/ensemble/plot_ensemble_oob.py,5,0.004173622704507512,0,0,true,Refined the docstring for plot_ensemble_oobpy (#4665) - Minor change to #4665: cleaned up the writing style of the documentation,,2697,0.7586206896551724,0.07178631051752922,42477,506.76836876427245,38.9858040822092,133.60171386868188,5628,56,1725,256,travis,kianho,kianho,true,kianho,1,1.0,4,27,1457,true,false,false,false,1,7,1,1,5,1,4
7446077,scikit-learn/scikit-learn,python,4686,1431031247,1432676240,1432676240,27416,27416,commits_in_master,false,false,false,17,2,1,1,7,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,36,0,82,0,4.317827351878323,0.09499858852229068,4,t3kcit@gmail.com,sklearn/covariance/graph_lasso_.py,4,0.0033585222502099076,0,0,false,Add enet_tol parameter to GraphLasso class/methods Per https://githubcom/scikit-learn/scikit-learn/pull/4613#issuecomment-96797817 adding enet_tol parameter to the rest of GraphLasso class,,2696,0.7585311572700296,0.07136859781696053,42299,506.96233953521363,38.9134494905317,133.5492564836048,5617,56,1724,273,travis,bnaul,GaelVaroquaux,false,GaelVaroquaux,1,1.0,1,0,1399,false,false,false,false,0,4,1,6,0,0,9
7443246,scikit-learn/scikit-learn,python,4684,1431020687,1432145218,1432145218,18742,18742,commits_in_master,false,false,false,10,1,1,2,7,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,26,15,26,9.150202224362243,0.20131802953354638,6,trev.stephens@gmail.com,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,4,0.003352891869237217,0,1,false,[MRG] FIX for LassoLarsCV on with readonly folds FIX #4597,,2695,0.7584415584415585,0.07124895222129086,42299,506.96233953521363,38.9134494905317,133.5492564836048,5616,56,1724,277,travis,ogrisel,amueller,false,amueller,118,0.8559322033898306,1151,124,2171,true,true,false,true,44,425,78,239,159,5,144
7442324,scikit-learn/scikit-learn,python,4683,1431017307,1431027081,1431027081,162,162,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,26,0,26,4.402899389176238,0.09687032128136326,3,olivier.grisel@ensta.org,sklearn/manifold/spectral_embedding_.py,3,0.0025041736227045075,0,0,false,Typos in comments corrected ,,2694,0.7583518930957683,0.0709515859766277,42299,506.96233953521363,38.9134494905317,133.5492564836048,5616,56,1724,258,travis,yanlend,ogrisel,false,ogrisel,1,0.0,0,1,800,false,false,false,false,0,0,0,0,0,0,-1
7434770,scikit-learn/scikit-learn,python,4681,1430973419,1430986117,1430986117,211,211,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.6064707007974315,0.10134905535173055,7,you@example.com,doc/related_projects.rst,7,0.005833333333333334,0,0,false,[MRG] add gplearn link to related projects doc page ,,2693,0.7582621611585593,0.07333333333333333,42299,506.96233953521363,38.9134494905317,133.5492564836048,5608,56,1723,258,travis,trevorstephens,GaelVaroquaux,false,GaelVaroquaux,11,0.7272727272727273,126,51,632,false,true,true,false,1,49,3,1,2,0,211
7425564,scikit-learn/scikit-learn,python,4680,1430934991,1433183528,1433183528,37475,37475,commits_in_master,false,false,false,10,3,2,3,5,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,27,0,46,0,8.855806638346237,0.19484062647323547,9,t3kcit@gmail.com,sklearn/pipeline.py|sklearn/pipeline.py,9,0.007468879668049793,0,5,false,[MRG] Pipeline named steps fix Fixes #4482 continuation of #4486,,2692,0.7581723625557206,0.07468879668049792,42299,506.96233953521363,38.9134494905317,133.5492564836048,5602,56,1723,284,travis,amueller,amueller,true,amueller,289,0.8477508650519031,1143,40,1657,true,true,false,false,157,1593,119,476,247,10,1033
7424442,scikit-learn/scikit-learn,python,4679,1430931188,1432202866,1432202866,21194,21194,commits_in_master,false,false,false,43,6,6,31,11,0,42,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,891,190,891,190,80.79414967163456,1.7775856439153783,16,vmehta94@gmail.com,sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py,7,0.005813953488372093,1,6,false,[WIP] change func_grad_hess into grad_hess I took over the work of @mblondel on issue #4672With logistic loss LogisticRegressionCV takes ~15% less time for example on iris datasetWith multinomial loss the gain is more subtle (~1%) and the refactor might be useless ,,2691,0.758082497212932,0.07475083056478406,42299,506.82049221021776,38.9134494905317,133.47833282110687,5602,56,1723,277,travis,TomDLT,agramfort,false,agramfort,1,1.0,6,3,77,true,false,true,true,0,7,3,0,3,0,5
7415398,scikit-learn/scikit-learn,python,4678,1430876473,1430931916,1430931916,924,924,commits_in_master,false,false,false,43,2,1,1,6,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,11,21,22,8.62802747214851,0.18983143046455392,8,ragvrv@gmail.com,sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py,6,0.004987531172069825,0,1,false,[MRG] FIX work-around for read only dataframes Fixes the second part of #4597Currently Im raising a DataConversionWarning which is not really right It should be a DataCopyWarning which we dont have yet Do we want to add that In light of #4660 ,,2690,0.7579925650557621,0.0773067331670823,42272,505.72482967448906,38.79636638909917,133.3506813020439,5597,56,1722,259,travis,amueller,amueller,true,amueller,288,0.8472222222222222,1143,40,1656,true,true,false,false,158,1601,117,482,246,10,30
7408268,scikit-learn/scikit-learn,python,4676,1430850124,1430855505,1430855505,89,89,commits_in_master,false,false,false,20,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.14671116295237,0.09123476756469999,6,t3kcit@gmail.com,sklearn/linear_model/base.py,6,0.0049833887043189366,0,0,false,Fix string literal concatenation whitespace Changes This LogisticRegression instance is not fittedyet to This LogisticRegression instance is not fitted yet,,2689,0.757902566009669,0.07724252491694353,42272,505.72482967448906,38.79636638909917,133.3506813020439,5593,56,1722,258,travis,zacstewart,agramfort,false,agramfort,0,0,117,66,1667,false,true,false,false,0,0,0,0,0,0,-1
7402362,scikit-learn/scikit-learn,python,4673,1430823913,,1431028305,3406,,unknown,false,false,false,6,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.100940417947445,0.09018394575249206,1,amueller@nyu.edu,sklearn/feature_extraction/text.py,1,0.0008312551953449709,0,0,false,Update textpy fix bug for selfvocabulary_,,2688,0.7581845238095238,0.07813798836242726,42724,524.6231626252223,40.258402771276096,136.57429079674188,5589,56,1722,260,travis,owengbs,ogrisel,false,,0,0,1,1,1153,false,false,false,false,0,0,0,0,0,0,14
7372507,scikit-learn/scikit-learn,python,4665,1430603989,1431088604,1431088604,8076,8076,commits_in_master,false,false,false,47,11,6,9,16,1,26,0,5,1,0,1,3,1,0,0,1,0,2,3,1,0,0,138,0,419,0,28.113333514870728,0.6185213971944121,0,,examples/ensemble/plot_ensemble_oob.py|examples/ensemble/plot_ensemble_oob.py|examples/ensemble/plot_ensemble_oob.py|examples/ensemble/plot_ensemble_oob.py|examples/ensemble/plot_ensemble_oob.py|examples/ensemble/plot_ensemble_oob.py,0,0.0,2,10,false,Script for example ensemble OOB error plot Fixes #4273 This script generates an example plot of the OOB error vs n_estimators using the RandomForestClassifier(warm_startTrue) approach for inclusion in the website docs/examples section as suggested by @glouppe and @amueller in #4273Please advise if further amendments are required,,2687,0.7580945292147376,0.08367854183927093,42267,504.19949369484465,38.68266023138619,133.27181962287364,5550,55,1719,264,travis,kianho,GaelVaroquaux,false,GaelVaroquaux,0,0,4,27,1451,true,false,false,false,1,3,0,0,2,0,1361
7363170,scikit-learn/scikit-learn,python,4663,1430521342,1432831467,1432831467,38502,38502,commits_in_master,false,true,false,34,12,6,0,21,1,22,0,5,0,1,3,4,1,1,1,0,1,3,4,1,1,1,468,0,468,0,43.54837588543681,0.9581061093635459,1,gael.varoquaux@normalesup.org,doc/themes/scikit-learn/static/sidebar.js|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/tune_toc.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/tune_toc.rst|doc/themes/scikit-learn/static/sidebar.js|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/tune_toc.rst|doc/themes/scikit-learn/static/sidebar.js,1,0.0,0,2,true,Sidebar hide in css A not yet fix But a proposal on removing the javascripts on this bug This is not completely responsive for all screen sizes But does not mess up the layout,,2686,0.7580044676098288,0.08526490066225166,42267,504.1521754560295,38.68266023138619,133.27181962287364,5541,54,1718,279,travis,Titan-C,GaelVaroquaux,false,GaelVaroquaux,5,0.8,7,3,1486,true,false,false,false,0,0,1,0,5,0,6759
7361902,scikit-learn/scikit-learn,python,4662,1430516012,1430519922,1430519922,65,65,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.880946164923669,0.10755826966532418,3,t3kcit@gmail.com,doc/testimonials/images/infonea.jpg|doc/testimonials/testimonials.rst,3,0.0024896265560165973,0,0,true,DOC: add a testimonial by infonea ,,2685,0.7579143389199255,0.08547717842323652,42267,504.1521754560295,38.68266023138619,133.27181962287364,5541,54,1718,259,travis,GaelVaroquaux,agramfort,false,agramfort,53,0.7924528301886793,627,3,1894,true,true,false,true,21,122,14,34,19,0,0
7359050,scikit-learn/scikit-learn,python,4659,1430503719,1430508706,1430508706,83,83,commits_in_master,false,false,false,7,2,1,3,2,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,10,0,4.129962596296771,0.09100934507755136,2,t3kcit@gmail.com,sklearn/grid_search.py,2,0.0016611295681063123,0,0,true,remove useless |samples| variable out of ParameterSampler__iter__() ,,2684,0.7578241430700448,0.08554817275747509,42267,504.1521754560295,38.68266023138619,133.27181962287364,5537,54,1718,259,travis,MAnyKey,amueller,false,amueller,0,0,2,4,1173,false,false,false,false,0,0,0,0,0,0,38
7358540,scikit-learn/scikit-learn,python,4658,1430501132,,1455993040,424865,,unknown,false,true,false,217,31,3,1,58,0,59,0,7,0,0,1,70,1,0,0,16,2,68,86,24,4,3,66,0,5234,6,14.048697322921639,0.3095821603077716,4,t3kcit@gmail.com,doc/conf.py|doc/conf.py|doc/conf.py,4,0.0033222591362126247,0,11,true,[WIP] Sphinx-gallery use This is a proposal to incorporate sphinx-gallery into scikit-learn There are still some issues to work on so for know one works with the github PR of sphinx-gallery(https://githubcom/sphinx-gallery/sphinx-gallery/pull/26) as the main development versionTo work on:- [ ] **Backreferences** The documented API gives at the end of every module a small gallery of the examples using that module Examples are properly identified but the html output fails recognizing the correct path of the image and the html file In scikit-learns gen_rstpy this paths are hard coded to its website structure Sphinxgallery uses a configuration dictionary This issue is mostly solved with  https://githubcom/sphinx-gallery/sphinx-gallery/pull/26- [ ] **the sidebar hide behavior** It just messes up with the scikit-learn design And it is manually deactivated in the gallery implementation with the promise to fix it later:https://githubcom/scikit-learn/scikit-learn/blob/master/doc/sphinxext/gen_rstpy#L494-508- [ ] **The carousel**: There is an additional image processing step for some images displayed in the carousel done in gen_rstpy that sphinx-gallery does not perform- [ ] **Image naming**: All images generated by sphinx-gallery have a sphx_glr_ prefix If images of the gallery are manually called by the documentation this would be broken- [ ] More stuff Im not aware of- [ ] Maybe as in nilearn put a local copy of sphinx-gallery inside scikit-learn ,,2683,0.7581065970928066,0.08554817275747509,42267,504.1521754560295,38.68266023138619,133.27181962287364,5535,54,1718,499,travis,Titan-C,Titan-C,true,,4,1.0,7,3,1486,true,false,false,false,0,0,0,0,1,0,121
7349861,scikit-learn/scikit-learn,python,4655,1430437993,1430752010,1430752010,5233,5233,commit_sha_in_comments,false,false,false,94,1,1,1,8,0,9,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,0,9,0,3.5254622504142517,0.07768835746685193,2,g.louppe@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,2,0.001652892561983471,0,2,false,Store values for all nodes instead of just leaves for decision trees Store values for all nodes instead of just leaves as discussed in #4644 and required in #2937Not surprisingly the impact on performance is minimalBenchmark:    from sklearndatasets import make_classification    from sklearntree import DecisionTreeClassifier    X y  make_classification(n_samples50000 n_features50 n_informative5                           n_redundant2 n_repeated0 n_classes8                           n_clusters_per_class1 random_state0)    def test_dt(X y):        dt  DecisionTreeClassifier()        dtfit(Xy)predict(X)Master:        In [4]: %timeit -r5 test_dt(Xy)    1 loops best of 5: 596 s per loopPR:    In [6]: %timeit -r5 test_dt(Xy)    1 loops best of 5: 596 s per loop,,2682,0.7580164056674124,0.08512396694214876,42267,504.1521754560295,38.68266023138619,133.27181962287364,5525,55,1717,260,travis,andosa,ogrisel,false,ogrisel,1,0.0,7,0,840,false,false,false,false,0,1,0,1,0,0,33
7347808,scikit-learn/scikit-learn,python,4654,1430429525,1431004877,1431004877,9589,9589,commits_in_master,false,false,false,29,1,1,0,4,0,4,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.213153493114178,0.0928427741366751,0,,sklearn/ensemble/partial_dependence.py,0,0.0,1,2,false,[MRG] FIX pass percentiles to partial_dependence in plotting Fixes #4625 Ping @pprettI dont know how to unit-test for this and Im not sure it is worth the hassle,,2681,0.7579261469600895,0.08519437551695616,42239,503.96552948696706,38.68462795047231,133.26546556499915,5524,55,1717,263,travis,amueller,ogrisel,false,ogrisel,287,0.8466898954703833,1140,40,1651,true,true,true,false,153,1556,120,395,250,10,9487
7344759,scikit-learn/scikit-learn,python,4653,1430417782,1430685296,1430685296,4458,4458,commits_in_master,false,false,false,7,18,6,0,12,0,12,0,3,0,0,1,4,1,0,0,0,0,4,4,4,0,0,94,0,321,167,26.608145075123943,0.5863478432744881,17,xuewei4d@gmail.com,sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py,17,0.014096185737976783,0,1,false,Added verbose flag to GMM Issue #4633 ,,2680,0.7578358208955224,0.08623548922056384,42239,503.96552948696706,38.68462795047231,133.26546556499915,5523,53,1717,263,travis,ssaeger,GaelVaroquaux,false,GaelVaroquaux,2,0.5,2,0,1264,true,false,false,false,1,3,0,0,1,0,12
7341851,scikit-learn/scikit-learn,python,4650,1430405010,1430740551,1430740551,5592,5592,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,12,3,12,9.238360621754623,0.20358022781069132,11,t3kcit@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py,8,0.006661115736885929,0,1,false,FIX DBSCAN fit with precomputed matrix in edge cases and add test Fix #4641,,2679,0.757745427398283,0.08659450457951708,42238,503.9774610540272,38.68554382309768,133.26862067332735,5522,52,1717,262,travis,lesteve,ogrisel,false,ogrisel,11,1.0,4,0,1100,false,false,false,false,0,23,6,16,8,0,194
7339359,scikit-learn/scikit-learn,python,4647,1430392152,1430392486,1430392486,5,5,commits_in_master,false,false,false,33,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.653661197788259,0.1026543522068071,4,loic.esteve@ymail.com,README.rst,4,0.003336113427856547,0,0,false,[MRG] Visual tweak to appveyor badge on READMErst It now looks visually different from the Travis one Result can be seen on my [branch](https://githubcom/lesteve/scikit-learn/tree/better-appveyor-badge)The link I used is from the AppVeyor [doc](http://wwwappveyorcom/docs/status-badges#badges-for-projects-with-public-repositories-on-github-and-bitbucket),,2678,0.7576549663928305,0.08757297748123437,42166,506.5455580325381,38.870179765688,133.35388701797658,5519,52,1717,258,travis,lesteve,agramfort,false,agramfort,10,1.0,4,0,1100,false,false,false,false,0,23,5,16,8,0,4
7329846,scikit-learn/scikit-learn,python,4645,1430339543,1430765883,1430765883,7105,7105,commits_in_master,false,false,false,34,31,8,31,36,0,67,0,4,0,0,12,18,12,0,0,0,0,18,18,17,0,0,147,36,954,535,145.14524760129285,3.2017353079955084,60,t3kcit@gmail.com,sklearn/cluster/k_means_.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/random.py|sklearn/utils/validation.py|sklearn/datasets/base.py|sklearn/datasets/twenty_newsgroups.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/preprocessing/data.py|sklearn/utils/graph.py|sklearn/manifold/spectral_embedding_.py|sklearn/preprocessing/data.py|sklearn/utils/graph.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/validation.py|sklearn/cluster/k_means_.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/random.py|sklearn/utils/validation.py|sklearn/datasets/base.py|sklearn/datasets/twenty_newsgroups.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/preprocessing/data.py|sklearn/utils/graph.py|sklearn/manifold/spectral_embedding_.py|sklearn/preprocessing/data.py|sklearn/utils/graph.py|sklearn/utils/validation.py,18,0.003305785123966942,0,5,false,Astype fix I extended check_array to add support for accept_sparseTrueand solve the 3 errors by changing astype into check_array when sparse matrix are possibleI will check the other uses of astype tomorrow,,2677,0.7575644378035113,0.09008264462809917,42166,506.5455580325381,38.870179765688,133.35388701797658,5512,52,1716,284,travis,TomDLT,TomDLT,true,TomDLT,0,0,6,3,70,false,false,false,false,0,2,0,0,0,0,130
7322705,scikit-learn/scikit-learn,python,4642,1430311015,1430767369,1430767369,7605,7605,commits_in_master,false,false,false,54,1,1,2,10,0,12,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,20,2,20,8.862225263308245,0.1954903794775783,5,ragvrv@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py,3,0.002477291494632535,0,1,false,[MRG ]Bagging : raise ValueError if sample weight are passed but unsupported base estimatorCurrently sample weight are ignored if the base estimator doesnt support fit with sample weight This is missleading as the user expect that sample weightare taken into accountThe pull request now raises a value error in that case,,2676,0.757473841554559,0.0916597853014038,42166,506.5455580325381,38.870179765688,133.35388701797658,5510,52,1716,263,travis,arjoly,glouppe,false,glouppe,82,0.8292682926829268,29,26,1226,true,true,true,true,8,65,8,72,20,0,0
7301444,scikit-learn/scikit-learn,python,4640,1430201294,1430281287,1430281287,1333,1333,commits_in_master,false,false,false,103,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.131593525428131,0.09114708341194484,13,trev.stephens@gmail.com,sklearn/ensemble/forest.py,13,0.010743801652892562,0,0,false,ForestClassifierpredict docstring correction RandomForestClassifier predicts based on the mean probability estimate rather than a majority vote as in the original Breiman paper This is mentioned in the narrative docs But the predict docstring currently says The predicted class of an input sample is computed as the majority prediction of the trees in the forestwhich is incorrect or at best quite misleadingIt would also be good to explain somewhere the rationale for this difference from the original method I assume its just so predict and predict_proba are consistent(Theres some discussion [in this crossvalidated question](http://statsstackexchangecom/q/127077/9964) which is what brought me here),,2675,0.7573831775700934,0.09256198347107437,42142,506.83403730245357,38.892316453893976,133.4298324711689,5479,51,1715,260,travis,dougalsutherland,amueller,false,amueller,6,1.0,34,28,2345,false,true,false,false,0,0,0,1,0,0,527
7289436,scikit-learn/scikit-learn,python,4636,1430147963,1430175546,1430175546,459,459,commits_in_master,false,false,false,10,2,1,1,1,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,9,0,4.3639430255147404,0.09627821806130961,10,t3kcit@gmail.com,sklearn/metrics/classification.py,10,0.008210180623973728,0,0,false,Update confusion_matrix docstring Improve confusion_matrix docstring with labels parameters usage,,2674,0.7572924457741211,0.09359605911330049,42142,505.76621897394523,38.82112856532675,133.23999810165628,5470,50,1714,262,travis,scls19fr,amueller,false,amueller,0,0,7,32,2100,false,false,false,false,0,0,0,0,0,0,303
7261136,scikit-learn/scikit-learn,python,4631,1429915218,1430261560,1430261560,5772,5772,commits_in_master,false,false,false,24,5,2,10,6,0,16,0,5,0,0,3,3,1,0,0,0,0,3,3,1,0,0,38,0,46,0,13.656980825751438,0.3013031495400259,10,t3kcit@gmail.com,doc/modules/tree.rst|sklearn/datasets/descr/diabetes.rst|sklearn/svm/classes.py,9,0.0008064516129032258,0,2,true,Documentation improvements Addresses documentation issues referenced in the following PR: https://githubcom/scikit-learn/scikit-learn/issues/4477Also fixes a few grammar and wording issues in some of the docstrings,,2672,0.7574850299401198,0.09838709677419355,42142,505.76621897394523,38.82112856532675,133.23999810165628,5435,50,1711,262,travis,Aerlinger,amueller,false,amueller,0,0,28,53,1537,false,true,false,false,1,1,0,0,0,0,7
7172024,scikit-learn/scikit-learn,python,4621,1429464538,1429520250,1429520250,928,928,merged_in_comments,false,false,false,67,10,1,31,39,0,70,0,7,0,0,2,2,2,0,0,0,0,2,2,2,0,0,128,6,323,63,8.686565116917315,0.19164474344804242,23,thomas.unterthiner@gmx.net,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,16,0.01263823064770932,0,12,false,Fix Bug ##4618 cross_validationShuffleSplit setting train_size without setting test_size the sum of train_size and test_size is not equal to 1 Now the sum of them is 01(default value of test_size) + train_sizeWhen setting test_size without setting train_size the train_size is autocomputed by 1 - test_size so we hope when setting train_size the test_size is autocomputed by 1 - train_size as well not defualt value 01https://githubcom/scikit-learn/scikit-learn/issues/4618,,2670,0.7576779026217229,0.10821484992101106,42294,502.3880455856623,38.61067763748995,132.57199602780537,5346,50,1706,371,travis,SnakeHunt2012,SnakeHunt2012,true,SnakeHunt2012,0,0,11,28,865,false,false,false,false,0,0,0,0,0,0,90
7157587,scikit-learn/scikit-learn,python,4613,1429301160,1430157772,1430157772,14276,14276,commits_in_master,false,false,false,36,10,3,13,8,0,21,0,4,0,0,1,3,1,0,0,0,0,3,3,2,0,1,0,82,30,260,14.558251689615869,0.3211870713632067,0,,sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/tests/test_graph_lasso.py,0,0.0,0,3,true,Graph lasso tests Added/improved some GraphLasso tests per #41341) Hard-coded optimal values for a well-known dataset2) Test lars method as well as cd3) Fix warning in test_graph_lasso_cv due to badly chosen warm starts,,2669,0.7575871112776321,0.11212121212121212,42294,502.3880455856623,38.61067763748995,132.57199602780537,5327,53,1704,273,travis,bnaul,ogrisel,false,ogrisel,0,0,1,0,1379,false,false,false,false,0,0,0,0,0,0,46
7148081,scikit-learn/scikit-learn,python,4612,1429284998,,1429285052,0,,unknown,false,false,false,2,1,1,0,0,0,0,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,0,47,0,47,0,4.632123483364161,0.10219483819445675,0,,sklearn/semi_supervised/boostcluster.py,0,0.0,0,0,true,Unfinished boostcluster ,,2668,0.7578710644677661,0.11346444780635401,42294,502.3880455856623,38.61067763748995,132.57199602780537,5326,53,1704,268,travis,christophesaintjean,christophesaintjean,true,,0,0,5,1,1241,false,false,false,false,0,0,0,0,0,0,-1
7140382,scikit-learn/scikit-learn,python,4610,1429238466,1429806733,1429806733,9471,9471,commits_in_master,false,false,false,26,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.638456144587082,0.10233455063764635,0,,examples/applications/wikipedia_principal_eigenvector.py,0,0.0,0,1,false,Change urlopen and iteritems functions for Python3 compat The Wikipedia example had used Python2-only modules and methods These have been changed to their equivalents in six ,,2667,0.7577802774653168,0.11320754716981132,42294,502.3880455856623,38.61067763748995,132.57199602780537,5316,53,1703,270,travis,sseg,ogrisel,false,ogrisel,2,0.5,0,0,357,false,false,false,false,0,5,2,0,0,0,673
7137188,scikit-learn/scikit-learn,python,4608,1429224805,1429234710,1429234710,165,165,commits_in_master,false,false,false,16,2,2,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,20,5,20,13.424101528474598,0.2961663339933911,2,rajotte.jeanfrancois@gmail.com,sklearn/datasets/tests/test_base.py|sklearn/datasets/base.py|sklearn/datasets/tests/test_base.py,2,0.0015094339622641509,0,3,false,Fix (sub)issue #4600 adding test to Bunch class Testing Bunch consistency after pickleSee issue #4600,,2666,0.7576894223555889,0.11396226415094339,42291,502.3527464472346,38.613416566172475,132.58140029793572,5314,53,1703,267,travis,jfraj,ogrisel,false,ogrisel,2,1.0,2,0,266,false,false,false,false,1,3,2,1,0,0,0
7129979,scikit-learn/scikit-learn,python,4606,1429197356,1429218260,1429218260,348,348,commits_in_master,false,false,false,15,1,1,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,4,11,4,9.04008626431376,0.19944470396896924,22,sseg@users.noreply.github.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,17,0.012658227848101266,0,0,false,Dont set / check order of sparse matrices (clean history) Please see #4554 for discussion,,2665,0.7575984990619137,0.1169024571854058,42293,502.3289906131039,38.611590570543584,132.57513063627553,5312,54,1703,268,travis,ibayer,ogrisel,false,ogrisel,19,0.6842105263157895,9,0,1139,true,true,false,false,0,2,2,0,13,0,214
7127241,scikit-learn/scikit-learn,python,4605,1429183228,1429554541,1429554541,6188,6188,merged_in_comments,false,false,false,15,6,1,12,6,0,18,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,18,2,152,2,12.61443346369952,0.27830286950196403,21,trev.stephens@gmail.com,sklearn/ensemble/forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,15,0.006716417910447761,0,3,false,FIX raise error properly when n_features differ in fit and apply Aim to fix https://githubcom/scikit-learn/scikit-learn/issues/4518,,2664,0.7575075075075075,0.11716417910447761,42293,502.3289906131039,38.611590570543584,132.57513063627553,5310,54,1703,271,travis,arjoly,arjoly,true,arjoly,81,0.8271604938271605,29,26,1213,true,true,false,false,9,57,5,60,15,0,5
7125576,scikit-learn/scikit-learn,python,4604,1429170594,1429473275,1429473275,5044,5044,commits_in_master,false,false,false,22,1,1,3,10,0,13,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.3427884832997465,0.09581166307369039,5,t3kcit@gmail.com,sklearn/neighbors/approximate.py,5,0.0037369207772795215,0,1,false,[MRG] ENH faster LSHForest: use take rather than fancy indexing An absurd amount of time was being taken up by fancy indexing,,2663,0.7574164476154712,0.11733931240657698,42228,503.0785260964289,38.576300085251496,132.61343184616842,5308,54,1703,271,travis,jnothman,ogrisel,false,ogrisel,110,0.6818181818181818,30,1,2179,true,true,false,false,29,301,24,151,28,1,168
7125334,scikit-learn/scikit-learn,python,4603,1429168532,,1429169071,8,,unknown,false,false,false,22,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.337567241890487,0.0956964707670298,5,t3kcit@gmail.com,sklearn/neighbors/approximate.py,5,0.0037369207772795215,0,0,false,[MRG] ENH faster LSHForest: use take rather than fancy indexing An absurd amount of time was being taken up by fancy indexing,,2662,0.7577009767092412,0.11733931240657698,42228,503.0785260964289,38.576300085251496,132.61343184616842,5307,54,1703,267,travis,jnothman,jnothman,true,,109,0.6880733944954128,30,1,2179,true,true,false,false,29,300,22,151,28,1,-1
7123510,scikit-learn/scikit-learn,python,4602,1429155320,,1434071340,81933,,unknown,false,true,false,384,8,2,6,19,0,25,0,5,2,0,7,11,0,0,0,2,0,9,11,1,0,0,0,0,4,0,77.99627818414305,1.7207729906990148,89,vmehta94@gmail.com,doc/developers/contributing.rst|doc/developers/index.rst|doc/documentation.rst|doc/index.rst|doc/modules/classes.rst|doc/preface.rst|doc/tutorial/index.rst|doc/user_guide.rst|doc/whats_new.rst|doc/developers/contributing.rst|doc/developers/index.rst|doc/documentation.rst|doc/index.rst|doc/modules/classes.rst|doc/preface.rst|doc/tutorial/index.rst|doc/user_guide.rst|doc/whats_new.rst,70,0.0007473841554559044,0,5,false,[MRG] DOC organise table of contents overall TOC should have better structure particularly if users want PDF doc This PR produces:* Welcome to scikit-learn * Installing scikit-learn * Frequently Asked Questions * Support * Related Projects * About us * Who is using scikit-learn * Release history* scikit-learn Tutorials * An introduction to machine learning with scikit-learn * A tutorial on statistical-learning for scientific data processing * Working With Text Data * Choosing the right estimator * External Resources Videos and Talks* User Guide * 1 Supervised learning * 2 Unsupervised learning * 3 Model selection and evaluation * 4 Dataset transformations * 5 Dataset loading utilities * 6 Strategies to scale computationally: bigger data * 7 Computational Performance* Examples * General examples * Examples based on real world datasets * Biclustering * Calibration * Classification * Clustering * Covariance estimation * Cross decomposition * Dataset examples * Decomposition * Ensemble methods * Tutorial exercises * Feature Selection * Gaussian Process for Machine Learning * Generalized Linear Models * Manifold learning * Gaussian Mixture Models * Model Selection * Nearest Neighbors * Neural Networks * Semi Supervised Classification * Support Vector Machines * Working with text documents * Decision Trees* API Reference * sklearnbase: Base classes and utility functions * sklearncluster: Clustering * sklearnclusterbicluster: Biclustering * sklearncovariance: Covariance Estimators * sklearncross_validation: Cross Validation * sklearndatasets: Datasets * sklearndecomposition: Matrix Decomposition * sklearndummy: Dummy estimators * sklearnensemble: Ensemble Methods * sklearnfeature_extraction: Feature Extraction * sklearnfeature_selection: Feature Selection * sklearngaussian_process: Gaussian Processes * sklearngrid_search: Grid Search * sklearnisotonic: Isotonic regression * sklearnkernel_approximation Kernel Approximation * sklearnkernel_ridge Kernel Ridge Regression * sklearnlda: Linear Discriminant Analysis * sklearnlearning_curve Learning curve evaluation * sklearnlinear_model: Generalized Linear Models * sklearnmanifold: Manifold Learning * sklearnmetrics: Metrics * sklearnmixture: Gaussian Mixture Models * sklearnmulticlass: Multiclass and multilabel classification * sklearnnaive_bayes: Naive Bayes * sklearnneighbors: Nearest Neighbors * sklearnneural_network: Neural network models * sklearncalibration: Probability Calibration * sklearncross_decomposition: Cross decomposition * sklearnpipeline: Pipeline * sklearnpreprocessing: Preprocessing and Normalization * sklearnqda: Quadratic Discriminant Analysis * sklearnrandom_projection: Random projection * sklearnsemi_supervised Semi-Supervised Learning * sklearnsvm: Support Vector Machines * sklearntree: Decision Trees * sklearnutils: Utilities* Developers Guide * Contributing * Developers’ Tips for Debugging * Utilities for Developers * How to optimize for speed * Maintainer / core-developer information,,2661,0.7579857196542653,0.11733931240657698,42228,503.0785260964289,38.576300085251496,132.61343184616842,5304,54,1702,314,travis,jnothman,jnothman,true,,108,0.6944444444444444,30,1,2178,true,true,false,false,29,300,20,151,28,1,867
7122761,scikit-learn/scikit-learn,python,4601,1429150828,,1429155356,75,,unknown,false,true,false,24,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,3.4762201229083645,0.07669321968290076,70,trev.stephens@gmail.com,doc/whats_new.rst,70,0.052316890881913304,0,0,false,DOC chapter heading for whats_new In PDF documentation the fact that each version in whats_new is a separate chapter is a bit more blatant,,2660,0.7582706766917293,0.11733931240657698,42228,503.0785260964289,38.576300085251496,132.61343184616842,5304,54,1702,267,travis,jnothman,jnothman,true,,107,0.7009345794392523,30,1,2178,true,true,false,false,29,301,19,151,28,1,57
7118985,scikit-learn/scikit-learn,python,4600,1429134817,1429183103,1429183103,804,804,commits_in_master,false,false,false,58,2,2,3,5,0,8,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,41,30,41,30,22.6915501995204,0.5006265376950249,1,amueller@nyu.edu,sklearn/datasets/tests/test_20news.py|sklearn/datasets/twenty_newsgroups.py|sklearn/datasets/base.py|sklearn/datasets/tests/test_20news.py|sklearn/datasets/twenty_newsgroups.py,1,0.0,0,0,false,Implementing fix for fetching 20newsgroup dataset Fixing issue 4435:the fetched bunch behaved differently for data and [data](similary for target and filenames)Fixed by updating both in the fetching (same as was suggested in the issue)A test has been added to check that both data and [data] have the same length(testing also target and filenames),,2659,0.7581797668296352,0.11808669656203288,42228,503.0785260964289,38.576300085251496,132.61343184616842,5302,54,1702,267,travis,jfraj,agramfort,false,agramfort,1,1.0,2,0,265,false,false,false,false,1,0,1,0,0,0,55
7110880,scikit-learn/scikit-learn,python,4596,1429101652,1429111346,1429111346,161,161,commits_in_master,false,false,false,31,1,1,0,0,0,0,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,18,0,18,0,4.2755159770621445,0.09432951139753085,15,thomas.unterthiner@gmx.net,sklearn/cross_validation.py,15,0.011202389843166542,0,2,false,[MRG] DOC train_test_split use standart (X y) notation Hi this docstring confused me once to often :) I would suggest to change (a b) to our standart( X y) notation,,2658,0.7580887885628292,0.1179985063480209,42228,503.1732499763191,38.576300085251496,132.61343184616842,5295,54,1702,269,travis,ibayer,agramfort,false,agramfort,18,0.6666666666666666,9,0,1138,true,true,false,false,0,2,1,0,13,0,-1
7106982,scikit-learn/scikit-learn,python,4595,1429075709,1429118608,1429118608,714,714,commits_in_master,false,false,false,39,4,3,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,8,0,9.143497975356077,0.20173043466728108,0,,examples/bicluster/bicluster_newsgroups.py|examples/bicluster/bicluster_newsgroups.py,0,0.0,0,0,false,Add python3 xrange support in bicluster newsgroups example The bicluster_newsgroupspy example did not have the xrange function defined in Python3 This raised a NameError from the function call at line 150 Added the function import from sklearnexternalssixmoves to resolve,,2657,0.757997741814076,0.11791044776119403,42228,503.1732499763191,38.576300085251496,132.61343184616842,5291,54,1702,268,travis,sseg,GaelVaroquaux,false,GaelVaroquaux,1,0.0,0,0,356,false,false,false,false,0,3,1,0,0,0,4
7103718,scikit-learn/scikit-learn,python,4594,1429057226,1429121401,1429121401,1069,1069,merged_in_comments,false,false,false,36,6,6,0,4,0,4,0,2,0,0,3,3,1,0,0,0,0,3,3,1,0,0,3,0,3,0,13.933559574475384,0.3074122219961352,16,trev.stephens@gmail.com,doc/modules/tree.rst|sklearn/tree/tree.py|doc/tutorial/basic/tutorial.rst,16,0.000741839762611276,0,1,false,Add conventions section to userguide #4508 (rebase) Adds conventions section with two subsections about a) type casting and b) updating model parameters to quick-start userguide (#4508) Similar to #4566 but branch was first rebased to master,,2656,0.7579066265060241,0.11943620178041543,42228,503.1732499763191,38.576300085251496,132.61343184616842,5288,54,1701,267,travis,cangermueller,amueller,false,amueller,4,0.5,10,0,659,true,false,false,false,1,19,4,0,14,0,910
7102838,scikit-learn/scikit-learn,python,4593,1429053987,1430432975,1430432975,22983,22983,commits_in_master,false,false,false,27,6,6,40,9,4,53,0,3,0,0,4,4,3,0,0,0,0,4,4,3,0,0,391,166,391,166,86.01424705693667,1.8977082395754568,96,trev.stephens@gmail.com,sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|doc/whats_new.rst|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|doc/whats_new.rst|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|doc/whats_new.rst|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,72,0.011843079200592153,0,6,false,Adding a fit_predict method for the GMM With low iterations the prediction might not be 100% accurate due tothe final maximization step in the EM algorithm,,2655,0.7578154425612053,0.12065136935603257,42228,503.1732499763191,38.576300085251496,132.61343184616842,5287,54,1701,275,travis,clorenz7,amueller,false,amueller,1,1.0,4,1,1175,true,false,false,false,0,0,1,0,2,0,10
7101801,scikit-learn/scikit-learn,python,4592,1429050076,1429903425,1429903425,14222,14222,commits_in_master,false,false,false,80,2,2,2,5,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,10,0,10,9.224433333734503,0.20351611784215637,3,ragvrv@gmail.com,sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,3,0.0022271714922048997,0,0,false,Fix Float Resolution Bug on Gaussian Process Test Observed with:Ubuntu 14041 LTS (vagrant/VirtualBox)scikit-learn017dev0scipy0133numpy182Was causing:FAIL: sklearngaussian_processteststest_gaussian_processtest_2d----------------------------------------------------------------------Traceback (most recent call last):  File /vagrant/plangrid-workers/venv/local/lib/python27/site-packages/nose/casepy line 197 in runTest    selftest(*selfarg)  File /vagrant/scikit-learn/sklearn/gaussian_process/tests/test_gaussian_processpy line 69 in test_2d    assert_true(npall(gptheta_  thetaL))  # Lower bounds of hyperparametersAssertionError: False is not true    False is not true  self_formatMessage(False is not true %s is not true % safe_repr(False))  raise selffailureException(False is not true)    ----------------------------------------------------------------------because 00001  1e-4,,2654,0.7577241899020347,0.12100965107646622,42228,503.1732499763191,38.576300085251496,132.61343184616842,5287,53,1701,270,travis,clorenz7,GaelVaroquaux,false,GaelVaroquaux,0,0,4,1,1175,false,false,false,false,0,0,0,0,0,0,122
7098311,scikit-learn/scikit-learn,python,4590,1429036998,1429118561,1429118561,1359,1359,commits_in_master,false,false,false,54,2,2,2,10,0,12,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,26,0,26,12.736642400855983,0.2810062842371156,4,t3kcit@gmail.com,sklearn/utils/testing.py|sklearn/utils/tests/test_testing.py|sklearn/utils/testing.py,4,0.0029717682020802376,0,3,false,Fixing bug in assert_raise_message #4559 assert_raise_message now only calls assert_raises_regex with the message argument changed into a literal texttrashing assert_raise_message and using directly assert_raises_regex as suggested in the bug report was not possible because some of the string message had non-alphanumeric charactersFinally the testing has been updated to raise the expected error,,2653,0.7576328684508103,0.1188707280832095,42227,503.1851658891231,38.57721363108911,132.61657233523573,5287,53,1701,271,travis,jfraj,ogrisel,false,ogrisel,0,0,2,0,264,false,false,false,false,1,0,0,0,0,0,13
7100314,scikit-learn/scikit-learn,python,4589,1429036073,1429044669,1429044669,143,143,merged_in_comments,false,false,false,29,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.0287847215970665,0.08888636337400575,13,olivier.grisel@ensta.org,sklearn/neighbors/base.py,13,0.009658246656760773,0,0,false,[MRG]  trying to make k_neighbors warning even more explicit  before it didnt say that %d was n_samples which I find slightly confusingMerging if travis is green as minor,,2652,0.7575414781297134,0.1188707280832095,42227,503.1851658891231,38.57721363108911,132.61657233523573,5286,53,1701,268,travis,amueller,amueller,true,amueller,286,0.8461538461538461,1130,40,1635,true,true,false,false,193,1778,131,510,262,10,-1
7087689,scikit-learn/scikit-learn,python,4588,1428978681,1429004582,1429004582,431,431,commits_in_master,false,false,false,48,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,47,0,47,0,8.474619287005341,0.18697392091560305,18,trev.stephens@gmail.com,sklearn/ensemble/forest.py|sklearn/tree/tree.py,14,0.010408921933085501,0,3,false,[MRG] dont to input validation in each tree for RandomForestpredict Fixes #4556pythonfrom sklearnensemble import RandomForestClassifierfrom sklearndatasets import fetch_mldatafrom time import timerf  RandomForestClassifier(n_estimators1000 max_depth1)mnist  fetch_mldata(MNIST original)rffit(mnistdata mnisttarget)start  time()rfpredict(mnistdata)print(time() - start)master: 3137sthis branch: 867s,,2651,0.7574500188608072,0.11895910780669144,42218,503.1503150315032,38.58543749111754,132.59747027334313,5281,53,1700,268,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,285,0.8456140350877193,1130,40,1634,true,true,true,false,194,1788,134,527,262,10,1
7087271,scikit-learn/scikit-learn,python,4586,1428976439,,1438099579,152052,,unknown,false,false,false,34,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.377592029467541,0.09658198418110514,1,amueller@nyu.edu,sklearn/semi_supervised/label_propagation.py,1,0.0007446016381236039,0,0,false,Added parameter for the unlabaled target in semi_supervised estimators I should be able to define my own unlabeled target instead of the value -1 that was usedThere is no reason to be static,,2650,0.7577358490566037,0.11839166046165302,42218,503.1503150315032,38.58543749111754,132.59747027334313,5281,53,1700,345,travis,chrsrds,chrsrds,true,,0,0,0,1,2,false,false,false,false,0,0,0,0,0,0,8
7085683,scikit-learn/scikit-learn,python,4585,1428969719,,1429050005,1338,,unknown,false,false,false,14,3,2,0,11,0,11,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,32,30,32,17.83170183112291,0.39341575122935607,13,t3kcit@gmail.com,sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py,8,0.005934718100890208,0,1,false,[MRG] astype sparse matrix Changes astype() in utils/fixespy which failed on SciPy sparse matrices,,2649,0.7580218950547376,0.11943620178041543,42214,502.7952811863363,38.54171601838253,132.56265693845643,5280,54,1700,269,travis,sseg,ogrisel,false,,0,0,0,0,354,false,false,false,false,0,1,0,0,0,0,1
7081689,scikit-learn/scikit-learn,python,4584,1428954396,1428972466,1428972466,301,301,commits_in_master,false,false,false,7,3,1,3,3,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,19,20,66,43,8.862256125235945,0.1955254290429723,8,t3kcit@gmail.com,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,4,0.0029542097488921715,0,1,false,[ENH] implements fit_predict on pipelines  FIX #4572,,2648,0.7579305135951662,0.12186115214180207,42214,502.7952811863363,38.54171601838253,132.56265693845643,5279,54,1700,268,travis,sdegryze,ogrisel,false,ogrisel,0,0,0,2,1510,false,true,false,false,0,1,0,0,0,0,42
7080711,scikit-learn/scikit-learn,python,4583,1428950861,1440939660,1440939660,199813,199813,merged_in_comments,false,false,false,165,31,7,14,18,0,32,0,9,0,0,2,4,2,0,0,0,0,4,4,2,0,0,231,69,333,131,26.798513403070046,0.5912479572699794,22,thomas.unterthiner@gmx.net,sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,15,0.01107011070110701,0,4,false,ShuffleLabelsOut cross-validation iterator This PR implements a stochastic variant of leave-p-labels-out CV splitting  Rather than iterating over all n_classes \choose p subsets of labels for the test points a user-specified number (fraction) of the labels are randomly selected  The number of iterations is also user-specified as in ShuffleSplit---This is similar to #4444  except that no balance is maintained at the level of individual samplesIm not sure if theres another PR implementing this exactly this functionality already but I couldnt find any  However if I missed it and someones already implemented this feel free to ignore---A realistic example use case for this is in music information retrieval where it is common to have a collection of recordings indexed by artist each artist has a small number of data points and all of an artists points should reside on one side of the train/test split  In this setting the number of labels is large so an exhaustive LeavePLabelsOut search is undesirable,,2647,0.7578390630902909,0.12250922509225093,42214,502.7952811863363,38.54171601838253,132.56265693845643,5278,54,1700,359,travis,bmcfee,glouppe,false,glouppe,0,0,175,141,1248,true,true,false,false,0,1,0,0,5,0,217
7067150,scikit-learn/scikit-learn,python,4581,1428856929,1428856998,1428856998,1,1,commits_in_master,false,false,false,18,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.879796053445575,0.10766144435121773,70,trev.stephens@gmail.com,doc/whats_new.rst,70,0.051319648093841645,0,0,false,Correct spelling of my name in whats_newrst Correct the spelling of my name Its Erich with h -),,2646,0.7577475434618292,0.12463343108504399,42214,502.7952811863363,38.54171601838253,132.56265693845643,5257,55,1699,267,travis,kno10,agramfort,false,agramfort,2,1.0,6,1,745,false,false,false,false,0,13,1,2,1,1,-1
7062364,scikit-learn/scikit-learn,python,4576,1428800619,,1428800879,4,,unknown,false,false,false,41,14,14,0,0,0,0,0,0,1,1,8,10,7,0,0,1,1,8,10,7,0,0,298,144,298,144,83.98030688801481,1.852818062953487,114,trev.stephens@gmail.com,sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/whats_new.rst|sklearn/tree/tree.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/validation.py,72,0.009339080459770116,0,0,false,PERF: Prevent using copy in astype if not needed I went over all instances of astype in the code and if a copy was not needed because it was assigned back to the same variable used utilsfixesastype with copyFalseResolves #4573 ,,2645,0.7580340264650284,0.125,42214,502.7952811863363,38.54171601838253,132.56265693845643,5248,55,1698,269,travis,jrings,jrings,true,,1,0.0,10,6,1161,false,true,false,false,0,1,1,0,2,0,-1
7062067,scikit-learn/scikit-learn,python,4575,1428798035,,1428800625,43,,unknown,false,true,false,41,6,2,0,18,0,18,0,5,0,0,12,12,12,0,0,0,0,12,12,12,0,0,50,9,56,9,53.43854747785541,1.1789895714122285,65,wu@minus.com,sklearn/cluster/k_means_.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/random.py|sklearn/utils/validation.py|sklearn/datasets/base.py|sklearn/datasets/twenty_newsgroups.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/preprocessing/data.py|sklearn/utils/graph.py,24,0.004310344827586207,0,6,false,PERF: Prevent using copy in astype if not needed I went over all instances of astype in the code and if a copy was not needed because it was assigned back to the same variable used utilsfixesastype with copyFalseResolves #4573 ,,2644,0.7583207261724659,0.125,42214,502.7952811863363,38.54171601838253,132.56265693845643,5248,55,1698,279,travis,jrings,jrings,true,,0,0,10,6,1161,false,true,false,false,0,1,0,0,2,0,99
7058878,scikit-learn/scikit-learn,python,4574,1428774360,,1445342656,276138,,unknown,false,false,false,64,1,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,3.9805921502386856,0.08782193481440659,6,trev.stephens@gmail.com,sklearn/gaussian_process/gaussian_process.py,6,0.00425531914893617,2,2,false,WIP : allow to normalize y but not X in GP @rbardinet spotted that one cannot scale y and not X is GP Using a periodic kernel with known period scaling X is catastrophic but you still want to scale y maybe @cangermueller you can have a lookI am thinking of a new parameter normalize_y but we should not break backward compat,,2643,0.7586076428301173,0.12624113475177304,42214,502.7952811863363,38.54171601838253,132.56265693845643,5245,56,1698,391,travis,agramfort,glouppe,false,,43,0.9069767441860465,177,187,1956,true,true,true,false,8,153,17,59,22,0,17
7057274,scikit-learn/scikit-learn,python,4571,1428757245,,1428920170,2715,,unknown,false,false,false,32,1,1,0,20,0,20,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,0,8,0,4.258248740977952,0.09394814662768056,0,,sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsefuncs_fast.pyx,0,0.0,0,10,false,Raise to the power of 2 instead of multiplying by the same value This makes the code a bit more readable and also saves access to the same variable (or array) twice,,2642,0.75889477668433,0.1259731068648266,42208,502.0138362395754,38.49981046247157,132.29719484457922,5244,56,1698,271,travis,thedrow,glouppe,false,,2,0.5,105,1357,2268,false,false,false,false,0,0,2,0,0,0,1580
7057086,scikit-learn/scikit-learn,python,4570,1428754997,1428851915,1428851915,1615,1615,commits_in_master,false,false,false,34,1,1,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,0,24,0,4.330208226684021,0.09553576180133376,4,xin_soft@foxmail.com,sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx,4,0.0028308563340410475,0,0,false,Code readability improvement for K-Means Those for loops shouldnt be evaluated if the condition isnt true anyway so why not intend themAs a bonus range and enumerate wont be called in these cases,,2641,0.7588034835289663,0.1259731068648266,42208,502.0138362395754,38.49981046247157,132.29719484457922,5244,56,1698,271,travis,thedrow,glouppe,false,glouppe,1,0.0,105,1357,2268,false,false,false,false,0,0,1,0,0,0,161
7051839,scikit-learn/scikit-learn,python,4568,1428711134,,1428955905,4079,,unknown,false,false,false,29,1,1,0,8,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.165971777246065,0.09191227454824691,11,trev.stephens@gmail.com,sklearn/cluster/k_means_.py,11,0.007757404795486601,0,0,true,Check if X is sparse only once when calculating centers Its probably not a very significant performance improvement but Im sure it does make the loop a bit faster,,2640,0.759090909090909,0.12552891396332863,42208,502.0138362395754,38.49981046247157,132.29719484457922,5233,56,1697,271,travis,thedrow,ogrisel,false,,0,0,105,1357,2267,false,false,false,false,0,0,0,0,0,0,3992
7048947,scikit-learn/scikit-learn,python,4566,1428700680,1429121430,1429121430,7012,7012,commit_sha_in_comments,false,true,false,27,7,5,5,12,2,19,0,4,0,0,3,3,1,0,0,0,0,3,3,1,0,0,3,0,3,0,13.911300972878387,0.30691424089746777,11,trev.stephens@gmail.com,doc/modules/tree.rst|sklearn/tree/tree.py|doc/tutorial/basic/tutorial.rst,11,0.0007032348804500703,0,3,true, Add conventions section to userguide #4508  Adds conventions section with two subsections about a) type casting and b) updating model parameters to [quick-start userguide](http://scikit-learnorg/stable/tutorial/basic/tutorialhtml)  See #4508,,2638,0.7592873388931008,0.12517580872011252,42206,501.46898545230533,38.477941524901674,132.09022413874806,5229,56,1697,273,travis,cangermueller,amueller,false,amueller,3,0.3333333333333333,10,0,655,true,false,false,false,1,18,3,0,13,0,16
7037011,scikit-learn/scikit-learn,python,4563,1428634598,1428638308,1428638308,61,61,commits_in_master,false,false,false,5,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.0179396601868325,0.08864438259765026,6,trev.stephens@gmail.com,sklearn/decomposition/fastica_.py,6,0.004198740377886634,0,0,false,Improve FastICA convergence warning message ,,2637,0.7591960561243838,0.1245626312106368,42207,501.45710427180325,38.477029876560756,132.08709455777478,5213,56,1696,266,travis,mspacek,GaelVaroquaux,false,GaelVaroquaux,1,1.0,4,3,1421,false,true,false,false,0,0,1,0,0,0,61
7034793,scikit-learn/scikit-learn,python,4562,1428623894,1435192238,1435192238,109472,109472,commits_in_master,false,false,false,126,8,5,1,10,0,11,0,6,0,0,2,4,2,0,0,0,0,4,4,3,0,0,17,0,139,2,12.51361245176341,0.27607717976562546,0,,sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx,0,0.0,0,7,false,Emptylabelset When using sklearn/datasets/svmlight_formatpy to load multi-label data sets it is possible that the label set is empty For instance line 10 of http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/multilabel/mediamill/train-exp1svmbz2In that case the original code would wrongly try to parse the first feature index:feature as the label-set list (which should be a string with comma as delimiters) causing parsing exceptionsThe fix is to detect : when parsing the label-set string If detected then the label-set should be empty and the string containing : should belong to features insteadThe fix also improve earlier code that copies line_parts (the parsed tokens) to features (the features to be parsed) but almost never uses the latter Now all the feature-parsing part uses the copied list variable features instead of line_partsThanks for reviewing,,2636,0.7591047040971168,0.1245626312106368,42207,501.45710427180325,38.477029876560756,132.08709455777478,5213,56,1696,320,travis,hsuantien,agramfort,false,agramfort,0,0,40,0,421,true,false,false,false,0,0,0,0,3,0,15
7032976,scikit-learn/scikit-learn,python,4561,1428616914,1428617305,1428617305,6,6,commits_in_master,false,false,false,34,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.284162111778326,0.09451782193493077,4,trev.stephens@gmail.com,sklearn/decomposition/fastica_.py,4,0.0027874564459930314,0,0,false,DOC fix fastica source matrix output shape Not only does it not make much sense the way its documented Ive confirmed via testing that the output S matrix really does have shape (n_samples n_components),,2635,0.7590132827324478,0.12613240418118468,42207,501.45710427180325,38.477029876560756,132.08709455777478,5213,56,1696,265,travis,mspacek,amueller,false,amueller,0,0,4,3,1421,false,true,false,false,0,0,0,0,0,0,6
7031818,scikit-learn/scikit-learn,python,4560,1428612295,1428707558,1428707558,1587,1587,commits_in_master,false,false,false,75,4,4,3,9,0,12,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,89,99,89,99,30.531544126026784,0.6735881094790623,27,trev.stephens@gmail.com,sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py|sklearn/utils/validation.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_utils.py,20,0.0041841004184100415,0,1,false,[MRG] shuffle pass-through I noticed while working on my pycon tutorial that the change in check_array in 0160 make shuffle try to convert its inputs to numpy arrays with a numeric dtype by default In 0152 shuffle would happily shuffle arrays with dtypeobjectHowever I think it should not even use check_array but instead rely on safe_indexing as cross-val and param-search tools doI think we should backport this to 016X for inclusing in 0161,,2634,0.7589217919514047,0.12622036262203626,42207,501.45710427180325,38.477029876560756,132.08709455777478,5213,56,1696,268,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,117,0.8547008547008547,1135,124,2143,true,true,true,true,41,397,69,295,163,5,30
7026487,scikit-learn/scikit-learn,python,4555,1428592430,1428766812,1428766812,2906,2906,commits_in_master,false,false,false,25,3,3,8,6,0,14,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,50,2,50,13.570986367980215,0.29940362706979523,9,t3kcit@gmail.com,sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|sklearn/utils/tests/test_fixes.py,5,0.0035014005602240898,0,8,false,[MRG] Fix the behavior of astype copytrue The astype backport should copy memory by default to be consistent with the default behavior of recent numpy,,2633,0.7588302316748956,0.1253501400560224,42207,501.45710427180325,38.477029876560756,132.08709455777478,5211,56,1696,270,travis,ogrisel,jnothman,false,jnothman,116,0.853448275862069,1135,124,2143,true,true,false,false,41,392,68,288,159,5,90
7026355,scikit-learn/scikit-learn,python,4554,1428591653,1429224459,1429224459,10546,10546,merged_in_comments,false,true,false,212,34,15,3,13,2,18,0,5,1,1,11,26,8,0,0,1,1,24,26,20,0,1,335,225,526,326,111.14424180579303,2.452068569097638,121,trev.stephens@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/gradient_boosting.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/install.rst|doc/whats_new.rst|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,70,0.0070028011204481795,0,3,false,[WIP] Dont set / check order of sparse matrices HiI run in this issue when using the awesome tools in utils/validationpyPreliminaries------------------ about sparse matrix order flags: pythonIn [4]: Xtodense()Out[4]: matrix([[0 1 2]        [3 4 5]        [6 7 8]])In [5]: XdataflagsOut[5]:   C_CONTIGUOUS : True  F_CONTIGUOUS : True  OWNDATA : False  WRITEABLE : True  ALIGNED : True  UPDATEIFCOPY : FalseThe data of a sparse matrix is stored in a 1 dim array for which both the C & F flags are trueMessing with the data sets both flags to falsepythonIn [17]: Xdata  Xdata[::-1]In [18]: XdataflagsOut[18]:   C_CONTIGUOUS : False  F_CONTIGUOUS : False  OWNDATA : False  WRITEABLE : True  ALIGNED : True  UPDATEIFCOPY : FalseThis is nice since the sparse matrix is now totally messed uppythonIn [19]: Xtodense()Out[19]: matrix([[0 2 1]        [8 7 6]        [5 4 3]])Conclusion-------------------1) the order flag for sparse matrices needs no checking2) forcing the flag as in _ensure_sparse_format is actually harmfullyas it hides the warning that someone messed with Xdata3) The tests for sparse matrices in test_validation/test_ordering() should be removedPlease let me know if this makes sense and should be fixedBestImmanuel,,2632,0.7587386018237082,0.1253501400560224,42207,501.45710427180325,38.477029876560756,132.08709455777478,5211,56,1696,274,travis,ibayer,ibayer,true,ibayer,17,0.6470588235294118,9,0,1132,true,true,false,false,0,0,0,0,2,0,11
7016463,scikit-learn/scikit-learn,python,4552,1428536833,,1438646051,168486,,unknown,false,false,false,53,2,1,3,14,0,17,0,6,0,0,5,5,5,0,0,0,0,5,5,5,0,0,150,2,162,2,22.451988702316214,0.4953366446719092,50,trev.stephens@gmail.com,sklearn/base.py|sklearn/pipeline.py|sklearn/preprocessing/label.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,35,0.0056022408963585435,0,16,false,[WIP] Allow transformers on y Shot at #4143Adds a new function to the interface with the uninspired name pipeexample:pythonfrom sklearnpreprocessinglabel import _LabelTransformerfrom sklearnpreprocessing import StandardScalerfrom sklearndatasets import make_regressionclass TargetScaler(StandardScaler _LabelTransformer):    passX y  make_regression()Xt yt  TargetScaler()fit_pipe(X y)print(ymean())print(ytmean()) 155246170559-111022302463e-18,,2631,0.7590269859369061,0.1253501400560224,42207,501.45710427180325,38.477029876560756,132.08709455777478,5204,55,1695,416,travis,amueller,amueller,true,,284,0.8485915492957746,1125,40,1629,true,true,false,false,191,1772,138,540,267,11,2
7014386,scikit-learn/scikit-learn,python,4550,1428529438,1430403366,1430403366,31232,31232,commits_in_master,false,true,false,24,13,4,9,11,0,20,0,6,1,0,5,8,4,0,0,1,0,7,8,5,0,0,486,743,507,820,70.87079908359549,1.5635543153317049,59,vmehta94@gmail.com,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|doc/developers/index.rst|doc/modules/classes.rst|sklearn/base.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_estimator_checks.py|doc/developers/index.rst|doc/modules/classes.rst|sklearn/base.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_estimator_checks.py,35,0.011220196353436185,0,6,false,WIP Common test refactoring Fixes #3810Todo:* [ ] tests* [ ] make sure all tests are run as they were before,,2630,0.75893536121673,0.12552594670406733,42207,501.45710427180325,38.477029876560756,132.08709455777478,5202,55,1695,274,travis,amueller,ogrisel,false,ogrisel,283,0.8480565371024735,1125,40,1629,true,true,true,false,190,1767,137,539,267,11,14
7000132,scikit-learn/scikit-learn,python,4542,1428460749,1428612789,1428612789,2534,2534,commits_in_master,false,false,false,84,3,3,3,11,0,14,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,13.876172935017024,0.3061382296621088,2,amueller@nyu.edu,doc/install.rst|doc/install.rst|doc/install.rst,2,0.0013831258644536654,0,1,false,DOC add warning for partial pip upgrades Add warnings to the install doc to make it explicit that as of now pip cannot properly upgrade scikit-learn if it was installed with python setuppy install or by conda / AnacondaThis issue caused problems such as #4472Note: future versions of pip will refuse to uninstall (and therefore upgrade) packages that lacks the missing metadata in particular the list of installed files that is usually stored in the old-style site-packages/package_name-versionegg-info/installed-filestxt or in the new-style site-packages/package_name-versiondist-info/RECORD,,2629,0.7588436667934576,0.12932226832641772,42207,501.45710427180325,38.477029876560756,132.08709455777478,5187,55,1694,265,travis,ogrisel,ogrisel,true,ogrisel,115,0.8521739130434782,1134,124,2141,true,true,false,false,41,390,67,287,158,5,0
6996047,scikit-learn/scikit-learn,python,4541,1428444116,1429037123,1429037123,9883,9883,commits_in_master,false,false,false,22,2,1,11,7,0,18,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,20,25,40,9.02302545411777,0.19906735464135164,26,trev.stephens@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,21,0.014492753623188406,0,5,false,FIX be robust to columns name dtype robust dtype checking Fixes #4540 and makes checks robust to thing holding dtype object data,,2628,0.758751902587519,0.1276742581090407,42207,501.45710427180325,38.477029876560756,132.08709455777478,5185,55,1694,272,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,282,0.8475177304964538,1125,40,1628,true,true,true,false,190,1788,137,555,267,11,1
6981089,scikit-learn/scikit-learn,python,4537,1428367477,1428371176,1428371176,61,61,commits_in_master,false,false,false,7,2,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,6,0,4.137451453709024,0.09128104258705781,9,olivier.grisel@ensta.org,sklearn/calibration.py,9,0.006202618883528601,0,1,false,FIX make CalibratedClassifierCV deterministic by default #4536,,2626,0.758948971820259,0.12818745692625774,42207,501.45710427180325,38.477029876560756,132.08709455777478,5170,55,1693,258,travis,amueller,amueller,true,amueller,281,0.8469750889679716,1123,40,1627,true,true,false,false,188,1754,138,552,267,12,11
6978181,scikit-learn/scikit-learn,python,4534,1428355814,1429567155,1429567155,20189,20189,commits_in_master,false,false,false,223,14,4,21,42,0,63,0,3,0,0,2,3,2,0,0,0,0,3,3,3,0,0,311,4,414,309,34.472219029779644,0.7605310004318897,18,xuewei4d@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,14,0.009621993127147767,0,3,false,[WIP] optimize rfecv by eliminating the for loop Fixes #4382  I am not sure this is good enough Any comment is warmly welcome Basically in this PR RFE has  _fit which is almost identical to previous fit _fit has a third parameter step_score which is a function evaluating the score of a parameter step_score takes estimator and support then gives a score In RFECV there is no for loop right now For each fold one rfe_fit is called with the return value of _step_score_on_test _step_score_on_test acts like a factory method It accepts X_test y_test and builds a step_score function which use _score just like the implementation in the for loop before this PR By doing so RFE is not aware of the test data Because I think it is weird that implementing a new fit function in RFE which explicitly accepts test data to get the scoreThe number of subsets of features needs to be evaluated is ceil((n_features - n_features_to_select)/step) + 1 For example n_features  10 n_features_to_select1 step2 The sizes of each subsets of features are [10 8 6 4 2 1] In RFEfits for loop the subsets which has size of [10 8 6 4 2] is evaluated The last one is evaluated in the final estimators fit after for loop This explains the size of scores_ in RFE,,2624,0.7591463414634146,0.12920962199312716,42207,501.45710427180325,38.477029876560756,132.08709455777478,5170,55,1693,273,travis,xuewei4d,amueller,false,amueller,6,0.5,14,69,1245,true,true,true,false,7,48,6,7,19,0,3
6977150,scikit-learn/scikit-learn,python,4530,1428351611,1428852207,1428852207,8343,8343,commits_in_master,false,false,false,7,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.407661659567843,0.09724245859022215,28,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py,28,0.019230769230769232,1,0,false,[MRG] DOC GradientBoostingXestimators_ docstring shape ping @pprett ,,2622,0.7593440122044242,0.12843406593406592,42207,501.45710427180325,38.477029876560756,132.08709455777478,5169,55,1693,269,travis,amueller,glouppe,false,glouppe,279,0.8494623655913979,1123,40,1627,true,true,true,true,186,1748,137,563,268,12,8343
6976929,scikit-learn/scikit-learn,python,4529,1428350630,,1428350925,4,,unknown,false,false,false,58,1,1,0,4,0,4,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,6,0,6,4.586230940382968,0.10118207946775865,14,xuewei4d@gmail.com,sklearn/feature_selection/tests/test_rfe.py,14,0.009621993127147767,0,0,false,add set_random_state in rfe test In test_rfe_features_importance I find sometimes RandomForestClassifier will generate different support_ values and cause the test failed (got this failure in PR #3659)So I add set_random_state for each classifier in the test(Note: For RandomForestClassifier I have to set random state to 1 to pass the test For SVC any state works fine),,2621,0.7596337275848912,0.12783505154639174,42207,501.45710427180325,38.477029876560756,132.08709455777478,5169,55,1693,254,travis,chyikwei,chyikwei,true,,4,0.75,17,1,923,true,true,false,false,0,17,1,2,13,0,0
6972003,scikit-learn/scikit-learn,python,4527,1428320996,1430428699,1430428699,35128,35128,merged_in_comments,false,false,false,19,2,1,2,7,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,6,0,9,4.212181087272568,0.09292958593428663,5,trev.stephens@gmail.com,sklearn/utils/testing.py,5,0.0034411562284927736,0,0,false,Fix for issue #4524     - Changed import to import matplotlibpyplt as plt    - Narrowed exception to catch the ImportError,,2619,0.759831996945399,0.12801101169993118,42207,501.314947757481,38.477029876560756,132.0634018053877,5158,55,1693,273,travis,jcm1317,amueller,false,amueller,0,0,1,0,727,false,false,false,false,1,0,0,0,0,0,4
6968581,scikit-learn/scikit-learn,python,4526,1428284685,1430428648,1430428648,35732,35732,commits_in_master,false,false,false,10,4,1,13,17,0,30,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,7,0,19,4.214755268524401,0.09298637779410848,5,trev.stephens@gmail.com,sklearn/utils/testing.py,5,0.0034411562284927736,0,0,false,Changed import statement and checked for ImportError in if_matplotlib #4524 ,,2618,0.7597402597402597,0.12801101169993118,42207,501.314947757481,38.477029876560756,132.0634018053877,5156,55,1692,273,travis,vortex-ape,amueller,false,amueller,13,0.9230769230769231,8,15,702,true,false,true,false,4,125,13,30,74,0,4
6968079,scikit-learn/scikit-learn,python,4525,1428280499,,1441963573,228051,,unknown,false,false,false,7,5,4,10,8,0,18,0,6,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,19.392623165996227,0.42784210926757527,3,trev.stephens@gmail.com,doc/modules/neighbors.rst|doc/modules/neighbors.rst|doc/modules/neighbors.rst|doc/modules/neighbors.rst,3,0.0020646937370956643,0,2,false,Listed valid metrics for neighbors algorithms #4521 ,,2617,0.7600305693542224,0.12801101169993118,42207,501.314947757481,38.477029876560756,132.0634018053877,5156,55,1692,360,travis,vortex-ape,vortex-ape,true,,12,1.0,8,15,702,true,false,false,false,4,120,12,29,74,0,1
6962146,scikit-learn/scikit-learn,python,4519,1428217000,1428230282,1428230282,221,221,commits_in_master,false,false,false,12,1,1,0,1,0,1,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.39705857724455,0.09700838996506637,0,,doc/modules/preprocessing.rst,0,0.0,0,0,false,DOC Grammar fix in preprocessing Just a minor grammar fix:[image](https://cloudgithubusercontentcom/assets/7017045/6995696/096025c0-db0d-11e4-982c-88f60e4ce8b0png)[image](https://cloudgithubusercontentcom/assets/7017045/6995695/fee0ed78-db0c-11e4-8bb6-f5c00ed3a889png),,2615,0.7602294455066921,0.12836438923395446,42208,501.3030705079605,38.47611827141774,132.06027293404094,5150,54,1692,252,travis,ClimbsRocks,agramfort,false,agramfort,0,0,109,95,381,false,false,false,false,0,0,0,0,0,0,8
6960856,scikit-learn/scikit-learn,python,4517,1428200540,1428345922,1428345922,2423,2423,commit_sha_in_comments,false,false,false,10,5,2,0,8,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,34,0,8.40966619227919,0.18553498051597184,3,t3kcit@gmail.com,sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py,3,0.002070393374741201,0,1,false,Univariate fast_mcd: npreshape(X (-1 1)) Fix issue #4512 as stated,,2614,0.7601377199693956,0.12836438923395446,42208,501.3030705079605,38.47611827141774,132.06027293404094,5149,54,1691,255,travis,ThatGeoGuy,ThatGeoGuy,true,ThatGeoGuy,0,0,8,6,622,false,false,false,false,0,0,0,0,0,0,7
6958428,scikit-learn/scikit-learn,python,4516,1428180649,1428231875,1428231875,853,853,commits_in_master,false,false,false,9,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.601766344321884,0.10152467523046353,6,trev.stephens@gmail.com,sklearn/metrics/pairwise.py,6,0.004143646408839779,0,0,false,[MRG] DOC try to clarify pairwise_distances docstring for Y ,,2613,0.7600459242250287,0.12845303867403315,42208,501.3030705079605,38.47611827141774,132.06027293404094,5147,54,1691,251,travis,amueller,agramfort,false,agramfort,276,0.855072463768116,1123,40,1625,true,true,true,false,185,1690,134,558,265,9,37
6953496,scikit-learn/scikit-learn,python,4514,1428123178,1428171065,1428171065,798,798,commits_in_master,false,false,false,16,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,0,7,0,9.143233215181375,0.20171932758302485,11,trev.stephens@gmail.com,sklearn/isotonic.py|sklearn/learning_curve.py,11,0.007607192254495159,0,0,true,CLN: Added missing __all__ Added __all__ attributes to isotonic and learning_curves listing methods/classes on API doc,,2611,0.7602451168134814,0.12863070539419086,42205,501.3387039450302,38.478853216443554,132.06965999289184,5141,54,1690,249,travis,sinhrks,agramfort,false,agramfort,1,1.0,49,4,1067,false,false,false,false,0,0,1,0,2,0,798
6951372,scikit-learn/scikit-learn,python,4511,1428108529,1441820534,1441820534,228533,228533,merged_in_comments,false,true,false,19,10,1,0,22,0,22,0,5,0,0,3,23,3,0,0,0,0,23,23,23,0,0,27,2,99,144,13.13918420283608,0.28987857358651214,58,trev.stephens@gmail.com,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py,40,0.016597510373443983,0,6,true,FIX make check_array reshape sensibly reshape in check_arrays for  ndim1 using reshape(-1 1) not reshape(1 -1)See 4509 #4466,,2610,0.7601532567049808,0.12863070539419086,42205,501.3387039450302,38.478853216443554,132.06965999289184,5140,54,1690,354,travis,amueller,amueller,true,amueller,275,0.8545454545454545,1123,40,1624,true,true,false,false,184,1690,133,558,265,9,1231
6945275,scikit-learn/scikit-learn,python,4507,1428081178,,1429036326,15919,,unknown,false,true,false,36,4,1,1,16,0,17,0,5,0,0,2,3,2,0,0,0,0,3,3,3,0,0,3,0,11,11,4.453553983384835,0.09825494917831096,2,loic.esteve@ymail.com,sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx,2,0.00138217000691085,0,3,true,Fix a bug in _k_meanspyx when KMeansfit(XT) bad case: PythonX  nparray([[0 0 0] [0 1 1]])estimator  KMeans(n_clusters2 n_init1 max_iter20 \                   precompute_distancesFalse)estimatorfit(XT)print XTprint estimatorlabels_print estimatorcluster_centers_print estimatorn_iter_,,2609,0.7604446147949406,0.12854181064270906,42205,501.3387039450302,38.478853216443554,132.06965999289184,5135,54,1690,271,travis,zhaipro,amueller,false,,1,1.0,1,0,214,false,false,false,false,0,0,1,0,0,0,201
6945111,scikit-learn/scikit-learn,python,4506,1428080344,1428345093,1428345093,4412,4412,commits_in_master,false,false,false,72,2,2,0,5,0,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,7,2,7,8.605990420371503,0.18986659969482753,11,trev.stephens@gmail.com,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,7,0.0048375950241879755,0,0,true,fixed reverse sorting issue in label_binarize() label_binarize was not properly setting indices when the classes keyword argument was setEg previous output:In [75]: label_binarize([0123] classes[3201])Out[75]: array([[0 0 0 1]       [0 0 1 0]       [1 0 0 0]       [0 1 0 0]])Output after this PR:In [2]: label_binarize([0123] classes[3201])Out[2]: array([[0 0 1 0]       [0 0 0 1]       [0 1 0 0]       [1 0 0 0]]),,2608,0.7603527607361963,0.12854181064270906,42205,501.3387039450302,38.478853216443554,132.06965999289184,5135,54,1690,254,travis,mheilman,amueller,false,amueller,0,0,21,4,1906,false,false,false,false,0,0,0,0,1,0,9
6942807,scikit-learn/scikit-learn,python,4505,1428063905,1428065706,1428065706,30,30,commits_in_master,false,false,false,68,1,1,0,4,0,4,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,0,14,0,4.501102102010341,0.09930448854237771,2,olivier.grisel@ensta.org,sklearn/_isotonic.c|sklearn/_isotonic.pyx,2,0.0013879250520471894,0,1,true,FIX _make_unique of _isotonicpyx avoids coercing float64 to float This addresses issue #4460Implicit conversion from float64 to float in _make_unique could result in two different entries of X to look identical when converting them to float removes the (tiny) difference This in turn results in uninitialized entries in x_out y_out and weights_out at the end of _make_unique because the final value of i is smaller than unique_values,,2607,0.7602608362102034,0.12907702984038863,42205,501.3387039450302,38.478853216443554,132.06965999289184,5133,54,1690,246,travis,jmetzen,GaelVaroquaux,false,GaelVaroquaux,17,0.5882352941176471,14,2,1271,true,true,false,false,2,52,4,45,248,0,1
6930002,scikit-learn/scikit-learn,python,4502,1428004383,1429484165,1429484165,24663,24663,commits_in_master,false,false,false,13,12,8,6,9,0,15,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,107,135,107,205,52.70586563629715,1.16280878375963,29,trev.stephens@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/tests/test_common.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py|sklearn/tests/test_common.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py,21,0.007670850767085077,0,0,false,Add test for custom get_params() functions tries to fix #4467 #4465 and #4461,,2605,0.7604606525911708,0.12761506276150628,42204,500.80561084257414,38.43237607809686,131.88323381669983,5127,53,1689,271,travis,banilo,amueller,false,amueller,5,0.4,5,0,744,true,false,false,false,0,3,2,2,3,0,19
6927161,scikit-learn/scikit-learn,python,4500,1427999717,1430929099,1430929099,48823,48823,merged_in_comments,false,false,false,76,13,2,35,17,0,52,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,168,109,369,331,17.353602963587683,0.3828591317555405,8,trev.stephens@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py,6,0.004195804195804196,0,2,false,Added warm_start to bagging BaggingClassifier and BaggingRegressor now support warm_starts Addedbasic tests and documentation of the new functionality Heavilyinspired by work on warm_start for Random forestsThere is a brief discussion of this in #3419 but it seemed deadQuestions:* How best to document that the oob score is only calculated on the last batch of training samples* What other invariants exist between cold and warm started classifiers that are worth testing,,2603,0.7606607760276604,0.12727272727272726,42204,500.80561084257414,38.43237607809686,131.88323381669983,5126,52,1689,269,travis,betatim,arjoly,false,arjoly,1,1.0,41,44,1139,true,false,false,false,0,0,3,0,6,0,2
6930079,scikit-learn/scikit-learn,python,4496,1427990442,1427997473,1427997473,117,117,commits_in_master,false,false,false,16,1,1,0,3,1,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,20,17,20,8.609599177438364,0.18994656319924977,16,xuewei4d@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,12,0.00851063829787234,0,1,false,[MRG+1] Fix RFE Add the possibility to use feature_importances rather than coef_ when existing see #2121,,2601,0.7608612072279892,0.12907801418439716,42157,500.62860260454966,38.4277818630358,131.8405009844154,5125,52,1689,243,travis,vmichel,amueller,false,amueller,2,0.5,17,1,1764,true,true,false,false,0,0,0,0,2,0,8
6929576,scikit-learn/scikit-learn,python,4493,1427984257,,1429484174,24998,,unknown,false,true,false,9,1,1,0,6,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,3,1,3,8.817939212085363,0.19454292998315376,11,trev.stephens@gmail.com,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,7,0.004978662873399715,0,0,false,FeatureUnion: add weights in get_params tries to address #4465,,2598,0.7617397998460355,0.12944523470839261,42157,500.62860260454966,38.4277818630358,131.8405009844154,5124,50,1689,273,travis,banilo,amueller,false,,4,0.5,5,0,744,true,false,false,false,0,3,1,2,3,0,7
6921546,scikit-learn/scikit-learn,python,4491,1427982778,1431115236,1431115236,52207,52207,merged_in_comments,false,false,false,76,9,3,57,26,0,83,0,9,0,0,4,4,3,0,0,0,0,4,4,3,0,0,974,233,1335,275,57.35027494169745,1.2652718797614293,20,trev.stephens@gmail.com,doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py,11,0.007112375533428165,0,10,false,rebasing pr/3474 (multioutput regression metrics) I have tried to rebase PR #3474 It it passes Travis I guess mentionned PR can be replacedBasically it is a support of multioutput support for regression metrics (MAE MSE R2 explained variance) in a way that we can get a whole array of scores instead of some kind of averaging BTW I corrected a small mistake in docs: it states that median absolute error supports multioutput while it doesnt,,2596,0.7619414483821263,0.12944523470839261,42157,500.62860260454966,38.4277818630358,131.8405009844154,5123,50,1689,272,travis,kshmelkov,amueller,false,amueller,0,0,2,1,59,false,false,false,false,0,0,0,0,1,0,6
6928386,scikit-learn/scikit-learn,python,4489,1427980608,1427986608,1427986608,100,100,github,false,false,false,38,2,2,0,4,0,4,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,8.981203169721532,0.1981448882089096,2,olivier.grisel@ensta.org,doc/modules/ensemble.rst|doc/modules/ensemble.rst,2,0.0014265335235378032,0,2,false,[MRG+1] Document oob estimates of RandomForest* and Bagging* Added a sentence each to the RandomForest* and Bagging* sections of the narrative documentation mentioning how to enable out-of-bag estimates of the generalisation error This is meant to fix #4290,,2594,0.7621434078643022,0.12981455064194009,42157,500.62860260454966,38.4277818630358,131.8405009844154,5123,50,1689,241,travis,betatim,betatim,true,betatim,0,0,41,44,1139,true,false,false,false,0,0,0,0,4,0,5
6928353,scikit-learn/scikit-learn,python,4488,1427980560,1428771452,1428771452,13181,13181,commits_in_master,false,false,false,22,11,8,16,19,2,37,0,6,2,1,3,7,4,0,0,2,1,4,7,4,0,0,489,163,496,182,62.45252588803908,1.377838639946191,19,trev.stephens@gmail.com,examples/tree/plot_tree_apply.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py|examples/tree/plot_tree_feat.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,15,0.0042796005706134095,1,5,false,[MRG] Public apply method for decision trees This supersedes #4065 I rebased @galv branch on top of master and rewrote the tests ,,2593,0.762051677593521,0.12981455064194009,42157,500.62860260454966,38.4277818630358,131.8405009844154,5123,50,1689,270,travis,glouppe,glouppe,true,glouppe,52,0.9615384615384616,158,26,1603,true,true,false,false,11,41,5,28,78,0,0
6926150,scikit-learn/scikit-learn,python,4486,1427977525,,1433183604,86767,,unknown,false,true,false,23,1,1,1,6,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.3977999428863255,0.09702503791319736,7,trev.stephens@gmail.com,sklearn/pipeline.py,7,0.005010737294201861,1,2,false,[Pipeline] add named_steps attribute documentation #4482 Added Attribute section with named_steps documentation + docstring exampleCompleted with @timvg80 at Paris Sprint April 2015,,2592,0.7623456790123457,0.13027916964924838,42157,500.62860260454966,38.4277818630358,131.8405009844154,5123,49,1689,287,travis,daveudaimon,amueller,false,,0,0,0,0,561,false,false,false,false,1,0,0,0,0,0,4
6926723,scikit-learn/scikit-learn,python,4484,1427970266,1427996131,1427996131,431,431,commits_in_master,false,false,false,8,1,1,0,3,0,3,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.474642154382178,0.09872034433471362,12,trev.stephens@gmail.com,doc/developers/index.rst,12,0.008602150537634409,1,0,false,Removes array2d from developer docs #4483 ping @GaelVaroquaux ,,2590,0.7625482625482626,0.13046594982078852,42157,500.62860260454966,38.4277818630358,131.8405009844154,5121,49,1689,243,travis,vortex-ape,amueller,false,amueller,11,1.0,8,15,699,true,false,true,false,4,110,11,22,66,0,4
6916178,scikit-learn/scikit-learn,python,4481,1427912656,1430939326,1430939326,50444,50444,commit_sha_in_comments,false,false,false,20,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.551538488855828,0.10041751243751378,14,trev.stephens@gmail.com,sklearn/cluster/k_means_.py,14,0.010050251256281407,0,0,false,DOC fix verbose docs in KMeans fix verbose docs in KMeansfix the parameter of verbose default value in KMeans__init__,,2589,0.7624565469293163,0.13208901651112706,42138,500.8543357539513,38.37391428164602,131.686363852105,5110,49,1688,271,travis,zhaipro,amueller,false,amueller,0,0,1,0,212,false,false,false,false,0,0,0,0,0,0,20
6906361,scikit-learn/scikit-learn,python,4478,1427861055,1444919987,1444919987,284315,284315,commits_in_master,false,true,false,19,3,1,2,30,0,32,0,5,0,0,1,4,1,0,0,0,0,4,4,4,0,0,4,0,16,24,4.295730084536059,0.09477378478689727,4,olivier.grisel@ensta.org,sklearn/utils/extmath.py,4,0.0028776978417266188,0,14,false,Fix randomized_svd transpose heuristic Fixes #4455This fixes two things: the heuristic ( vs ) and a logic error,,2588,0.7623647604327666,0.13237410071942446,42138,500.8543357539513,38.37391428164602,131.686363852105,5102,49,1687,362,travis,amueller,ogrisel,false,ogrisel,274,0.8540145985401459,1120,40,1621,true,true,true,false,175,1576,126,514,251,9,2
6895645,scikit-learn/scikit-learn,python,4476,1427822204,1427825995,1427825995,63,63,commits_in_master,false,false,false,6,2,1,0,3,0,3,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.682130328876056,0.10329888461926766,2,t3kcit@gmail.com,doc/modules/linear_model.rst,2,0.0014336917562724014,0,0,false,Added import statement for Lasso example ,,2587,0.7622729029764206,0.13189964157706094,42141,500.6763009895351,38.347452599606086,131.6532592961724,5099,49,1687,236,travis,CaMeLCa5e,agramfort,false,agramfort,0,0,6,11,245,false,false,false,false,0,0,0,0,1,0,7
6883589,scikit-learn/scikit-learn,python,4471,1427757570,1427833123,1427833123,1259,1259,commits_in_master,false,false,false,8,1,1,0,8,0,8,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,3.4965789793025706,0.07714278070674853,0,,examples/decomposition/plot_pca_vs_fa_model_selection.py,0,0.0,0,1,false,Typo: PCA is not the abbreviation of Probablisitic ,,2586,0.7621809744779582,0.13161659513590845,42138,500.68821491290527,38.35018273292515,131.63890075466324,5093,49,1686,236,travis,baharev,amueller,false,amueller,0,0,1,1,891,false,true,false,false,0,0,0,0,1,0,5
6873266,scikit-learn/scikit-learn,python,4467,1427696482,1429484118,1429484118,29793,29793,commits_in_master,false,true,false,100,3,3,6,11,0,17,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,102,66,102,66,48.74498474129817,1.0754293527217291,49,trev.stephens@gmail.com,sklearn/pipeline.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/pipeline.py|sklearn/tests/test_common.py|sklearn/tests/test_pipeline.py|sklearn/utils/estimator_checks.py|sklearn/pipeline.py|sklearn/tests/test_common.py|sklearn/tests/test_pipeline.py|sklearn/utils/estimator_checks.py,38,0.013571428571428571,0,1,false,BUG: get_param in FeatureUnion disjoint for shallow/deep params - Previously params in a FeatureUnion or Pipeline were differentwhen deepFalse and deepTrue making it impossible to set shallowparams during GridSearch (issue #4461)- Added test to enforce deepTrue results to be superset of deepFalse- Updated deep get_param output in pipeline estimators with shallow paramsTST: Test estimators for get_params invariance (issue #4465)- Estimators that support get_params should always be consistentin that get_params(deepFalse) should be a subset of get_params(deepTrue) - Implemented test over all normal and other estimators provided byall_estimators- Ignoring grid_search estimators for now,,2585,0.7620889748549323,0.13142857142857142,42138,500.68821491290527,38.35018273292515,131.63890075466324,5091,49,1686,276,travis,samzhang111,amueller,false,amueller,0,0,16,8,866,false,false,false,false,1,0,0,0,0,0,8
6872300,scikit-learn/scikit-learn,python,4464,1427670614,1427933537,1427933537,4382,4382,commits_in_master,false,false,false,6,1,1,3,2,0,5,0,4,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,8.69310376171665,0.19178983215364342,8,t3kcit@gmail.com,doc/modules/linear_model.rst|sklearn/linear_model/logistic.py,6,0.004252303330970942,0,1,false,Modifies LogisiticRegressionCV doc and docstring #4463 ,,2584,0.7619969040247678,0.13040396881644223,42138,500.6644833641843,38.35018273292515,131.63890075466324,5089,49,1685,237,travis,vortex-ape,amueller,false,amueller,10,1.0,8,15,695,true,false,true,false,4,101,10,21,59,0,3
6865030,scikit-learn/scikit-learn,python,4456,1427545809,1427933808,1427933808,6466,6466,commits_in_master,false,false,false,194,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.400989163109007,0.09709572152504908,3,trev.stephens@gmail.com,sklearn/neighbors/dist_metrics.pyx,3,0.0021231422505307855,0,0,false,Fix signature not compatible with previous declaration Fixes a compile error with Cython 022:Error compiling Cython file:------------------------------------------------------------        cdef DTYPE_t sin_0  sin(05 * (x1[0] - x2[0]))        cdef DTYPE_t sin_1  sin(05 * (x1[1] - x2[1]))        return 2 * asin(sqrt(sin_0 * sin_0                             + cos(x1[0]) * cos(x2[0]) * sin_1 * sin_1))    cdef inline DTYPE_t _rdist_to_dist(self DTYPE_t rdist):        ^------------------------------------------------------------dist_metricspyx:984:9: Signature not compatible with previous declarationError compiling Cython file:------------------------------------------------------------    cdef int pdist(self DTYPE_t[: ::1] X DTYPE_t[: ::1] D) except -1    cdef int cdist(self DTYPE_t[: ::1] X DTYPE_t[: ::1] Y                   DTYPE_t[: ::1] D) except -1    cdef DTYPE_t _rdist_to_dist(self DTYPE_t rdist) except -1                              ^------------------------------------------------------------dist_metricspxd:73:31: Previous declaration is hereError compiling Cython file:------------------------------------------------------------                             + cos(x1[0]) * cos(x2[0]) * sin_1 * sin_1))    cdef inline DTYPE_t _rdist_to_dist(self DTYPE_t rdist):        return 2 * asin(sqrt(rdist))    cdef inline DTYPE_t _dist_to_rdist(self DTYPE_t dist):        ^------------------------------------------------------------dist_metricspyx:987:9: Signature not compatible with previous declarationError compiling Cython file:------------------------------------------------------------    cdef int cdist(self DTYPE_t[: ::1] X DTYPE_t[: ::1] Y                   DTYPE_t[: ::1] D) except -1    cdef DTYPE_t _rdist_to_dist(self DTYPE_t rdist) except -1    cdef DTYPE_t _dist_to_rdist(self DTYPE_t dist) except -1                              ^------------------------------------------------------------dist_metricspxd:75:31: Previous declaration is here,,2581,0.7624951569159241,0.1316348195329087,42138,500.6644833641843,38.35018273292515,131.63890075466324,5077,49,1684,236,travis,cgohlke,amueller,false,amueller,4,1.0,32,0,1593,false,false,false,false,0,0,0,0,1,0,4
6835937,scikit-learn/scikit-learn,python,4451,1427392449,1427398264,1427398265,96,96,github,false,false,false,10,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.676833411265081,0.10318227439954267,15,trev.stephens@gmail.com,sklearn/ensemble/tests/test_gradient_boosting.py,15,0.010668563300142247,2,1,false,[MRG+1] DOC/TST docstring -- comment Following #4432 Ping @arjoly @pprett ,,2580,0.7624031007751938,0.12731152204836416,42117,504.5706009449866,38.70171189780849,132.67801600303918,5060,49,1682,233,travis,ragv,amueller,false,amueller,41,0.6097560975609756,3,1,146,true,true,false,false,17,399,44,320,267,0,6
6818989,scikit-learn/scikit-learn,python,4448,1427304239,1427389097,1427389097,1414,1414,commits_in_master,false,false,false,9,4,3,2,9,0,11,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,22,9,28,13.393735638029996,0.29549857097155063,29,trev.stephens@gmail.com,sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py,26,0.01863799283154122,1,2,false,Fix gbrt min leaf weight Fixes https://githubcom/scikit-learn/scikit-learn/issues/4447cc @arjoly ,,2579,0.762310973245444,0.12688172043010754,42117,503.95327302514426,38.67796851627609,132.60678585844195,5044,48,1681,232,travis,pprett,pprett,true,pprett,48,0.875,149,29,2059,true,true,false,false,1,5,0,0,5,0,10
6803716,scikit-learn/scikit-learn,python,4444,1427226226,,1427234120,131,,unknown,false,true,false,13,27,1,50,40,0,90,0,13,0,0,2,4,2,0,0,0,0,4,4,2,0,0,65,56,449,162,9.200805245868498,0.20299291649273887,30,trev.stephens@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,21,0.01507537688442211,0,11,false,Added subject independent KFold Added a SubjectIndependentKFold class to create subject independent folds,,2578,0.7626066718386346,0.12634601579325197,42117,503.95327302514426,38.67796851627609,132.60678585844195,5028,47,1680,345,travis,JeanKossaifi,amueller,false,,8,1.0,7,13,1387,false,true,true,false,0,0,0,0,0,0,11
6801637,scikit-learn/scikit-learn,python,4443,1427218624,,1430186993,49472,,unknown,false,false,false,14,2,1,0,8,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,6,0,3.804163269101617,0.08392941445600574,11,trev.stephens@gmail.com,sklearn/cross_decomposition/pls_.py,11,0.007874015748031496,0,1,false,FIX _PLS transform or predict with copyFalse Fix for bug as described in https://githubcom/scikit-learn/scikit-learn/issues/4122#issuecomment-78056759,,2577,0.7629025999223904,0.12598425196850394,42117,503.95327302514426,38.67796851627609,132.60678585844195,5026,47,1680,273,travis,hzpc-joostk,amueller,false,,0,0,0,2,105,false,false,false,false,0,2,0,0,0,0,7
6792301,scikit-learn/scikit-learn,python,4440,1427160965,,1427248560,1459,,unknown,false,false,false,21,5,5,0,14,0,14,0,5,0,0,6,6,1,0,0,0,0,6,6,1,0,0,3,0,3,0,27.50572545712093,0.6074572419523973,17,trev.stephens@gmail.com,doc/modules/tree.rst|sklearn/tree/tree.py|doc/datasets/index.rst|doc/datasets/labeled_faces.rst|doc/datasets/mldata.rst|doc/datasets/twenty_newsgroups.rst,17,0.0,0,7,false,Link to functions in dataset docs #4426  Corrects cross-references and some inline codes in datasets documentation See #4426 for more details,,2576,0.7631987577639752,0.12589413447782546,42126,503.3945781702511,38.645966861320794,132.53097849309214,5020,47,1679,235,travis,cangermueller,amueller,false,,2,0.5,10,0,637,true,false,false,false,0,10,2,0,13,0,1
6791343,scikit-learn/scikit-learn,python,4439,1427156742,1427247990,1427247990,1520,1520,commits_in_master,false,false,false,22,2,1,0,5,0,5,0,4,0,0,4,4,0,0,0,0,0,4,4,0,0,0,0,0,0,0,18.468027136132037,0.4078618775536582,0,,doc/datasets/index.rst|doc/datasets/labeled_faces.rst|doc/datasets/mldata.rst|doc/datasets/twenty_newsgroups.rst,0,0.0,0,3,false,[MRG] Link to functions and fix typos in dataset docs This PR solves issue #4426Also some typos and inconsistencies were fixed,,2575,0.7631067961165049,0.12535816618911175,42126,503.3945781702511,38.645966861320794,132.53097849309214,5019,47,1679,235,travis,bryandeng,ogrisel,false,ogrisel,1,1.0,43,161,978,true,false,true,false,0,10,1,0,7,0,6
6790933,scikit-learn/scikit-learn,python,4438,1427155085,1431217876,1431217876,67713,67713,commits_in_master,false,false,false,162,4,2,8,7,0,15,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,39,14,64,21,15.96049814676249,0.3524837110561895,30,trev.stephens@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,21,0.015064562410329985,0,2,false,Stratified train_test_split This is a first stab at #4437 It features a hack I am not particularly happy with but cant think of a better option at the moment Your input is greatly appreciated The issue is that internally train_test_split uses a ShuffleSplit iterator and both take a train_size/ test_size parameter To implement stratification I used a StatifiedKFold iterator which takes n_folds To maintain the semantics of train_size/ test_size n_folds has to be computed out of these However as far as I can tell there isnt a value of n_folds which can generate train/test sets such that train_size  test_size which is possible with the current implementation When the user requests that train_size  test_size I generate a train/test where train_size and  test_size are swapped (easy to do with current implementation of StatifiedKFold) and then swap them againMaybe a better solution would have been to extract the stratification logic out of StatifiedKFold and generate appropriately sized train/test sets to begin with ,,2574,0.763014763014763,0.12482065997130559,42126,503.3945781702511,38.645966861320794,132.53097849309214,5019,47,1679,273,travis,mbatchkarov,amueller,false,amueller,5,0.2,14,19,1337,false,false,false,false,0,1,0,0,0,0,5
6785156,scikit-learn/scikit-learn,python,4436,1427133278,1427141553,1427141553,137,137,commits_in_master,false,false,false,26,3,1,0,4,0,4,0,4,0,0,3,4,3,0,0,0,0,4,4,3,0,0,32,38,32,52,13.200316692452434,0.2914788044317794,14,wu@minus.com,sklearn/preprocessing/_weights.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,12,0.002886002886002886,0,0,false,[MRG+2] FIX make StandardScaler & scale more numerically stable This is a rebased and fixed version of #3747 I will merge if travis is still green ,,2573,0.7629226583754373,0.12554112554112554,42107,440.3780844040183,34.19858930819104,112.19037214714893,5012,47,1679,234,travis,ogrisel,amueller,false,amueller,114,0.8508771929824561,1124,124,2126,true,true,false,true,41,401,67,340,147,5,20
6782270,scikit-learn/scikit-learn,python,4435,1427119468,,1429305803,36438,,unknown,false,true,false,29,1,1,0,17,0,17,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.219187564412556,0.0931647153322897,0,,sklearn/datasets/twenty_newsgroups.py,0,0.0,0,0,false,When fetching both training and test set of the 20newsgroup dataset When fetching both training and test set of the 20newsgroup dataset the data[data] data[target] data[filenames] should be updated,,2572,0.7632192846034215,0.1254506128334535,42107,440.3780844040183,34.19858930819104,112.19037214714893,5009,47,1679,280,travis,hhchen1105,amueller,false,,0,0,6,0,987,false,false,false,false,0,0,0,0,0,0,4
6777370,scikit-learn/scikit-learn,python,4434,1427079642,1429903544,1429903544,47065,47065,commits_in_master,false,false,false,26,3,3,4,8,0,12,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,37,24,37,26.873476424516625,0.593398591265111,25,trev.stephens@gmail.com,sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,19,0.013678905687544997,0,0,false,[MRG] Friendly error when lvmpdf has non positive definite covariance Catch linalgLinAlgError and raise ValueError when log_multivariate_normal_density has full type but non positive definite covariance #4429 ,,2571,0.7631271878646441,0.12526997840172785,42107,440.3780844040183,34.19858930819104,112.19037214714893,5000,47,1678,274,travis,xuewei4d,GaelVaroquaux,false,GaelVaroquaux,3,0.6666666666666666,14,69,1230,true,true,true,false,3,16,3,0,10,0,4
6762627,scikit-learn/scikit-learn,python,4433,1426944178,1426974182,1426974182,500,500,commits_in_master,false,false,false,6,2,1,1,6,0,7,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.687976833884602,0.10351601580101022,11,you@example.com,doc/related_projects.rst,11,0.007845934379457917,0,0,false,sklearn-theano https://githubcom/sklearn-theano/sklearn-theanomade by - http://kastnerkylegithubio,,2570,0.7630350194552529,0.12410841654778887,42107,440.3780844040183,34.19858930819104,112.19037214714893,4979,47,1677,233,travis,gwulfs,GaelVaroquaux,false,GaelVaroquaux,2,0.5,52,125,601,false,true,false,false,0,1,3,0,0,0,5
6761363,scikit-learn/scikit-learn,python,4432,1426928741,1427137785,1427137785,3484,3484,commits_in_master,false,false,false,130,7,4,1,7,0,8,0,5,0,0,83,83,83,0,0,0,0,83,83,83,0,0,0,2233,0,6560,459.1741533672259,10.139102772824682,196,vmehta94@gmail.com,sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_bicluster.py|sklearn/cluster/tests/test_birch.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/cluster/tests/test_spectral.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/decomposition/tests/test_factor_analysis.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/tests/test_sparse_pca.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_base.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py|sklearn/ensemble/tests/test_partial_dependence.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/feature_extraction/tests/test_feature_hasher.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_selection/tests/test_chi2.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/tests/test_variance_threshold.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_bayes.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_passive_aggressive.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_score_objects.py|sklearn/mixture/tests/test_gmm.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/neighbors/tests/test_kde.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neural_network/tests/test_rbm.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/tests/test_label.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_base.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_init.py|sklearn/tests/test_kernel_approximation.py|sklearn/tests/test_lda.py|sklearn/tests/test_metaestimators.py|sklearn/tests/test_multiclass.py|sklearn/tests/test_naive_bayes.py|sklearn/tests/test_pipeline.py|sklearn/tests/test_qda.py|sklearn/tests/test_random_projection.py|sklearn/tree/tests/test_export.py|sklearn/tree/tests/test_tree.py|sklearn/utils/tests/test_class_weight.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_fixes.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/tests/test_utils.py|sklearn/utils/tests/test_validation.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/decomposition/tests/test_nmf.py|sklearn/tests/test_base.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_pipeline.py|sklearn/tests/test_random_projection.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/ensemble/tests/test_forest.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/neural_network/tests/test_rbm.py,18,0.0021474588403722263,2,0,false,[MRG + 1] Travis ignore docstring Sorry I just screwed up #4250 beyond any recovery :| Reopening a new PR**ISSUE**When docstrings are added to test_* functions nose uses them as headers when a particular test fails This is not very convenient compared to the function name and the passed arguments (in case of generators)And also as pointed out in the link shared by Joel test generators do not respect the generators docstring Most generators use naming similar to check_* Docstrings need not be removed in such casesWhether the generators need to be decorated by a method which sets its description is debatable**FIX*** Convert all docstring to comments* Decorate all test generators with a @test_describe(description) (  )@amueller @ogrisel Please take a look,,2569,0.7629427792915532,0.1231209735146743,42107,440.3780844040183,34.19858930819104,112.19037214714893,4975,47,1677,235,travis,ragv,ogrisel,false,ogrisel,40,0.6,3,1,141,true,true,true,true,17,402,45,312,262,0,0
6761077,scikit-learn/scikit-learn,python,4431,1426925069,1433184386,1433184386,104321,104321,commits_in_master,false,false,false,16,8,3,3,20,0,23,0,6,0,0,4,4,3,0,0,0,0,4,4,3,0,0,36,106,72,203,45.06769359912478,0.9951474267112228,7,trev.stephens@gmail.com,doc/modules/classes.rst|sklearn/datasets/__init__.py|sklearn/datasets/lfw.py|sklearn/datasets/tests/test_lfw.py|doc/modules/classes.rst|sklearn/datasets/lfw.py|sklearn/datasets/tests/test_lfw.py|doc/modules/classes.rst|sklearn/datasets/lfw.py|sklearn/datasets/tests/test_lfw.py,6,0.0007168458781362007,2,2,false,Deprecates load_lfw_pairs and load_lfw_people #4425 I was wondering if this needs a DeprecationWarningping @amueller @ogrisel ,,2568,0.7628504672897196,0.12329749103942653,42107,440.3780844040183,34.19858930819104,112.19037214714893,4975,47,1677,289,travis,vortex-ape,amueller,false,amueller,9,1.0,8,15,687,true,false,true,false,3,84,9,20,49,0,19
6754278,scikit-learn/scikit-learn,python,4428,1426886013,1427855642,1427855642,16160,16160,commits_in_master,false,false,false,14,3,1,7,6,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,16,84,54,9.16406491316597,0.2023532733949982,5,t3kcit@gmail.com,sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py,5,0.0035945363048166786,0,0,true,[MRG] FIX dont raise memory error in ledoit wolf Fixes part A of #4415,,2567,0.7627580833657966,0.12293314162473042,42107,440.3780844040183,34.19858930819104,112.19037214714893,4973,47,1676,242,travis,amueller,amueller,true,amueller,273,0.8534798534798534,1116,40,1610,true,true,false,false,162,1431,119,510,222,9,0
6753522,scikit-learn/scikit-learn,python,4427,1426883295,1427147941,1427147941,4410,4410,commits_in_master,false,false,false,17,1,1,4,6,0,10,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,10,8,10,9.024435301617345,0.1992700882334502,13,trev.stephens@gmail.com,sklearn/lda.py|sklearn/tests/test_lda.py,13,0.009345794392523364,0,3,true,[MRG] FIX LDA(solverlsqr): make sure the right error is raised on transform Fixes part B of #4415,,2566,0.7626656274356975,0.12293314162473042,42107,440.3780844040183,34.19858930819104,112.19037214714893,4973,47,1676,234,travis,amueller,ogrisel,false,ogrisel,272,0.8529411764705882,1116,40,1610,true,true,true,false,162,1431,118,510,222,9,6
6745876,scikit-learn/scikit-learn,python,4423,1426839738,1427800544,1427800544,16013,16013,commits_in_master,false,false,false,10,2,2,2,4,0,6,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,22,8,22,17.763301082753813,0.3922283416316937,21,trev.stephens@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,18,0.012977649603460706,0,2,true,Raising an error when n_clusters  0 in AgglomerativeClustering #4417 ,,2565,0.7625730994152047,0.12112472963229992,42087,439.85078527811436,34.167320075082564,112.12488416850809,4964,47,1676,247,travis,vortex-ape,ogrisel,false,ogrisel,8,1.0,8,15,686,true,false,true,false,2,78,8,19,48,0,7
6745764,scikit-learn/scikit-learn,python,4422,1426838743,1427118594,1427118594,4664,4664,commits_in_master,false,false,false,91,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.126955549562584,0.0911265830405945,6,trev.stephens@gmail.com,sklearn/linear_model/least_angle.py,6,0.004329004329004329,1,2,true,[FIX] LarsCV and LassoLarsCV fails for numpy 180 fixes #4399@amueller Also should I also update y_numeric for https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/kernel_ridgepy#L144 and https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/stochastic_gradientpy#L868  Both are explicitly regressors Theres a couple others that share fit between classifiers and regressors too: gbm adaboost bagging with no y_numericTrue on check_X_y Presumably all of the above get through tests because no call to the offending numpy function thoughCan confirm all tests pass locally on this branch and 2 fails on an up to date master with np 180 installed on python 279 ubuntu 1404 :feelsgood: ,,2564,0.7624804992199687,0.12121212121212122,42087,439.85078527811436,34.167320075082564,112.12488416850809,4963,47,1676,239,travis,trevorstephens,ogrisel,false,ogrisel,10,0.7,120,51,585,true,true,true,false,3,79,7,11,6,0,4
6745397,scikit-learn/scikit-learn,python,4421,1426835993,1441902382,1441902382,251106,251106,merged_in_comments,false,true,false,68,31,5,21,62,0,83,0,8,0,0,1,19,1,0,0,2,2,17,21,14,0,0,65,0,5643,1212,24.38723405385954,0.5384902460059183,13,trev.stephens@gmail.com,sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|sklearn/lda.py,13,0.00939306358381503,0,10,true,Renames LDA and QDA to LinearDiscriminantAnalysis and QuadraticDiscriminantAnalysis #4398 Since sklearn will soon have LDA (Latent Dirichlet Allocation) #3659 renaming LDA and QDA- [ ] Rename LDA to LinearDiscriminantAnalysis- [ ] Rename QDA to QuadraticDiscriminantAnalysis- [ ] Add backward compatibility by raising DeprecationWarning- [ ] Merge LDA and QDA into one module discriminant_analysis (Other module name suggestions are welcome though this one is spot-on),,2563,0.7623878267655092,0.12066473988439307,42087,439.85078527811436,34.167320075082564,112.12488416850809,4963,47,1676,349,travis,vortex-ape,ogrisel,false,ogrisel,7,1.0,8,15,686,true,false,true,false,2,77,7,19,47,0,30
6741002,scikit-learn/scikit-learn,python,4418,1426812546,1427998557,1427998557,19766,19766,commits_in_master,false,false,false,46,14,3,21,32,0,53,0,8,0,0,16,16,15,0,0,0,0,16,16,15,0,0,319,60,646,111,92.72120403132622,2.0473606747909128,116,trev.stephens@gmail.com,sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/base.py|sklearn/grid_search.py|sklearn/pipeline.py|doc/developers/index.rst|sklearn/base.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/grid_search.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/multiclass.py|sklearn/pipeline.py|sklearn/svm/base.py|sklearn/tests/test_common.py|sklearn/tests/test_multiclass.py|sklearn/utils/estimator_checks.py,39,0.007246376811594203,0,14,false,[WIP] Classifier and regressor tags This improved is_classifier by making it not depend on inheritance and introduces is_regressorIt also fixes #2588 using ranking scorers on regressorsTodo:* [ ] Describe the estimator_type in the dev docs* [ ] remove decision_function from all regressors,,2561,0.7625927372120266,0.11884057971014493,42087,439.85078527811436,34.167320075082564,112.12488416850809,4957,47,1675,252,travis,amueller,amueller,true,amueller,270,0.8555555555555555,1114,40,1609,true,true,false,false,158,1422,116,512,211,8,0
6734175,scikit-learn/scikit-learn,python,4416,1426788944,1426860243,1426860243,1188,1188,commits_in_master,false,false,false,9,2,2,0,12,0,12,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,24,0,24,0,8.299595192990584,0.18326190856054583,15,trev.stephens@gmail.com,sklearn/svm/base.py|sklearn/svm/base.py,15,0.010752688172043012,1,3,false,Added docstrings for kernel  precomputed #4397 ping @amueller ,,2560,0.7625,0.11827956989247312,42087,439.85078527811436,34.167320075082564,112.12488416850809,4952,47,1675,234,travis,vortex-ape,ogrisel,false,ogrisel,6,1.0,8,15,685,true,false,true,false,2,72,6,19,41,0,8
6725413,scikit-learn/scikit-learn,python,4412,1426736356,1426738566,1426738566,36,36,commits_in_master,false,false,false,37,5,5,0,5,0,5,0,3,0,5,20,25,23,0,1,0,5,20,25,23,0,1,1949,973,1949,973,111.0002260097565,2.450976786594813,95,trev.stephens@gmail.com,sklearn/cross_validation.py|.gitattributes|doc/modules/hmm.rst|sklearn/__init__.py|sklearn/_hmmc.c|sklearn/_hmmc.pyx|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/dummy.py|sklearn/hmm.py|sklearn/linear_model/base.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_sgd.py|sklearn/preprocessing/tests/test_label.py|sklearn/setup.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_hmm.py|sklearn/tests/test_learning_curve.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/__init__.py|sklearn/cross_validation.py|sklearn/cross_validation.py,23,0.0035945363048166786,1,1,false,MAINT Remove the deprecated n_iterations from StratifiedShuffleSplit @amueller Sorry for not doing this along with #4411 :/ I was a bit sloppy in #4411 :| Just now saw the travis fail on master due to the same,,2559,0.7624071903087143,0.11790079079798706,42088,439.8403345371603,34.16650826839004,112.1222201102452,4939,47,1674,229,travis,ragv,amueller,false,amueller,39,0.5897435897435898,3,1,138,true,true,false,false,17,401,43,319,267,0,3
6724753,scikit-learn/scikit-learn,python,4411,1426733094,1426733195,1426733195,1,1,commits_in_master,false,false,false,6,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,3.9396461525646544,0.08699072939754525,19,trev.stephens@gmail.com,sklearn/cross_validation.py,19,0.013639626704953339,0,0,false,MAINT remove the deprecated n_iterations param ,,2558,0.7623143080531666,0.11773151471643933,42088,439.8403345371603,34.16650826839004,112.1222201102452,4939,47,1674,229,travis,ragv,amueller,false,amueller,38,0.5789473684210527,3,1,138,true,true,false,false,17,399,42,319,267,0,1
6722634,scikit-learn/scikit-learn,python,4409,1426724796,1426868383,1426868383,2393,2393,commits_in_master,false,false,false,38,7,6,0,27,0,27,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,38,12,42,12,47.99252820454288,1.0597157390461687,18,trev.stephens@gmail.com,sklearn/feature_selection/rfe.py|sklearn/metrics/pairwise.py|examples/applications/plot_species_distribution_modeling.py|sklearn/manifold/t_sne.py|examples/applications/plot_species_distribution_modeling.py|sklearn/feature_selection/rfe.py|sklearn/manifold/t_sne.py|sklearn/metrics/pairwise.py|examples/applications/plot_species_distribution_modeling.py|sklearn/manifold/t_sne.py|sklearn/metrics/pairwise.py,8,0.0050578034682080926,0,4,false,[MRG] Use immutable default arguments throughtout repo Hi developers:Ive now fixed the issue that some functions are using mutable default arguments as shown in https://landscapeio/github/scikit-learn/scikit-learn/76/messages/error (not including the one in ridgepy which has been fixed by DonBeo),,2557,0.7622213531482206,0.11849710982658959,42088,439.8403345371603,34.16650826839004,112.1222201102452,4937,47,1674,233,travis,bryandeng,ogrisel,false,ogrisel,0,0,43,161,973,true,false,true,false,0,2,0,0,4,0,2
6722456,scikit-learn/scikit-learn,python,4408,1426724134,,1440534980,230180,,unknown,false,false,false,22,7,1,0,19,2,21,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,15,0,4.343821044205191,0.09591525390213705,14,trev.stephens@gmail.com,sklearn/lda.py,14,0.01012292118582791,0,2,false,DONT MERGE THIS - debugging travis not finding error This is the reverse of #4117 trying to see why travis didnt fail,,2556,0.7625195618153364,0.11858279103398409,42088,439.8403345371603,34.16650826839004,112.1222201102452,4936,47,1674,339,travis,amueller,amueller,true,,269,0.8587360594795539,1111,40,1608,true,true,false,false,157,1388,116,512,215,8,45
6718268,scikit-learn/scikit-learn,python,4407,1426708031,1426721116,1426721116,218,218,commits_in_master,false,false,false,22,1,1,1,6,0,7,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,16,6,16,8.96384660639486,0.1969069238090049,8,olivier.grisel@ensta.org,sklearn/calibration.py|sklearn/tests/test_calibration.py,7,0.005090909090909091,0,2,false,FIX : allow NaN in input of calibration if estimator handles it if base_estimator can handle NaNs then calibration should support it,,2555,0.762426614481409,0.11927272727272727,42672,446.26452943382077,34.61286089238845,113.32958380202474,4930,46,1674,230,travis,agramfort,ogrisel,false,ogrisel,42,0.9047619047619048,176,187,1932,true,true,true,true,9,163,22,63,27,0,7
6706551,scikit-learn/scikit-learn,python,4402,1426639929,1426720306,1426720306,1339,1339,commits_in_master,false,false,false,26,4,1,0,15,0,15,0,5,0,0,2,3,2,0,0,0,0,3,3,3,0,0,10,0,50,18,9.291668445334157,0.20410793440164607,39,trev.stephens@gmail.com,sklearn/linear_model/omp.py|sklearn/utils/estimator_checks.py,37,0.026928675400291122,0,3,false,[MRG] fix ompcv on old scipy versions Fixes #4387We have an if old scipy version ignore error which made this error hard to track down,,2554,0.7623335943617854,0.11717612809315867,42751,445.4398727515146,34.54889944094875,113.12016093190802,4917,49,1673,234,travis,amueller,ogrisel,false,ogrisel,268,0.8582089552238806,1111,40,1607,true,true,true,false,153,1349,115,505,209,8,44
6701623,scikit-learn/scikit-learn,python,4401,1426620384,1426621922,1426621922,25,25,github,false,false,false,22,2,2,0,3,0,3,0,3,2,0,4,6,5,0,1,2,0,4,6,5,0,1,99,34,99,34,26.657962278387924,0.5868058236166765,19,trev.stephens@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py,16,0.0,1,0,false,[MRG+2] Rebased version of the Cythonized DBSCAN main loop Rebased + fixed version of @larsmans #4157 to get travis running on it,,2553,0.7622405013709361,0.11585807385952208,42761,444.8679871845841,34.51743411052127,112.90662051869694,4911,49,1673,235,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,113,0.8495575221238938,1121,124,2120,true,true,true,true,35,371,52,335,127,4,5
6665511,scikit-learn/scikit-learn,python,4395,1426412195,1426416460,1426416461,71,71,github,false,false,false,2,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.651665920878377,0.10239436319795309,2,amueller@nyu.edu,examples/applications/plot_outlier_detection_housing.py,2,0.0014419610670511895,0,0,false,Fix typo ,,2552,0.7621473354231975,0.11824080749819754,42759,444.8887953413316,34.519048621342876,112.91190158796978,4872,52,1671,240,travis,akitty,agramfort,false,agramfort,0,0,2,0,1444,true,false,false,false,0,0,0,0,2,0,9
6659935,scikit-learn/scikit-learn,python,4392,1426305996,1426547670,1426547670,4027,4027,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.284605789972388,0.09431448622510745,11,trev.stephens@gmail.com,doc/developers/index.rst,11,0.007947976878612716,0,0,true,DOC: Updated set_params doc I think this is required to make grid search work,,2550,0.7623529411764706,0.11849710982658959,42759,444.8887953413316,34.519048621342876,112.91190158796978,4866,52,1669,238,travis,sinhrks,amueller,false,amueller,0,0,49,4,1046,false,false,false,false,0,0,0,0,2,0,9
6655692,scikit-learn/scikit-learn,python,4390,1426280936,1426550378,1426550378,4490,4490,commits_in_master,false,false,false,12,1,1,0,6,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,11,0,11,8.6564999887154,0.19055040042519356,31,trev.stephens@gmail.com,sklearn/linear_model/tests/test_sgd.py|sklearn/neighbors/tests/test_approximate.py,27,0.01970802919708029,0,1,true,[MRG+1] Replace assert_array_equal with assert_almost_equal to compare arrays of floats Fixes #4386,,2549,0.7622597096900745,0.11897810218978103,42759,444.8887953413316,34.519048621342876,112.91190158796978,4865,52,1669,237,travis,Barmaley-exe,amueller,false,amueller,4,0.25,6,4,1615,true,false,false,false,2,19,5,9,26,0,42
6651179,scikit-learn/scikit-learn,python,4389,1426262792,1427195637,1427195637,15547,15547,commits_in_master,false,false,false,85,6,3,6,20,0,26,0,5,1,0,1,3,2,0,0,1,0,2,3,2,0,0,51,80,51,126,28.131611128607496,0.61924448630398,0,,sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py,0,0.0,1,1,true,Fixes #4385: Move newton_cg test out of optimize #4385 I had to make new grad and hess functions to resolve errors Still Im getting an error with fmin_ncg:preTraceback (most recent call last):  File testpy line 25 in module    assert_almost_equal(newton_cg(func_grad_hess func grad x0) fmin_ncg(ffunc x0x0 fprimegrad fhesshess))  File /usr/lib/python34/site-packages/scipy/optimize/optimizepy line 1313 in fmin_ncg    callbackcallback **opts)  File /usr/lib/python34/site-packages/scipy/optimize/optimizepy line 1405 in _minimize_newtoncg    Ap  numpydot(A psupi)TypeError: unsupported operand type(s) for *: function and float/preCan anyone help me to resolve thisping @amueller,,2548,0.7621664050235479,0.11812179016874541,42759,444.8887953413316,34.519048621342876,112.91190158796978,4864,52,1669,248,travis,vortex-ape,GaelVaroquaux,false,GaelVaroquaux,5,1.0,8,15,679,true,false,false,false,1,63,5,16,37,0,12
6643539,scikit-learn/scikit-learn,python,4388,1426210595,1426720681,1426720681,8501,8501,commits_in_master,false,false,false,51,4,3,6,9,0,15,0,6,0,0,3,3,2,0,0,0,0,3,3,2,0,0,3,30,3,30,14.217280405608316,0.31295656907675856,20,trev.stephens@gmail.com,doc/conf.py|sklearn/svm/tests/test_svm.py|doc/modules/svm.rst,12,0.005891016200294551,0,4,false,[MRG] Add SVR mathematical description to the tutorial and a test for decision_function This PR adds SVRs description to the [tutorial](http://scikit-learnorg/stable/modules/svmhtml#mathematical-formulation) (Mathematical formulation section) adds a test to reproduce its decision_function (#4367) and makes sphinx use six that comes with sklearn being built not the one already installed in a system,,2547,0.7620730270906949,0.11782032400589101,42759,444.8887953413316,34.519048621342876,112.91190158796978,4860,52,1668,237,travis,Barmaley-exe,ogrisel,false,ogrisel,3,0.0,6,4,1614,true,false,false,false,1,18,4,6,21,0,3
6609995,scikit-learn/scikit-learn,python,4380,1426026442,1426038005,1426038005,192,192,merged_in_comments,false,false,false,31,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.55371419712753,0.1002382635021188,6,trev.stephens@gmail.com,sklearn/metrics/ranking.py,6,0.0044411547002220575,0,0,false,FIX numpy deprecation warning from unsafe type comparison Small modification to fix a line a that launch a large bunch of warningsI intend to merge this whenever travis is green,,2544,0.7625786163522013,0.12139156180606958,42762,444.5535756045087,34.46985641457369,112.85720967213882,4843,52,1666,235,travis,arjoly,arjoly,true,arjoly,80,0.825,29,26,1176,true,true,false,false,4,44,5,25,26,0,192
6609718,scikit-learn/scikit-learn,python,4379,1426025390,1431974534,1431974534,99152,99152,commits_in_master,false,true,false,33,4,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,20,0,4.361387456888419,0.09600468677067783,38,trev.stephens@gmail.com,sklearn/utils/estimator_checks.py,38,0.028189910979228485,0,6,false,[MRG] TST test that all default arguments are not mutable Check that default parameter values are of a basic typeSee #4376 for a fix for one of them #4373 for the issue,,2543,0.7624852536374361,0.12091988130563798,42762,444.5535756045087,34.46985641457369,112.85720967213882,4843,52,1666,294,travis,amueller,amueller,true,amueller,266,0.8609022556390977,1106,40,1600,true,true,false,false,149,1287,115,474,201,4,2
6609605,scikit-learn/scikit-learn,python,4378,1426024957,1426026980,1426026980,33,33,github,false,false,false,15,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.797061792347743,0.1055949326553295,93,trev.stephens@gmail.com,doc/whats_new.rst,93,0.06899109792284866,0,0,false,Vainly add link to my GH profile in Whats New Title says it all :smile: ,,2542,0.7623918174665618,0.12091988130563798,42762,444.5535756045087,34.46985641457369,112.85720967213882,4843,52,1666,234,travis,dan-blanchard,dan-blanchard,true,dan-blanchard,1,1.0,51,23,1306,false,true,false,false,0,2,0,0,1,0,-1
6605289,scikit-learn/scikit-learn,python,4377,1426006747,1426866972,1426866972,14337,14337,commits_in_master,false,false,false,5,14,6,18,26,0,44,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,36,81,100,157,53.09331847584027,1.169807299945155,22,trev.stephens@gmail.com,sklearn/svm/base.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,15,0.008982035928143712,0,9,false,Fixes #4374: LinearSVC(intercept_scaling0) breaks #4374,,2541,0.7622983077528532,0.1220059880239521,42690,445.3033497306161,34.527992504099316,113.04755211993441,4843,51,1666,246,travis,vortex-ape,ogrisel,false,ogrisel,4,1.0,8,15,676,true,false,true,false,1,48,4,10,27,0,5
6604144,scikit-learn/scikit-learn,python,4376,1426000621,1426010555,1426010555,165,165,commits_in_master,false,true,false,39,114,11,0,2,5,7,0,6,0,0,21,139,18,0,0,0,5,134,139,131,0,1,210,74,2647,3460,137.1733540226498,3.0223462292468284,212,wu@minus.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|doc/whats_new.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/about.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/isotonic.py|sklearn/lda.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/metrics/classification.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/preprocessing/label.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/externals/joblib/format_stack.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/externals/joblib/format_stack.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/externals/joblib/format_stack.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,93,0.006766917293233083,0,1,false,replace mutable default argument in _RidgeGCV This is a very small change to make me familiar with the repository and the git system  The mutable default value is replaced with a numpy array This code passes al the tests,,2540,0.7622047244094489,0.12180451127819548,42690,445.3033497306161,34.527992504099316,113.04755211993441,4842,51,1666,234,travis,DonBeo,DonBeo,true,DonBeo,0,0,0,0,661,true,true,false,false,0,1,0,0,3,0,77
6599130,scikit-learn/scikit-learn,python,4372,1425965161,,1426029495,1072,,unknown,false,false,false,19,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,0,9,0,9.103812497064206,0.2005846803726105,0,,benchmarks/bench_multilabel_metrics.py|benchmarks/bench_sparsify.py,0,0.0,0,0,false,[WIP] remove some style errors in bench code This PR is mostly to test whether the landscapeio bot comments,,2539,0.7625049231981095,0.12274096385542169,42690,445.3033497306161,34.527992504099316,113.04755211993441,4839,50,1666,235,travis,amueller,amueller,true,,265,0.8641509433962264,1106,40,1600,true,true,false,false,146,1257,111,464,197,4,733
6597538,scikit-learn/scikit-learn,python,4371,1425956379,1430229558,1430229558,71219,71219,commits_in_master,false,false,false,24,4,3,8,15,0,23,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,71,0,77,0,13.119269604098962,0.2890574142546222,4,trev.stephens@gmail.com,setup.py|setup.py|setup.py,4,0.0030234315948601664,0,3,false,FIX: Ensure dependencies installed Ported https://githubcom/scikit-learn/scikit-learn/pull/4332- Without numpy without scipy: https://travis-ciorg/saketkc/scikit-learn/builds/53724568- With numpy with scipy: https://travis-ciorg/saketkc/scikit-learn/builds/53724630- With numpy without scipy: https://travis-ciorg/saketkc/scikit-learn/builds/53724858,,2538,0.7624113475177305,0.12320483749055178,42690,445.3033497306161,34.527992504099316,113.04755211993441,4838,50,1665,282,travis,saketkc,ogrisel,false,ogrisel,3,0.6666666666666666,95,94,1449,true,true,false,false,1,10,4,0,5,0,2
6596915,scikit-learn/scikit-learn,python,4370,1425953103,1426723321,1426723321,12836,12836,commits_in_master,false,false,false,81,4,1,0,5,0,5,0,4,0,3,12,26,14,0,0,0,5,21,26,24,0,1,1677,852,5376,2771,53.237320659203085,1.1729801060560126,108,trev.stephens@gmail.com,doc/modules/hmm.rst|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/dummy.py|sklearn/hmm.py|sklearn/linear_model/base.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/preprocessing/label.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_hmm.py|sklearn/utils/extmath.py,44,0.006056018168054504,0,3,false,[MRG] remove deprecated stuff from 017 This PR removes most of the deprecated things for 017Two items are left afterwards:* sequence of sequence multi-label format* possibly refactoring cv to not accept boolean masksIm not sure if we want cross_val_score and GridSearchCV to support iterators that support boolean masks I think the intention was to remove this support completely but Im not sure For now I only removed the option to return boolean masks from the cross-validation iterators,,2537,0.762317698068585,0.12339137017411052,42690,445.3033497306161,34.527992504099316,113.04755211993441,4838,50,1665,238,travis,amueller,ogrisel,false,ogrisel,264,0.8636363636363636,1106,40,1599,true,true,true,false,146,1250,110,458,195,4,1232
6596062,scikit-learn/scikit-learn,python,4369,1425949773,,1425956482,111,,unknown,false,false,false,6,1,1,0,2,0,2,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.6710548261971265,0.10291754576643962,11,you@example.com,doc/related_projects.rst,11,0.008327024981074944,0,0,false,Kayak & nolearn more related projects,,2536,0.7626182965299685,0.12339137017411052,42690,445.3033497306161,34.527992504099316,113.04755211993441,4838,50,1665,232,travis,gwulfs,gwulfs,true,,1,1.0,51,125,589,false,true,false,false,0,0,1,0,0,0,24
6595792,scikit-learn/scikit-learn,python,4368,1425948783,1426868157,1426868157,15322,15322,commits_in_master,false,false,false,14,3,1,5,7,0,12,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,19,51,53,8.628849312955602,0.19011979673139925,18,trev.stephens@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,13,0.00984102952308857,0,4,false,Deprecate estimator_params in RFE and RFECV #4292 Fix #4292  Is the warning message OK,,2535,0.7625246548323471,0.12339137017411052,42690,445.3033497306161,34.527992504099316,113.04755211993441,4837,50,1665,242,travis,xuewei4d,ogrisel,false,ogrisel,2,0.5,14,68,1217,true,true,true,false,1,9,2,0,6,0,44
6589037,scikit-learn/scikit-learn,python,4364,1425924179,1425927423,1425927423,54,54,commits_in_master,false,false,false,12,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.741676028062857,0.10447377094925547,2,loic.esteve@ymail.com,README.rst,2,0.0015220700152207,0,0,false,[MRG] add appveyor badge to alert us of failures Currently failing ^^,,2533,0.7627319384129491,0.1232876712328767,42680,447.8444236176195,34.74695407685099,113.7066541705717,4833,50,1665,232,travis,amueller,agramfort,false,agramfort,263,0.8631178707224335,1106,40,1599,true,true,true,false,146,1207,106,447,191,4,-1
6588479,scikit-learn/scikit-learn,python,4363,1425921870,1425924991,1425924991,52,52,commits_in_master,false,false,false,41,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.856994970978636,0.1070146035065602,0,,doc/modules/decomposition.rst,0,0.0,0,0,false,DOC Fix NMF inconsistency and broken links In the text X is decomposed into matrices V and H and in the equation into matrices W and HThe common literature uses V instead of X and V  WHhttp://wwwjmlrorg/papers/volume5/hoyer04a/hoyer04apdfhttp://hebbmitedu/people/seung/papers/ls-lponm-99pdf,,2532,0.7626382306477093,0.12338156892612338,42680,447.8444236176195,34.74695407685099,113.7066541705717,4833,50,1665,232,travis,arimbr,amueller,false,amueller,0,0,21,25,913,false,false,true,false,0,0,0,0,1,0,51
6582089,scikit-learn/scikit-learn,python,4362,1425875236,1432299018,1432299018,107063,107063,commits_in_master,false,false,false,8,12,4,2,7,0,9,0,5,0,0,2,4,2,0,0,0,0,4,4,4,0,0,20,0,20,148,16.77800632246681,0.36967131013264604,3,trev.stephens@gmail.com,sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx,3,0.002284843869002285,0,5,false,FIX Include PyFuncDistance attributes while pickling fixes #4360,,2531,0.7625444488344528,0.12490479817212491,42680,447.8444236176195,34.74695407685099,113.7066541705717,4830,50,1664,288,travis,ragv,ogrisel,false,ogrisel,37,0.5675675675675675,3,1,128,true,true,true,true,17,382,40,327,256,0,410
6570546,scikit-learn/scikit-learn,python,4359,1425772941,1425777473,1425777473,75,75,commits_in_master,false,false,false,17,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.766351551046998,0.10501746012399507,9,you@example.com,doc/related_projects.rst,9,0.006891271056661562,0,0,false,Related projects linking to wrong URL There was a spelling error linking to githupio instead of githubio,,2530,0.7624505928853755,0.1332312404287902,42680,447.8444236176195,34.74695407685099,113.7066541705717,4824,50,1663,230,travis,josephlewis42,agramfort,false,agramfort,0,0,9,3,1440,false,false,false,false,0,0,0,0,2,0,-1
6564996,scikit-learn/scikit-learn,python,4356,1425722359,1427935481,1427935481,36885,36885,commits_in_master,false,false,false,6,21,6,9,12,0,21,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,146,0,194,0,26.620185527007738,0.5865250916862709,2,t3kcit@gmail.com,sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/dict_vectorizer.py,2,0.0015313935681470138,0,4,false,Fixes issue #4355: DictVectorizerrestrict docstring unclear ,,2529,0.7623566627125345,0.1339969372128637,42680,447.8444236176195,34.74695407685099,113.7066541705717,4821,50,1663,251,travis,vortex-ape,amueller,false,amueller,3,1.0,8,15,673,true,false,true,false,1,38,3,2,13,0,5
6556646,scikit-learn/scikit-learn,python,4352,1425674486,1425681760,1425681760,121,121,commits_in_master,false,false,false,18,2,1,0,5,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,23,20,23,9.245901355123799,0.20371657492246745,14,trev.stephens@gmail.com,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,10,0.0077459333849728895,1,2,true,[MRG + 1] Adding fix for issue #4297 isotonic infinite loop Fix by @mjbommar rebased by me ),,2527,0.7625643055005936,0.13555383423702555,42678,447.63109798959647,34.725151131730634,113.71198275458082,4820,50,1662,230,travis,amueller,amueller,true,amueller,262,0.8625954198473282,1103,40,1596,true,true,false,false,143,1181,104,440,180,4,2
6547969,scikit-learn/scikit-learn,python,4350,1425624148,1427290480,1427290480,27772,27772,commits_in_master,false,false,false,49,9,4,20,8,0,28,0,5,0,0,2,5,2,0,0,0,0,5,5,5,0,0,12,0,66,0,21.47831945256183,0.4732598241667856,9,trev.stephens@gmail.com,sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py|sklearn/datasets/lfw.py|sklearn/feature_selection/univariate_selection.py,9,0.007003891050583658,0,2,true,Fixes #2274: Running tests should not print anything on stdout / stderr or warnings #2274 Im still going through all the warnings Fixed the fprob is deprecated warning Tested with:prenosetests -s sklearn doc/*rst doc/modules/ doc/datasets/ \        doc/developers doc/tutorial/basic doc/tutorial/statistical_inference \        doc/tutorial/text_analytics/preand its showing no errors,,2526,0.7624703087885986,0.133852140077821,42682,446.745700763788,34.628180497633664,113.51389344454337,4813,50,1662,250,travis,vortex-ape,ogrisel,false,ogrisel,2,1.0,8,15,672,true,false,true,false,1,31,2,0,8,0,507
6546638,scikit-learn/scikit-learn,python,4349,1425615462,,1425934187,5312,,unknown,false,true,false,14,14,7,8,22,0,30,0,3,0,0,8,8,6,0,1,0,0,8,8,6,0,1,142,50,222,50,58.467559618132576,1.2882919934891877,152,trev.stephens@gmail.com,sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/validation.py|setup.cfg|setup.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/tests/test_dbscan.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/validation.py,92,0.0140625,0,4,false,Add warning for KFold CV if random_state must be changed but shuffle is False ,,2525,0.7627722772277228,0.13359375,42682,446.745700763788,34.628180497633664,113.51389344454337,4813,49,1661,233,travis,alexsavio,amueller,false,,0,0,18,14,884,false,false,false,false,0,0,0,0,0,0,738
6545146,scikit-learn/scikit-learn,python,4347,1425608805,1433200537,1433200537,126528,126528,commits_in_master,false,false,false,32,18,1,14,33,0,47,0,5,0,0,2,21,2,0,0,0,0,21,21,19,0,0,10,0,1603,1571,8.993605524648746,0.19816865655088747,46,trev.stephens@gmail.com,sklearn/utils/class_weight.py|sklearn/utils/estimator_checks.py,41,0.03203125,0,14,false,[WIP] Use more natural class_weightauto heuristic Fix for #4324No test yet Travis passes so we dont have very strict tests for this ^^ (apart from the manual reimplementation in the test),,2524,0.7626782884310618,0.13359375,42678,446.60012184263553,34.60799475139416,113.40737616570598,4813,49,1661,295,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,261,0.8620689655172413,1103,40,1595,true,true,true,false,143,1170,103,438,179,4,17
6542934,scikit-learn/scikit-learn,python,4346,1425600739,1425940270,1425940270,5658,5658,merged_in_comments,false,false,false,14,7,7,12,9,0,21,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,256,85,256,85,31.78608653911972,0.70039698138436,20,trev.stephens@gmail.com,sklearn/tests/test_naive_bayes.py|sklearn/naive_bayes.py|CONTRIBUTING.md|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,13,0.005439005439005439,0,1,false,[WIP] Sample-weight support for GaussianNB This PR adds support for sample weights to GaussianNB,,2523,0.7625842251288149,0.13286713286713286,42673,446.65245002694917,34.6120497738617,113.42066412016966,4813,49,1661,235,travis,jmetzen,amueller,false,amueller,16,0.5625,14,2,1242,true,true,false,false,2,62,6,40,234,0,17
6536946,scikit-learn/scikit-learn,python,4345,1425577966,1425588086,1425588086,168,168,commits_in_master,false,false,false,26,2,2,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,46,0,46,0,9.666354371643646,0.21300026576509604,4,t3kcit@gmail.com,examples/text/document_classification_20newsgroups.py|examples/text/document_classification_20newsgroups.py,4,0.00306044376434583,0,0,false,update 20newsgroups text classification example for best practice As perhttp://scikit-learnorg/stable/modules/feature_selectionhtml#feature-selection-as-part-of-a-pipelineIve updated the 20newsgroups classification example to use a pipeline for the L1 feature selection,,2522,0.7624900872323552,0.1315990818668707,42665,446.1033634126333,34.59510137114731,113.11379350755888,4808,49,1661,232,travis,bendavies,amueller,false,amueller,0,0,24,8,1476,true,false,false,false,0,0,0,0,2,0,40
6535188,scikit-learn/scikit-learn,python,4342,1425567758,1425589222,1425589222,357,357,commits_in_master,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.3683550953114585,0.09625767486728697,2,manojkumarsivaraj334@gmail.com,CONTRIBUTING.md,2,0.0015290519877675841,0,0,false,DOC remove unnecessary backticks in CONTRIBUTING The backticks shouldnt be there,,2521,0.7623958746529155,0.13149847094801223,42665,446.1033634126333,34.59510137114731,113.11379350755888,4808,48,1661,231,travis,sotte,amueller,false,amueller,0,0,22,21,2136,false,true,false,false,0,0,0,0,1,0,-1
6528365,scikit-learn/scikit-learn,python,4339,1425515207,1425588275,1425588275,1217,1217,commits_in_master,false,false,false,9,3,1,2,5,0,7,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.070921547104608,0.11173888269353552,0,,doc/presentations.rst,0,0.0,0,0,false,[MRG] add scipy2013 tutorial links to presentations on website ,,2519,0.7626042080190552,0.1318181818181818,42665,446.1033634126333,34.59510137114731,113.11379350755888,4805,48,1660,231,travis,amueller,amueller,true,amueller,260,0.8615384615384616,1102,40,1594,true,true,false,false,141,1147,99,437,178,4,82
6522264,scikit-learn/scikit-learn,python,4338,1425495413,1425495947,1425495947,8,8,commits_in_master,false,false,false,12,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.437657172066141,0.0977847615226369,1,olivier.grisel@ensta.org,sklearn/manifold/tests/test_spectral_embedding.py,1,0.0007468259895444362,0,0,false,[MRG] fix typo pyagm - pygamg in SkipTest As per the title,,2518,0.7625099285146942,0.1321882001493652,42665,446.1033634126333,34.59510137114731,113.11379350755888,4804,48,1660,229,travis,lesteve,amueller,false,amueller,9,1.0,4,0,1043,false,false,false,false,0,2,5,8,5,0,-1
6521693,scikit-learn/scikit-learn,python,4337,1425492891,1426707376,1426707376,20241,20241,merged_in_comments,false,false,false,136,2,1,5,9,0,14,0,3,0,0,14,15,11,0,0,0,0,15,15,11,0,0,38,0,38,0,54.597255317947415,1.203062648611567,186,trev.stephens@gmail.com,doc/modules/feature_selection.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/isotonic.py|sklearn/lda.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/metrics/classification.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/preprocessing/label.py|sklearn/svm/classes.py|sklearn/tree/tree.py,94,0.008221225710014948,0,1,false,Remove sphinx warnings when generating the doc and fixes a few other doc problems while I was at itThe main take-home message is to remember that you need a backslash whenyour parameter type description exceeds one lineJust in case we care about them the only remaining warnings are document isnt included in any toctree:/home/lesteve/dev/scikit-learn/doc/modules/hmmrst:: WARNING: document isnt included in any toctree/home/lesteve/dev/scikit-learn/doc/related_projectsrst:: WARNING: document isnt included in any toctree/home/lesteve/dev/scikit-learn/doc/themes/scikit-learn/static/ML_MAPS_READMErst:: WARNING: document isnt included in any toctree/home/lesteve/dev/scikit-learn/doc/tune_tocrst:: WARNING: document isnt included in any toctreeI guess: * hmmrst is fine since it is deprecated* related_projectsrst not sure: it looks like related_projectshtml is included via a raw html snippet in doc/indexrst* ML_MAPS_READMErst probably fine but maybe it should be moved alongside ml_mappng in doc/images* tune_tocrst probably fine,,2517,0.7624155740961462,0.13228699551569506,42665,446.1033634126333,34.59510137114731,113.11379350755888,4804,48,1660,246,travis,lesteve,ogrisel,false,ogrisel,8,1.0,4,0,1043,false,false,false,false,0,2,4,8,4,0,46
6519974,scikit-learn/scikit-learn,python,4335,1425483733,1426015136,1426015136,8856,8856,commits_in_master,false,false,false,30,1,1,0,6,0,6,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.399380051422156,0.0969413205512778,12,trev.stephens@gmail.com,sklearn/mixture/dpgmm.py,12,0.008988764044943821,0,1,false,Fix DPGMM lower_bound and doc Fix #2555Todos:- [x] Fix typo of DPGMM doc (docstring)- [ ] Add References for DPGMM (docstring)- [ ] Fix DPGMM lower_bound,,2516,0.7623211446740858,0.13183520599250936,42665,446.1033634126333,34.59510137114731,113.11379350755888,4803,48,1660,240,travis,martin0258,amueller,false,amueller,1,1.0,7,8,1240,true,true,false,false,1,6,1,0,4,0,262
6519490,scikit-learn/scikit-learn,python,4334,1425481018,1426627739,1426627739,19112,19112,merged_in_comments,false,false,false,75,8,8,9,18,0,27,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,203,68,203,68,34.97724663863412,0.7707314300570222,17,trev.stephens@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py,15,0.011244377811094454,0,11,false,MRG: Faster vectorization of DBSCAN (plain python) This pull request includes: fixes #4066 fixes #4073This is still a plain python variant of DBSCAN with the same drawbacks (worst case O(n^2) memory due to materialization of neighborhoods) as the current head version But I vectorized operations differently which is strictly faster on all my experiments As such it may also be useful as baseline for benchmarking the pending Cython rewrite (#4157) and is less invasive,,2515,0.7622266401590457,0.13193403298350825,42665,446.1033634126333,34.59510137114731,113.11379350755888,4803,48,1660,244,travis,kno10,ogrisel,false,ogrisel,1,1.0,5,1,706,true,false,false,false,0,13,1,1,9,1,2010
6525865,scikit-learn/scikit-learn,python,4332,1425476976,,1425945014,7800,,unknown,false,false,false,83,4,1,4,26,0,30,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,32,0,4.424686687010449,0.09749895791017021,2,trev.stephens@gmail.com,setup.py,2,0.0014992503748125937,0,4,false,[RFC] setuppy: Fix numpy installation Pass build_requires if numpy is not foundFixes #4164Runs:- Doing pip install /scikit-learn$ pip install Unpacking /media/data1/Development_Version_Controlled/OpenSource/saketkc/forks/scikit-learn  Running setuppy (path:/tmp/pip-ekp_12-build/setuppy) egg_info for package from file:///media/data1/Development_Version_Controlled/OpenSource/saketkc/forks/scikit-learn    Partial import of sklearn during the build process    Downloading/unpacking numpy162 (from scikit-learn016dev)  Downloading numpy-192targz (40MB): 40MB downloaded-  python setuppy buildscikit-learn$ python setuppy buildPartial import of sklearn during the build process[Error] Numpy is required to setup scikit-learnPlease install numpy from http://numpyorg,,2514,0.7625298329355609,0.13193403298350825,42665,446.1033634126333,34.59510137114731,113.11379350755888,4803,48,1660,235,travis,saketkc,saketkc,true,,2,1.0,95,94,1444,false,true,false,false,1,0,2,0,0,0,2
6517313,scikit-learn/scikit-learn,python,4331,1425466441,1426014476,1426014476,9133,9133,commits_in_master,false,false,false,32,12,2,10,17,0,27,0,5,0,0,2,2,1,0,0,0,0,2,2,1,0,0,122,0,266,0,13.592069071119909,0.299504368190747,20,trev.stephens@gmail.com,sklearn/feature_selection/univariate_selection.py|doc/developers/index.rst|sklearn/feature_selection/univariate_selection.py,11,0.008106116433308769,0,3,false,Add See also for selectors and scoring functions of univariate feature selection Fix #4263 Better univariate feature selection docsChange: Add See also for selectors and scoring functions of univariate feature selection,,2513,0.7624353362514923,0.13190862196020633,42665,446.1033634126333,34.59510137114731,113.11379350755888,4803,48,1660,239,travis,martin0258,amueller,false,amueller,0,0,7,8,1240,true,true,false,false,0,2,0,0,3,0,139
6502310,scikit-learn/scikit-learn,python,4328,1425383579,1425439725,1425439725,935,935,commits_in_master,false,false,false,10,40,30,0,0,0,0,0,2,47,3,68,130,57,1,9,47,3,80,130,64,1,9,7709,1208,7753,1298,823.1595284499077,18.138482685421636,145,trev.stephens@gmail.com,.gitignore|sklearn/pipeline.py|doc/images/hinge.png|doc/images/piecewise_linear.png|doc/images/simple_earth_example.png|doc/modules/classes.rst|doc/modules/earth.rst|doc/modules/earth_bibliography.bib|doc/supervised_learning.rst|.gitignore|examples/earth/classifier_comp.py|examples/earth/pyearth_vs_earth.py|examples/earth/sine_wave.py|examples/earth/v_function.py|sklearn/earth/__init__.py|sklearn/earth/_basis.c|sklearn/earth/_basis.pxd|sklearn/earth/_basis.pyx|sklearn/earth/_forward.c|sklearn/earth/_forward.pxd|sklearn/earth/_forward.pyx|sklearn/earth/_forward.pyxdep|sklearn/earth/_pruning.c|sklearn/earth/_pruning.pxd|sklearn/earth/_pruning.pyx|sklearn/earth/_record.c|sklearn/earth/_record.pxd|sklearn/earth/_record.pyx|sklearn/earth/_util.c|sklearn/earth/_util.pxd|sklearn/earth/_util.pyx|sklearn/earth/earth.py|sklearn/earth/setup.py|sklearn/earth/tests/__init__.py|sklearn/earth/tests/earth_regress.txt|sklearn/earth/tests/forward_regress.txt|sklearn/earth/tests/test_basis.py|sklearn/earth/tests/test_data.csv|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/earth/tests/test_pruning.py|sklearn/earth/tests/test_record.py|sklearn/earth/tests/test_util.py|sklearn/earth/tests/testing_utils.py|sklearn/setup.py|doc/modules/earth.rst|examples/earth/README.txt|examples/earth/classifier_comp.py|examples/earth/pyearth_vs_earth.py|examples/earth/sine_wave.py|examples/earth/v_function.py|sklearn/earth/_forward.pyx|sklearn/earth/earth.py|sklearn/earth/_basis.c|sklearn/earth/_forward.c|sklearn/earth/_forward.pxd|sklearn/earth/_forward.pyx|sklearn/earth/_pruning.c|sklearn/earth/_record.c|sklearn/earth/_record.pxd|sklearn/earth/_record.pyx|sklearn/earth/_util.c|sklearn/earth/earth.py|sklearn/earth/tests/earth_linvars_regress.txt|sklearn/earth/tests/test_basis.py|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/earth/tests/test_record.py|sklearn/earth/tests/test_util.py|sklearn/earth/_basis.c|sklearn/earth/_basis.pyx|sklearn/earth/_forward.c|sklearn/earth/_forward.pyx|sklearn/earth/_pruning.c|sklearn/earth/_pruning.pyx|sklearn/earth/_record.c|sklearn/earth/_record.pyx|sklearn/earth/_util.c|sklearn/earth/earth.py|sklearn/earth/tests/pathological_data/issue_44.csv|sklearn/earth/tests/pathological_data/issue_44.txt|sklearn/earth/tests/pathological_data/readme.txt|sklearn/earth/_basis.c|sklearn/earth/_forward.c|sklearn/earth/_forward.pxd|sklearn/earth/_pruning.c|sklearn/earth/_record.c|sklearn/earth/_record.pyx|sklearn/earth/_util.c|sklearn/earth/_basis.c|sklearn/earth/_forward.c|sklearn/earth/_pruning.c|sklearn/earth/_record.c|sklearn/earth/_util.c|sklearn/earth/tests/test_basis.py|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/earth/tests/test_record.py|sklearn/earth/_basis.c|sklearn/earth/_basis.pxd|sklearn/earth/_basis.pyx|sklearn/earth/_forward.c|sklearn/earth/_forward.pyx|sklearn/earth/_pruning.c|sklearn/earth/_record.c|sklearn/earth/_util.c|sklearn/earth/earth.py|sklearn/earth/tests/test_basis.py|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/earth/earth.py|examples/earth/classifier_comp.py|examples/earth/pyearth_vs_earth.py|examples/earth/sine_wave.py|examples/earth/v_function.py|sklearn/earth/__init__.py|sklearn/earth/_basis.c|sklearn/earth/_basis.pxd|sklearn/earth/_basis.pyx|sklearn/earth/_forward.c|sklearn/earth/_forward.pxd|sklearn/earth/_forward.pyx|sklearn/earth/_pruning.c|sklearn/earth/_pruning.pxd|sklearn/earth/_pruning.pyx|sklearn/earth/_record.c|sklearn/earth/_record.pxd|sklearn/earth/_record.pyx|sklearn/earth/_util.c|sklearn/earth/_util.pxd|sklearn/earth/_util.pyx|sklearn/earth/earth.py|sklearn/earth/setup.py|sklearn/earth/tests/test_basis.py|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/earth/tests/test_pruning.py|sklearn/earth/tests/test_record.py|sklearn/earth/tests/test_util.py|sklearn/earth/tests/testing_utils.py|.gitignore|sklearn/pipeline.py|doc/modules/earth.rst|examples/earth/classifier_comp.py|examples/earth/pyearth_vs_earth.py|examples/plot_classifier_comparison.py|doc/modules/earth.rst|examples/earth/sine_wave.py|examples/earth/v_function.py|examples/plot_classifier_comparison.py|sklearn/earth/__init__.py|sklearn/earth/earth.py|sklearn/earth/tests/earth_linvars_regress.txt|sklearn/earth/tests/earth_regress.txt|sklearn/earth/tests/pathological_data/issue_44.txt|sklearn/earth/tests/test_earth.py|examples/earth/plot_sine_wave.py|examples/earth/plot_v_function.py|examples/earth/plot_sine_wave.py|examples/earth/plot_v_function.py|doc/images/simple_earth_example.png|doc/modules/earth.rst|sklearn/earth/_basis.c|sklearn/earth/_forward.c|sklearn/earth/_forward.pxd|sklearn/earth/_forward.pyx|sklearn/earth/_pruning.c|sklearn/earth/_record.c|sklearn/earth/_util.c|sklearn/earth/_util.pxd|sklearn/earth/_util.pyx|sklearn/earth/earth.py|sklearn/earth/tests/pathological_data/issue_50.csv|sklearn/earth/tests/pathological_data/issue_50.txt|sklearn/earth/tests/pathological_data/issue_50_weight.csv|sklearn/earth/tests/pathological_data/readme.txt|sklearn/earth/tests/test_earth.py|sklearn/earth/tests/test_forward.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/cluster/tests/test_k_means.py|sklearn/decomposition/tests/test_fastica.py|sklearn/feature_extraction/tests/test_text.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/tests/test_bounds.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_hmm.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|doc/developers/index.rst|doc/modules/calibration.rst|doc/modules/clustering.rst|doc/modules/feature_selection.rst|examples/applications/plot_prediction_latency.py|sklearn/cross_decomposition/pls_.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/ranking.py|sklearn/preprocessing/label.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py,38,0.0,0,1,false,[WIP] Earth (MARS) continuation Continuation of this PR :  #2285,,2512,0.7623407643312102,0.1301518438177874,42661,445.3950915355946,34.38737957384965,113.28848362673168,4798,45,1659,233,travis,mehdidc,mehdidc,true,mehdidc,1,0.0,2,5,1550,false,true,false,false,0,5,1,0,0,0,-1
6498130,scikit-learn/scikit-learn,python,4325,1425353533,1425353546,1425353546,0,0,commits_in_master,false,false,false,20,3,3,2,4,0,6,0,3,0,0,9,9,5,0,0,0,0,9,9,5,0,0,52,0,52,0,78.08886542223071,1.7207035628350211,53,trev.stephens@gmail.com,doc/developers/index.rst|doc/modules/calibration.rst|doc/modules/clustering.rst|doc/modules/feature_selection.rst|sklearn/cross_decomposition/pls_.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/ranking.py|sklearn/preprocessing/label.py|examples/applications/plot_prediction_latency.py|doc/developers/index.rst|doc/modules/calibration.rst|doc/modules/clustering.rst|doc/modules/feature_selection.rst|examples/applications/plot_prediction_latency.py|sklearn/cross_decomposition/pls_.py|sklearn/linear_model/stochastic_gradient.py|sklearn/metrics/ranking.py|sklearn/preprocessing/label.py,20,0.0042643923240938165,0,0,false,[MRG] More minor doc fixes Checked the rendering of the calibration part Im surprised it worked before (but it did),,2510,0.7625498007968128,0.12864250177683015,42661,445.3950915355946,34.38737957384965,113.28848362673168,4795,45,1658,233,travis,amueller,amueller,true,amueller,259,0.861003861003861,1100,40,1592,true,true,false,false,138,1100,93,430,170,4,540
6497109,scikit-learn/scikit-learn,python,4323,1425349088,1425350028,1425350028,15,15,commits_in_master,false,false,false,25,1,1,0,3,0,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.539418962427844,0.10002675058204197,0,,doc/conf.py,0,0.0,0,2,false,[MRG] Generate example rst files for inclusion in docstrings Fixes #4321When autodoc parses a docstring we generate an empty examplesrst to avoid inclusion errors,,2509,0.762455161418892,0.12981455064194009,42661,445.3950915355946,34.38737957384965,113.28848362673168,4795,45,1658,231,travis,amueller,amueller,true,amueller,258,0.8604651162790697,1100,40,1592,true,true,false,false,138,1094,91,434,166,4,9
6304691,scikit-learn/scikit-learn,python,4257,1424147916,1424368153,1424368153,3670,3670,commit_sha_in_comments,false,false,false,36,2,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,31,0,42,0,4.3789812991170445,0.09732671737750401,11,trev.stephens@gmail.com,sklearn/neighbors/graph.py,11,0.006798516687268232,0,3,false,DOC: Reword docstring and deprecation warning for include_self This is minor and can be ignored if it doesnt seem like an improvement but I was seeking a clearer statement on the behavior when include_selfNone for kneighbors_graph,,2471,0.764063132335087,0.16749072929542647,42089,445.62712347644276,34.42704744707643,113.52134762051843,4716,59,1644,231,travis,chebee7i,MechCoder,false,MechCoder,1,0.0,20,9,1684,true,false,false,false,0,5,1,1,3,0,470
6302991,scikit-learn/scikit-learn,python,4256,1424138487,1425602089,1425602089,24393,24393,commits_in_master,false,false,false,35,8,3,0,7,0,7,0,3,0,0,16,18,15,0,0,0,0,18,18,16,0,0,124,2,135,2,79.45419726526607,1.7659395355838239,103,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/gaussian_process/gaussian_process.py|sklearn/linear_model/base.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/preprocessing/label.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|doc/modules/tree.rst|sklearn/tree/tree.py|doc/modules/tree.rst|sklearn/tree/tree.py,28,0.004947433518862091,0,5,false,Update documentation of predict_proba in tree module Adds short description on how class probabilties are computed to docstring of predict_proba() of DecisionTreeClassifierAlso adds example and description to narrative of tree moduleSee also #4209,,2470,0.7639676113360324,0.16759431045145332,42089,445.62712347644276,34.42704744707643,113.52134762051843,4716,59,1644,251,travis,cangermueller,amueller,false,amueller,0,0,10,0,602,true,false,false,false,0,1,0,0,4,0,1705
6301190,scikit-learn/scikit-learn,python,4254,1424130977,,1435633342,191706,,unknown,false,true,false,48,6,6,0,17,0,17,0,6,6,0,12,18,15,0,0,6,0,12,18,15,0,0,7931,18,7931,18,121.28872088903412,2.695748680503892,64,trev.stephens@gmail.com,sklearn/model_selection/partition.py|sklearn/model_selection/search.py|sklearn/model_selection/validate.py|sklearn/covariance/graph_lasso_.py|sklearn/feature_selection/rfe.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/model_selection/__init__.py|sklearn/model_selection/partition.py|sklearn/model_selection/scoring.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/model_selection/validate.py|sklearn/tests/test_cross_validation.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/model_selection/__init__.py|sklearn/model_selection/partition.py|sklearn/model_selection/scoring.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/model_selection/validate.py|sklearn/tests/test_cross_validation.py|sklearn/model_selection/partition.py|sklearn/model_selection/partition.py|sklearn/model_selection/partition.py,21,0.0018598884066955983,0,12,false,[WIP] Data independent cv and model_selection refactoring fixes #2904Continuing upon the work of #3340 **NOTE** - Kindly do not close this or review this yet I am trying to make the diff more cleaner by moving the files into model_selection before cherry picking commits from #3340 ,,2469,0.764277035236938,0.1680099194048357,42089,445.62712347644276,34.42704744707643,113.52134762051843,4716,58,1644,326,travis,ragv,rvraghav93,false,,29,0.6206896551724138,3,1,108,true,true,false,false,17,339,37,320,346,0,831
6299430,scikit-learn/scikit-learn,python,4253,1424122608,1440946819,1440946819,280403,280403,commits_in_master,false,false,false,47,3,1,7,4,0,11,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,66,0,4.845883706170705,0.10770403472831129,8,loic.esteve@ymail.com,sklearn/metrics/classification.py,8,0.004962779156327543,0,0,false,Add example for precision_recall_fscore_support The first example did not demonstrate averageNone it now doesThe second example demonstrates that the values do not have to beintegers they can be strings and it also demonstrates that the orderof the results is determined by the labels argument,,2468,0.7641815235008104,0.16811414392059554,42089,445.62712347644276,34.42704744707643,113.52134762051843,4714,58,1644,358,travis,Flimm,GaelVaroquaux,false,GaelVaroquaux,0,0,3,5,1656,false,false,false,false,0,0,0,0,0,0,2524
6295674,scikit-learn/scikit-learn,python,4251,1424105937,1424106609,1424106609,11,11,commits_in_master,false,false,false,100,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,46,0,46,0,4.498636511632138,0.09998645859130842,23,trev.stephens@gmail.com,sklearn/utils/validation.py,23,0.01435705368289638,0,0,false,[MRG] FIX broken test under 64 bit Python 2 / Windows This is to fix the following broken tests with 64 bit Python 2 under Windows:FAIL: sklearnutilsteststest_validationtest_check_array_min_samples_and_features_messages----------------------------------------------------------------------Traceback (most recent call last):File C:\Python27-x64\lib\site-packages\nose\casepy line 197 in runTestselftest(*selfarg)File C:\Python27-x64\lib\site-packages\sklearn\utils\tests\test_validationpy line 217 in test_check_array_min_samples_and_features_messagesassert_raise_message(ValueError msg check_array [])File C:\Python27-x64\lib\site-packages\sklearn\utils\testingpy line 398 in assert_raise_messageassert_in(message error_message)AssertionError: 0 feature(s) (shape(1 0)) while a minimum of 1 is required not found in Found array with 0 feature(s) (shape(1L 0L)) while a minimum of 1 is required Eg: https://ciappveyorcom/project/sklearn-ci/scikit-learn/build/10475/job/gtgifkr1kdl82i31I will merge when travis is green,,2467,0.7640859343331983,0.16916354556803995,42071,445.8177842219106,34.34669962682133,113.45107080887072,4712,58,1644,225,travis,ogrisel,ogrisel,true,ogrisel,110,0.8545454545454545,1107,124,2091,true,true,false,false,26,314,36,326,62,0,-1
6287410,scikit-learn/scikit-learn,python,4250,1424039752,,1426928412,48144,,unknown,false,false,false,18,13,2,7,20,0,27,0,4,0,0,4,88,3,0,1,0,0,88,88,87,0,1,28,0,28,6624,36.93269442456461,0.820864112505065,3,loic.esteve@ymail.com,Makefile|continuous_integration/install.sh|continuous_integration/test_script.sh|setup.cfg|Makefile|continuous_integration/install.sh|continuous_integration/test_script.sh|setup.cfg,3,0.0,2,5,false,[MRG] Prevent nose from using docstring to name the tests in results @amueller @ogrisel Please take a look,,2466,0.7643957826439578,0.16979949874686717,42071,445.8177842219106,34.34669962682133,113.45107080887072,4709,58,1643,261,travis,ragv,ragv,true,,28,0.6428571428571429,3,1,107,true,false,false,false,17,332,36,317,346,0,201
6286707,scikit-learn/scikit-learn,python,4249,1424034275,,1424952985,15311,,unknown,false,false,false,5,15,3,33,39,0,72,0,5,0,0,2,32,2,0,0,0,0,32,32,29,0,0,0,30,385,632,13.546214421610081,0.3010774451268419,8,trev.stephens@gmail.com,sklearn/manifold/spectral_embedding_.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_spectral_embedding.py,8,0.005018820577164366,0,11,false,Making Spectral Embedding Deterministic #4236 ,,2465,0.7647058823529411,0.17001254705144292,42071,445.8177842219106,34.34669962682133,113.45107080887072,4709,58,1643,238,travis,Hasil-Sharma,ogrisel,false,,0,0,14,4,748,true,false,false,false,1,3,0,0,1,0,6
6284435,scikit-learn/scikit-learn,python,4248,1424011965,1426500719,1426500719,41479,41479,merged_in_comments,false,false,false,17,1,1,0,7,0,7,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.536338338965547,0.10082441594515623,2,loic.esteve@ymail.com,README.rst,2,0.0012539184952978057,0,0,false,Add gitter chat room link to readme Just trying to make our gitter room more alive ),,2464,0.7646103896103896,0.16990595611285267,42071,445.8177842219106,34.34669962682133,113.45107080887072,4708,57,1643,261,travis,ragv,ragv,true,ragv,27,0.6296296296296297,3,1,107,true,false,false,false,16,317,35,307,346,0,22
6284078,scikit-learn/scikit-learn,python,4247,1424007085,,1424176535,2824,,unknown,false,true,false,67,1,1,0,6,0,6,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.766425716889005,0.10593832583503786,2,loic.esteve@ymail.com,README.rst,2,0.0012539184952978057,1,1,false,Add a Gitter chat badge to READMErst ### scikit-learn/scikit-learn now has a Chat Room on Gitter@ragv has just created a chat room You can visit it here: [https://gitterim/scikit-learn/scikit-learn](https://gitterim/scikit-learn/scikit-learnutm_sourcebadge&utm_mediumbadge&utm_campaignpr-badge&contentbody_link)This pull-request adds this badge to your READMErst:[[Gitter](https://badgesgitterim/Join%20Chatsvg)](https://gitterim/scikit-learn/scikit-learnutm_sourcebadge&utm_mediumbadge&utm_campaignpr-badge&utm_contentbody_badge)If my aim is a little off please [let me know](https://githubcom/gitterHQ/readme-badger/issues)Happy chattingPS: [Click here](https://gitterim/settings/badger/opt-out) if you would prefer not to receive automatic pull-requests from Gitter in future,,2463,0.7649208282582217,0.16990595611285267,42071,445.8177842219106,34.34669962682133,113.45107080887072,4707,57,1643,229,travis,gitter-badger,ogrisel,false,,0,0,38,0,178,false,false,false,false,0,0,0,0,0,0,3
6273834,scikit-learn/scikit-learn,python,4245,1423886045,1424958435,1424958435,17873,17873,commits_in_master,false,false,false,42,16,6,14,10,0,24,0,5,0,0,10,11,9,0,0,0,0,11,11,10,0,0,193,99,342,122,88.20409063896679,1.960422388572234,147,trev.stephens@gmail.com,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/dummy.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/kernel_approximation.py|sklearn/linear_model/coordinate_descent.py|sklearn/preprocessing/label.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,83,0.011912225705329153,0,4,true,[WIP] Fix empty input data common checks This PR includes #4214 but also additional common tests that currently fail on some estimators that do not have a consistent behavior and that probably need to be fixed on a case by case basis,,2462,0.764825345247766,0.17053291536050158,42054,445.33219194369144,34.33680506016074,113.28292195748324,4700,58,1641,239,travis,ogrisel,ogrisel,true,ogrisel,109,0.8532110091743119,1107,124,2088,true,true,false,false,26,320,37,326,64,1,996
6263018,scikit-learn/scikit-learn,python,4244,1423834916,1423843000,1423843000,134,134,commits_in_master,false,false,false,16,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.672079476738731,0.1038422474385003,0,,examples/feature_selection/plot_rfe_with_cross_validation.py,0,0.0,0,0,true,Update plot_rfe_with_cross_validationpy minor modification (import statement for matplotlibpyplot has been moved to beginning of the file),,2461,0.7647297846403901,0.17031934877896054,42043,444.97300382941273,34.34578883524011,113.24120543253335,4694,58,1641,225,travis,ugurcaliskan,agramfort,false,agramfort,0,0,0,0,163,false,false,false,false,0,0,0,0,0,0,11
6247210,scikit-learn/scikit-learn,python,4242,1423749637,1444544106,1444544106,346574,346574,commits_in_master,false,false,false,6,13,3,80,56,5,141,0,7,0,0,4,16,4,0,0,1,0,16,17,14,0,0,729,248,1080,550,53.20525976198508,1.1825472411795388,14,trev.stephens@gmail.com,sklearn/feature_selection/__init__.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/utils/testing.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/utils/testing.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/utils/testing.py,11,0.0018656716417910447,0,46,false,Implemented SelectFromModel meta-transformer Continuation of https://githubcom/scikit-learn/scikit-learn/pull/3011/,,2460,0.7646341463414634,0.17101990049751245,42041,444.9703860517114,34.34742275397826,113.22280630812779,4689,58,1640,376,travis,MechCoder,MechCoder,true,MechCoder,63,0.8571428571428571,84,41,968,true,true,false,false,9,291,35,173,93,0,3
6234756,scikit-learn/scikit-learn,python,4241,1423681256,,1423749949,1144,,unknown,false,true,false,37,1,1,6,5,0,11,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,43,19,43,19,18.479803276121302,0.4107345867585048,15,trev.stephens@gmail.com,sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py,11,0.004944375772558714,0,2,false,[MRG] Inherit LinearModels from _LearntSelectorMixin Fixes #41801 Make 1e-5 1e-5 times max coefficient2 Specify clearly that for l1 based models this is the threshold3 Inherit LinearModels from _LearntSelectorMixin effectively adding transform to all LinearModels,,2459,0.7649450996339976,0.1699629171817058,42041,444.9703860517114,34.34742275397826,113.22280630812779,4681,58,1639,227,travis,MechCoder,MechCoder,true,,62,0.8709677419354839,84,41,967,true,true,false,false,9,288,34,169,93,0,1
6231000,scikit-learn/scikit-learn,python,4240,1423661155,1429009803,1429009803,89144,89144,commits_in_master,false,false,false,27,3,3,2,9,0,11,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,39,197,39,197,39.94389813053403,0.8877984384913977,22,trev.stephens@gmail.com,doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py,20,0.0018529956763434219,0,3,false,[MRG] add a prior strategy for the dummy estimator I wanted a strategy that is able to predict the prior with predict_proba but failed to find one,,2458,0.7648494711147275,0.169857936998147,42041,444.9703860517114,34.34742275397826,113.22280630812779,4678,58,1639,302,travis,arjoly,arjoly,true,arjoly,79,0.8227848101265823,29,26,1149,true,true,false,false,5,79,10,64,58,5,58990
6223332,scikit-learn/scikit-learn,python,4234,1423613421,1425435146,1425435146,30362,30362,commits_in_master,false,false,false,76,1,1,4,6,0,10,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,8,8,8,8.945808395874604,0.19883073752457836,8,trev.stephens@gmail.com,sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py,5,0.003190810465858328,0,0,false,[MRG] Fix gibbs sampling behavior in RBM with integral random_state Fixes #4227If random_state was an integer all samplings were identical That is not really the expected behavior I think Also if random_state was None or a RandomState object they were already differentIt seems to me that we might have more places where passing a RandomState object might have different behavior than passing a seed and I am wondering what behavior we want in general,,2456,0.7650651465798045,0.17549457562220805,42041,444.9703860517114,34.34742275397826,113.22280630812779,4673,57,1638,250,travis,amueller,amueller,true,amueller,252,0.8571428571428571,1091,40,1572,true,true,false,false,119,930,82,400,129,6,12
6220576,scikit-learn/scikit-learn,python,4233,1423602147,1423858567,1423858567,4273,4273,commits_in_master,false,false,false,13,2,1,3,7,0,10,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.569127206821324,0.10155403426646732,1,olivier.grisel@ensta.org,doc/modules/ensemble.rst,1,0.0006373486297004461,3,2,false,DOC minor improvement in Ensemble user guide See http://stackoverflowcom/questions/28411976/unconclusive-randomforest-documentation-in-scikitlearnPing @glouppe @arjoly @ogrisel ,,2455,0.764969450101833,0.17590822179732313,42041,444.9703860517114,34.34742275397826,113.22280630812779,4673,57,1638,226,travis,amueller,ogrisel,false,ogrisel,251,0.8565737051792829,1091,40,1572,true,true,true,false,119,924,81,400,129,6,275
6217245,scikit-learn/scikit-learn,python,4232,1423588139,1423599883,1423599883,195,195,commits_in_master,false,false,false,33,1,1,0,3,0,3,0,2,0,0,8,8,8,0,0,0,0,8,8,8,0,0,0,28,0,28,35.03119841990029,0.7786072702934685,26,trev.stephens@gmail.com,sklearn/cluster/tests/test_birch.py|sklearn/cluster/tests/test_dbscan.py|sklearn/feature_extraction/tests/test_image.py|sklearn/feature_selection/tests/test_chi2.py|sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/utils/tests/test_fixes.py|sklearn/utils/tests/test_graph.py,17,0.0,0,0,false,[MRG] Use absolute imports in tests Excerpt from the scikit-learn developers guideline [here](http://scikit-learnorg/stable/developers/#coding-guidelines): Unit tests are an exception to the previous rule they should use absolute imports exactly as client code would,,2454,0.7648736756316219,0.17579617834394903,42034,445.0206975305705,34.35314269400961,113.24166151210925,4671,58,1638,223,travis,lesteve,amueller,false,amueller,6,1.0,4,0,1021,true,false,false,false,0,4,3,9,2,0,11
6208198,scikit-learn/scikit-learn,python,4229,1423531731,1423531989,1423531989,4,4,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.199783498691538,0.09334480138101026,13,trev.stephens@gmail.com,sklearn/naive_bayes.py,13,0.008296107211231652,0,0,false,[MRG] Clean up minor PEP8 issues ,,2453,0.7647778230737872,0.1761327377153797,42034,445.0206975305705,34.35314269400961,113.24166151210925,4666,58,1637,224,travis,ragv,agramfort,false,agramfort,26,0.6153846153846154,3,1,101,true,false,false,false,15,290,34,285,289,0,-1
6208178,scikit-learn/scikit-learn,python,4228,1423531657,1425676653,1425676653,35749,35749,merged_in_comments,false,true,false,51,2,2,2,23,0,25,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,106,38,106,38,31.338070529659586,0.6965230398582543,39,trev.stephens@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/utils/multiclass.py|sklearn/utils/validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/utils/multiclass.py,20,0.007657945118059987,0,19,false,[WIP] ENH support sparse y in GridSearchCV Fixes #4225 - [ ] Add support for sparse y in GridSearchCV- [ ] Add support for sparse y in CrossValidationCV- [ ] Add support for sparse y in other related helpers- [ ] Add tests using sparse y in GridSearchCV,,2452,0.7646818923327896,0.1761327377153797,42034,445.0206975305705,34.35314269400961,113.24166151210925,4666,58,1637,257,travis,ragv,ragv,true,ragv,25,0.6,3,1,101,true,false,false,false,15,290,33,285,289,0,80
6199562,scikit-learn/scikit-learn,python,4226,1423491031,1423773865,1423773865,4713,4713,commits_in_master,false,false,false,49,18,6,35,20,0,55,0,7,0,0,1,2,1,0,0,0,0,2,2,2,0,0,109,0,115,209,28.325278200907125,0.6295604210429635,11,trev.stephens@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,11,0.0070242656449553,1,10,false,Handle numerical instability in ElasticNetCV Fixes #4224 - [x] Return a 0 array of n_alphas when the maximum alpha is 0- [x] Use nanmax and nanargmin instead of max to search through the alpha values- [ ] Add NRT@GaelVaroquaux does this seem like a reasonable fix,,2451,0.7645858833129335,0.17624521072796934,42034,445.0206975305705,34.35314269400961,113.24166151210925,4662,58,1637,228,travis,ragv,MechCoder,false,MechCoder,24,0.5833333333333334,3,1,101,true,false,false,false,15,284,32,284,288,0,1
6193392,scikit-learn/scikit-learn,python,4222,1423437481,1423568661,1423568661,2186,2186,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.195846459746463,0.09325689645885386,0,,benchmarks/bench_plot_omp_lars.py,0,0.0,0,0,false,FIX precompute_gram-precompute as in #2224 ,,2450,0.7644897959183673,0.17307692307692307,42028,445.08422956124485,34.35804701627487,113.25782811459027,4657,58,1636,225,travis,saketkc,MechCoder,false,MechCoder,1,1.0,93,94,1420,false,true,false,false,0,0,1,0,0,0,2186
6193327,scikit-learn/scikit-learn,python,4221,1423436870,1423443007,1423443007,102,102,commits_in_master,false,false,false,3,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.059543976754419,0.0902274370481199,9,trev.stephens@gmail.com,sklearn/neighbors/graph.py,9,0.0057692307692307696,0,0,false,DOC: Fix is-if ,,2449,0.7643936300530829,0.17307692307692307,42028,445.08422956124485,34.35804701627487,113.25782811459027,4657,58,1636,223,travis,saketkc,amueller,false,amueller,0,0,93,94,1420,false,true,false,false,0,0,0,0,0,0,102
6173813,scikit-learn/scikit-learn,python,4214,1423258740,1423945912,1423945912,11452,11452,commits_in_master,false,true,false,90,3,2,6,7,0,13,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,118,128,118,135,25.86117043001773,0.5747933740128718,23,trev.stephens@gmail.com,sklearn/dummy.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/dummy.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,20,0.005750798722044728,1,0,true,[MRG] add validation for non-empty input data This follows the discussion in #4206 This makes check_array and check_X_y reject input arrays with less than 1 samples and less than 1 feature for 2D inputs while providing informative error message to the caller FYI I also have implemented some common checks based on this PR in edcd0793ec8adad7aef1d2de5bdad1c403ced1c6 but this reveals some missing input validation in several estimators Those missing validation checks should better be added once @amuellers own #4136 is merged in master to avoid conflict resolution pain and redundant work,,2444,0.7655482815057283,0.17124600638977636,42019,444.8939765344249,34.365406125800234,113.21069040196102,4640,60,1634,232,travis,ogrisel,agramfort,false,agramfort,108,0.8518518518518519,1105,124,2081,true,true,true,true,24,293,33,295,58,1,862
6157558,scikit-learn/scikit-learn,python,4206,1423167831,1423320886,1423320886,2550,2550,commits_in_master,false,false,false,50,3,2,2,22,0,24,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,21,40,21,58,26.650495301401058,0.592327626292055,3,trev.stephens@gmail.com,sklearn/feature_selection/base.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/base.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,3,0.0,0,9,false,[MRG] explicit exception message for strict selectors This is a fix for #4059 to raise a ValueError at transform time with an explicit error message instead of crashing when calling get_support() with a cryptic error messageNote that for consistency the behavior of SelectKBest(k0) is also impacted by this change,,2443,0.7654523127302497,0.1694373401534527,42008,443.2012949914302,34.27918491715864,112.93087031041706,4624,60,1633,226,travis,ogrisel,jnothman,false,jnothman,107,0.8504672897196262,1105,124,2080,true,true,false,false,21,276,31,283,53,1,10
6152948,scikit-learn/scikit-learn,python,4205,1423144556,,1423239053,1574,,unknown,false,false,false,6,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.208155553948089,0.09352947336054798,3,trev.stephens@gmail.com,sklearn/neural_network/rbm.py,3,0.0019169329073482429,0,0,false,Fix forgotten return self in rbmpartial_fit ,,2442,0.7657657657657657,0.16932907348242812,42008,443.2012949914302,34.27918491715864,112.93087031041706,4621,60,1633,224,travis,Treora,Treora,true,,0,0,2,0,658,false,false,false,false,0,0,0,0,0,0,118
6147732,scikit-learn/scikit-learn,python,4204,1423104808,1424001190,1424001190,14939,14939,commits_in_master,false,false,false,13,1,1,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.434984176310089,0.09857107125985519,38,trev.stephens@gmail.com,sklearn/cluster/hierarchical.py,38,0.024265644955300127,0,4,false,DOC: fix correct shape of children and distance small fix in the documentation,,2441,0.7656698074559607,0.1685823754789272,42008,443.2012949914302,34.27918491715864,112.93087031041706,4615,60,1632,233,travis,mvdoc,agramfort,false,agramfort,2,1.0,9,17,421,true,false,false,false,1,13,1,8,13,0,803
6144816,scikit-learn/scikit-learn,python,4203,1423091965,1423135712,1423135712,729,729,commits_in_master,false,false,false,6,1,1,0,4,0,4,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.531698919735636,0.10072063380328004,5,t3kcit@gmail.com,doc/related_projects.rst,5,0.0031887755102040817,0,1,false,PyMC add PyMC to related_projectsrst doc,,2440,0.7655737704918033,0.1683673469387755,42008,443.2012949914302,34.27918491715864,112.93087031041706,4613,60,1632,223,travis,gwulfs,GaelVaroquaux,false,GaelVaroquaux,0,0,51,125,556,false,false,false,false,0,0,0,0,0,0,5
6128317,scikit-learn/scikit-learn,python,4201,1423000789,1423004311,1423004311,58,58,commits_in_master,false,false,false,20,3,1,2,3,0,5,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.619891939866633,0.10268061803709483,3,mathieu@mblondel.org,doc/related_projects.rst,3,0.0018987341772151898,1,1,false,[MRG] website: add skll to related projects Adding scikit-learn laboratory for some exposure@dan-blanchard is that a reasonable one-sentence summary,,2439,0.7654776547765477,0.16582278481012658,42006,442.93672332523926,34.25701090320431,112.91244107984573,4606,61,1631,224,travis,amueller,amueller,true,amueller,250,0.856,1086,40,1565,true,true,false,false,117,923,82,380,124,6,36
6124105,scikit-learn/scikit-learn,python,4198,1422981584,1422989997,1422989997,140,140,commits_in_master,false,false,false,24,2,2,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,8,7,8,17.029102274368608,0.378484087413758,18,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,17,0.010800508259212199,0,1,false,[MRG] FIX Ensure at least 1 feature is sampled when max_features is a float This mimics what has been done in the tree module,,2438,0.7653814602132896,0.1645489199491741,41978,443.0654152174949,34.27986087950831,112.94011148696936,4602,61,1631,225,travis,arjoly,larsmans,false,larsmans,78,0.8205128205128205,29,26,1141,true,true,true,true,6,89,10,59,58,5,-1
6100817,scikit-learn/scikit-learn,python,4193,1422818871,1422991539,1422991539,2877,2877,commits_in_master,false,false,false,210,6,4,18,5,0,23,0,6,0,0,20,20,19,0,0,0,0,20,20,19,0,0,114,0,120,0,90.60502063799672,2.013763011913502,108,wu@minus.com,continuous_integration/install.sh|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cross_validation.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/feature_extraction/text.py|sklearn/lda.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/supervised.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/utils/class_weight.py|sklearn/utils/fixes.py|sklearn/utils/multiclass.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|README.rst|continuous_integration/install.sh,46,0.001913265306122449,0,0,false,Deactivate travis virtual env for DISTRIBubuntu Two aspects:* Travis default python virtualenv includes numpy 191 (this is mentioned [here](http://docstravis-cicom/user/ci-environment/#Python-VM-images)) which means that DISTRIBubuntu uses numpy 191 rather than the one available through apt-get ie 161 You can see that numpy 191 is used in Travis [here](https://travis-ciorg/scikit-learn/scikit-learn/jobs/48901775) for example* The tests are broken with numpy 161 because npbincount raise ans exception with empty input arrays The test failures can be seen in my first commit [here](https://travis-ciorg/lesteve/scikit-learn/builds/49084036)I added a fixed bincount for numpy 161 in sklearnutilsfixesbincount modelled after this [numpy fix](https://githubcom/numpy/numpy/commit/40f0844846a9d7665616b142407a3d74cb65a040) and used it everywhere in the code instead of npbincountTwo questions:* the failing tests were actually coming from only three places in the code (sklearn/preprocessing/datapy sklearn/utils/multiclasspy sklearn/utils/sparsefuncspy) Is it enough to use utilsfixesbincount in these three places rather than everywhere in the code For example do we know that npbincount is never going to be called with an empty input array in some part of the code* sklearn/cluster/_k_meanspyx was using npbincount Given that npbincount is actually a C function is there any performance penalty in using the python (numpy 161 only) function utilsfixesbincount Also I regenerated the c by hand via cython sklearn/cluster/_k_meanspyx just in case there is some best practices that I didnt follow,,2437,0.7652851867049651,0.16262755102040816,41962,443.2343548925218,34.29293170010962,112.98317525380105,4573,61,1629,225,travis,lesteve,larsmans,false,larsmans,5,1.0,4,0,1012,true,false,false,false,0,4,2,1,2,0,7
6100077,scikit-learn/scikit-learn,python,4192,1422811408,1425385296,1425385296,42898,42898,commits_in_master,false,false,false,65,1,1,5,15,0,20,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,111,49,111,49,17.761167561178056,0.3947549708751892,15,trev.stephens@gmail.com,sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py,12,0.003826530612244898,0,4,false,TST/FIX in future averagebinary iff 2 labels in y one of which is pos_label As per [my comment elsewhere](https://githubcom/scikit-learn/scikit-learn/pull/2610#issuecomment-72362227) this attempts to complete #2679 by implementing the converse in the precision-recall-fscore family: binary data should not be handled specially when average  binary This PR fixes a bug where pos_labelNone -- which makes binary data not be handled specially -- was not being treated appropriately,,2436,0.7651888341543513,0.16262755102040816,41962,443.2343548925218,34.29293170010962,112.98317525380105,4572,61,1629,259,travis,jnothman,ogrisel,false,ogrisel,105,0.7047619047619048,30,1,2105,true,true,false,false,33,558,42,513,94,9,2659
6097257,scikit-learn/scikit-learn,python,4190,1422773684,1423221867,1423221867,7469,7469,commits_in_master,false,false,false,166,5,2,5,14,0,19,0,4,0,0,5,6,5,0,0,0,0,6,6,6,0,0,197,99,213,107,21.224971361701932,0.4717405498740554,33,trev.stephens@gmail.com,sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/utils/__init__.py|sklearn/utils/class_weight.py|sklearn/utils/tests/test_class_weight.py,26,0.005108556832694764,1,7,false,Refactor - Farm out class_weight calcs to utils With #4114 bringing a few more ensembles onboard the class_weight bandwagon a fair bit of duplicated code is being proposed I know that @amueller was originally concerned about code duplication in the original RF/Tree PR and I think that this function may alleviate some of thatThis PR farms out the calculations and some of the error checks for the expanded_class_weight variable to a new sklearnutils function: compute_sample_weight(class_weight y indicesNone) and refactors the code from #3961 to utilise it It also adds a bit more rigor to the input checks and testsBenefits: - Better unit testing to ensure the class_weightsubsample option is doing what we think it is doing - Will make transitioning classifiers that dont support multi-output such as the meta-estimators somewhat easier in some distant future - Removes duplicated code (the main point really)If merged I shall also make appropriate mods to the code in #4114 to also take advantage of this helper function,,2434,0.7654067378800329,0.16283524904214558,41962,443.2343548925218,34.29293170010962,112.98317525380105,4569,60,1629,228,travis,trevorstephens,glouppe,false,glouppe,8,0.75,117,51,538,true,true,true,false,5,45,6,8,14,0,10
6083907,scikit-learn/scikit-learn,python,4189,1422657027,1422744996,1422744996,1466,1466,commits_in_master,false,true,false,7,20,7,8,32,2,42,0,7,0,0,7,7,6,0,0,0,0,7,7,6,0,0,1255,283,2868,1000,132.77456167494145,2.9510119785999627,84,trev.stephens@gmail.com,sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|doc/whats_new.rst|sklearn/svm/base.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py,80,0.0,0,19,true,WIP Sparse decision function Rebase of #1586,,2433,0.7653103164817098,0.16325224071702946,41962,443.2343548925218,34.29293170010962,112.98317525380105,4562,60,1627,271,travis,amueller,amueller,true,amueller,249,0.8554216867469879,1083,40,1561,true,true,true,false,115,892,80,375,121,6,1280
6079224,scikit-learn/scikit-learn,python,4188,1422634002,1422643217,1422643217,153,153,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.676638910298697,0.10394173693389996,0,,doc/modules/feature_selection.rst,0,0.0,0,0,true,5 of 6 are zeros not ones ,,2432,0.7652138157894737,0.16282051282051282,41962,443.2343548925218,34.29293170010962,112.98317525380105,4562,60,1627,222,travis,miniyou,larsmans,false,larsmans,0,0,1,3,1598,false,false,false,false,0,0,0,0,0,0,-1
6074565,scikit-learn/scikit-learn,python,4186,1422597507,1423240377,1423240377,10714,10714,commits_in_master,false,false,false,17,3,2,8,7,0,15,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,0,250,0,258,27.70937307435841,0.6158611819175701,27,trev.stephens@gmail.com,sklearn/feature_selection/tests/test_rfe.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_pipeline.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_pipeline.py,20,0.00701530612244898,0,1,true,Fixes issue #4050 Explicitly tests RFE cross_validation and pipeline for non-inheriting estimators as mentions in issue #4050 ,,2430,0.7654320987654321,0.16198979591836735,41962,443.2343548925218,34.29293170010962,112.98317525380105,4556,61,1627,226,travis,xbhsu,ogrisel,false,ogrisel,2,0.0,2,4,277,true,false,false,false,0,6,3,0,3,0,153
6073911,scikit-learn/scikit-learn,python,4185,1422593259,,1425140591,42455,,unknown,false,true,false,19,7,1,4,42,0,46,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,11,0,132,4.725998655503783,0.10503879354862664,5,trev.stephens@gmail.com,sklearn/tests/test_isotonic.py,5,0.0031887755102040817,0,25,false,Adding unit test to cover ties/duplicate x values in Isotonic Regression Unit test to highlight regression in issue #4184 ,,2429,0.7657472210786331,0.16198979591836735,41962,443.2343548925218,34.29293170010962,112.98317525380105,4556,61,1626,251,travis,mjbommar,mjbommar,true,,11,0.9090909090909091,39,14,2085,false,false,false,false,1,5,0,0,3,0,924
6071623,scikit-learn/scikit-learn,python,4183,1422581693,,1423447293,14426,,unknown,false,false,false,7,5,1,10,8,0,18,0,5,0,0,1,2,1,0,0,0,0,2,2,1,0,0,23,0,50,0,4.550339996321364,0.10113469425394302,4,trev.stephens@gmail.com,sklearn/mixture/gmm.py,4,0.0025673940949935813,0,5,false,ENH: improve GMM convergence check (issue #4178) ,,2428,0.7660626029654036,0.16110397946084723,41962,443.11519946618364,34.29293170010962,112.98317525380105,4556,61,1626,230,travis,hbredin,ogrisel,false,,0,0,14,12,1221,false,false,false,false,1,3,0,0,1,0,21
6070540,scikit-learn/scikit-learn,python,4182,1422577213,1442731046,1442731046,335897,335897,merged_in_comments,false,true,false,11,5,1,0,11,5,16,0,3,0,0,3,4,3,0,0,0,0,4,4,3,0,0,15,39,99,247,13.585969155447643,0.3019582795550425,8,trev.stephens@gmail.com,sklearn/mixture/dpgmm.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py,8,0.0,0,0,false,[MRG] Give dpgmm a covars_ property allows for sampling Fixes #1637,,2427,0.7659662134322208,0.16025641025641027,41962,443.11519946618364,34.29293170010962,112.98317525380105,4555,61,1626,364,travis,amueller,amueller,true,amueller,248,0.8548387096774194,1083,40,1560,true,true,false,false,113,873,79,372,112,6,685
6067907,scikit-learn/scikit-learn,python,4179,1422565943,1423270038,1423270038,11734,11734,commits_in_master,false,false,false,15,1,1,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,18,1,18,9.064961327871966,0.2014755145899655,3,trev.stephens@gmail.com,sklearn/kernel_approximation.py|sklearn/tests/test_kernel_approximation.py,3,0.0019230769230769232,0,0,false,[MRG] Make nystroem approximation robust to singular kernel Fixes #1860Sorry for the long turnaround,,2425,0.7661855670103093,0.16025641025641027,41962,443.11519946618364,34.29293170010962,112.98317525380105,4555,61,1626,226,travis,amueller,larsmans,false,larsmans,247,0.854251012145749,1083,40,1560,true,true,true,true,112,872,78,372,112,6,-1
6062641,scikit-learn/scikit-learn,python,4177,1422541138,,1422556112,249,,unknown,false,false,false,14,23,23,0,3,2,5,2,2,4,0,5,9,6,0,1,4,0,5,9,6,0,1,1957,0,1957,0,123.0064729169953,2.7339278767104718,9,trev.stephens@gmail.com,.gitignore|doc/modules/feature_extraction.rst|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|examples/text_topics_extractor.py|.gitignore|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tokenize_utils.py|sklearn/feature_extraction/text.py|.gitignore|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/readability.py|sklearn/feature_extraction/syllables_en.py|sklearn/feature_extraction/text.py,7,0.0,1,1,false,Fix lda lsi super constructor @kolia1985 please take a look and merge if needed,,2424,0.7665016501650165,0.15839694656488548,41956,443.1785680236438,34.2978358280103,112.99933263418819,4552,61,1626,216,travis,nadersoliman,nadersoliman,true,,0,0,4,3,1241,false,false,false,false,0,0,0,0,0,0,230
6055859,scikit-learn/scikit-learn,python,4176,1422496243,1423326368,1423326368,13835,13835,commits_in_master,false,false,false,16,3,1,4,3,0,7,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,29,24,88,72,9.087747539382104,0.20198363258990046,8,trev.stephens@gmail.com,sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py,6,0.0038192234245703373,0,0,false,Better error messages in MeanShift slightly more robust to bad binning Fixes #2356 (see discussion there),,2423,0.7664052827073875,0.1578612348822406,41842,443.07155489699346,34.31958319391998,112.75751637111037,4548,61,1625,228,travis,amueller,amueller,true,amueller,246,0.8536585365853658,1082,40,1559,true,true,false,false,110,850,77,372,112,6,6
6053826,scikit-learn/scikit-learn,python,4175,1422487510,,1442731045,337392,,unknown,false,true,false,38,1,1,0,4,0,4,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.458895526218775,0.09910309587954941,8,trev.stephens@gmail.com,sklearn/mixture/dpgmm.py,8,0.005098789037603569,0,0,false,[MRG] Fix gamma update in DPGMM Fixes #1764Tests dont change I have no idea how to test for thisIn the light of #2454 it seems unlikely we can currently test for this in a sensible way,,2422,0.7667217175887696,0.15806246016571066,41842,443.07155489699346,34.31958319391998,112.75751637111037,4548,61,1625,364,travis,amueller,amueller,true,,245,0.8571428571428571,1082,40,1559,true,true,false,false,110,845,76,371,112,6,31045
6053560,scikit-learn/scikit-learn,python,4174,1422486336,1422544264,1422544264,965,965,commits_in_master,false,false,false,14,6,4,10,5,0,15,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,95,10,95,20,36.13177484875551,0.8030622664467468,0,,sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_robust_covariance.py,0,0.0,0,0,false,MAINT threshold - threshold_ with deprecation warning to be removed in 018 fixes #4173 ,,2421,0.7666253614209004,0.15806246016571066,41842,443.07155489699346,34.31958319391998,112.75751637111037,4547,61,1625,217,travis,ragv,GaelVaroquaux,false,GaelVaroquaux,23,0.5652173913043478,3,1,89,true,false,false,false,12,257,31,272,287,0,3
6041108,scikit-learn/scikit-learn,python,4171,1422415011,1422459348,1422459348,738,738,commits_in_master,false,false,false,14,1,1,0,7,0,7,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.775483235471079,0.10613944581374599,0,,doc/tutorial/statistical_inference/model_selection.rst,0,0.0,0,1,false,[MRG] Doc: search C not gamma of linear svm in tutorial Maybe fixes #3389,,2420,0.7665289256198347,0.15772669220945082,41842,443.07155489699346,34.31958319391998,112.75751637111037,4542,61,1624,215,travis,amueller,larsmans,false,larsmans,244,0.8565573770491803,1082,40,1558,true,true,true,true,109,822,75,366,112,6,2
6037119,scikit-learn/scikit-learn,python,4169,1422397539,1422458870,1422458870,1022,1022,commit_sha_in_comments,false,false,false,6,3,1,10,4,0,14,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,76,0,96,0,4.538476625748796,0.1008719093917714,7,trev.stephens@gmail.com,sklearn/svm/classes.py,7,0.004487179487179487,0,0,false,minor documentation corrections to ensure consistency ,,2419,0.7664324100868127,0.1576923076923077,41842,443.04765546579995,34.29568376272645,112.75751637111037,4540,61,1624,216,travis,MMKrell,larsmans,false,larsmans,0,0,7,7,537,false,true,false,false,0,0,0,0,1,0,40
6025951,scikit-learn/scikit-learn,python,4165,1422331937,1424827735,1424827735,41596,41596,commits_in_master,false,false,false,52,2,1,2,2,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,21,4,42,8.8377726049111,0.19642798679746928,19,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,18,0.011620400258231117,1,0,false,[MRG] make defensive copies in GradientBoosting*staged_decision function Fixes #3741@pprett do you see any problem with thisI feel like a function that is solely for debugging should not worry that much about memory consumption Also the memory consumption is really not that large as GRBT is usually used for binary cases,,2418,0.7663358147229115,0.15752098127824402,41842,442.4023708235744,34.24788490033937,112.63801921514268,4530,61,1623,253,travis,amueller,amueller,true,amueller,243,0.8559670781893004,1082,40,1557,true,true,false,false,106,796,74,348,112,6,734
6020924,scikit-learn/scikit-learn,python,4163,1422308996,1445720039,1445720039,390184,390184,commits_in_master,false,true,false,16,30,3,211,150,2,363,0,10,9,0,7,34,13,0,0,12,5,20,37,25,0,2,1229,161,5856,572,73.87088276862553,1.641853387539061,45,trev.stephens@gmail.com,sklearn/tree/benchmark_iforest.py|sklearn/tree/plot_iforest.py|sklearn/tree/iforest.py|benchmarks/bench_isolation_forest.py|doc/datasets/kddcup99.rst|doc/modules/classes.rst|doc/modules/outlier_detection.rst|examples/covariance/plot_outlier_detection.py|examples/ensemble/plot_isolation_forest.py|sklearn/datasets/__init__.py|sklearn/datasets/kddcup99.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/iforest.py|sklearn/ensemble/tests/test_iforest.py,26,0.0,0,56,false,WIP: Iforest - new anomaly detection algo Isolation Forest (iForest) algorithmBased on previous discussion http://sourceforgenet/p/scikit-learn/mailman/message/32485020,,2417,0.7662391394290443,0.15643180349062702,41841,442.19784421978443,34.22480342248034,112.61681126168114,4529,61,1623,399,travis,ngoix,glouppe,false,glouppe,1,1.0,4,1,469,true,false,false,false,0,2,0,2,1,0,2
6019883,scikit-learn/scikit-learn,python,4162,1422304324,,1434484168,202997,,unknown,false,true,false,25,4,4,0,20,0,20,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,121,20,121,20,34.470073294561246,0.766131451070625,29,trev.stephens@gmail.com,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,25,0.016170763260025874,1,1,false,[MRG after #3907] TST Add test to check if estimators reset model when fit is called @amueller Please take a look if this is sufficient,,2416,0.7665562913907285,0.15653298835705046,41841,442.19784421978443,34.22480342248034,112.61681126168114,4529,61,1623,323,travis,ragv,rvraghav93,false,,22,0.5909090909090909,3,1,87,true,false,false,false,11,244,30,262,287,0,11
6013883,scikit-learn/scikit-learn,python,4161,1422265181,1431080498,1431080498,146921,146921,commits_in_master,false,true,false,68,324,21,75,112,0,187,0,12,4,0,24,251,25,0,2,11,8,239,258,217,0,6,1155,424,10014,6991,207.9458272425369,4.62177492090737,66,wu@minus.com,doc/modules/ensemble.rst|sklearn/ensemble/__init__.py|sklearn/ensemble/ensemble_classifier.py|sklearn/ensemble/tests/test_ensemble_classifier.py|setup.cfg|setup.py|examples/svm/plot_rbf_parameters.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/base.py|sklearn/mixture/gmm.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/tests/test_metaestimators.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/tests/test_dbscan.py|.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/ensemble.rst|sklearn/ensemble/__init__.py|sklearn/ensemble/ensemble_classifier.py|sklearn/ensemble/tests/test_ensemble_classifier.py|doc/modules/ensemble.rst|sklearn/ensemble/__init__.py|sklearn/ensemble/ensemble_classifier.py|sklearn/ensemble/tests/test_ensemble_classifier.py|doc/modules/ensemble.rst|sklearn/ensemble/__init__.py|sklearn/ensemble/ensemble_classifier.py|sklearn/ensemble/tests/test_ensemble_classifier.py,19,0.0012970168612191958,0,35,false,EnsembleClassifier implementation This is an implementation of an estimator to combine scikit-learn classification estimators into an ensemble classifier for majority rule class label prediction or classification based on weighted probability averagesI added a section at the bottom of the ensemblerst documentation with a more detailed description and examplesId be happy to make improvements or adjustments if necessary Please let me know I am happy to help,,2415,0.7664596273291926,0.1549935149156939,41841,440.59654405965443,34.12920341292034,112.30611123061112,4526,60,1623,299,travis,rasbt,agramfort,false,agramfort,0,0,1168,41,478,true,true,false,false,0,0,0,0,9,0,6
6013297,scikit-learn/scikit-learn,python,4160,1422258680,1422265368,1422265368,111,111,commits_in_master,false,false,false,35,1,1,0,2,0,2,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,3.9916742859485983,0.08871839532375218,17,trev.stephens@gmail.com,doc/modules/model_evaluation.rst,17,0.011031797534068787,1,2,false,[MRG] Fix model evaluation documentation table - Fixes #4098 Looks like the problem @amueller mentioned in #4098 was simply a problem in the RST formatting My build now looks like this with this fix:[doc-fix](https://cloudgithubusercontentcom/assets/5210848/5895143/4b8ddea6-a4cb-11e4-9b2d-bd640f54f03fpng),,2414,0.7663628831814416,0.15509409474367294,41841,440.59654405965443,34.12920341292034,112.30611123061112,4526,60,1623,217,travis,trevorstephens,jnothman,false,jnothman,7,0.7142857142857143,117,51,532,true,true,true,false,5,43,6,8,14,0,111
6008595,scikit-learn/scikit-learn,python,4159,1422219083,1422222001,1422222001,48,48,commits_in_master,false,false,false,73,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,4.482348707692243,0.09962427207990333,0,,examples/gaussian_process/gp_diabetes_dataset.py,0,0.0,0,1,false,DOC: copy-edits and active voice Thanks for this amazing project Ive been using it extensively in a course for health metricians this quarterSome of my students have been very interested in the Gaussian processes part of sklearn and they inspired me to edit the introduction to the gp_diabetes_datasetpy example  Here are those edits as a pull request for your consideration for inclusion in sklearnThanks again for all your work on this,,2413,0.7662660588479072,0.15474642392717816,41841,440.59654405965443,34.12920341292034,112.30611123061112,4524,60,1622,216,travis,aflaxman,GaelVaroquaux,false,GaelVaroquaux,2,1.0,97,9,2183,false,false,false,false,0,0,0,0,0,0,5
6006173,scikit-learn/scikit-learn,python,4158,1422192280,1422193424,1422193424,19,19,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.533681760416062,0.10076519612029072,74,trev.stephens@gmail.com,doc/whats_new.rst,74,0.048145738451528954,0,0,false,DOC Correct whats new entry of Jaccard Similarity Score ,,2412,0.7661691542288557,0.1541964866623292,41841,440.59654405965443,34.12920341292034,112.30611123061112,4522,60,1622,216,travis,ragv,jnothman,false,jnothman,21,0.5714285714285714,3,1,86,true,false,false,false,11,238,29,254,287,0,19
6003395,scikit-learn/scikit-learn,python,4157,1422159020,,1426628885,74497,,unknown,false,true,false,65,4,4,11,30,0,41,0,8,2,0,3,5,4,0,1,2,0,3,5,4,0,1,425,0,425,0,70.71338069342559,1.5716691312819224,13,trev.stephens@gmail.com,.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py|.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py|.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py|.gitattributes|sklearn/cluster/_dbscan_inner.cpp|sklearn/cluster/_dbscan_inner.pyx|sklearn/cluster/dbscan_.py|sklearn/cluster/setup.py,13,0.0,0,2,false,[MRG] optimize DBSCAN by rewriting in Cython Follow-up to #4151: Cython/C++ version of DBSCAN ~30% off the running time for a 37e5 set of 3-d points ball tree radius queries are now the bottleneck at 72% of the time The inner loop/DFS algorithm takes 12% of the timeI have some ideas about optimizing the radius queries but Ill leave them out of this PR,,2411,0.7664869348817918,0.1541964866623292,41841,440.59654405965443,34.12920341292034,112.30611123061112,4520,60,1621,267,travis,larsmans,ogrisel,false,,128,0.75,147,38,1651,true,true,true,true,26,85,34,24,83,2,1
6002779,scikit-learn/scikit-learn,python,4156,1422154195,1422301835,1422301835,2460,2460,commits_in_master,false,false,false,3,1,1,0,8,0,8,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,9,2,9,9.216746247081627,0.20485055905046656,3,trev.stephens@gmail.com,sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py,3,0.001954397394136808,0,0,false,Fix issue #4154 ,,2410,0.766390041493776,0.15374592833876222,41841,440.59654405965443,34.12920341292034,112.30611123061112,4520,60,1621,221,travis,AlexanderFabisch,amueller,false,amueller,10,0.8,35,27,1311,false,true,true,false,2,18,0,27,0,0,2460
6001376,scikit-learn/scikit-learn,python,4155,1422143531,1422301572,1422301572,2634,2634,commits_in_master,false,false,false,30,1,1,0,2,0,2,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,4,24,4,24,13.531434688047522,0.3007484296835346,86,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,74,0.007807417046193884,1,0,false,[MRG] NB partial_fit with class_prior - Fixes #3186 Fixes #3186 @amueller This actually affected both BernoulliNB and MultinomialNB as they share the partial_fit function Added a test for this too,,2409,0.7662930676629307,0.15354586857514638,41841,440.59654405965443,34.12920341292034,112.30611123061112,4520,60,1621,216,travis,trevorstephens,amueller,false,amueller,6,0.6666666666666666,116,51,530,true,true,true,false,5,46,5,8,14,0,78
5985923,scikit-learn/scikit-learn,python,4153,1422024461,1422045212,1422045212,345,345,commits_in_master,false,false,false,28,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.595565608176408,0.10214011724623409,16,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py,16,0.010329244673983214,0,0,true,Fix for overflow warning Use expit function to compute the negative gradient and avoid exp overflow This is a patch for Issue #4152 (RuntimeWarning: overflow encountered in exp),,2408,0.7661960132890365,0.14912846998063267,41812,440.97388309576195,34.17679135176505,112.33617143403808,4508,62,1620,219,travis,hsalamin,larsmans,false,larsmans,0,0,0,1,1691,false,false,false,false,0,0,0,0,0,0,-1
5985415,scikit-learn/scikit-learn,python,4151,1422021315,1422026778,1422026778,91,91,commits_in_master,false,true,false,54,2,2,0,0,0,0,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,8.413863572226418,0.18700484010760213,11,trev.stephens@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py,11,0.0071013557133634605,0,3,true,ENH: optimize DBSCAN (~10% faster on my data) Ive found that on a set of 380k samples (3 features point clouds) radius_neighbors takes 33% of the time npunique 32% and intersect1d 15% This speeds up intersect1d by a factor two and takes away some minor overheadAlso fixes the path of an example script,,2407,0.7660988782717075,0.14912846998063267,41812,440.97388309576195,34.17679135176505,112.33617143403808,4508,62,1620,219,travis,larsmans,larsmans,true,larsmans,127,0.7480314960629921,147,38,1650,true,true,false,false,24,77,28,24,78,0,-1
5983168,scikit-learn/scikit-learn,python,4150,1422001685,1422113846,1422113846,1869,1869,commit_sha_in_comments,false,false,false,37,9,7,6,9,0,15,0,5,0,0,5,5,3,0,0,0,0,5,5,3,0,0,44,36,50,36,35.49016623252432,0.788797299211406,124,trev.stephens@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|doc/modules/classes.rst|doc/whats_new.rst|doc/whats_new.rst|sklearn/ensemble/gradient_boosting.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,72,0.01610824742268041,0,6,true,fix agglomerative clustering w/ precomputed distances & connectivity matrix linkage_tree doesnt calculate distances when affinity  precomputed and connectivity is not None PR includes fix & test case let me know if theres anything I should change,,2406,0.7660016625103907,0.148840206185567,41812,440.97388309576195,34.17679135176505,112.33617143403808,4506,62,1620,217,travis,cathydeng,jnothman,false,jnothman,0,0,49,0,1085,false,false,false,false,0,0,0,0,1,0,10
5979756,scikit-learn/scikit-learn,python,4148,1421979563,1422123725,1422123725,2402,2402,commits_in_master,false,false,false,12,2,1,2,2,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,11,0,36,8.734721060180732,0.19413610036363077,25,trev.stephens@gmail.com,sklearn/cluster/tests/test_hierarchical.py|sklearn/datasets/tests/test_svmlight_format.py,25,0.016129032258064516,0,1,false,[MRG] remove tempfiles that we create in testing Fixes #4007 and #4099,,2405,0.765904365904366,0.14838709677419354,41813,440.96333676129433,34.1759739793844,112.33348480137757,4506,62,1619,216,travis,amueller,larsmans,false,larsmans,242,0.8553719008264463,1076,40,1553,true,true,true,true,61,684,69,334,108,6,709
5978707,scikit-learn/scikit-learn,python,4147,1421974917,1422376354,1422376354,6690,6690,commits_in_master,false,false,false,21,4,1,9,10,0,19,0,4,0,0,2,4,2,0,0,0,0,4,4,3,0,0,2,12,8,96,8.952744632861423,0.1989818471134074,4,manojkumarsivaraj334@gmail.com,sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py,4,0.00258732212160414,0,8,false,[MRG] Sort labels in precision_recall_fscore_support Fixes #3670The code already stores and restores the ordering so we can discard it here,,2404,0.7658069883527454,0.14877102199223805,41813,440.96333676129433,34.1759739793844,112.33348480137757,4506,62,1619,221,travis,amueller,larsmans,false,larsmans,241,0.8547717842323651,1076,40,1553,true,true,true,true,60,679,68,334,105,6,4
5978393,scikit-learn/scikit-learn,python,4146,1421973620,1424830520,1424830520,47615,47615,commits_in_master,false,false,false,49,12,7,4,11,0,15,0,6,0,0,4,4,2,0,0,0,0,4,4,2,0,0,15,326,42,782,53.58411522147324,1.190949521734338,83,trev.stephens@gmail.com,doc/whats_new.rst|doc/modules/classes.rst|doc/whats_new.rst|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,70,0.010996119016817595,2,5,false,[MRG + 1] Fdr treshold bug Continues #2932 Fixes #2771These are some minor fixes on top of #2932 where @bthirion already gave his +1Maybe @arjoly wants to have a look as he commented thereThis is a good bug fix that I think we should include asap,,2403,0.7657095297544736,0.14877102199223805,41813,440.96333676129433,34.1759739793844,112.33348480137757,4505,62,1619,255,travis,amueller,amueller,true,amueller,240,0.8541666666666666,1076,40,1553,true,true,false,false,60,679,67,334,105,6,932
5968375,scikit-learn/scikit-learn,python,4142,1421919368,1421963395,1421963395,733,733,merged_in_comments,false,false,false,6,1,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.28920104998761,0.09533090017427594,12,trev.stephens@gmail.com,sklearn/naive_bayes.py,12,0.0077269800386349004,0,2,false,FIX Return correct variance Fixes #4135 ,,2402,0.7656119900083264,0.14810045074050227,41813,440.96333676129433,34.1759739793844,112.33348480137757,4494,62,1619,215,travis,ragv,amueller,false,amueller,20,0.55,3,1,83,true,false,false,false,11,234,28,247,287,0,149
5963632,scikit-learn/scikit-learn,python,4141,1421888059,1422123860,1422123860,3930,3930,commits_in_master,false,false,false,27,1,1,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,2,12,2,8.661535320783457,0.19250969483331404,18,trev.stephens@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,15,0.009664948453608248,0,2,false,FIX check for integer 1 before checking for floating 1 pythonisinstance(1 numbersreal)  Trueso we need to change the order of evaluationFixes #4070,,2401,0.7655143690129113,0.14755154639175258,41794,429.65497439823895,33.40192372110829,109.08264344164235,4493,62,1618,217,travis,amueller,larsmans,false,larsmans,239,0.8535564853556485,1076,40,1552,true,true,true,true,57,643,63,326,104,6,-1
5956207,scikit-learn/scikit-learn,python,4139,1421855475,1421856923,1421856923,24,24,commits_in_master,false,false,false,8,2,2,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,8.923014333843282,0.19832120031645994,0,,sklearn/ensemble/base.py|sklearn/ensemble/base.py,0,0.0,0,0,false,[MRG] FIX Support for negative values of n_jobs ,,2400,0.7654166666666666,0.1470019342359768,41794,429.65497439823895,33.40192372110829,109.08264344164235,4490,62,1618,215,travis,MSusik,glouppe,false,glouppe,0,0,5,0,344,false,false,false,false,0,0,0,0,0,0,3
5950036,scikit-learn/scikit-learn,python,4137,1421809339,,1424958868,52492,,unknown,false,false,false,7,2,1,5,7,0,12,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,6,4,12,12,9.069457092984017,0.20157590534835385,10,trev.stephens@gmail.com,sklearn/ensemble/bagging.py|sklearn/tests/test_metaestimators.py,7,0.004510309278350515,0,0,false,[MRG] Make BaggingClassifier use if_delegate_has_method in decision_function ,,2399,0.7657357232180075,0.14497422680412372,41794,429.65497439823895,33.40192372110829,109.08264344164235,4488,62,1617,251,travis,amueller,ogrisel,false,,238,0.8571428571428571,1075,40,1551,true,true,true,false,56,633,62,308,98,6,44
5949642,scikit-learn/scikit-learn,python,4136,1421807463,1423880039,1423880039,34542,34542,commits_in_master,false,false,false,15,12,5,38,18,0,56,0,5,0,0,16,17,16,0,0,0,0,17,17,17,0,0,456,252,528,301,284.875308562347,6.331580561670999,87,trev.stephens@gmail.com,sklearn/cluster/spectral.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/omp.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/manifold/mds.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/approximate.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/omp.py|sklearn/manifold/mds.py|sklearn/manifold/t_sne.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/approximate.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/omp.py|sklearn/manifold/mds.py|sklearn/manifold/t_sne.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/approximate.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/omp.py|sklearn/manifold/mds.py|sklearn/manifold/t_sne.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/approximate.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py,35,0.002577319587628866,0,3,false,Test more estimators test dtype handling Hack for #4134 fixing parts of #4056 fixing #4124,,2398,0.7656380316930775,0.14497422680412372,41794,429.65497439823895,33.40192372110829,109.08264344164235,4488,62,1617,239,travis,amueller,amueller,true,amueller,237,0.8565400843881856,1075,40,1551,true,true,false,false,55,632,61,308,98,6,1174
5946145,scikit-learn/scikit-learn,python,4131,1421792155,1422321521,1422321521,8822,8822,commits_in_master,false,false,false,41,2,1,1,3,0,4,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.6696209028960824,0.1037859311534901,4,joel.nothman@gmail.com,doc/faq.rst,4,0.0025789813023855577,1,2,false,[MRG] Explain why we are somewhat selective lower citiation rule of thumb I lowered the 1000+ citations to 200+ citations (I think suggested by @mblondel) and tried to add an explanation and link to the discussion about why we are selective,,2397,0.7655402586566542,0.14442295293359123,41794,429.7028281571517,33.40192372110829,109.08264344164235,4488,62,1617,221,travis,amueller,jnothman,false,jnothman,236,0.8559322033898306,1075,40,1551,true,true,false,false,53,618,60,301,96,6,462
5933751,scikit-learn/scikit-learn,python,4125,1421713501,1432688562,1432688562,182917,182917,commits_in_master,false,false,false,77,21,1,56,25,0,81,0,8,0,0,3,5,2,0,0,2,0,5,7,4,0,0,253,94,533,129,13.773185791360309,0.3064339207864814,13,wu@minus.com,doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,11,0.003236245954692557,0,11,false,ENH RobustScaler This PR adds RobustScaler and robust_scale as alternative to StandardScaler and scale They use robust estimates of data center/scale (median & interquartile range) which will work better for data with outliersParts of this were discussed before in #2514 I originally wanted to submit this only after #3639 was merged but sending the PR now allows it to be discussed concurrently (if either this or #3639 get merged Ill of course update the other commit),,2394,0.7660818713450293,0.1436893203883495,41733,430.3309131862075,33.45074641171256,109.24208659813577,4484,62,1616,300,travis,untom,amueller,false,amueller,10,0.7,10,0,700,true,false,false,false,0,12,1,2,3,0,129
5921102,scikit-learn/scikit-learn,python,4121,1421621757,1425158711,1425158711,58949,58949,commit_sha_in_comments,false,true,false,34,6,3,3,20,0,23,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,4,15,4,66,12.519781397462369,0.27854744422460426,10,trev.stephens@gmail.com,sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/base.py,7,0.0045336787564766836,0,9,false,[WIP] Fix for issue #4072 This PR makes the radius_neighbors for brute force include points lying on boundary of chosen radius This is the same behavior as the KD Tree and Ball Tree radius_neighbors ,,2393,0.7659841203510238,0.14378238341968913,41733,430.3309131862075,33.45074641171256,109.24208659813577,4478,62,1615,251,travis,seowyanyi,ogrisel,false,ogrisel,1,1.0,6,16,484,true,false,false,false,0,2,2,0,3,0,319
5920069,scikit-learn/scikit-learn,python,4120,1421613231,1421620243,1421620243,116,116,merged_in_comments,false,false,false,2,3,2,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,15,2,15,8.654363725848686,0.192547363304114,10,trev.stephens@gmail.com,sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py,7,0.004548408057179987,0,1,false,Fix 4072 ,,2392,0.7658862876254181,0.1442495126705653,41733,430.3309131862075,33.45074641171256,109.24208659813577,4478,61,1615,209,travis,seowyanyi,seowyanyi,true,seowyanyi,0,0,6,16,484,false,false,false,false,0,0,0,0,0,0,81
5917700,scikit-learn/scikit-learn,python,4118,1421589096,1421589806,1421589806,11,11,commits_in_master,false,false,false,24,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.6664232836944715,0.10382142375529259,15,trev.stephens@gmail.com,doc/modules/classes.rst,15,0.009823182711198428,0,0,false,MAINT kernel_ridge added to classesrst This is fixes an issue introduced in PR #3942 where I forgot adding the new kernel_ridge module to classesrst,,2390,0.7661087866108787,0.14341846758349705,41723,430.2183447978333,33.43479615559764,109.14843132085421,4476,61,1615,209,travis,jmetzen,agramfort,false,agramfort,14,0.5,13,2,1196,true,true,false,false,2,37,5,24,278,0,10
5916829,scikit-learn/scikit-learn,python,4117,1421574662,1421579761,1421579761,84,84,commits_in_master,false,false,false,26,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.343821044205191,0.09664388422198116,44,trev.stephens@gmail.com,sklearn/lda.py,44,0.028871391076115485,0,1,false,fix import error in ldapy Fix import error in python 34 environmentwhen I run from sklearnlda import LDA  got ImportError: No module named six,,2389,0.7660108832147342,0.14238845144356954,41723,430.2183447978333,33.43479615559764,109.14843132085421,4475,60,1615,209,travis,chyikwei,mblondel,false,mblondel,3,0.6666666666666666,17,1,845,true,true,false,false,0,9,0,2,11,0,-1
5916050,scikit-learn/scikit-learn,python,4116,1421563785,1422127277,1422127277,9391,9391,commits_in_master,false,false,false,33,13,2,8,7,0,15,0,5,0,0,2,5,2,0,0,0,0,5,5,4,0,0,94,70,168,70,17.75869139109183,0.3951055216998678,15,trev.stephens@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,12,0.007879185817465528,3,2,false,[MRG] Simplify sample_weight support in Ridge I simplified the code by doing the rescaling upfront This allows to support sample_weight in all solvers and fixes the long-standing bug #1190 Ping @eickenberg @fabianp @agramfort ,,2388,0.7659128978224455,0.14182534471437952,41723,430.1464420104019,33.43479615559764,109.14843132085421,4474,60,1615,219,travis,mblondel,mblondel,true,mblondel,31,0.6774193548387096,433,32,1755,true,true,false,false,4,64,1,26,31,1,9
5914314,scikit-learn/scikit-learn,python,4115,1421545004,,1452479579,515576,,unknown,false,false,false,32,14,5,27,18,0,45,0,7,0,0,2,4,1,0,0,0,0,4,4,3,0,0,38,0,56,246,22.941971710632853,0.512594614574367,67,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/metrics/classification.py|doc/whats_new.rst|sklearn/metrics/classification.py|doc/whats_new.rst,67,0.04425363276089828,1,6,false,Adding sample_weight support to matthews_corrcoef metric Partially fixes #3450 Based on Jatin Shahs implementation of wcorrcoeff [here](https://githubcom/scikit-learn/scikit-learn/issues/3450#issuecomment-51236631)@jatinshah Have credited you in the whats new entry ThanksShould there be any tests,,2387,0.7662337662337663,0.14002642007926025,41525,431.11378687537626,33.42564720048164,109.2594822396147,4473,59,1614,474,travis,ragv,MechCoder,false,,18,0.6111111111111112,3,1,78,true,false,false,false,11,218,25,233,273,0,16
5914225,scikit-learn/scikit-learn,python,4114,1421544201,,1423292327,29135,,unknown,false,true,false,240,3,1,2,8,0,10,0,4,0,0,3,8,3,0,0,0,0,8,8,7,0,0,235,0,361,169,13.103493753381404,0.2927725835778604,22,trev.stephens@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py,17,0.005284015852047556,3,3,false,[WIP] class_weight for Bagging AdaBoost & GradientBoosting classifiers Initial early crack at the rest of the ensembles carrying on from #3961 My biggest question is how (or if) to handle class_weightsubsample in GradientBoostingClassifier In RandomForestClassifier I weighted based on the bootstrap sample as I have done in BaggingClassifier in this PR But the applicability to GBM is not so clear to me Right now I simply use the auto weighting as a initialization of the sample weights as the fit sample_weight param does right now and as is implemented in AdaBoostClassifier in this PR My initial thought for subsample was to fit the residuals to the re-weighted boosting sample_weight while not actually altering it and letting the iterations do their things but I feel that we may be venturing far away from the literature thereAny thoughtsAdditionally Im certain Travis will be upset with me on the infamous check_class_weight_classifiers in estimator_checks GBM may get by from the hack here: https://githubcom/scikit-learn/scikit-learn/pull/3961#discussion-diff-22450369 but the two meta estimators do not call min_weight_fraction_leaf directly and are sure to fail As one solution I could add another bandaid to check if the estimator has base_estimator and set a decision tree with a similar bias against lightly weighted leaf nodes Will add individual estimator checks on top of this soon of courseCalling the previous reviewers for input: @glouppe @amueller @GaelVaroquaux  plus any of you other fine people who are willing to comment :-),,2386,0.7665549036043587,0.14002642007926025,41525,431.11378687537626,33.42564720048164,109.2594822396147,4473,59,1614,236,travis,trevorstephens,trevorstephens,true,,5,0.8,114,51,523,true,true,false,false,4,41,4,5,14,0,2001
5913117,scikit-learn/scikit-learn,python,4113,1421534959,1421542628,1421542628,127,127,commits_in_master,false,false,false,18,1,1,0,0,1,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.18700227968884,0.09355064480846184,6,trev.stephens@gmail.com,sklearn/isotonic.py,6,0.003997335109926716,1,0,false,[MRG] MAINT Remove temporary fix #3995 in view of the change to slinear @jmetzen Please take a look,,2385,0.7664570230607967,0.14057295136575615,41526,431.10340509560274,33.424842267495066,109.2568511294129,4471,59,1614,209,travis,ragv,agramfort,false,agramfort,17,0.5882352941176471,3,1,78,true,false,false,false,11,218,24,232,272,0,-1
5911563,scikit-learn/scikit-learn,python,4112,1421519802,1442949880,1442949880,357167,357167,commit_sha_in_comments,false,false,false,9,1,1,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,30,0,4.480166064189423,0.10010083495465796,9,trev.stephens@gmail.com,sklearn/qda.py,9,0.006020066889632107,0,0,false,[WIP]Parameters moved from fit to the constructor Fixes #4107,,2384,0.7663590604026845,0.1411371237458194,41526,431.10340509560274,33.424842267495066,109.2568511294129,4471,59,1614,359,travis,akshayah3,amueller,false,amueller,9,0.5555555555555556,7,5,635,true,true,false,false,1,14,7,4,2,0,10
5907446,scikit-learn/scikit-learn,python,4111,1421465712,1421513971,1421513971,804,804,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.085345191737123,0.09127969323129546,4,trev.stephens@gmail.com,sklearn/isotonic.py,4,0.00267379679144385,0,0,true,FIX Use slinear interpolation for isotonic regression Fixes #4101,,2383,0.766261015526647,0.13970588235294118,41508,431.19398670135877,33.415245253926955,109.30423050978125,4466,59,1613,209,travis,ragv,GaelVaroquaux,false,GaelVaroquaux,16,0.5625,3,1,77,true,false,false,false,11,216,23,230,269,0,751
5907010,scikit-learn/scikit-learn,python,4110,1421463073,,1421546456,1389,,unknown,false,false,false,17,2,2,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,0,8,0,15.790598326941751,0.3528125295598269,4,trev.stephens@gmail.com,sklearn/covariance/graph_lasso_.py|sklearn/gaussian_process/gaussian_process.py|sklearn/covariance/graph_lasso_.py|sklearn/gaussian_process/gaussian_process.py,3,0.002005347593582888,1,0,true,[MRG] MAINT use raise without argument for cleaner traceback Thanks to @jnothman for teaching me this ),,2382,0.7665827036104114,0.13970588235294118,41508,431.19398670135877,33.415245253926955,109.30423050978125,4466,59,1613,211,travis,ragv,ragv,true,,15,0.6,3,1,77,true,false,false,false,11,213,22,229,269,0,737
5906105,scikit-learn/scikit-learn,python,4109,1421458455,1423003937,1423003937,25758,25758,commits_in_master,false,false,false,13,6,4,2,3,0,5,0,2,0,0,2,3,2,0,0,0,0,3,3,2,0,0,51,56,75,60,37.27470135915895,0.8328361853568222,5,trev.stephens@gmail.com,sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,5,0.0033444816053511705,0,2,true,[MRG] Gmm tied covariance fixes from #4039 Following up on #4039 minor fixes,,2381,0.7664846703065938,0.13979933110367893,41508,431.19398670135877,33.415245253926955,109.30423050978125,4464,59,1613,238,travis,amueller,agramfort,false,agramfort,235,0.8553191489361702,1073,40,1547,true,true,true,false,47,570,59,280,95,6,647
5899896,scikit-learn/scikit-learn,python,4106,1421425919,1421434055,1421434055,135,135,commit_sha_in_comments,false,false,false,72,3,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,13,4.14631851635324,0.09264098568926217,6,trev.stephens@gmail.com,sklearn/manifold/spectral_embedding_.py,6,0.004043126684636119,0,0,true,Lower memory usage of spectral_embedding When calling eigsh -laplacian is given as an argument Both laplacianand -laplacian are thus in memory By assigning -laplacian to laplacianthe extra copy is freed This copy is not needed by the end of thefunction call The memory reclaimed is therefore available for the eigshcall In case of a RuntimeError the laplacian is reverted to its oldvalue to process lobpcg as usual,,2380,0.7663865546218488,0.13881401617250674,41441,429.622837286745,33.27622402934293,109.3844260514949,4461,59,1613,212,travis,scharron,ogrisel,false,ogrisel,1,1.0,16,4,1584,false,false,false,false,0,0,0,0,0,0,3
5892128,scikit-learn/scikit-learn,python,4105,1421367016,1421369320,1421369320,38,38,github,false,false,false,9,2,2,0,1,0,1,0,1,0,0,35,35,23,0,0,0,0,35,35,23,0,0,83,0,83,0,155.303115469666,3.4699286973934007,211,trev.stephens@gmail.com,doc/about.rst|doc/developers/index.rst|doc/developers/performance.rst|doc/developers/utilities.rst|doc/faq.rst|doc/install.rst|doc/modules/clustering.rst|doc/modules/cross_validation.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/unsupervised_reduction.rst|doc/whats_new.rst|examples/applications/plot_outlier_detection_housing.py|examples/cluster/plot_agglomerative_clustering_metrics.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/covariance/plot_sparse_cov.py|examples/datasets/plot_random_multilabel_dataset.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_digits.py|examples/model_selection/plot_underfitting_overfitting.py|examples/model_selection/plot_validation_curve.py|sklearn/datasets/base.py|sklearn/datasets/samples_generator.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/omp.py|sklearn/linear_model/stochastic_gradient.py|sklearn/neighbors/approximate.py|sklearn/pipeline.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/utils/validation.py|sklearn/metrics/ranking.py,65,0.002018842530282638,0,0,false,[MRG] Minor spelling fixes Using tools from in #2102,,2379,0.7662883564522909,0.13660834454912515,41443,429.60210409478077,33.27461815023044,109.37914726250513,4458,60,1612,212,travis,amueller,amueller,true,amueller,234,0.8547008547008547,1073,40,1546,true,true,false,false,45,530,53,248,92,6,38
5891865,scikit-learn/scikit-learn,python,4104,1421365774,1440950801,1440950801,326417,326417,commits_in_master,false,false,false,18,2,1,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,12,0,4.682518331049076,0.10462124139519546,1,olivier.grisel@ensta.org,examples/plot_johnson_lindenstrauss_bound.py,1,0.0006734006734006734,0,0,false,DOC Modified math expressions to theoretical bounds I think these lines need to be displayed as math expressions,,2378,0.7661900756938604,0.1367003367003367,41443,429.60210409478077,33.27461815023044,109.37914726250513,4458,60,1612,361,travis,maheshakya,GaelVaroquaux,false,GaelVaroquaux,14,0.5,3,0,1092,true,false,false,false,2,35,2,30,65,0,22
5882731,scikit-learn/scikit-learn,python,4097,1421207682,1421229620,1421229620,365,365,github,false,false,false,6,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.085192833251319,0.09136980684415523,1,t3kcit@gmail.com,sklearn/metrics/cluster/bicluster.py,1,0.0006702412868632708,0,0,false,[MRG] Remove unnecessary initialization in bicluster_pairwise_similarity ,,2377,0.7660917122423223,0.13270777479892762,41441,429.5745759030911,33.27622402934293,109.3844260514949,4441,61,1610,216,travis,amueller,amueller,true,amueller,233,0.8540772532188842,1071,40,1544,true,true,false,false,43,478,48,213,86,5,321
5845342,scikit-learn/scikit-learn,python,4096,1421199006,,1421265673,1111,,unknown,false,false,false,82,3,3,2,7,0,9,0,5,0,0,5,5,2,0,0,0,0,5,5,2,0,0,4,0,4,0,22.068693308357034,0.49358824120105604,14,trev.stephens@gmail.com,sklearn/__check_build/__init__.py|sklearn/neighbors/nearest_centroid.py|doc/documentation.rst|doc/faq.rst|doc/related_projects.rst,11,0.0006711409395973154,0,2,false,[MRG] More prominent other packages Following the discussion on the mailing list about a possible contrib package I think we came to the conclusion that we would rather like to encourage individual small packagesTo facilitate this this PR makes the list of related packages more prominent and also moves some more packages from the wiki to the websiteMost people I spoke to were not aware that we had a related packages section which seemed to be an issue for me,,2376,0.7664141414141414,0.13221476510067115,41441,429.3573996766487,33.27622402934293,109.36029535966796,4441,61,1610,218,travis,amueller,amueller,true,,232,0.8577586206896551,1071,40,1544,true,true,false,false,41,460,46,200,84,5,958
5862385,scikit-learn/scikit-learn,python,4095,1421196265,1421256617,1421256617,1005,1005,commits_in_master,false,false,false,52,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.628706662906527,0.10352562106223455,0,,sklearn/__check_build/__init__.py,0,0.0,0,1,false,BUG: Correct check for local import With the move of the check_build directory to __check_build in  cd7404706 the comparison for displaying the error message indicating a import from the source tree that has not been build inplace was incorrect  This fixes this bug so that the **INPLACE_MSG** is displayed in these cases,,2375,0.7663157894736842,0.13180901143241425,41441,429.3573996766487,33.27622402934293,109.36029535966796,4441,61,1610,218,travis,jjhelmus,ogrisel,false,ogrisel,0,0,29,22,1217,true,true,false,false,0,0,0,0,2,0,58
5880335,scikit-learn/scikit-learn,python,4094,1421195102,,1421200292,86,,unknown,false,false,false,210,1,1,0,4,0,4,0,3,3,0,2,5,5,0,0,3,0,2,5,5,0,0,259,0,259,0,17.661321823515557,0.39501300119358373,5,trev.stephens@gmail.com,sklearn/cluster/__init__.py|sklearn/cluster/_gaussian_factors.c|sklearn/cluster/_gaussian_factors.pyx|sklearn/cluster/gaussian_factors.py|sklearn/cluster/setup.py,5,0.0,0,1,false,[WIP] Gaussian Factors hierarchical clustering A fast implementation of the hierarchical clustering algorithm described in Giada and Marsili Data clustering and noise undressing for correlation matrices Phys Rev E 63 061101 (2001) [also available on arXiv]In a nutshellpython# exampleimport numpy as npfrom sklearncluster import gaussian_factorsfrom sklearndatasets import make_blobsX y  make_blobs(n_samples100 n_features1000)C  npcorrcoef(X)Z labels  gaussian_factors(C)assert(labelsmax() + 1  3)I havent found a compelling example yet (other than make_blobs ) ) but this algorithm gives results similar to affinity_propagation and is much much fasterI added a damping parameter to control the case of perfectly correlated samples You can play with it to control the number of clustersAs described in the docstring the output linkage matrix is not a true scipy linkage matrix since the distances are replaced by log-likehoods You need to map thoose to positive floats before plotting a dendrogramI tried to make this as fast as possible However for big matrices ( 5k x 5k) the implementation struggles since its complexity is O(N^3) [see the note on the bottleneck in the code] Using rb trees or something similar might be an alternative for big matricesGive me a feedback if youre interested,,2374,0.7666385846672283,0.13198653198653199,41441,429.3573996766487,33.27622402934293,109.36029535966796,4441,60,1610,218,travis,x0l,x0l,true,,3,1.0,4,25,694,true,false,false,false,0,1,0,0,2,0,17
5859565,scikit-learn/scikit-learn,python,4093,1421187950,1421247501,1421247501,992,992,commits_in_master,false,false,false,6,1,1,1,5,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.004450103706251,0.08956350319650641,11,trev.stephens@gmail.com,sklearn/neighbors/nearest_centroid.py,11,0.007432432432432433,0,0,false,FIX: Regression in NearestCentroids Fixes https://githubcom/scikit-learn/scikit-learn/issues/4074,,2373,0.7665402444163506,0.1304054054054054,41441,429.3573996766487,33.27622402934293,109.36029535966796,4440,60,1610,217,travis,MechCoder,jnothman,false,jnothman,61,0.8688524590163934,84,41,938,true,true,false,false,12,395,49,205,117,0,11
5846648,scikit-learn/scikit-learn,python,4092,1421176858,1424903301,1424903301,62107,62107,commits_in_master,false,false,false,45,1,1,3,9,0,12,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,158,0,158,0,4.902344797074238,0.10964580966978715,0,,examples/svm/plot_rbf_parameters.py,0,0.0,0,3,false,[MRG+1] ENH better RBF parameters heat map I refactored the [RBF hyperparameteres example](http://scikit-learnorg/015/auto_examples/svm/plot_rbf_parametershtml) to:- use more readable (bi-chromic) color maps- provide an analysis of the results- adapt the range of hyper-parameters to make it more interestingHere are generated figures:[figure_1](https://cloudgithubusercontentcom/assets/89061/5722848/0a76c7c8-9b40-11e4-8864-35a88a52de12png)[figure_2](https://cloudgithubusercontentcom/assets/89061/5722849/0a7840c6-9b40-11e4-8961-3098a85abf69png),,2372,0.7664418212478921,0.1294672960215779,41441,429.3573996766487,33.27622402934293,109.36029535966796,4439,60,1610,263,travis,ogrisel,amueller,false,amueller,106,0.8490566037735849,1096,124,2057,true,true,false,true,17,242,27,204,49,1,4
5854619,scikit-learn/scikit-learn,python,4090,1421119299,1440958542,1440958542,330654,330654,merged_in_comments,false,true,false,79,28,16,15,18,0,33,0,4,0,0,7,9,6,0,0,0,0,9,9,8,0,0,416,694,490,776,284.019146966457,6.352314703663264,77,t3kcit@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,66,0.0033200531208499337,1,2,false,[MRG] Support metricprecomputed in nearest neighbors This fixes up #2532 (sorry to not offer the changes back there @robertlayton: the rebase was too messy with my changes to validation in sklearnmetricspairwise) to handle queries that differ from the index data This case is a little awkward in that the index data doesnt matter (until we support XNone as in #4046) but really should be supported for KNNClassifier et al)This also provides stronger validation to precomputed metrics in pairwise_{distanceskernels},,2371,0.766343315056938,0.12682602921646746,41321,429.5394593548075,33.30025894823455,109.5568839089083,4430,60,1609,362,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,104,0.7019230769230769,30,1,2085,true,true,false,false,28,471,32,455,79,11,1451
5852098,scikit-learn/scikit-learn,python,4089,1421103877,1421104279,1421104279,6,6,github,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.323313479757478,0.09669435443884945,5,ragvrv@gmail.com,sklearn/preprocessing/data.py,5,0.0033377837116154874,0,0,false,Fix a typo in OneHotEncoders docstring were -- where,,2370,0.7662447257383966,0.1268357810413885,41321,429.5394593548075,33.30025894823455,109.5568839089083,4430,60,1609,214,travis,wujiang,wujiang,true,wujiang,0,0,6,28,2090,true,false,false,false,0,0,0,0,1,0,6
5849716,scikit-learn/scikit-learn,python,4088,1421091781,,1421091798,0,,unknown,false,false,true,1,185,185,0,0,0,0,0,0,8,0,173,181,116,1,9,8,0,173,181,116,1,9,1795,920,1795,920,1582.3846016025536,35.3912937940642,361,trev.stephens@gmail.com,doc/conf.py|sklearn/__init__.py|doc/Makefile|doc/README|sklearn/linear_model/tests/test_least_angle.py|continuous_integration/exclude_joblib_mp.txt|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/datasets/tests/test_samples_generator.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ransac.py|sklearn/tests/test_common.py|doc/modules/outlier_detection.rst|doc/install.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/covariance.rst|doc/modules/cross_decomposition.rst|doc/modules/decomposition.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/gaussian_process.rst|doc/modules/isotonic.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/lda_qda.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/modules/neural_networks.rst|doc/modules/outlier_detection.rst|doc/modules/random_projection.rst|doc/modules/scaling_strategies.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/sphinxext/gen_rst.py|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/sphinxext/gen_rst.py|sklearn/linear_model/tests/test_omp.py|sklearn/utils/testing.py|sklearn/decomposition/sparse_pca.py|sklearn/tests/test_common.py|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|doc/whats_new.rst|doc/whats_new.rst|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/_random.c|sklearn/utils/_random.pxd|sklearn/utils/_random.pyx|sklearn/utils/random.py|sklearn/utils/setup.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/tests/test_extmath.py|sklearn/cluster/k_means_.py|doc/install.rst|sklearn/cluster/k_means_.py|doc/install.rst|doc/install.rst|doc/whats_new.rst|doc/conf.py|doc/whats_new.rst|doc/index.rst|sklearn/tests/test_common.py|doc/whats_new.rst|sklearn/__init__.py|doc/documentation.rst|doc/themes/scikit-learn/layout.html|sklearn/tests/test_common.py|sklearn/cluster/k_means_.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|doc/support.rst|Makefile|sklearn/cluster/k_means_.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/utils/__init__.py|sklearn/tests/test_cross_validation.py|sklearn/utils/tests/test_utils.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/feature_extraction/tests/test_text.py|sklearn/utils/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/decomposition/pca.py|appveyor.yml|continuous_integration/appveyor/install.ps1|sklearn/feature_selection/tests/test_feature_select.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/utils/fixes.py|sklearn/utils/testing.py|continuous_integration/test_script.sh|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/utils/testing.py|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/linear_model/stochastic_gradient.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/semi_supervised/label_propagation.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/validation.py|sklearn/preprocessing/label.py|sklearn/feature_selection/tests/test_feature_select.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/whats_new.rst|doc/whats_new.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/fixes.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|doc/whats_new.rst|sklearn/linear_model/tests/test_least_angle.py|doc/whats_new.rst|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|doc/conf.py|doc/index.rst|sklearn/__init__.py|doc/whats_new.rst|sklearn/utils/testing.py|sklearn/utils/fixes.py|sklearn/utils/testing.py|sklearn/__init__.py|appveyor.yml|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast_helpers.h|continuous_integration/install.sh|doc/modules/svm.rst|doc/developers/index.rst|doc/developers/performance.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/biclustering.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/covariance.rst|doc/modules/cross_decomposition.rst|doc/modules/cross_validation.rst|doc/modules/decomposition.rst|doc/modules/dp-derivation.rst|doc/modules/feature_selection.rst|doc/modules/grid_search.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/lda_qda.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/model_persistence.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/modules/preprocessing.rst|doc/modules/random_projection.rst|doc/modules/scaling_strategies.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/support.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/index.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|sklearn/neighbors/kde.py|sklearn/neighbors/tests/test_kde.py|sklearn/neighbors/tests/test_kde.py|doc/logos/scikit-learn-logo.png|doc/testimonials/images/change-logo.png|doc/testimonials/images/howaboutwe.png|doc/testimonials/images/lovely.png|doc/testimonials/images/peerindex.png|doc/modules/outlier_detection.rst|doc/modules/linear_model.rst|doc/modules/outlier_detection.rst|doc/modules/mixture.rst|sklearn/datasets/california_housing.py|sklearn/datasets/covtype.py|sklearn/datasets/lfw.py|sklearn/datasets/species_distributions.py|sklearn/datasets/california_housing.py|sklearn/datasets/covtype.py|sklearn/datasets/lfw.py|sklearn/datasets/species_distributions.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/_isotonic.c|sklearn/_isotonic.pyx|doc/modules/classes.rst|MANIFEST.in|sklearn/metrics/tests/test_metrics.py|sklearn/cluster/k_means_.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|sklearn/feature_extraction/tests/test_text.py|doc/modules/linear_model.rst|sklearn/preprocessing/tests/test_label.py|sklearn/utils/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|benchmarks/bench_plot_neighbors.py|doc/modules/model_evaluation.rst|sklearn/metrics/pairwise.py|examples/linear_model/plot_sparse_recovery.py|sklearn/linear_model/randomized_l1.py|sklearn/feature_extraction/image.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/locally_linear.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_shortest_path.py|sklearn/ensemble/base.py|sklearn/ensemble/tests/test_base.py|CONTRIBUTING.md|doc/developers/index.rst|examples/applications/plot_out_of_core_classification.py|doc/modules/neighbors.rst|doc/modules/outlier_detection.rst|doc/modules/kernel_approximation.rst|sklearn/covariance/robust_covariance.py|doc/install.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|doc/sphinxext/numpy_ext/docscrape_sphinx.py|examples/plot_rfe_digits.py|sklearn/feature_selection/rfe.py|sklearn/externals/joblib/numpy_pickle.py|doc/modules/manifold.rst|examples/cluster/plot_kmeans_digits.py|sklearn/manifold/mds.py|sklearn/datasets/twenty_newsgroups.py|doc/whats_new.rst|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/gp_diabetes_dataset.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py|doc/faq.rst|doc/whats_new.rst|sklearn/datasets/samples_generator.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|examples/plot_kernel_approximation.py|doc/modules/cross_validation.rst|sklearn/feature_extraction/text.py|continuous_integration/install.sh|continuous_integration/test_script.sh|setup.py|examples/applications/plot_out_of_core_classification.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|.gitignore|doc/tutorial/statistical_inference/supervised_learning.rst|examples/exercises/digits_classification_exercise.py|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|examples/exercises/plot_cv_diabetes.py|doc/install.rst|doc/whats_new.rst|sklearn/lda.py|sklearn/tests/test_lda.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py|doc/tutorial/statistical_inference/supervised_learning.rst|doc/whats_new.rst|sklearn/neighbors/base.py|sklearn/gaussian_process/gaussian_process.py|sklearn/utils/extmath.py|doc/modules/clustering.rst|examples/imputation.py|doc/modules/scaling_strategies.rst|sklearn/cluster/k_means_.py|doc/whats_new.rst|sklearn/cluster/k_means_.py|sklearn/naive_bayes.py|setup.py|sklearn/datasets/base.py|sklearn/datasets/tests/test_svmlight_format.py|sklearn/datasets/twenty_newsgroups.py|sklearn/tests/test_common.py|sklearn/__init__.py|sklearn/tests/test_common.py|continuous_integration/install.sh|sklearn/utils/testing.py|continuous_integration/appveyor/requirements.txt|sklearn/linear_model/least_angle.py|sklearn/gaussian_process/tests/test_gaussian_process.py|appveyor.yml|continuous_integration/appveyor/requirements.txt|doc/whats_new.rst|doc/conf.py|sklearn/__init__.py|appveyor.yml|sklearn/__init__.py,66,0.0,0,0,false,015x ,,2369,0.7665681722245673,0.12575250836120402,41321,429.5394593548075,33.30025894823455,109.5568839089083,4427,59,1609,214,travis,irogerthat,irogerthat,true,,0,0,5,13,24,false,false,false,false,0,0,0,0,0,0,-1
5858435,scikit-learn/scikit-learn,python,4086,1421083355,1421085191,1421085191,30,30,github,false,false,false,10,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.887112459618481,0.10930417100023196,64,t3kcit@gmail.com,doc/whats_new.rst,64,0.042666666666666665,1,0,false,DOC/MAINT Update whats new entry for NotFittedError @jnothman ping :),,2367,0.7667934093789607,0.12466666666666666,41321,429.5394593548075,33.30025894823455,109.5568839089083,4425,60,1609,214,travis,ragv,ragv,true,ragv,14,0.5714285714285714,3,1,73,true,false,false,false,10,191,19,198,260,0,30
5840365,scikit-learn/scikit-learn,python,4085,1421081630,1421509492,1421509492,7131,7131,commits_in_master,false,false,false,69,6,2,25,10,0,35,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,222,115,275,124,23.11064809967556,0.5168926087896272,13,trev.stephens@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/utils/fixes.py,11,0.0013351134846461949,0,2,false,[MRG] ENH use parallelism for all metrics in pairwise_{kernelsdistances} Up till now parallelism was only provided for inbuilt metrics Its useful for scipy etc too (even if more cores are needed for cdist over a split array to outperform pdist for YNone)As noted in a TODO it may be worth exploring threading backend in some cases (particularly following https://githubcom/scipy/scipy/pull/4397) but that can be left for a future PR,,2366,0.7666948436179205,0.12483311081441922,41441,429.3573996766487,33.27622402934293,109.36029535966796,4425,60,1609,219,travis,jnothman,jnothman,true,jnothman,102,0.7058823529411765,30,1,2085,true,true,false,false,28,472,31,453,70,11,97
5839324,scikit-learn/scikit-learn,python,4082,1421046047,1424979119,1424979119,65551,65551,commit_sha_in_comments,false,false,false,49,3,1,5,9,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,20,13,47,9.292075801671743,0.20782469213985197,15,ragvrv@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,14,0.009389671361502348,0,0,false,[MRG+1] Test check_consistent_length and TypeError with ensemble arg #4081s code will now raise:TypeError: Expected dataset but found estimator RandomForestRegressor(bootstrapTrue criterionmse max_depthNone           max_featuresauto max_leaf_nodesNone min_samples_leaf1           min_samples_split2 min_weight_fraction_leaf00           n_estimators10 n_jobs1 oob_scoreFalse random_stateNone           verbose1 warm_startFalse)instead of ValueError: Found arrays with inconsistent numbers of samples: [10 17],,2365,0.7665961945031713,0.12340710932260228,41321,429.5394593548075,33.30025894823455,109.5568839089083,4422,60,1609,258,travis,jnothman,ogrisel,false,ogrisel,101,0.7029702970297029,30,1,2085,true,true,false,false,28,472,30,452,60,11,322
5804293,scikit-learn/scikit-learn,python,4078,1420977282,1421016133,1421016133,647,647,commits_in_master,false,false,false,22,2,1,1,5,0,6,0,5,0,0,12,12,12,0,0,0,0,12,12,12,0,0,46,53,60,55,50.62569269020818,1.1322852357354407,80,th0mas.ko6er@gmail.com,sklearn/cluster/__init__.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/common.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/cluster/tests/test_spectral.py,32,0.002044989775051125,0,1,false,MANIT: pep8ize to an extent cluster module Fix/pep8ize most of the cluster codebase- Remove unused imports- Indents spaces brackets fixes,,2364,0.766497461928934,0.12201772324471712,41063,431.6781530818498,33.50948542483501,110.05041034507951,4412,60,1608,213,travis,pratapvardhan,amueller,false,amueller,0,0,30,7,679,false,false,false,false,0,1,0,0,0,0,5
5801957,scikit-learn/scikit-learn,python,4076,1420943733,1421772609,1421772609,13814,13814,commits_in_master,false,false,false,31,11,2,45,19,1,65,0,4,1,0,0,3,1,0,0,1,0,2,3,1,0,0,266,0,848,0,9.194516264663534,0.20564291495060397,0,,examples/cluster/plot_kmeans_silhouette_analysis.py|examples/cluster/plot_kmeans_silhouette_analysis.py,0,0.0,2,10,false,[MRG] Add silhouette analysis plot for KMeans Addresses #4068 @jnothman @amueller Please take a lookThese are the output of the example ( as 3 separate figures )[image](https://cloudgithubusercontentcom/assets/9487348/5693531/e7ea4b4e-9946-11e4-9b5e-9d98f38ba20cpng)[image](https://cloudgithubusercontentcom/assets/9487348/5693532/eaa6b700-9946-11e4-808f-a2ec7fadada5png)[image](https://cloudgithubusercontentcom/assets/9487348/5693534/ee0e76ee-9946-11e4-9801-1cf75e80b77cpng),,2363,0.766398645789251,0.12218430034129693,41063,431.6781530818498,33.50948542483501,110.05041034507951,4409,59,1607,227,travis,ragv,jnothman,false,jnothman,13,0.5384615384615384,3,1,71,true,false,false,false,8,171,18,172,244,0,4
5801839,scikit-learn/scikit-learn,python,4075,1420942703,1421017019,1421017019,1238,1238,commits_in_master,false,false,false,5,2,0,1,1,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,FIX: typo in Pipeline error ,,2362,0.7662997459779848,0.12201772324471712,41063,431.6781530818498,33.50948542483501,110.05041034507951,4409,59,1607,212,travis,banilo,amueller,false,amueller,3,0.3333333333333333,5,0,662,true,false,false,false,0,8,3,1,4,0,13
5798756,scikit-learn/scikit-learn,python,4073,1420916137,,1425609165,78217,,unknown,false,false,false,47,4,3,3,5,0,8,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,52,101,52,105,33.85084648422716,0.7571020099573033,10,manojkumarsivaraj334@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/neighbors/base.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/neighbors/base.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py,7,0.0034036759700476512,0,0,false,[MRG pending #4072] FIX/TST boundary cases in dbscan #3994 handled the min_samples boundary case differently to the prior implementation Unfortunately when properly testing boundary cases I found the inconsistency reported at #4072 I fix it here for brute search without tests pending a complete patch for #4072,,2361,0.7666243117323168,0.12185159972770593,41063,431.6781530818498,33.50948542483501,110.05041034507951,4405,60,1607,266,travis,jnothman,jnothman,true,,100,0.71,30,1,2083,true,true,false,false,27,441,29,405,57,11,2619
5793064,scikit-learn/scikit-learn,python,4071,1420851941,1420901782,1420901782,830,830,commits_in_master,false,false,false,15,1,1,1,2,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,0,28,0,8.658799730708871,0.19366130657492336,9,larsmans@users.noreply.github.com,examples/model_selection/grid_search_digits.py|sklearn/grid_search.py,7,0.004794520547945206,0,0,true,[MRG] Doc best_estimator_ depends on refitTrue Fixes #2976 Fixes #2976 and a typo in grid_search_digitspy,,2360,0.7665254237288136,0.1226027397260274,41063,431.6781530818498,33.50948542483501,110.05041034507951,4402,60,1606,212,travis,amueller,jnothman,false,jnothman,231,0.8571428571428571,1068,40,1540,true,true,false,false,37,412,41,170,76,5,731
5787346,scikit-learn/scikit-learn,python,4070,1420822596,,1422123859,21687,,unknown,false,false,false,34,1,1,0,3,0,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.115120763384429,0.09526499555581447,0,,sklearn/cross_validation.py,0,0.0,0,0,true,Fix interpretation of parameter train_size (test_size) in cross_validationBootstrap Bootstrap can now return train/test sets with only one 1 exampleBefore train_size1 indicated entire set (100%) Now train_size1 results in train set with one example,,2359,0.7668503603221705,0.11956521739130435,37507,448.50294611672484,35.51337083744368,113.5787986242568,4394,61,1606,233,travis,tomazc,larsmans,false,,0,0,0,0,111,false,false,false,false,0,0,0,0,0,0,12
5774232,scikit-learn/scikit-learn,python,4067,1420736512,,1451944925,520140,,unknown,false,true,false,25,1,1,2,2,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,23,0,23,0,4.645129614412809,0.10389221285999381,6,manojkumarsivaraj334@gmail.com,sklearn/metrics/classification.py,6,0.004081632653061225,0,0,false,[WIP]Sample weight support to hamming loss Partial fix for #3450 Have to add tests should i test this in the corresponding hamming_loss tests in test_classification,,2358,0.767175572519084,0.11428571428571428,41063,431.6781530818498,33.50948542483501,110.05041034507951,4380,61,1605,471,travis,akshayah3,TomDLT,false,,8,0.625,7,5,626,true,true,false,false,1,12,6,4,2,0,4
5773625,scikit-learn/scikit-learn,python,4066,1420732486,1425609181,1425609181,81278,81278,merged_in_comments,false,true,false,48,11,1,11,47,0,58,0,9,0,0,1,2,1,0,0,0,0,2,2,2,0,0,31,0,201,68,4.511925350730343,0.10091298798036537,7,larsmans@gmail.com,sklearn/cluster/dbscan_.py,7,0.004758667573079538,0,15,false,Do not shuffle by default for DBSCAN Shuffling is not necessary the effect on the result is usually nonexistant (except for permuted cluster numbering) DBSCAN is mostly deterministic except for rare border casesAdd a note about the increased memory complexity of this implementation compared to original DBSCAN,,2357,0.7670767925328807,0.1142080217539089,41063,431.6781530818498,33.50948542483501,110.05041034507951,4379,61,1605,268,travis,kno10,ogrisel,false,ogrisel,0,0,5,1,651,false,false,false,false,0,0,0,0,0,0,29
5770412,scikit-learn/scikit-learn,python,4065,1420702033,,1428771474,134490,,unknown,false,true,false,63,8,5,8,9,0,17,0,5,1,0,3,4,3,0,0,1,0,3,4,3,0,0,122,2,150,36,26.46726740399871,0.5919652588345063,6,olivier.grisel@ensta.org,examples/tree/plot_tree_feat.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|examples/tree/plot_tree_feat.py,4,0.0027155465037338763,0,0,false,Tree apply apply is now a public method for all decision trees for #3832Added docstring and example demonstrating two uses of apply(): Reducing number of classes to predict and make a one-hot feature encodingThere is currently no function to do the one-hot encoding automatically Should we consider adding one transform() is already used to select only the features with highest importance,,2356,0.767402376910017,0.11405295315682282,41063,431.6781530818498,33.50948542483501,110.05041034507951,4379,61,1605,311,travis,galv,glouppe,false,,0,0,0,2,566,true,false,false,false,0,0,0,0,8,0,21
5769997,scikit-learn/scikit-learn,python,4064,1420697974,1423261361,1423261361,42723,42723,commits_in_master,false,true,false,13,15,5,11,34,0,45,0,4,0,0,13,13,11,0,0,0,0,13,13,11,0,0,296,86,520,101,141.68326319658567,3.1688790644860036,115,th0mas.ko6er@gmail.com,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/k_means_.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|doc/developers/index.rst|doc/whats_new.rst|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/incremental_pca.py|sklearn/neural_network/rbm.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,67,0.0034036759700476512,0,4,false,[MRG] Pipeline scoring y none Fixes #4063On top of #4058 (please review),,2355,0.767303609341826,0.11436351259360109,41063,431.6781530818498,33.50948542483501,110.05041034507951,4379,61,1605,249,travis,amueller,ogrisel,false,ogrisel,230,0.8565217391304348,1068,40,1539,true,true,true,false,35,379,40,155,77,4,8
5768353,scikit-learn/scikit-learn,python,4061,1420686241,1420731622,1420731622,756,756,commits_in_master,false,false,false,53,3,3,0,1,0,1,0,3,0,0,24,24,24,0,0,0,0,24,24,24,0,0,68,77,68,77,109.97594882133168,2.4597152405599054,62,trev.stephens@gmail.com,sklearn/cluster/k_means_.py|sklearn/datasets/base.py|sklearn/datasets/olivetti_faces.py|sklearn/dummy.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_passive_aggressive.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/metrics/tests/test_common.py|sklearn/mixture/dpgmm.py|sklearn/naive_bayes.py|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/dist_metrics.pyx|sklearn/svm/__init__.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_isotonic.py|sklearn/tree/tests/test_tree.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/estimator_checks.py|sklearn/base.py|sklearn/cross_decomposition/pls_.py|sklearn/neural_network/tests/test_rbm.py|sklearn/tests/test_base.py|sklearn/utils/estimator_checks.py,12,0.002044989775051125,3,0,false,Pep8 cleanup Cleaned up the following PEP8 inconsistencies:E101 indentation contains mixed spaces and tabsE111 indentation is not a multiple of fourE112 expected an indented blockE113 unexpected indentationW191 indentation contains tabsW291 trailing whitespaceW293 blank line contains whitespaceW391 blank line at end of file@amueller @agramfort @jakevdp ,,2354,0.7672047578589635,0.11451942740286299,41063,431.6781530818498,33.50948542483501,110.05041034507951,4378,61,1604,211,travis,ragv,MechCoder,false,MechCoder,12,0.5,3,1,68,true,false,false,false,8,161,16,154,229,0,9
5765294,scikit-learn/scikit-learn,python,4060,1420671321,,1420681675,172,,unknown,false,false,false,150,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.12348020765749,0.09222550220866954,24,olivier.grisel@ensta.org,sklearn/tree/_tree.pyx,24,0.016359918200409,0,2,false,Rolling back a030c5d5d96dcc8ef8edafa40ca640d9b1e0ad7b Commit a030c5d5d96dcc8ef8edafa40ca640d9b1e0ad7b incorrectly made the impurities computed for regression trees be equal to the sum of variances on each side of the split not accounting for the different sizes on each side The correct regression impurity is the sum of squared deviations from the mean on each side of the split as in equation (913) in Elements of Statistical Learning page 307 (page 326 in the pdf http://statwebstanfordedu/~tibs/ElemStatLearn/printings/ESLII_print10pdf) The variables var_right and var_left are indeed misnamed as variances but they were correctly used in correctly implementing the regression tree algorithm before commit a030c5d5d96dcc8ef8edafa40ca640d9b1e0ad7b Using sum of variances instead of sum of squared errors is a terribly egregious error that has made scikit use the completely wrong algorithm for regression trees and forest one of the most popular methods in regression and hence leading to a serious handicap for machine learning with Python compared with standards like R,,2353,0.7675308117297067,0.11451942740286299,41063,431.6781530818498,33.50948542483501,110.05041034507951,4374,61,1604,209,travis,nathankallus,glouppe,false,,0,0,0,0,0,false,false,false,false,0,0,0,0,0,0,41
5755256,scikit-learn/scikit-learn,python,4058,1420599853,1421331248,1421331248,12189,12189,commits_in_master,false,false,false,58,3,1,11,5,0,16,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,42,64,154,198,9.287959756570519,0.20773408819375422,14,olivier.grisel@ensta.org,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,9,0.006130790190735695,0,3,false,[MRG] slight cleanup of common tests Cleanup of common tests (somewhat in anticipation of #4052)The goal would is to have one function for all including meta-estimators one for all default-constructible estimators and then more specialized ones in the spirit of #3810 and keeping in mind untested estimators from #4056This is only some of the way there,,2352,0.7674319727891157,0.11376021798365123,41063,431.6781530818498,33.50948542483501,110.05041034507951,4363,60,1603,221,travis,amueller,ogrisel,false,ogrisel,229,0.8558951965065502,1065,40,1537,true,true,true,false,31,347,38,128,71,4,1313
5755071,scikit-learn/scikit-learn,python,4057,1420598813,1424828001,1424828001,70486,70486,commits_in_master,false,false,false,12,19,2,14,26,0,40,0,6,0,0,1,15,1,0,0,0,0,15,15,14,0,0,16,0,1313,107,8.792602443594362,0.19665494891686247,12,matteo.visconti.gr@dartmouth.edu,sklearn/utils/validation.py|sklearn/utils/validation.py,12,0.008174386920980926,0,7,false,WIP make check_array convert object to float * [ ] add tests,,2351,0.767333049766057,0.11376021798365123,41063,431.6781530818498,33.50948542483501,110.05041034507951,4363,60,1603,267,travis,amueller,ogrisel,false,ogrisel,228,0.8552631578947368,1065,40,1537,true,true,true,false,31,346,37,128,71,3,1274
5752377,scikit-learn/scikit-learn,python,4054,1420584500,1420600100,1420600100,260,260,commits_in_master,false,false,false,16,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.381464791332188,0.09799564352510758,3,olivier.grisel@ensta.org,sklearn/cluster/affinity_propagation_.py,3,0.0020491803278688526,0,3,false,fix for issue #4051 replaced X  npasarray(X) with check_array() in affinity_propagation_py related issues #4051 #4052 ,,2350,0.7672340425531915,0.11270491803278689,41063,431.6781530818498,33.50948542483501,110.05041034507951,4363,60,1603,209,travis,tttthomasssss,amueller,false,amueller,0,0,10,10,1514,false,false,false,false,1,5,0,0,0,0,135
5750294,scikit-learn/scikit-learn,python,4052,1420574795,1420597868,1420597868,384,384,commits_in_master,false,true,false,12,19,1,10,19,0,29,0,4,0,0,2,6,2,0,0,0,0,6,6,6,0,0,28,2,628,307,8.792766619420497,0.19665882317749495,14,olivier.grisel@ensta.org,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,9,0.006155950752393981,0,2,false,[WIP] TST add test for sparse matrix handling in clustering See #4051,,2349,0.767134951042997,0.11285909712722299,40963,432.73197763835657,33.591289700461395,110.3190684276054,4361,60,1603,223,travis,amueller,jnothman,false,jnothman,227,0.8546255506607929,1064,40,1537,true,true,false,false,28,325,35,109,67,3,16
5734738,scikit-learn/scikit-learn,python,4049,1420469813,1420578308,1420578308,1808,1808,commits_in_master,false,false,false,36,2,1,5,3,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,9,34,11,8.912645329001428,0.19934069742696428,16,olivier.grisel@ensta.org,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,11,0.007362784471218206,1,2,false,[MRG] Sparse inputs for fit_params @ogrisel I closed my previous PR as i had messed up my commits I have addressed your comments* Added a non regression test with public cross_val_score* Added safe_indexing utility,,2348,0.7670357751277683,0.11044176706827309,40967,431.00544340566796,33.539190079820344,109.77127932238143,4325,61,1602,211,travis,akshayah3,amueller,false,amueller,7,0.5714285714285714,7,5,623,true,false,false,false,1,13,4,2,5,0,200
5730253,scikit-learn/scikit-learn,python,4047,1420420763,1420464511,1420464511,729,729,commits_in_master,false,false,false,25,1,1,0,2,0,2,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,0,12,0,3.484672551250336,0.07793834737884177,0,,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx,0,0.0,1,0,false,FIX : avoid unecessary computation sparse dual gap cc @MechCoder this must have been a speed regression when you removed gil from from this code,,2347,0.7669365146996165,0.11036789297658862,40967,431.00544340566796,33.539190079820344,109.77127932238143,4323,62,1601,211,travis,agramfort,mblondel,false,mblondel,41,0.9024390243902439,171,186,1859,true,true,true,true,17,281,39,173,50,1,188
5722085,scikit-learn/scikit-learn,python,4046,1420317475,1422536771,1422536771,36988,36988,commits_in_master,false,false,false,119,18,4,83,80,1,164,0,4,0,0,3,5,3,0,0,0,0,5,5,4,0,0,272,131,1172,811,54.18763509865126,1.2119499288791078,6,manojkumarsivaraj334@gmail.com,sklearn/neighbors/base.py|sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py,5,0.0019946808510638296,0,28,false,FIX/ENH: Fix kneighbors_graph and allow XNone Following the discussion in https://githubcom/scikit-learn/scikit-learn/pull/4019 this PR1 Allows X to be None in neighborskneighbors which skips the first neighbor in both distance and connectivity mode2 Do not do anything special in other cases    In [2]: knn  NearestNeighbors(n_neighbors1)fit([[0] [1]])    In [3]: knnkneighbors_graph([[0] [1]])toarray()    Out[3]:     array([[ 1  0]           [ 0  1]])    In [5]: knnkneighbors_graph([[2] [1]])toarray()    Out[5]:     array([[ 0  1]           [ 0  1]])    In [8]: knnkneighbors_graph()toarray()    Out[8]:     array([[ 0  1]           [ 1  0]])    In [6]: knnkneighbors_graph([[2] [1]] modedistance)toarray()    Out[6]:     array([[ 0  1]           [ 0  0]])    In [7]: knnkneighbors_graph([[0] [1]] modedistance)toarray()    Out[7]:     array([[ 0  0]           [ 0  0]])    In [9]: knnkneighbors_graph(modedistance)toarray()    Out[9]:     array([[ 0  1]           [ 1  0]])Also a happy 2015 to everyone,,2346,0.7668371696504689,0.10970744680851063,40946,431.2264934303717,33.55639134469789,109.82757778537585,4312,62,1600,243,travis,MechCoder,MechCoder,true,MechCoder,60,0.8666666666666667,83,41,928,true,true,false,false,16,436,53,228,119,0,1
5720516,scikit-learn/scikit-learn,python,4045,1420297339,,1420298579,20,,unknown,false,false,false,13,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,0,23,0,8.912448859836637,0.19933406841160609,33,matteo.visconti.gr@dartmouth.edu,sklearn/cluster/hierarchical.py|sklearn/feature_selection/from_model.py,31,0.020611702127659573,1,0,false,Added threshold parameter to the class object Partial fix for #1975Ping @jnothman ,,2345,0.7671641791044777,0.10970744680851063,40946,431.2264934303717,33.55639134469789,109.82757778537585,4312,62,1600,212,travis,akshayah3,akshayah3,true,,6,0.6666666666666666,7,5,621,true,false,false,false,1,14,2,2,6,0,15
5717097,scikit-learn/scikit-learn,python,4044,1420249602,1420250466,1420250466,14,14,commits_in_master,false,false,false,8,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,3.9743771609847527,0.08889015049447246,1,isaac.slavitt@gmail.com,sklearn/preprocessing/label.py,1,0.0006618133686300463,0,0,true,add MultiLabelBinarizer to __all__ Prevents spurious pylint error,,2344,0.7670648464163823,0.10919920582395765,40945,431.2370252778117,33.55721089266088,109.83026010501892,4308,64,1599,212,travis,andreasvc,agramfort,false,agramfort,0,0,21,23,1707,false,true,false,false,0,0,0,0,0,0,-1
5712031,scikit-learn/scikit-learn,python,4042,1420204313,1420800048,1420800048,9928,9928,commits_in_master,false,false,false,10,3,1,9,8,0,17,0,8,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,27,0,4.310787203765165,0.09641423241205106,1,dsullivan7@hotmail.com,sklearn/feature_extraction/text.py,1,0.0006548788474132286,0,1,true,Add dependence on max_features to docstring As requested in https://githubcom/scikit-learn/scikit-learn/issues/4032#issuecomment-68478422,,2343,0.7669654289372599,0.10805500982318271,40945,431.2370252778117,33.55721089266088,109.83026010501892,4304,64,1599,219,travis,lmichelbacher,GaelVaroquaux,false,GaelVaroquaux,1,1.0,0,0,1221,false,false,false,false,0,1,1,0,0,0,9
5703888,scikit-learn/scikit-learn,python,4039,1420066439,1423091085,1423091085,50410,50410,merged_in_comments,false,true,false,104,1,1,4,6,0,10,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,18,13,18,9.377690058764838,0.20973946210633598,2,jakevdp@gmail.com,sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,2,0.0012903225806451613,0,4,false,Corrected two bugs related to tied covariance_type in mixtureGMM() a dded test closes #4036The first bug was with the Gaussian log-density calculation for the tied covariance_typeRather than fix this equation I reformatted the covars data structure so that it could be fedinto the full covariance_type log-density calculation It might be a tiny bit slower but I thinkit is better to have the least amount of potentially redundant code as this should minimizethe potential for such errorsThe second bug I fixed was related to the _covar_mstep_tied() function only the first partof the equation should be divided by Xshape[0],,2342,0.7668659265584971,0.1064516129032258,40945,431.2126022713396,33.55721089266088,109.83026010501892,4297,66,1597,251,travis,wadawson,amueller,false,amueller,0,0,9,31,744,false,false,false,false,1,3,0,0,1,0,1129
5703039,scikit-learn/scikit-learn,python,4038,1420059689,1420092205,1420092205,541,541,commits_in_master,false,false,false,24,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,1,7,1,8.323040067602356,0.18615138012978086,11,matteo.visconti.gr@dartmouth.edu,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,10,0.006464124111182934,0,0,false,make utilscheck_symmetric future-proof Two things:- convert to CSR CSC COO if necessary- add dok input to tests which was left out previously,,2341,0.7667663391712943,0.10730446024563671,40945,431.2126022713396,33.55721089266088,109.83026010501892,4295,67,1597,211,travis,jakevdp,jnothman,false,jnothman,45,0.8888888888888888,1606,0,1330,false,true,false,false,2,55,3,13,0,0,10
5698960,scikit-learn/scikit-learn,python,4037,1420008893,1422845707,1422845707,47280,47280,commits_in_master,false,false,false,44,25,11,7,24,2,33,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,41,326,245,326,61.320124084726366,1.371473119844001,3,larsmans@gmail.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,2,0.0012453300124533001,2,8,false,[MRG] Make storing stop words optional Fixes #4032 - [x] Add store_stop_words parameter to exclude stop_words_ from getting stored- [x] Tests for store_stop_words- [x] Tests to make sure excluding stop_words_ does not affect pickling / unpicking@jnothman @amueller Please take a look,,2340,0.7666666666666667,0.10460772104607721,40945,431.2126022713396,33.55721089266088,109.83026010501892,4294,68,1597,252,travis,ragv,jnothman,false,jnothman,11,0.45454545454545453,3,1,61,true,false,false,false,8,113,15,117,196,0,9
5688989,scikit-learn/scikit-learn,python,4031,1419913444,1420572021,1420572021,10976,10976,commits_in_master,false,false,false,40,8,1,9,6,0,15,0,3,0,0,1,4,1,0,0,0,0,4,4,3,0,0,10,0,12,76,4.245445982121156,0.09495276033988254,7,larsmans@users.noreply.github.com,sklearn/metrics/scorer.py,7,0.004377736085053158,0,2,false,[WIP] fixes to check_scoring and regression tests  This changes the behavior of check_scoringIf scoring is provided it MUST be used Also a scorer might not need predict so that check is droppedFixes remaining issues from #3848 and #2853,,2337,0.7672229353872486,0.1050656660412758,40945,431.2126022713396,33.55721089266088,109.83026010501892,4282,68,1595,213,travis,amueller,ogrisel,false,ogrisel,226,0.8539823008849557,1062,40,1529,true,true,true,false,25,289,34,86,66,3,88
5688736,scikit-learn/scikit-learn,python,4030,1419910980,1420324873,1420324873,6898,6898,commits_in_master,false,false,false,12,2,1,1,4,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,92,0,96,0,4.209022749006945,0.09413812589646668,9,larsmans@users.noreply.github.com,sklearn/pipeline.py,9,0.005628517823639775,0,1,false,Add parameters to pipeline methods add some missing docstrings Also see #4023,,2336,0.7671232876712328,0.1050656660412758,40945,431.2126022713396,33.55721089266088,109.83026010501892,4282,68,1595,212,travis,amueller,ogrisel,false,ogrisel,225,0.8533333333333334,1062,40,1529,true,true,true,false,25,286,33,82,66,3,170
5688564,scikit-learn/scikit-learn,python,4029,1419909734,1421028308,1421028308,18642,18642,commits_in_master,false,false,false,136,72,51,67,24,0,91,0,5,0,0,52,52,52,0,0,0,0,52,52,52,0,0,4940,596,5959,891,2366.2863173051537,52.92386678074271,194,trev.stephens@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/svm/base.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/validation.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/birch.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/base.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/locally_linear.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py,38,0.001876172607879925,1,7,false,[WIP] Add NotFittedError class and fix all modules to raise uniform errors when not fitted Fixes #3627 - [x] Add utilvalidationNotFittedError- [x] Add _is_fitted method to check and return boolean if the passed estimator has all_or_any of the passed attributes- [x] Add check_is_fitted to raise NotFittedError when the given attribute(s) arent available in the passed estimator instance- [x] Refactor existing modules to use check_is_fitted- [x] Add tests to ensure all the unfitted estimators {regressors classifiers transformers} raise NotFittedError when predict ( or related methods are called )Thanks to @jnothman for [this](https://githubcom/jnothman/scikit-learn/compare/scikit-learn:masterjnothman:unfitted-error)- [x] Run smoke test and fix all modules which do not currently raise an error when predict like methods are called before fit-ing- [ ] Make travis happyThe check_is_fitted should behave more ideally after #3937 is merged,,2335,0.7670235546038544,0.1050656660412758,40945,431.2126022713396,33.55721089266088,109.83026010501892,4282,68,1595,219,travis,ragv,jnothman,false,jnothman,10,0.4,3,1,59,true,false,false,false,8,99,14,85,185,0,5
5685292,scikit-learn/scikit-learn,python,4027,1419885165,,1420469817,9744,,unknown,false,false,false,23,2,1,3,10,0,13,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,6,0,6,10,4.145183423161247,0.09261887857866014,10,matteo.visconti.gr@dartmouth.edu,sklearn/cross_validation.py,10,0.00630119722747322,0,7,false,Sparse inputs for fit_params Fixes #3689 Have to add tests Should i go on and create a seperate method in the cross_validation tests,,2333,0.7676810972996142,0.10586011342155009,40948,431.03448275862064,33.55475236885806,109.79779232196933,4279,68,1595,215,travis,akshayah3,akshayah3,true,,5,0.8,7,5,616,true,false,false,false,1,17,1,3,8,0,87
5683428,scikit-learn/scikit-learn,python,4026,1419869563,1419878430,1419878430,147,147,commits_in_master,false,false,false,10,5,3,0,8,0,8,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,20,0,36,0,21.446748356859086,0.4792006247113229,16,trev.stephens@gmail.com,sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/tree/tree.py,10,0.005069708491761723,0,5,false,Documentation of max depth parameter in the DecisionTreeClassifier Fixes #3843 ,,2332,0.7675814751286449,0.10646387832699619,40946,431.0555365603478,33.55639134469789,109.80315537537244,4276,68,1595,205,travis,akshayah3,glouppe,false,glouppe,4,0.75,7,5,616,true,false,false,false,1,13,0,3,10,0,74
5681696,scikit-learn/scikit-learn,python,4025,1419849450,,1457682104,630544,,unknown,false,true,false,304,105,38,132,84,4,220,0,9,3,0,38,53,40,0,0,10,2,48,60,51,0,0,11855,64,19187,1437,280.10864150613423,6.25867464939348,39,olivier.grisel@ensta.org,sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/setup.py|sklearn/__check_build/_check_build.c|sklearn/_hmmc.c|sklearn/_isotonic.c|sklearn/cluster/_k_means.c|sklearn/datasets/_svmlight_format.c|sklearn/ensemble/_gradient_boosting.c|sklearn/feature_extraction/_hashing.c|sklearn/linear_model/cd_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/manifold/_utils.c|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/pairwise_fast.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/svm/liblinear.c|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/utils/_logistic_sigmoid.c|sklearn/utils/_random.c|sklearn/utils/arrayfuncs.c|sklearn/utils/graph_shortest_path.c|sklearn/utils/lgamma.c|sklearn/utils/murmurhash.c|sklearn/utils/seq_dataset.c|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/weight_vector.c|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/setup.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/_utils.c|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/_utils.pyx|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/bhtsne.c|sklearn/__check_build/_check_build.c|sklearn/_hmmc.c|sklearn/_isotonic.c|sklearn/cluster/_k_means.c|sklearn/datasets/_svmlight_format.c|sklearn/ensemble/_gradient_boosting.c|sklearn/feature_extraction/_hashing.c|sklearn/linear_model/cd_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/pairwise_fast.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/svm/liblinear.c|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/utils/_logistic_sigmoid.c|sklearn/utils/_random.c|sklearn/utils/arrayfuncs.c|sklearn/utils/graph_shortest_path.c|sklearn/utils/lgamma.c|sklearn/utils/murmurhash.c|sklearn/utils/seq_dataset.c|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/weight_vector.c|benchmarks/bench_covertype.py|benchmarks/bench_mnist.py,24,0.0,0,39,false,Cemoody/bhtsne ## IntroductionThis PR presents the Barnes-Hut implementation of t-SNE t-SNE is used to visualize high-dimensional data in a low dimensional space that attempts preserve the pairwise high-dimensional similarities in a low-dimensional embedding The Barnes-Hut algorithm which is used by astrophysicists to perform N-body simulations allows the calculation of the t-SNE embedding in O(NlogN) time instead of O(N^2) This effectively allows us to learn embeddings of data sets with millions of elements instead of tens of thousands## ExampleAn iPython Notebook is available where weve put together simple install instructions & a demo of both methods:http://nbvieweripythonorg/urls/gistgithubusercontentcom/cemoody/01135ef2f26837548360/raw/76ce7f0bac916a516501ead719b513b22430cad0/Barnes-Hut%20t-SNE%20Demoipynb[bhtsne](https://cloudgithubusercontentcom/assets/3419930/5566392/24d8b7dc-8edb-11e4-9e67-4f319fc6dc59png)## PerformanceThis compares the timings for the Barnes-Hut approximation against the current method (note the log y-axis)[timing](https://cloudgithubusercontentcom/assets/3419930/5566359/f0c7579c-8ed9-11e4-974e-903a839fe9cdpng)The following diagram shows the relative speedup between both versions:[speedup](https://cloudgithubusercontentcom/assets/3419930/5566360/f4802f94-8ed9-11e4-891c-da999a0426e8png)## TODO - [ ] Create fit & transform methods to update using new data- [ ] Change sparse inputs to use methodstandard- [ ] Change dimension  4 to use methodstandard- [ ] Warnings for 64bit being transformed to 32bit- [ ] Do we have memory leaks- [ ] Segfaults on data larger than 50k elements- [x] PEP8 the code- [x] Include usage documentation- [x] Remove extra imports print statements pdb statements- [x] Ensure python 3 works- [x] Ensure output dimensionality works for 2D or 3D- [x] Am I using memviews or returning full arrays appropriately- [x] Incorporate into SKLearn- [x] Remove GIL as much as possible in Cython code- [x] Add answer tests- [x] Ensure old t-SNE tests works- [x] Changed perplexity calculation to nearest neighbors- [x] Changed positive gradient calc to nearest neighbors## Learn- You can read more about the technique in general here:http://lvdmaatengithubio/tsne/- The Barnes-Hut approximation is here:http://lvdmaatengithubio/publications/papers/JMLR_2014pdf,,2331,0.767910767910768,0.10680228862047043,40946,431.0555365603478,33.55639134469789,109.80315537537244,4273,68,1595,513,travis,cemoody,jnothman,false,,0,0,71,21,699,false,false,false,false,0,0,0,0,0,0,133
5681339,scikit-learn/scikit-learn,python,4024,1419844151,1419896599,1419896599,874,874,commits_in_master,false,false,false,55,6,2,9,9,0,18,0,5,0,0,4,5,4,0,0,0,0,5,5,5,0,0,56,15,99,52,22.095567279820745,0.49369760980813476,8,olivier.grisel@ensta.org,sklearn/manifold/spectral_embedding_.py|sklearn/manifold/mds.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/__init__.py|sklearn/utils/validation.py,6,0.0031766200762388818,0,3,false,MAINT: create ensure_symmetric utility function to check matrix symmetry (first commit of this is from PR #4021)This PR creates a utility function called ensure_symmetric which checks whether a matrix is symmetric This also fixes a (possible) bug in the validation check for spectral_embedding: it compares the absolute difference of the matrix with its transpose,,2330,0.7678111587982832,0.10673443456162643,40946,431.0555365603478,33.55639134469789,109.80315537537244,4272,68,1595,207,travis,jakevdp,agramfort,false,agramfort,44,0.8863636363636364,1605,0,1328,false,true,false,true,2,40,2,11,0,0,238
5680956,scikit-learn/scikit-learn,python,4023,1419837954,,1421441849,26731,,unknown,false,true,false,61,72,18,185,36,0,221,0,6,0,0,52,55,52,0,0,1,0,55,56,55,0,0,7887,295,8739,2299,2267.874373049419,50.67279527790895,215,trev.stephens@gmail.com,sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/svmlight_format.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/passive_aggressive.py|sklearn/metrics/pairwise.py|sklearn/mixture/gmm.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/utils/__init__.py|sklearn/utils/multiclass.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/svmlight_format.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/utils/__init__.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/validation.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/hmm.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/linear_model/ridge.py|sklearn/metrics/scorer.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py|sklearn/cluster/birch.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/base.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/passive_aggressive.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/base.py|sklearn/metrics/pairwise.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/unsupervised.py|sklearn/preprocessing/data.py|sklearn/qda.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/utils/multiclass.py|sklearn/utils/random.py|sklearn/utils/sparsefuncs.py|sklearn/utils/stats.py|sklearn/utils/testing.py|sklearn/utils/validation.py,36,0.0019047619047619048,3,11,false,Fix doc signature mismatch Fixes #2062 This is almost done but I havent cross checked with #2084 yet and a few more (  8 ) are yet to be doneIll refrain from force pushing over this commit to allow review comments to be visible ( Ill squash those commits later on )@jnothman @amueller @NelleV Kindly take a look ,,2329,0.7681408329755259,0.10666666666666667,40946,431.0555365603478,33.55639134469789,109.80315537537244,4272,68,1595,225,travis,ragv,amueller,false,,9,0.4444444444444444,3,1,59,true,false,false,false,8,94,13,69,169,0,3
5680165,scikit-learn/scikit-learn,python,4021,1419826243,1419858461,1419858461,536,536,commits_in_master,false,false,false,26,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.351944767579166,0.09723872225417167,0,,sklearn/manifold/spectral_embedding_.py,0,0.0,0,1,false,BUG: only symmetrize matrix when it is not already symmetric This looks like an indentation problem which caused unnecessary computation when a non-symmetric array was passed,,2328,0.7680412371134021,0.10623409669211197,40946,431.0555365603478,33.55639134469789,109.80315537537244,4271,68,1594,207,travis,jakevdp,agramfort,false,agramfort,43,0.8837209302325582,1605,0,1327,false,true,false,true,2,33,1,11,0,0,5
5679924,scikit-learn/scikit-learn,python,4020,1419822907,,1419831808,148,,unknown,false,false,false,17,3,3,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,4,7,4,13.461318435869835,0.3007761987955185,12,larsmans@users.noreply.github.com,sklearn/feature_extraction/tests/test_text.py|sklearn/grid_search.py|sklearn/feature_extraction/tests/test_text.py,10,0.006373486297004461,2,1,false,Refactor ParameterGrid to use OrderedDict to keep the parameters sorted alphabetically See #4017 and #3267@amueller @larsmans ,,2327,0.7683712935109583,0.10579987253027406,40946,431.0555365603478,33.55639134469789,109.80315537537244,4271,68,1594,204,travis,ragv,ragv,true,,8,0.5,3,1,58,true,false,false,false,7,90,10,65,162,0,44
5679816,scikit-learn/scikit-learn,python,4019,1419821286,,1420317539,8270,,unknown,false,false,false,9,5,4,0,51,0,51,0,3,0,0,4,5,4,0,0,0,0,5,5,4,0,0,102,79,102,79,59.70974189636489,1.33413894665732,8,manojkumarsivaraj334@gmail.com,sklearn/manifold/isomap.py|sklearn/manifold/tests/test_isomap.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_isomap.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_isomap.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,5,0.0019120458891013384,0,20,false,[MRG] Fix bug in kneighbors_graph when modedistance Fixes https://githubcom/scikit-learn/scikit-learn/issues/4015,,2326,0.7687016337059329,0.10579987253027406,40946,431.0555365603478,33.55639134469789,109.80315537537244,4271,68,1594,215,travis,MechCoder,MechCoder,true,,59,0.8813559322033898,83,41,922,true,true,false,false,16,429,53,234,122,0,1
5679636,scikit-learn/scikit-learn,python,4018,1419818685,1421434545,1421434545,26931,26931,commits_in_master,false,false,false,12,8,5,5,9,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,34,45,41,72,36.107823499215655,0.806783819178839,5,manojkumarsivaraj334@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,4,0.0025493945188017845,0,5,false,FIX scale - Raise ValueError when input contains non finite Fixes #3782,,2325,0.7686021505376344,0.10579987253027406,40946,431.0555365603478,33.55639134469789,109.80315537537244,4271,68,1594,224,travis,ragv,ogrisel,false,ogrisel,7,0.42857142857142855,3,1,58,true,false,true,true,6,90,9,63,162,0,197
5677975,scikit-learn/scikit-learn,python,4017,1419797125,,1419830716,559,,unknown,false,false,false,13,1,1,8,0,0,8,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.6904625142902505,0.104802474762193,0,,examples/svm/plot_rbf_parameters.py,0,0.0,0,1,false,[MRG] DOC Refactor example to remove ambiguity in understanding param order Fixes #3267,,2324,0.7689328743545611,0.10586734693877552,40946,431.0555365603478,33.55639134469789,109.80315537537244,4269,68,1594,204,travis,ragv,ragv,true,,6,0.5,3,1,58,true,false,false,false,6,87,8,60,162,0,241
5673273,scikit-learn/scikit-learn,python,4013,1419718918,1419899293,1419899293,3006,3006,commits_in_master,false,false,false,52,3,2,0,3,0,3,0,3,0,1,5,9,6,0,0,0,1,8,9,9,0,0,78,0,78,28,41.8317970645173,0.9346787961218024,73,trev.stephens@gmail.com,sklearn/cross_validation.py|sklearn/decomposition/factor_analysis.py|sklearn/feature_selection/selector_mixin.py|sklearn/lda.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/cross_validation.py|sklearn/decomposition/factor_analysis.py|sklearn/feature_selection/selector_mixin.py|sklearn/lda.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py,36,0.0063411540900443885,1,2,false,[MRG] Remove all 016 deprecated parameters / classes remove deprecated n_bootstraps parameter in Bootstrap cv object verbose in FactorAnalysis and SelectorMixin Also adjust some deprecation stringsThis takes care of all deprecations for 016I also checked that all mentions of DeprecationWarning and @deprecated are properly tagged with either 017 or 018,,2323,0.7688334050796384,0.10526315789473684,40946,431.0555365603478,33.55639134469789,109.80315537537244,4267,68,1593,206,travis,amueller,ogrisel,false,ogrisel,224,0.8526785714285714,1061,40,1527,true,true,true,false,24,264,33,70,62,3,2
5670578,scikit-learn/scikit-learn,python,4012,1419673091,1419685528,1419685528,207,207,commits_in_master,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.385586095371686,0.09828785003200897,23,manojkumarsivaraj334@gmail.com,doc/modules/model_evaluation.rst,23,0.014603174603174604,0,0,false,Small doc typo fix in Model evaluation ,,2322,0.7687338501291989,0.10539682539682539,40948,428.5435186089674,33.43264628309075,109.30936797890006,4264,68,1593,204,travis,sylvinus,agramfort,false,agramfort,1,1.0,125,3,1558,false,false,false,false,0,0,1,0,0,0,-1
5669044,scikit-learn/scikit-learn,python,4011,1419645567,1419699250,1419699250,894,894,commits_in_master,false,true,false,9,2,2,0,2,0,2,0,3,0,0,4,4,3,0,0,0,0,4,4,3,0,0,221,43,221,43,30.293839476068534,0.6789317116063495,73,trev.stephens@gmail.com,doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py,69,0.005714285714285714,0,2,true,[MRG] MAINT: Remove deprecation warnings in enet_path and lasso_path ,,2321,0.7686342093925033,0.10539682539682539,40948,428.519097391814,33.43264628309075,109.30936797890006,4262,68,1592,204,travis,MechCoder,larsmans,false,larsmans,58,0.8793103448275862,83,41,920,true,true,false,false,14,420,52,233,122,0,678
5668784,scikit-learn/scikit-learn,python,4010,1419642380,1419645894,1419645894,58,58,commits_in_master,false,false,false,9,2,2,0,0,0,0,0,1,0,0,3,3,2,0,0,0,0,3,3,2,0,0,4,24,4,24,22.39068790536942,0.5018098836776729,73,trev.stephens@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,69,0.005714285714285714,0,0,true,FIX: Bug in sparse coordinate solver in lazy centering ,,2320,0.7685344827586207,0.10539682539682539,40948,428.519097391814,33.43264628309075,109.30936797890006,4262,68,1592,204,travis,MechCoder,agramfort,false,agramfort,57,0.8771929824561403,83,41,920,true,true,true,false,14,420,51,233,122,0,-1
5667893,scikit-learn/scikit-learn,python,4009,1419631968,1440946582,1440946582,355243,355243,commits_in_master,false,false,false,25,29,2,33,92,0,125,0,8,0,0,5,15,5,0,0,0,0,15,15,14,0,0,232,94,4932,207,43.65141381352943,0.9782955745133801,6,manojkumarsivaraj334@gmail.com,sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py,5,0.0,0,42,true,[MRG] ENH: Parallelize neighbors search through multiprocessing #3536 I used the most simple approach with multiprocessing joblib backend A simple benchmark could be seen [here](http://nbvieweripythonorg/gist/nmayorov/7531d9b59608ae2d76dc),,2319,0.7684346701164295,0.10546378653113088,40948,428.519097391814,33.43264628309075,109.30936797890006,4262,68,1592,361,travis,nmayorov,GaelVaroquaux,false,GaelVaroquaux,5,0.2,5,0,275,true,false,false,false,0,41,2,17,16,0,65
5663638,scikit-learn/scikit-learn,python,4004,1419560353,,1420736123,19596,,unknown,false,false,false,31,4,1,14,19,0,33,0,4,0,0,1,27,1,0,0,0,0,27,27,27,0,0,4,0,455,31,4.205274520911494,0.0942466942994292,2,trev.stephens@gmail.com,sklearn/linear_model/passive_aggressive.py,2,0.0012531328320802004,1,3,false,COSMIT Minor fix in passive aggressive docstring Make the [n_classes n_samples] appear without any extra spaces inbetween the two wordsA very minor PR @jnothman could you take a look,,2318,0.7687661777394306,0.10401002506265664,40948,428.519097391814,33.43264628309075,109.30936797890006,4257,69,1591,219,travis,ragv,ragv,true,,5,0.6,3,1,55,true,false,false,false,3,68,7,47,162,0,11
5661820,scikit-learn/scikit-learn,python,4003,1419527917,1419528749,1419528749,13,13,commits_in_master,false,false,false,23,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.378285149699913,0.0981241291128792,0,,doc/tutorial/text_analytics/working_with_text_data.rst,0,0.0,0,0,false,Spelling in heading newgroups - newsgroups The section title is newgroups this fixes it so that it matches the rest of the document,,2317,0.7686663789382823,0.10246913580246914,40948,428.519097391814,33.43264628309075,109.30936797890006,4253,70,1591,201,travis,ClashTheBunny,jnothman,false,jnothman,0,0,7,4,1710,false,false,false,false,0,0,0,0,0,0,13
5661251,scikit-learn/scikit-learn,python,4002,1419517934,,1419699383,3024,,unknown,false,false,false,14,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.455677317059942,0.09986161079053117,7,ragvrv@gmail.com,doc/developers/index.rst,7,0.004320987654320987,1,0,false,Update Contributors guide Based on @jnothmans suggestion #3912Placeholder PR No work done yet,,2316,0.7689982728842832,0.10246913580246914,40956,425.7007520265651,33.3284500439496,108.94618615099131,4253,70,1591,203,travis,ragv,larsmans,false,,4,0.75,3,1,55,true,false,false,false,3,63,6,47,162,0,3024
5659164,scikit-learn/scikit-learn,python,4001,1419474235,1453189560,1453189560,561922,561922,commit_sha_in_comments,false,false,false,46,5,2,5,9,0,14,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,29,40,31,52,27.281608301339762,0.6114574874346709,14,manojkumarsivaraj334@gmail.com,sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py,12,0.003708281829419036,0,1,false,Added sample weight support to confusion matrix Added a sample_weight parameter inside confusion_matrix whichwill be used to build the confusion matrix If not present itwill default to npones() of size equal to the number of samplesTests were created in test_classificationpy instead oftest_commonpy,,2315,0.7688984881209503,0.10259579728059333,40971,424.10485465329134,33.26743306241,108.3205193917649,4248,70,1590,474,travis,DanielSidhion,MechCoder,false,MechCoder,0,0,3,0,1850,false,false,false,false,0,2,0,0,0,0,0
5657284,scikit-learn/scikit-learn,python,4000,1419451128,1419463389,1419463389,204,204,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.831435799672791,0.10828605202193367,3,larsmans@gmail.com,doc/modules/svm.rst,3,0.0018529956763434219,0,2,false,Fix a typo on svmrst ,,2314,0.7687986171132238,0.102532427424336,40971,424.10485465329134,33.26743306241,108.3205193917649,4246,70,1590,202,travis,he7d3r,MechCoder,false,MechCoder,0,0,26,15,949,false,true,false,false,0,0,0,0,0,0,71
5657150,scikit-learn/scikit-learn,python,3999,1419449603,1419523905,1419523905,1238,1238,commit_sha_in_comments,false,false,false,74,1,1,3,5,0,8,0,4,1,0,2,3,3,0,0,1,0,2,3,3,0,0,52,0,52,0,10.510452724909594,0.2355687786683857,0,,setup.py|sklearn/__init__.py|sklearn/_version.py,0,0.0,0,0,false,[MRG] MAINT dev version to follow PEP440 This PR includes a few cosmits to setuppy and the use of the dev marker instead of the previously used -git marker for in the __version__ attribute for the dev branchThe version 60 of pip now enforce pep 440 convention in a stricter manner and the -git marker made those packages not visible in our windows CI for instance:https://ciappveyorcom/project/sklearn-ci/scikit-learn/build/10354/job/3aldvipl0mxow0iyThis PR should fix this issue,,2313,0.7686986597492435,0.102532427424336,40971,424.10485465329134,33.26743306241,108.3205193917649,4246,70,1590,203,travis,ogrisel,jnothman,false,jnothman,104,0.8557692307692307,1084,124,2037,true,true,false,false,15,231,23,194,44,4,270
5653963,scikit-learn/scikit-learn,python,3997,1419412100,1419418859,1419418859,112,112,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.521553459530619,0.10134060832621344,4,larsmans@gmail.com,sklearn/svm/classes.py,4,0.0024737167594310453,0,0,false,Fix typo in SVM docstrings ,,2312,0.768598615916955,0.10265924551638837,40967,423.95098493909734,33.246271389166886,108.28227597822637,4244,71,1590,204,travis,sylvinus,agramfort,false,agramfort,0,0,125,3,1555,false,false,false,false,0,0,0,0,0,0,9
5646757,scikit-learn/scikit-learn,python,3995,1419351915,1419438843,1419438843,1448,1448,commits_in_master,false,false,false,67,3,2,0,9,0,9,0,7,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,11,5,11,8.809436555370093,0.1974440128869037,3,peter.prettenhofer@gmail.com,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,3,0.001841620626151013,0,1,false,FIX Isotonic Regression for duplicate minimal value Fixes an issue of Isotonic Regression when the minimal value of fitting is duplicated eg:    ir  IsotonicRegression(increasingTrue out_of_boundsclip)    irfit([0 0 1] [0 0 1])    irpredict([0])would return nan (see test_isotonic_duplicate_min_entry) The deeper reason for this seems to be in interpolateinterp1d This issue is fixed by clipping not to minimal value observed in fitting but to minimal value + npfinfo(float)resolution,,2311,0.7684984855041108,0.10190300798035605,40967,423.95098493909734,33.246271389166886,108.28227597822637,4233,71,1589,203,travis,jmetzen,jnothman,false,jnothman,13,0.46153846153846156,11,2,1170,true,true,false,false,1,23,4,6,221,0,30
5646718,scikit-learn/scikit-learn,python,3994,1419350689,1419515773,1419515773,2751,2751,commits_in_master,false,false,false,74,9,2,6,7,2,15,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,144,0,326,172,8.488910995100396,0.19026014222108975,0,,sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py,0,0.0,0,2,false,[MRG] ENH vectorize DBSCAN implementation This reduces the cluster comparison toy examples under DBSCAN from ~6s each to 02s each on my machine(This could be sped up further by allowing a dual-tree radius_neighbors lookup that reuses the computed tree The toy examples have n_features2 so neighbor calculation does not take up as much of the overall time as it might otherwise)As an afterthought Ive thrown in sparse matrix support for good measure,,2310,0.7683982683982684,0.10196560196560196,40967,423.95098493909734,33.246271389166886,108.28227597822637,4232,71,1589,204,travis,jnothman,agramfort,false,agramfort,99,0.7070707070707071,29,1,2065,true,true,false,true,22,323,20,368,37,10,30
5645323,scikit-learn/scikit-learn,python,3993,1419339177,1419428611,1419428611,1490,1490,commit_sha_in_comments,false,false,false,9,3,1,8,7,0,15,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,14,6,19,6,9.456076203490536,0.21193700868909798,13,manojkumarsivaraj334@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,13,0.007975460122699387,0,0,false,[MRG] sparse metric support for paired distances Pretty straightforward,,2309,0.7682979644867908,0.10184049079754601,40967,423.95098493909734,33.246271389166886,108.28227597822637,4231,71,1589,203,travis,jnothman,jnothman,true,jnothman,98,0.7040816326530612,29,1,2065,true,true,false,false,23,323,19,368,37,10,70
5638067,scikit-learn/scikit-learn,python,3992,1419275916,1419277748,1419277748,30,30,commit_sha_in_comments,false,false,false,6,2,1,0,2,0,2,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,51,1,51,12,17.69083754623411,0.3964972196601089,16,larsmans@users.noreply.github.com,sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_ranking.py,8,0.004310344827586207,0,1,false,[MRG] MAINT remove deprecated auc_score function ,,2308,0.7681975736568457,0.1022167487684729,40963,422.0882259600127,33.17628103410395,108.0975514488685,4230,72,1588,202,travis,arjoly,arjoly,true,arjoly,77,0.8181818181818182,28,26,1098,true,true,false,false,6,172,15,254,117,5,2
5636681,scikit-learn/scikit-learn,python,3991,1419265593,1419899731,1419899731,10568,10568,commits_in_master,false,true,false,107,10,4,28,26,0,54,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,218,25,335,31,21.679448090694798,0.48589225180895745,35,larsmans@users.noreply.github.com,benchmarks/bench_plot_approximate_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py,26,0.011042944785276074,0,12,false,LSHForest: sparse support and vectorised _find_longest_prefix_match This adds sparse matrix support to LSHForest vectorises calls to hashertransform and vectorises _find_longest_prefix_match over queries These seem to speed things up a little but the benchmark script does not provide very stable timings for meSome operations cannot be easily vectorised such as gathering the set of candidates per query (which differ in candidate set cardinality) This is followed by calculating exact distances from each query to its candidates which may also not be trivial to do vectorised and certainly not with pairwise_distances Unvectorised operations make sparse matrix calculations particularly inefficient (because extracting a single row is not especially cheap),,2307,0.7680970957954053,0.10184049079754601,40963,422.0882259600127,33.17628103410395,108.0975514488685,4229,73,1588,210,travis,jnothman,ogrisel,false,ogrisel,97,0.7010309278350515,29,1,2064,true,true,false,false,23,327,18,373,32,10,502
5627994,scikit-learn/scikit-learn,python,3989,1419143310,1419147825,1419147825,75,75,commits_in_master,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.837450234238515,0.08600715856810133,9,olivier.grisel@ensta.org,sklearn/linear_model/sgd_fast.pyx,9,0.0054611650485436895,0,0,false,Small typo fix in the comments: whther -- whether ,,2306,0.7679965307892455,0.10072815533980582,40963,422.0882259600127,33.17628103410395,108.0975514488685,4220,74,1587,202,travis,hammer,jnothman,false,jnothman,1,1.0,499,1199,2368,true,true,false,false,0,1,3,0,0,0,75
5618197,scikit-learn/scikit-learn,python,3987,1419026082,1419515862,1419515862,8163,8163,commit_sha_in_comments,false,false,false,14,1,1,0,5,0,5,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,43,0,43,9.51393349095326,0.21346140576952102,13,matteo.visconti.gr@dartmouth.edu,sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py,8,0.0049079754601227,0,1,true,MRG TST add tests for nd grid-search can train_test_split Makes sure #3984 stays fixed,,2305,0.7678958785249458,0.10184049079754601,40851,421.5808670534381,33.21828107023084,108.10016890651391,4215,74,1585,209,travis,amueller,jnothman,false,jnothman,223,0.852017937219731,1055,40,1519,true,true,false,false,22,248,31,64,54,3,99
5617563,scikit-learn/scikit-learn,python,3986,1419022373,1419041354,1419041354,316,316,commits_in_master,false,false,false,35,2,1,0,2,0,2,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,13,0,35,4,4.27113709582847,0.09583028193090126,10,matteo.visconti.gr@dartmouth.edu,sklearn/cross_validation.py,10,0.006146281499692686,0,0,true,MRG FIX backward compatibility force_arrays was never in a release allow_nd was a legal option to train_test_split that raises an error in masterApparently I messed this up when I did the input validation refactoring,,2304,0.7677951388888888,0.10202827289489859,40851,421.5808670534381,33.21828107023084,108.10016890651391,4214,74,1585,206,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,222,0.8513513513513513,1055,40,1519,true,true,true,false,21,244,30,64,52,3,280
5617405,scikit-learn/scikit-learn,python,3985,1419021716,1419297457,1419297457,4595,4595,commits_in_master,false,false,false,26,6,2,10,10,0,20,0,5,0,0,3,3,2,0,0,0,0,3,3,2,0,0,60,38,66,154,27.12008532042256,0.6084856008917408,84,s8wu@uwaterloo.ca,doc/whats_new.rst|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|doc/whats_new.rst|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,66,0.011063306699446834,0,10,true,[MRG] ENH: Return distances option for linkage_trees On the lines of https://githubcom/scikit-learn/scikit-learn/pull/3158  Also a micro optimization for the ward_tree option by preallocating the distance array,,2303,0.7676943117672601,0.10202827289489859,40851,421.5808670534381,33.21828107023084,108.10016890651391,4214,74,1585,206,travis,MechCoder,agramfort,false,agramfort,56,0.875,83,41,913,true,true,true,false,13,400,44,232,115,0,0
5616084,scikit-learn/scikit-learn,python,3983,1419014429,1419044739,1419044739,505,505,commit_sha_in_comments,false,false,false,15,1,1,4,13,0,17,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,25,0,25,0,4.304993513349967,0.09658990869151828,18,matteo.visconti.gr@dartmouth.edu,sklearn/cluster/hierarchical.py,18,0.011090573012939002,0,4,true,[MRG] FIX: Remove lil matrix conversion Removed a XXX from the code (unnecessary LIL conversion),,2302,0.7675933970460469,0.10166358595194085,40851,421.5808670534381,33.21828107023084,108.10016890651391,4213,74,1585,205,travis,MechCoder,MechCoder,true,MechCoder,55,0.8727272727272727,83,41,913,true,true,false,false,13,398,43,231,114,0,0
5609044,scikit-learn/scikit-learn,python,3982,1418955128,1419699545,1419699545,12406,12406,commits_in_master,false,false,false,7,18,12,8,18,0,26,0,4,2,0,5,8,5,0,0,3,0,6,9,6,0,0,422,278,635,310,116.94470788949744,2.6238549910199187,14,n59_ru@hotmail.com,sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/utils/metaestimators.py|sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py|sklearn/utils/metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/utils/metaestimators.py|sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/tests/test_metaestimators.py|sklearn/utils/metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py,7,0.002449479485609308,0,15,false,[MRG] Metaestimator delegation Slight change to #2854,,2301,0.7674923946110387,0.10165339865278629,40851,421.5808670534381,33.21828107023084,108.10016890651391,4205,76,1584,206,travis,amueller,larsmans,false,larsmans,221,0.8506787330316742,1055,40,1518,true,true,true,true,20,238,29,64,45,3,9
5606142,scikit-learn/scikit-learn,python,3980,1418939454,1418943041,1418943041,59,59,commits_in_master,false,false,false,32,147,145,0,0,0,0,0,2,7,2,13,22,13,0,0,7,2,13,22,13,0,0,6466,1754,6478,1754,932.3062791348244,21.027666835189144,28,olivier.grisel@ensta.org,sklearn/utils/testing.py|sklearn/utils/testing.py|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/__init__.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|doc/modules/neighbors.rst|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|doc/modules/neighbors.rst|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|benchmarks/bench_plot_approximate_neighbors.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/classes.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/utils/testing.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/__init__.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|doc/modules/neighbors.rst|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|benchmarks/bench_plot_approximate_neighbors.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/classes.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/utils/testing.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/utils/testing.py,22,0.0,0,0,false,LSH forest #3894 in just over 80 commits Moved some LSH-specific utility code back out of the neighbors base classesI just want to see if the tests work on this one,,2300,0.7673913043478261,0.10196560196560196,40531,420.1475413880733,33.1844760800375,107.67067183143766,4205,76,1584,203,travis,larsmans,larsmans,true,larsmans,126,0.746031746031746,146,38,1614,true,true,false,false,30,145,37,42,137,4,-1
5603112,scikit-learn/scikit-learn,python,3979,1418924191,1418924273,1418924273,1,1,commits_in_master,false,false,false,2,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.4389308873941085,0.10011745457635683,5,ragvrv@gmail.com,doc/developers/index.rst,5,0.0030864197530864196,0,0,false,Fix typo ,,2299,0.7672901261418008,0.10246913580246914,40531,420.17221386099527,33.1844760800375,107.67067183143766,4204,76,1584,203,travis,lmichelbacher,agramfort,false,agramfort,0,0,0,0,1206,false,false,false,false,0,0,0,0,0,0,-1
5602352,scikit-learn/scikit-learn,python,3978,1418919237,1419079361,1419079361,2668,2668,commits_in_master,false,false,false,52,5,3,0,3,0,3,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,6,93,6,93,26.67764105832873,0.6016999237698412,11,matteo.visconti.gr@dartmouth.edu,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,7,0.004326328800988875,0,4,false,[MRG] FIX: Fix bug in computing full tree when n_clusters is large There was a bug where compute_full_tree was set to True when n_clusters greater than 100 or 002 * n_samples It should be reverse the tree should not be computed fully when n_clusters is very large since it spoils the purpose,,2298,0.7671888598781549,0.10259579728059333,40518,420.257663260773,33.19512315514093,107.70521743422675,4204,76,1584,206,travis,MechCoder,MechCoder,true,MechCoder,54,0.8703703703703703,83,41,912,true,true,false,false,14,400,45,242,112,2,1
5591981,scikit-learn/scikit-learn,python,3976,1418848386,1419179066,1419179066,5511,5511,commits_in_master,false,false,false,33,4,3,0,8,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,149,0,208,0,12.9822059589307,0.2928067110121894,7,matteo.visconti.gr@dartmouth.edu,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py,7,0.004375,1,2,false,[MRG] DOC: Minor Improvement to children_ attribute in AgglomerativeClustering Im sorry if this was obvious but I had trouble in finding that it worked out this way without reading the codeping @agramfort ,,2297,0.7670875054418808,0.10375,40518,420.257663260773,33.19512315514093,107.70521743422675,4200,75,1583,207,travis,MechCoder,MechCoder,true,MechCoder,53,0.8679245283018868,83,41,911,true,true,false,false,14,392,43,240,109,2,1092
5586986,scikit-learn/scikit-learn,python,3974,1418818889,1418819569,1418819569,11,11,commits_in_master,false,false,false,10,1,1,0,0,0,0,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,8.76925617904607,0.1977857827547151,15,manojkumarsivaraj334@gmail.com,doc/modules/clustering.rst|sklearn/metrics/cluster/unsupervised.py,14,0.008755472170106316,0,0,false,[MRG] Fix silhouette typo Fix minor typo: silho**eu**tte - silho**ue**tte,,2296,0.76698606271777,0.10381488430268918,40516,420.2537269226972,33.19676177312667,107.68585250271498,4198,75,1583,204,travis,lesteve,agramfort,false,agramfort,4,1.0,4,0,966,true,false,false,false,0,4,1,1,2,0,-1
5578611,scikit-learn/scikit-learn,python,3972,1418762456,,1418867669,1753,,unknown,false,false,false,101,1,1,0,13,0,13,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,21,0,21,0,4.0538667420408405,0.09143274986922263,7,matteo.visconti.gr@dartmouth.edu,sklearn/cluster/hierarchical.py,7,0.00435052827843381,0,2,false,[MRG] Connect only required components in fixing the connectivity matrix This is the beginning of some minor optimisations as Im working through the agglomerative code The previous code in fixing the connectivity matrix looped through all components to complete the connectivity matrixFor example if there are 3 components it connects 1 - 0  2 - 1 2 - 0 3 - 2 3 - 1 3 - 0Is it not sufficient to connect 1 - 0 2 - 1 and 3 - 2 to make sure all components are connected Sorry if this PR was out of place,,2295,0.7673202614379085,0.10316967060285892,40516,420.2537269226972,33.19676177312667,107.68585250271498,4191,78,1582,204,travis,MechCoder,MechCoder,true,,52,0.8846153846153846,83,41,910,true,true,false,false,15,386,42,238,108,2,54
5578397,scikit-learn/scikit-learn,python,3971,1418761618,1429889102,1429889102,185458,185458,commits_in_master,false,false,false,22,24,6,30,19,0,49,0,8,0,0,7,7,4,0,0,0,0,7,7,4,0,0,462,448,565,1035,168.64490522929015,3.803693712138576,98,s8wu@uwaterloo.ca,doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py,65,0.0074580484773151025,0,7,false,[WIP] add ranking_loss multilabel ranking metric This pull request add the ranking loss This is a common metric in multilabel ranking task,,2294,0.7672188317349607,0.10316967060285892,40516,420.2537269226972,33.19676177312667,107.68585250271498,4191,78,1582,304,travis,arjoly,arjoly,true,arjoly,76,0.8157894736842105,28,26,1092,true,true,false,false,8,181,14,280,114,6,16
5563022,scikit-learn/scikit-learn,python,3969,1418663038,1418681324,1418681324,304,304,commits_in_master,false,false,false,27,2,2,0,1,0,1,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,38,0,38,0,17.9209290630315,0.4041878425759872,64,s8wu@uwaterloo.ca,doc/whats_new.rst|sklearn/metrics/pairwise.py|doc/whats_new.rst|sklearn/metrics/pairwise.py,58,0.03609209707529558,1,0,false,Remove optional input validation from input_validation Manually removed check_X_y since there was still some useful stuff related to repeated typecasting in check_pairwise Fixes https://githubcom/scikit-learn/scikit-learn/issues/3963  ping @larsmans ,,2292,0.7674520069808028,0.10080896079651525,40532,419.841113194513,33.183657357149904,107.6433435310372,4180,79,1581,208,travis,MechCoder,agramfort,false,agramfort,51,0.8823529411764706,82,41,909,true,true,true,false,15,386,41,237,104,2,45
5562623,scikit-learn/scikit-learn,python,3968,1418660219,1418688313,1418688313,468,468,commits_in_master,false,false,false,15,4,2,5,6,0,11,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,73,50,107,73,18.56956647071769,0.4188171820206472,2,manojkumarsivaraj334@gmail.com,sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/graph.py|sklearn/neighbors/tests/test_neighbors.py,2,0.0012461059190031153,0,4,false,[MRG] ENH: Add metric support to neighbors_graph Use non-euclidean metrics for kneighbors and nearest_neighbors graph,,2291,0.7673505019642077,0.10093457943925234,40532,419.841113194513,33.183657357149904,107.6433435310372,4180,79,1581,207,travis,MechCoder,larsmans,false,larsmans,50,0.88,82,41,909,true,true,false,false,15,382,40,237,102,2,0
5555203,scikit-learn/scikit-learn,python,3966,1418584643,1418691760,1418691760,1785,1785,commits_in_master,false,false,false,14,5,1,3,4,0,7,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,3,25,3,63,13.916771593309067,0.313878361242438,57,s8wu@uwaterloo.ca,doc/whats_new.rst|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,56,0.0012445550715619166,0,2,false,FIX: ward_tree returns children in the same order for both structured and unstructured versions ,,2290,0.7672489082969433,0.09956440572495333,40528,419.6111330438216,33.186932491117254,107.55527043031978,4170,78,1580,208,travis,mvdoc,agramfort,false,agramfort,1,1.0,9,17,369,true,false,false,false,0,1,0,1,3,0,6
5550164,scikit-learn/scikit-learn,python,3965,1418511807,1425418436,1425418436,115110,115110,commits_in_master,false,false,false,27,18,1,0,38,0,38,0,5,0,0,1,12,1,0,0,0,0,12,12,7,0,0,18,0,304,574,4.665201392599285,0.10521878283028048,29,trev.stephens@gmail.com,sklearn/linear_model/stochastic_gradient.py,29,0.017967781908302356,0,10,false,WIP change default to shuffleTrue in SGDClassifier See #3960Tests fail because they seem to rely on iris being sorted which makes me question their validity somewhat,,2289,0.7671472258628222,0.09913258983890955,40528,419.6111330438216,33.186932491117254,107.55527043031978,4164,78,1579,271,travis,amueller,amueller,true,amueller,220,0.85,1052,40,1513,true,true,false,false,17,213,28,52,28,3,1043
5548686,scikit-learn/scikit-learn,python,3964,1418494603,1418497868,1418497868,54,54,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.7880599362644025,0.10798976211101126,0,,examples/ensemble/plot_adaboost_regression.py,0,0.0,0,0,false,CLN: Fix typo in comment ,,2288,0.7670454545454546,0.09784615384615385,40528,419.6111330438216,33.186932491117254,107.55527043031978,4164,78,1579,206,travis,bwignall,agramfort,false,agramfort,9,0.8888888888888888,10,17,439,false,true,false,false,0,0,0,0,0,0,-1
5537877,scikit-learn/scikit-learn,python,3962,1418404553,1418407002,1418407002,40,40,commits_in_master,false,false,false,17,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.390731819258551,0.09902830792498737,9,perimosocordiae@gmail.com,sklearn/metrics/pairwise.py,9,0.005597014925373134,0,4,true,[MRG] ENH: Use check_X_y in pairwise_distances_argmin This is a minor non-controversial PR that makes pairwise_distances_argmin  use check_X_y,,2287,0.7669435942282467,0.09763681592039801,40522,419.37712847342186,33.167168451705244,107.52183998815458,4157,78,1578,207,travis,MechCoder,agramfort,false,agramfort,49,0.8775510204081632,82,41,906,true,true,true,false,14,381,39,236,102,2,3
5533354,scikit-learn/scikit-learn,python,3961,1418361831,1421145284,1421145284,46390,46390,commits_in_master,false,false,false,102,10,1,17,36,0,53,0,5,0,0,3,6,3,0,0,0,0,6,6,5,0,0,81,47,483,191,13.389520127339818,0.301986438387608,19,trev.stephens@gmail.com,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/utils/estimator_checks.py,11,0.005025125628140704,0,17,true,[WIP] Add support for class_weight to the forests This PR adds support for specifying class_weight in the forest classifier constructors- Right now it only supports single-output classification problems It would be possible however to accept a list of dicts for each target (or just the preset string auto overall) and multiply the expanded weight vectors such that a sample with two minority classes becomes even more important etc I couldnt find any precedent in other classifiers to guide this- Should an exception warning or tantrum be thrown for an auto class_weight used with the warm_start optionAny other comments ,,2286,0.7668416447944006,0.09798994974874371,40522,419.37712847342186,33.167168451705244,107.52183998815458,4152,78,1578,234,travis,trevorstephens,GaelVaroquaux,false,GaelVaroquaux,4,0.75,108,51,487,true,true,true,false,1,29,4,4,16,0,12
5525389,scikit-learn/scikit-learn,python,3959,1418313200,,1418673841,6010,,unknown,false,false,false,119,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.922893012599046,0.0908149254662036,0,,sklearn/metrics/metrics.py,0,0.0,0,5,false,Fix usability issue in metricsclassification_report Fix usability issue in metricsclassification_reportIn following codes class 0 and class 1 will be printed while class 0 and class 2 are expectedpythony_true  [0 0 2 0 0]y_pred  [0 2 2 0 0]target_names  [class 0 class 1 class 2]print(metricsclassification_report(y_true y_pred target_namestarget_names)) console              precision    recall  f1-score   support    class 0       100      075      086         4    class 1       050      100      067         1avg / total       090      080      082         5In real world we dont know which label will be included in y_true or y_pred since those may come from random subset using KFold So include all the label names in target_names is safer and more intuitive,,2285,0.7671772428884026,0.09697732997481108,37507,448.50294611672484,35.51337083744368,113.5787986242568,4146,78,1577,213,travis,imkevinyang,imkevinyang,true,,0,0,1,0,1131,false,false,false,false,0,0,0,0,0,0,496
5523625,scikit-learn/scikit-learn,python,3957,1418300649,,1418677717,6284,,unknown,false,false,false,14,3,0,2,5,0,7,0,5,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,14,38,0,0.0,0,,,0,0.0,0,5,false,Fix bug in fit_params assumed to be an array Fixes issue mentioned in https://githubcom/scikit-learn/scikit-learn/pull/3690#discussion_r21658762,,2284,0.7675131348511384,0.09634760705289673,40144,421.68194499800717,33.33001195695496,108.01115982463133,4146,78,1577,212,travis,justhalf,larsmans,false,,1,1.0,1,1,1444,false,true,false,false,2,16,0,1,0,0,355
5520488,scikit-learn/scikit-learn,python,3955,1418268830,1418269689,1418269689,14,14,github,false,false,false,7,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.422651896833167,0.10006545641550191,23,trev.stephens@gmail.com,sklearn/linear_model/stochastic_gradient.py,23,0.0144745122718691,0,0,false,none is an acceptable value for penalty ,,2283,0.7674113009198423,0.09565764631843927,40144,421.68194499800717,33.33001195695496,108.01115982463133,4142,77,1576,206,travis,hammer,hammer,true,hammer,0,0,498,1198,2357,false,true,false,false,0,1,0,0,0,0,14
5520483,scikit-learn/scikit-learn,python,3954,1418268806,1418321853,1418321853,884,884,github,false,false,false,27,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.392772167798493,0.09938940756671798,23,trev.stephens@gmail.com,sklearn/linear_model/stochastic_gradient.py,23,0.0144745122718691,0,0,false,DOC explain t0 in the SGD docstring (currently somewhat confusing) The SGD optimal learning rate (which is the default) uses t0 but doesnt explain what it is,,2282,0.7673093777388256,0.09565764631843927,40144,421.68194499800717,33.33001195695496,108.01115982463133,4142,77,1576,207,travis,amueller,amueller,true,amueller,219,0.8493150684931506,1052,40,1510,true,true,false,false,17,206,25,51,27,3,884
5520341,scikit-learn/scikit-learn,python,3953,1418268044,1418648466,1418648466,6340,6340,commits_in_master,false,false,false,21,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,3.875010563009431,0.08767470505198045,14,thomas.delteil1@gmail.com,doc/sphinxext/gen_rst.py,14,0.00881057268722467,0,0,false, Use correct savefig arguments in gen_rstpy Only pass facecolor/edgecolor arguments to savefig when they are differentthan rcParams[figurefacecolor]/rcParams[figureedgecolor]From nilearn,,2281,0.7672073651907059,0.09565764631843927,40144,421.68194499800717,33.33001195695496,108.01115982463133,4142,77,1576,209,travis,Titan-C,GaelVaroquaux,false,GaelVaroquaux,3,1.0,7,2,1344,true,false,false,false,0,2,3,2,5,0,6340
5348901,scikit-learn/scikit-learn,python,3952,1418238704,1419044555,1419044555,13430,13430,commits_in_master,false,false,false,87,7,4,34,15,0,49,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,143,76,233,82,37.262078646939706,0.8430794450420667,2,larsmans@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,2,0.001257071024512885,1,5,false,[MRG] ENH: Allow connectivity to be a callable in AgglomerativeClutering etal Fixes https://githubcom/scikit-learn/scikit-learn/issues/3851 (It was marked easy but I went ahead and fixed it)The current AgglomerativeClustering conditions connectivity on the input data which is prevents the reuse of the model for different data (if connectivity is provided initially) and definitely not in a pipeline where the input data is modified (as pointed by @jnothman )This fixes it by allowing it to be an int or a callable If an int then the kneighbors_graph is used,,2280,0.7671052631578947,0.09428032683846638,40144,421.68194499800717,33.33001195695496,108.01115982463133,4140,78,1576,213,travis,MechCoder,agramfort,false,agramfort,48,0.875,82,41,904,true,true,true,false,14,391,38,237,103,2,392
5493098,scikit-learn/scikit-learn,python,3949,1418081507,1418081782,1418081782,4,4,github,false,false,false,38,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,4.377167799918368,0.09903447811208974,8,trev.stephens@gmail.com,sklearn/ensemble/forest.py,8,0.0050062578222778474,2,0,false,[MRG] clarify RandomTreesEmbedding docstring I noticed n_out was mentioned in some method docstrings but never defined Also noting that parameters are tree-specific isnt necessary in a tree-specific ensemble :)@glouppe @arjoly Care to give this a quick look,,2278,0.7673397717295873,0.09073842302878599,40125,421.40809968847356,33.345794392523366,108.06230529595015,4120,78,1574,203,travis,larsmans,larsmans,true,larsmans,125,0.744,146,38,1604,true,true,false,false,29,122,30,36,136,4,1
5304411,scikit-learn/scikit-learn,python,3946,1418070046,1418663242,1418663242,9886,9886,commits_in_master,false,false,false,17,4,1,28,16,0,44,0,7,0,0,1,1,1,0,0,0,0,1,1,1,0,0,71,0,109,0,4.41712790487725,0.09994047240382778,1,dsullivan7@hotmail.com,sklearn/cluster/hierarchical.py,1,0.0006277463904582549,0,11,false,[MRG+1] Fix FeatureAgglomeration docs The docs are not displayed properly here http://scikit-learnorg/stable/modules/generated/sklearnclusterFeatureAgglomerationhtml (I keep running into these),,2276,0.7675746924428822,0.08976773383553044,40144,421.68194499800717,33.33001195695496,108.01115982463133,4119,78,1574,211,travis,MechCoder,MechCoder,true,MechCoder,47,0.8723404255319149,82,41,902,true,true,false,false,14,387,40,222,100,2,0
5490656,scikit-learn/scikit-learn,python,3945,1418068402,1424831975,1424831975,112726,112726,merged_in_comments,false,false,false,74,7,1,15,17,0,32,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,7,0,13,141,4.568830076579363,0.10337088338840804,3,jakevdp@gmail.com,sklearn/mixture/gmm.py,3,0.0018856065367693275,0,3,false,Problem with non positive defined covariance matrices Due to round off errors the formula used to compute the weighted covariance matrix may lead to matrices with large negative eigenvaluesThe formula is:C  \sum_i w_i (x_i - \mu)(x_i - \mu)                         (1)C  (\sum_i w_i x_i x_i)  - \mu \mu                           (2)assuming \sum_i w_i  1 and \mu  \sum_i w_i x_iThe second formula does not guarantee that C is positive definite,,2275,0.7674725274725275,0.08862350722815839,40125,421.40809968847356,33.345794392523366,108.06230529595015,4119,78,1574,269,travis,AlexisMignon,ogrisel,false,ogrisel,0,0,0,0,0,false,false,false,false,0,1,0,0,2,0,4
5303563,scikit-learn/scikit-learn,python,3944,1417997706,1418821744,1418821744,13733,13733,commits_in_master,false,false,false,118,7,0,32,10,0,42,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,67,28,0,0.0,0,,,0,0.0,1,6,false,[MRG+1] GaussianNBpartial_fit - Raise ValueError when there is a mismatch between target and classes argument In GaussianNBpartial_fit when there is a mismatch between the target labels (or classes) y and the classes argument a KeyError was raised while attempting to access the class2idx ( refactored to class_to_index for readability ) with the mismatching keySample code that reproduces this KeyErrorgnbpartial_fit(X  [[12] [-22]] y  [12] classes  [01])* Now a ValueError with a message explaining that is raisedValueError: The target label in y does not exist in the initial classes argument classes[0 1] The conflicting target label is 2* Added NRT to test the same@GaelVaroquaux Could you take a look at this,,2274,0.7673702726473175,0.08829054477144646,40144,421.68194499800717,33.33001195695496,108.01115982463133,4108,78,1573,208,travis,ragv,larsmans,false,larsmans,3,0.6666666666666666,3,1,37,true,false,false,false,3,37,5,14,138,0,37
5299617,scikit-learn/scikit-learn,python,3942,1417962407,1421574610,1421574610,60203,60203,commit_sha_in_comments,false,true,false,72,47,14,75,14,0,89,0,6,4,0,4,12,4,0,0,4,0,8,12,5,0,0,920,84,1522,180,84.03479258144938,1.9013074679743274,9,trev.stephens@gmail.com,sklearn/kernel_ridge.py|examples/plot_kernel_ridge_regression.py|sklearn/tests/test_kernel_ridge.py|sklearn/kernel_ridge.py|sklearn/linear_model/ridge.py|examples/plot_kernel_ridge_regression.py|examples/plot_kernel_ridge_regression.py|doc/modules/kernel_ridge.rst|doc/supervised_learning.rst|sklearn/kernel_ridge.py|examples/plot_kernel_ridge_regression.py|sklearn/tests/test_kernel_ridge.py|sklearn/kernel_ridge.py|sklearn/linear_model/ridge.py|examples/plot_kernel_ridge_regression.py|examples/plot_kernel_ridge_regression.py|doc/modules/kernel_ridge.rst|doc/supervised_learning.rst,9,0.0,1,17,false,[WIP] Kernel Ridge Regression Adds an implementation of Kernel Ridge Regression (KRR)Basic implementation of KRR based on @mblondels code in lightning In addition to the lighning code documentation and an example comparing KRR and SVR is added The example illustrates the models learned by KRR and SVR:[plot_kernel_ridge_regression_001](https://cloudgithubusercontentcom/assets/1116263/5330658/9793f8f6-7e03-11e4-93a9-7da2f0634037png)Furthermore it compares the time required for fitting the model and predicting with the model for different sizes of the training set:[plot_kernel_ridge_regression_002](https://cloudgithubusercontentcom/assets/1116263/5330660/a1b7a4cc-7e03-11e4-8628-3d6cccad921apng),,2273,0.7672679278486582,0.08801498127340825,40125,421.40809968847356,33.345794392523366,108.06230529595015,4098,78,1573,232,travis,jmetzen,mblondel,false,mblondel,12,0.4166666666666667,11,2,1154,true,true,false,false,0,17,2,6,180,0,53
5298692,scikit-learn/scikit-learn,python,3940,1417906556,1417919625,1417919625,217,217,github,false,false,false,44,1,1,2,6,0,8,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.400318265917131,0.09955822874574578,3,olivier.grisel@ensta.org,doc/developers/index.rst,3,0.001869158878504673,1,1,false,[MRG] Added note to advice users not to __init__ params with trailing _ in custom defined estimators Motivated by #3937 Added a note to [developers/#parameters-and-init](http://scikit-learnorg/dev/developers/#parameters-and-init) section discouraging users not to define parameters with trailing _ in the __init__@larsmans Could you take a look,,2272,0.7671654929577465,0.08660436137071652,40125,421.40809968847356,33.345794392523366,108.06230529595015,4094,78,1572,199,travis,ragv,ragv,true,ragv,2,0.5,3,1,36,true,false,false,false,2,30,2,14,136,0,52
5296424,scikit-learn/scikit-learn,python,3939,1417814935,,1445360785,459097,,unknown,false,true,false,50,212,56,2,79,2,83,0,8,12,2,14,33,11,0,1,14,2,19,35,15,0,1,17420,4622,22792,6377,746.5467729875804,16.890795143201945,20,olivier.grisel@ensta.org,sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/fixes.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_network/README.txt|examples/neural_network/plot_mlp_alpha.py|examples/neural_network/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/neural_networks_supervised.rst|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/base.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|examples/neural_network/README.txt|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|doc/modules/neural_networks_supervised.rst|examples/neural_networks/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py,16,0.0,0,23,true,WIP Mlp finishing touches Slight update to #3204 putting on the finishing touches* [ ] add tests for momentum* [ ] use SGD in benchmark* [ ] add weight visualization example (maybe) * [ ] tune benchmark add SGDClassifier* [ ] add mlp to classifier comparison,,2271,0.7675033025099075,0.08598130841121496,40125,421.40809968847356,33.345794392523366,108.06230529595015,4079,78,1571,379,travis,amueller,amueller,true,,218,0.8532110091743119,1046,40,1505,true,true,false,false,16,198,24,50,27,3,0
5462389,scikit-learn/scikit-learn,python,3936,1417740484,1417740661,1417740661,2,2,github,false,false,false,21,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.598085596044679,0.08140733683512229,2,dsullivan7@hotmail.com,sklearn/externals/joblib/parallel.py,2,0.001266624445851805,0,0,false,Fixed error message typo in joblib Changed protect you main loop to protect your main loopMy first contribution to OSS,,2268,0.7680776014109347,0.08359721342621912,40125,421.50778816199374,33.345794392523366,108.06230529595015,4067,77,1570,197,travis,sethdandridge,sethdandridge,true,sethdandridge,0,0,2,0,0,false,false,false,false,0,0,0,0,0,0,2
5292961,scikit-learn/scikit-learn,python,3934,1417728670,1417747773,1417747773,318,318,commits_in_master,false,false,false,12,1,0,2,1,0,3,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,70,0,0.0,0,,,0,0.0,0,2,false,MRG Allow list of strings for type_filter in all_estimators Removes lines ),,2267,0.7679752977503308,0.08419023136246787,40125,421.50778816199374,33.345794392523366,108.06230529595015,4065,76,1570,197,travis,amueller,larsmans,false,larsmans,217,0.8525345622119815,1041,40,1504,true,true,true,true,16,182,23,43,22,3,245
5290411,scikit-learn/scikit-learn,python,3933,1417701588,1423601350,1423601350,98329,98329,commits_in_master,false,false,false,30,10,2,3,15,0,18,0,5,0,0,7,8,5,0,0,0,0,8,8,5,0,0,225,20,282,22,46.254804073056405,1.046523301045763,33,s8wu@uwaterloo.ca,sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_svm.py|doc/modules/model_evaluation.rst|doc/modules/svm.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/metrics/classification.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_svm.py,23,0.003816793893129771,0,2,false,Change loss names for LinearSVC and LinearSVR() the names are now consistent across methodsIn LinearSVC:   l1 - hinge   l2 - squared_hingeIn LinearSVR:   l1 - epsilon_insensitive   l2 - squared_epsilon_insensitive,,2266,0.7678729037952339,0.08396946564885496,40125,421.50778816199374,33.345794392523366,108.06230529595015,4060,76,1570,251,travis,fabianp,larsmans,false,larsmans,37,0.7027027027027027,220,25,1664,true,true,false,true,1,14,2,2,15,0,2
5290815,scikit-learn/scikit-learn,python,3931,1417637860,1418407058,1418407058,12819,12819,commits_in_master,false,false,false,21,5,1,5,14,0,19,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,35,58,84,8.857166091647091,0.20039321067959354,33,trev.stephens@gmail.com,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,27,0.01762402088772846,0,4,false,[MRG] adding support for class_weight in fit method This should solve #3928 unless we decide to go the route of warning ,,2265,0.7677704194260485,0.0835509138381201,40092,421.85473411154345,33.37324154444777,108.15125212012371,4054,76,1569,211,travis,dsullivan7,agramfort,false,agramfort,15,0.6,8,19,481,true,true,true,false,2,122,9,19,89,1,303
5283717,scikit-learn/scikit-learn,python,3929,1417576463,,1445733078,469276,,unknown,false,true,false,53,1,1,6,8,0,14,0,7,0,0,4,4,4,0,0,0,0,4,4,4,0,0,84,12,84,12,18.04105291089316,0.4081787901091879,15,manojkumarsivaraj334@gmail.com,sklearn/metrics/__init__.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_common.py,12,0.004005340453938585,1,2,false,[WIP] add balanced_accuracy_score metric #3506 It refers to the issue https://githubcom/scikit-learn/scikit-learn/issues/3506- It passes all the common tests for metrics and regression metrics but I still need to implement specific tests- It might be a duplicate of the work done by @lazywei at https://githubcom/scikit-learn/scikit-learn/pull/3511  so I will compare the two versions,,2264,0.7681095406360424,0.08411214953271028,40092,421.5055372642921,33.34829891250125,108.07642422428415,4047,76,1568,395,travis,ppuggioni,arjoly,false,,2,0.5,0,1,374,true,false,false,false,0,2,3,2,1,0,176
5279335,scikit-learn/scikit-learn,python,3925,1417491079,1417573592,1417573592,1375,1375,commits_in_master,false,false,false,41,5,5,5,4,0,9,0,5,1,1,4,6,3,2,1,1,1,4,6,3,2,1,91,0,91,0,28.653036677578335,0.6482829743636511,17,thomas.delteil1@gmail.com,doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/gallery.css|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/nature.css_t|doc/conf.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/js/examples.js,13,0.000663129973474801,0,1,false,Nilearn css template for the examples gallery This pull request ports the CSS templating of Nilearn into the example gallery it also removes the jquery function that highlights mouse hovered exampleModularizes the CSS file to later simplify integration to Sphinx-Gallery,,2263,0.7680070702607159,0.08090185676392574,40090,421.52656522823645,33.34996258418558,108.08181591419307,4037,77,1567,198,travis,Titan-C,larsmans,false,larsmans,2,1.0,7,2,1335,true,false,false,false,0,1,2,1,5,0,158
5420062,scikit-learn/scikit-learn,python,3923,1417454882,1417457129,1417457129,37,37,github,false,false,false,15,1,1,0,1,0,1,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.844349075711256,0.10960409236602489,4,manojkumarsivaraj334@gmail.com,doc/about.rst,4,0.002664890073284477,0,0,false,Fix tinyclues logo in doc/aboutrst We updated our website and the logo URL was broken,,2261,0.7682441397611677,0.07928047968021319,40090,421.52656522823645,33.34996258418558,108.08181591419307,4034,77,1567,198,travis,Lothiraldan,Lothiraldan,true,Lothiraldan,0,0,27,5,1692,true,false,false,false,0,0,0,0,1,0,37
5416072,scikit-learn/scikit-learn,python,3921,1417414520,1417465860,1417465860,855,855,commits_in_master,false,false,false,58,1,1,0,4,0,4,0,3,0,0,11,11,11,0,0,0,0,11,11,11,0,0,178,0,178,0,48.44470461705335,1.0960651562979797,86,trev.stephens@gmail.com,sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py,29,0.005291005291005291,0,0,false,Various docstring fixes for web docs Various fixes to get proper formatting (bold continuation lines & rendering of dicts basically) for the web API docs Just looked at the ensemble and linear_model modules so far but will try to find time to review others in a later PR Motivated by noticing a lot of formatting problems on http://scikit-learnorg/stable/modules/generated/sklearnlinear_modelSGDClassifierhtml,,2259,0.768481629039398,0.0787037037037037,40091,421.5160509840114,33.349130727594726,108.07912000199546,4033,76,1567,199,travis,trevorstephens,amueller,false,amueller,3,0.6666666666666666,106,51,476,true,true,true,false,1,29,3,4,16,0,43
5412910,scikit-learn/scikit-learn,python,3920,1417378091,1417396629,1417396629,308,308,commits_in_master,false,false,false,4,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,21,4,21,4,9.209218379678239,0.20835923280884738,4,jnothman@student.usyd.edu.au,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py,4,0.0025493945188017845,0,1,false,PLSRegression coefs to coef_ ,,2258,0.7683790965456155,0.07648183556405354,40086,421.5686274509804,33.353290425584994,108.0926009080477,4033,79,1566,198,travis,jlopezpena,agramfort,false,agramfort,3,1.0,0,0,659,false,false,false,false,1,4,3,1,0,0,2
5272170,scikit-learn/scikit-learn,python,3919,1417374761,,1417553670,2981,,unknown,false,false,false,48,3,2,1,2,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,28,2,55,8.545991022899761,0.19335366582814217,4,manojkumarsivaraj334@gmail.com,sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py,4,0.002554278416347382,0,2,false,[MRG] Nearest neighbor radius query should include points in the boundary Noticed that although kd_tree and ball_tree algorithms return points in the boundaries the brute method does not Included a fix for brute and added a toy testcase to check the consistency between the algorithms for this edgecase,,2257,0.7687195392113425,0.07662835249042145,40086,421.5686274509804,33.353290425584994,108.0926009080477,4032,78,1566,200,travis,tvarvadoukas,amueller,false,,0,0,2,0,873,false,false,false,false,0,0,0,0,0,0,8
5272146,scikit-learn/scikit-learn,python,3918,1417374417,1418678354,1418678354,21732,21732,commit_sha_in_comments,false,false,false,47,2,1,1,2,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,10,0,4.497152205456557,0.10174839435030131,2,dsullivan7@hotmail.com,examples/missing_values.py,2,0.001277139208173691,0,1,false,Fixed formatting of Script output in text introduction Added more details to the introduction on use cases for imputingFixed the script output in the introductionWarning: This is my first edit As such I couldnt replicate the documentation in html format for testing (ie make html),,2256,0.7686170212765957,0.07662835249042145,40086,421.5686274509804,33.353290425584994,108.0926009080477,4032,78,1566,215,travis,dfletchermaths,larsmans,false,larsmans,0,0,0,0,288,false,false,false,false,0,0,0,0,1,0,5
5272240,scikit-learn/scikit-learn,python,3917,1417373172,1417378184,1417378184,83,83,commit_sha_in_comments,false,true,false,20,4,2,1,3,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,41,0,43,0,8.953464678255273,0.20257278679144128,4,jnothman@student.usyd.edu.au,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/pls_.py,4,0.002554278416347382,0,0,false,Replace coefs by coef_ in PLSRegression Renamed the coefs attribute and created a property with a deprecation warning on PLSRegression,,2255,0.7685144124168515,0.07662835249042145,40086,421.5686274509804,33.353290425584994,108.0926009080477,4032,78,1566,198,travis,jlopezpena,jlopezpena,true,jlopezpena,2,1.0,0,0,659,false,false,false,false,0,3,2,1,0,0,0
5272037,scikit-learn/scikit-learn,python,3916,1417368860,1417445517,1417445517,1277,1277,commit_sha_in_comments,false,false,false,20,6,4,3,3,0,6,0,6,0,0,7,7,4,0,0,0,0,7,7,4,0,0,41,0,41,0,31.979360216266524,0.7235351712891248,33,perimosocordiae@gmail.com,doc/modules/metrics.rst|sklearn/metrics/pairwise.py|sklearn/decomposition/kernel_pca.py|sklearn/datasets/svmlight_format.py|sklearn/metrics/ranking.py|doc/modules/classes.rst|doc/modules/decomposition.rst,16,0.003848620910840282,0,1,false, Inconsistencies for the kernel documentation #2901  Documentation improvements Contributions made in the context of the Bloomberg Open Source Day ,,2254,0.7684117125110914,0.0750481077613855,40086,421.5686274509804,33.353290425584994,108.0926009080477,4032,78,1566,199,travis,zeppe,larsmans,false,larsmans,1,0.0,0,0,883,true,false,false,false,1,2,1,0,3,0,0
5412173,scikit-learn/scikit-learn,python,3915,1417368162,1417370775,1417370775,43,43,commits_in_master,false,false,false,45,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.362959429473689,0.0987122818854431,11,najera.oscar@gmail.com,doc/sphinxext/gen_rst.py,11,0.007060333761232349,2,4,false,[bug fix] reverted css to fix display bug in documentation example page Revert change introduced by @jnothman in https://githubcom/scikit-learn/scikit-learn/commit/937f36cba77f18af7b37f0798f76aaeaed0b02d8causing a big empty space in the documentation on the example page after the first example@jnothman was this line changed to fix another bug somewhere,,2253,0.7683089214380826,0.07445442875481387,40084,421.5896617104081,33.35495459534977,108.09799421215448,4032,78,1566,196,travis,ThomasDelteil,ogrisel,false,ogrisel,0,0,2,1,641,true,false,false,false,0,0,0,0,1,0,27
5272275,scikit-learn/scikit-learn,python,3914,1417366854,1417543472,1417543472,2943,2943,commits_in_master,false,false,false,13,1,1,6,1,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.820488014148889,0.1090638855209247,0,,examples/model_selection/plot_underfitting_overfitting.py,0,0.0,0,2,false,add crossvalidation Improve the plot_underfitting_overfittingpy example by adding a quantitative measure using crossvalidation,,2252,0.7682060390763765,0.07373868046571798,40084,421.5896617104081,33.35495459534977,108.09799421215448,4031,77,1566,200,travis,ppuggioni,amueller,false,amueller,1,0.0,0,1,372,true,false,false,false,0,1,2,0,1,0,209
5409405,scikit-learn/scikit-learn,python,3911,1417320378,1417347195,1417347195,446,446,commit_sha_in_comments,false,false,false,3,2,2,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,22,0,22,0,9.21821318561916,0.2085623912930889,1,mathieu@mblondel.org,examples/text/document_classification_20newsgroups.py|examples/text/document_classification_20newsgroups.py,1,0.0006697923643670462,0,1,false,Fix #3908 #3908,,2251,0.7681030653043092,0.07166778298727394,40075,421.6843418590143,33.36244541484716,108.12227074235808,4028,76,1565,199,travis,a4tunado,jnothman,false,jnothman,1,0.0,13,56,1562,false,false,false,false,1,0,2,0,0,0,192
5409118,scikit-learn/scikit-learn,python,3909,1417316256,,1417320018,62,,unknown,false,false,false,2,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,4.640643634980575,0.10499489227528826,1,mathieu@mblondel.org,examples/text/document_classification_20newsgroups.py,1,0.0006702412868632708,0,0,false,fix #3908 ,,2250,0.7684444444444445,0.07104557640750671,40077,421.8629138907603,33.36078049754223,108.11687501559499,4028,76,1565,200,travis,a4tunado,a4tunado,true,,0,0,13,56,1562,false,false,false,false,1,0,0,0,0,0,-1
5407292,scikit-learn/scikit-learn,python,3906,1417294434,1417347347,1417347347,881,881,commit_sha_in_comments,false,false,false,6,2,2,0,1,0,1,0,1,1,1,2,4,0,0,1,1,1,2,4,0,0,1,0,0,0,0,13.859208843509744,0.31356558572529114,8,larsmans@gmail.com,iris.dot|doc/modules/feature_extraction.rst|doc/modules/svm.rst|iris.dot,7,0.0013458950201884253,0,0,false,Fix broken links Fix broken links,,2248,0.7686832740213523,0.07133243606998654,40077,421.8629138907603,33.36078049754223,108.11687501559499,4028,76,1565,199,travis,fannix,jnothman,false,jnothman,7,0.8571428571428571,14,2,1417,false,true,false,false,0,0,0,0,1,0,881
5407081,scikit-learn/scikit-learn,python,3905,1417291696,1417346831,1417346831,918,918,commits_in_master,false,false,false,10,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.941269528198641,0.11179657448929549,0,,examples/decomposition/plot_pca_3d.py,0,0.0,0,0,false,[MRG] Fix example in plot_pca_3dpy (color array had wrong size) ,,2247,0.7685803293279929,0.07017543859649122,40077,421.8629138907603,33.36078049754223,108.11687501559499,4028,75,1565,200,travis,jlopezpena,agramfort,false,agramfort,1,1.0,0,0,658,false,false,false,false,0,1,1,1,0,0,0
5270742,scikit-learn/scikit-learn,python,3904,1417290122,,1417356434,1105,,unknown,false,false,false,27,0,0,2,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,add crossvalidation to the overfitting / underfitting example I have added a simple crossvalidation to quantify which model actually performs betterI added a brief comment too,,2246,0.7689225289403384,0.06954760297096556,40077,421.8629138907603,33.36078049754223,108.11687501559499,4027,75,1565,197,travis,ppuggioni,ppuggioni,true,,0,0,0,1,371,true,false,false,false,0,0,0,0,1,0,75
5270511,scikit-learn/scikit-learn,python,3902,1417286733,1417361699,1417361699,1249,1249,commits_in_master,false,false,false,21,2,1,2,1,0,3,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.407225415248072,0.09971378845357534,1,Florian.Wilhelm@gmail.com,doc/developers/index.rst,1,0.0006770480704129993,0,0,false,Minor doc enhancement: documented setup develop Just mentioned the possibility to use develop not to have to reinstall after every change,,2245,0.7688195991091314,0.06905890318212593,40077,421.8629138907603,33.36078049754223,108.11687501559499,4027,75,1565,196,travis,JeanKossaifi,ogrisel,false,ogrisel,7,1.0,7,13,1272,false,true,false,false,0,0,0,0,0,0,44
5406364,scikit-learn/scikit-learn,python,3901,1417282450,,1438809663,358786,,unknown,false,false,false,18,2,1,0,9,0,9,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,6,0,18,4.550999501193512,0.10296668022192128,9,s8wu@uwaterloo.ca,sklearn/tests/test_common.py,9,0.006081081081081081,0,2,false,#3404 wrapping all_estimators in ignore_warnings code committed as part of the BloombergLabs Open Source day Refer to https://githubcom/scikit-learn/scikit-learn/issues/3404,,2244,0.7691622103386809,0.06824324324324324,40075,421.88396756082346,33.36244541484716,108.12227074235808,4027,75,1565,358,travis,zeppe,amueller,false,,0,0,0,0,882,false,false,false,false,1,0,0,0,0,0,69
5406373,scikit-learn/scikit-learn,python,3900,1417282337,1417284318,1417284318,33,33,commits_in_master,false,false,false,10,1,1,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.571776396004662,0.10343675891198996,9,najera.oscar@gmail.com,doc/sphinxext/gen_rst.py,9,0.006081081081081081,0,1,false,Don’t embed hyperlinks during latex file generation Should fix #3830,,2243,0.7690592955862684,0.06824324324324324,40075,421.88396756082346,33.36244541484716,108.12227074235808,4027,75,1565,195,travis,dimazest,GaelVaroquaux,false,GaelVaroquaux,0,0,32,37,1248,true,true,false,false,1,2,0,0,1,0,11
5270463,scikit-learn/scikit-learn,python,3899,1417281810,1417368565,1417368565,1445,1445,commits_in_master,false,false,false,10,1,1,9,6,0,15,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,10,7,10,9.016451402870967,0.2039978400552306,9,mathieu@mblondel.org,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,9,0.006085192697768763,0,0,false,[MRG] Let OneVsOneClassifier use list of targets Fix for #3863 ,,2242,0.7689562890276539,0.068289384719405,40075,421.88396756082346,33.36244541484716,108.12227074235808,4027,74,1565,197,travis,jlopezpena,larsmans,false,larsmans,0,0,0,0,658,false,false,false,false,0,0,0,0,0,0,11
5267393,scikit-learn/scikit-learn,python,3898,1417210523,1417367668,1417367668,2619,2619,commits_in_master,false,false,false,21,3,1,1,6,0,7,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,27,0,44,0,4.193155453237257,0.09487043375000526,26,olivier.grisel@ensta.org,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,26,0.01753202966958867,3,2,true,ENH Release gil in feature importance @pprett Do you want that the gill be release for normalization operationcc @ogrisel @glouppe ,,2241,0.7688531905399375,0.06810519217801753,40075,421.88396756082346,33.36244541484716,108.12227074235808,4023,74,1564,196,travis,arjoly,larsmans,false,larsmans,75,0.8133333333333334,28,26,1074,true,true,true,true,7,176,13,286,105,5,13
5264560,scikit-learn/scikit-learn,python,3894,1417077452,,1418943131,31094,,unknown,false,true,false,14,131,119,42,25,33,100,32,5,7,2,16,26,15,0,0,7,2,17,26,16,0,0,3655,1960,3803,2065,637.987912999885,14.434577036259242,28,s8wu@uwaterloo.ca,sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/tests/test_common.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/lshashing.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/tests/test_common.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/tests/test_lsh_forest.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/tests/test_lsh_forest.py|sklearn/neighbors/__init__.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_locality_sensitive_hashing_forest_accuracies.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|doc/modules/neighbors.rst|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|benchmarks/bench_plot_approximate_neighbors.py|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_approximate.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|benchmarks/bench_plot_approximate_neighbors.py|benchmarks/bench_plot_approximate_neighbors.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|benchmarks/bench_plot_approximate_neighbors.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/neighbors.rst|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|doc/modules/classes.rst|doc/modules/neighbors.rst|examples/neighbors/plot_approximate_nearest_neighbors_scalability.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/approximate.py|sklearn/neighbors/tests/test_approximate.py|sklearn/neighbors/approximate.py|sklearn/utils/testing.py|examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.py,14,0.0,0,10,false,[MRG] Locality Sensitive Hashing for approximate nearest neighbor search New PR created from #3304,,2240,0.7691964285714286,0.06753962784286699,40059,421.87773034773704,33.375770738161215,108.14049277315958,4011,74,1563,215,travis,maheshakya,larsmans,false,,13,0.5384615384615384,3,0,1043,true,false,false,false,0,22,1,28,100,0,73
5387692,scikit-learn/scikit-learn,python,3893,1417071274,1417148662,1417148662,1289,1289,commits_in_master,false,false,false,55,1,1,0,2,0,2,0,3,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.593549426736528,0.10392977941904183,2,dsullivan7@hotmail.com,doc/themes/scikit-learn/static/nature.css_t,2,0.0013783597518952446,0,1,false,[MRG] DOC fix layout: ensure stable width of documentwrapper Fixes bugs described at https://githubcom/scikit-learn/scikit-learn/commit/937f36cba77f18af7b37f0798f76aaeaed0b02d8#commitcomment-8720925Ive only tested this by looking at BallTree (which is affected particularly because of its lack of content) and a class with examples Both issues seem to be fixed (and I have no idea why I originally made those CSS changes),,2239,0.7690933452434122,0.06753962784286699,40059,421.87773034773704,33.375770738161215,108.14049277315958,4011,74,1563,196,travis,jnothman,jnothman,true,jnothman,95,0.7052631578947368,29,1,2039,true,true,false,false,22,342,11,346,25,7,10
5386022,scikit-learn/scikit-learn,python,3892,1417055384,1417057286,1417057286,31,31,commits_in_master,false,false,false,217,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.585737473929361,0.10375345823004271,6,jnothman@student.usyd.edu.au,sklearn/naive_bayes.py,6,0.004137931034482759,0,0,false,DOC: COSMIT: Naive Bayes long url + PEP8 cleanup * Put long link to IR-book on its own line to make sure it renders as a  correct link in the docs* Put long ValueError messages that were breaking PEP8 compliance of  the file naive_bayespy into temporary variablesSigned-off-by: mrShu mr@shuio-------------This pull request might not be necessary at all but at least the change on line 572 is pretty important to me It took me a while to realize what the URL should be by looking in here (http://scikit-learnorg/dev/modules/generated/sklearnnaive_bayesMultinomialNBhtml#sklearnnaive_bayesMultinomialNB) and I also think this is just not right and shouldnt be left like thatI understand that this breaks PEP8 (especially the line length rule) and there are ways how to get around that (http://stackoverflowcom/questions/14414878/how-do-i-break-a-link-in-a-rst-docstring-to-satisfy-pep8) but I believe this is precisely the case in which the following from the original PEP8 document applies:  But most importantly: know when to be inconsistent -- sometimes the style guide just doesnt apply When in doubt use your best judgment Look at other examples and decide what looks best And dont hesitate to ask Two good reasons to break a particular rule: - When applying the rule would make the code less readable even for someone who is used to reading code that follows the rules,,2238,0.7689901697944593,0.06689655172413793,40060,421.69246130803793,33.37493759360959,108.08786819770344,4010,74,1562,196,travis,mrshu,amueller,false,amueller,1,1.0,46,4,1487,false,false,false,false,0,0,0,0,0,0,31
5261204,scikit-learn/scikit-learn,python,3891,1417055308,1423968987,1423968987,115227,115227,commits_in_master,false,true,false,29,134,28,80,58,0,138,0,7,0,0,4,4,2,0,0,0,0,4,4,2,0,0,1321,253,2874,1333,260.0438072183431,5.88355623137995,53,s8wu@uwaterloo.ca,doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/whats_new.rst|sklearn/multiclass.py|doc/modules/multiclass.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,51,0.006206896551724138,3,39,false,[WIP] Add decision_function for OneVsOneClassifier Adding decision_function support for the OneVsOneClassifierThis is based off of #1548 and addresses #1523@arjoly @amueller @mblondel  Pl review when you find time,,2237,0.7688869021010282,0.06689655172413793,40060,421.69246130803793,33.37493759360959,108.08786819770344,4010,74,1562,258,travis,ragv,amueller,false,amueller,0,0,3,1,26,true,false,false,false,2,3,0,0,43,0,46
5383096,scikit-learn/scikit-learn,python,3889,1417033098,1417037483,1417037483,73,73,github,false,false,false,9,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.235774119962013,0.0958354479403186,0,,examples/covariance/plot_outlier_detection.py,0,0.0,0,0,false,Update plot_outlier_detectionpy I think this word should be inliers,,2236,0.768783542039356,0.06643109540636043,40060,421.69246130803793,33.37493759360959,108.08786819770344,4008,74,1562,195,travis,jdcaballero,jdcaballero,true,jdcaballero,0,0,8,22,1315,true,false,false,false,0,0,0,0,1,0,-1
5257267,scikit-learn/scikit-learn,python,3887,1416991049,1417060629,1417060629,1159,1159,commit_sha_in_comments,false,false,false,109,2,1,1,6,0,7,0,4,0,0,3,3,1,0,0,0,0,3,3,1,0,0,0,9,0,9,13.849493903534881,0.3133478657249411,11,joel.nothman@gmail.com,doc/modules/grid_search.rst|doc/modules/kernel_approximation.rst|sklearn/tests/test_grid_search.py,9,0.005007153075822604,0,3,false,[MRG] DOC narrative docs for grid searchs robustness to failure A fairly minor doc and test improvement The error_score parameter in grid search wasnt mentioned in narrative docsI realised there were a number of best practice tips associated with parameter search not specific to GridSearchCV or RandomizedSearchCV Ive pulled these out into a separate section but Im not sure that the formatting is appropriate (see screen shot below) or if theres a better way of presenting in Sphinx what might be \paragraph in LaTeXApart from formatting this PR is very minor[Uploading 32 Grid Search  Searching for estimator parameters — scikit-learn 016-git documentation (1)png   ](),,2235,0.7686800894854586,0.058655221745350504,40060,421.5177234148777,33.34997503744383,108.03794308537195,4006,73,1562,197,travis,jnothman,jnothman,true,jnothman,94,0.7021276595744681,29,1,2038,true,true,false,false,23,344,9,355,25,2,228
5372040,scikit-learn/scikit-learn,python,3884,1416955397,1416956091,1416956091,11,11,commits_in_master,false,false,false,22,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.517264058356484,0.10220412733655487,2,dsullivan7@hotmail.com,sklearn/preprocessing/label.py,2,0.0014245014245014246,0,0,false,DOC: Fix LabelBinarizer docstring to rst format bug Also add line to make multilabel classification example more instructiveBefore:[selection_002](https://cloudgithubusercontentcom/assets/1284973/5189062/c35a7a1e-74a8-11e4-93a0-c7888a895bb8png)After:[selection_003](https://cloudgithubusercontentcom/assets/1284973/5189069/cbd31ec6-74a8-11e4-84e9-332af60869bdpng),,2232,0.7692652329749103,0.05698005698005698,40060,421.5177234148777,33.32501248127808,108.01298052920619,4003,74,1561,194,travis,isms,amueller,false,amueller,0,0,18,11,1066,true,false,false,false,0,0,0,0,1,0,-1
5370852,scikit-learn/scikit-learn,python,3883,1416949310,1417007755,1417007755,974,974,commits_in_master,false,false,false,115,2,2,0,6,0,6,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,15,77,15,77,8.81184882261362,0.19937008496787798,25,olivier.grisel@ensta.org,sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx,21,0.009978617248752673,0,1,false,[MRG] Improve stability of SGDClassifier / SGDRegressor with gradient clipping The squared_hinge loss of SGDClassifier (and potentially the squared loss of SGDRegressor) tend to trigger numerical overflows depeinding even on well conditioned data for some hyper parameter combinationsThis PR fixes that issue by clipping dloss to 1e12 All other tests passI have also add to prevent strong l2 regularization with large learning rates to trigger negative scales (which are meaningless) Instead I set the weights to zero in that case The non regression tests highlight this caseBoth non regression tests were inspired by #3040 They both fail at epoch #2 and #3 of the iris data with the sgd_fastpyx implementation from master,,2231,0.7691618108471537,0.05702066999287242,40060,421.5177234148777,33.32501248127808,108.01298052920619,4003,73,1561,196,travis,ogrisel,larsmans,false,larsmans,103,0.8543689320388349,1063,124,2008,true,true,true,true,22,229,36,171,81,8,17
5261861,scikit-learn/scikit-learn,python,3882,1416934472,1417116975,1417116975,3041,3041,commits_in_master,false,false,false,9,1,1,3,27,0,30,0,6,0,0,5,5,5,0,0,0,0,5,5,5,0,0,56,0,56,0,13.670709998531652,0.3093039000334531,1,Florian.Wilhelm@gmail.com,sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/neighbors/kd_tree.pyx,1,0.0007199424046076314,0,7,false,[MRG] DOC: Fix BallTree and KDTree Docs Fixes https://githubcom/scikit-learn/scikit-learn/issues/3734,,2230,0.7690582959641256,0.05687544996400288,40057,421.5492922585316,33.32750830067154,108.02106997528523,4000,72,1561,198,travis,MechCoder,ogrisel,false,ogrisel,46,0.8695652173913043,81,41,889,true,true,true,false,14,429,50,254,110,3,2
5250774,scikit-learn/scikit-learn,python,3881,1416888709,1417148638,1417148638,4332,4332,commits_in_master,false,false,false,10,2,0,3,2,0,5,0,2,0,0,0,2,0,0,0,0,0,2,2,1,0,1,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,WEB fix css for new version of sphinx Fixes #3876,,2229,0.768954688200987,0.05391804457225018,40057,421.5492922585316,33.32750830067154,108.02106997528523,3996,73,1560,197,travis,amueller,jnothman,false,jnothman,215,0.8558139534883721,1031,40,1494,true,true,false,false,12,121,15,19,14,0,143
5249664,scikit-learn/scikit-learn,python,3880,1416879052,1443546979,1443546979,444465,444465,merged_in_comments,false,true,false,59,13,2,28,23,0,51,0,5,0,0,5,8,5,0,0,0,0,8,8,8,0,0,264,39,520,167,18.471999460251673,0.41793338905767674,34,trev.stephens@gmail.com,sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|benchmarks/bench_covertype.py,28,0.001440922190201729,3,7,false,Sparse input support for Gradient Boosting adds sparse input support:  * Falls back to BestSparseSplitter  * Holds data in both csc and csr format during fit (needs the latter just to call treeapply to get the terminal leaf assignment)Todo:  - [ ] more & better tests  - [ ] benchmarks  - [ ] reviewcc @arjoly @jnothman @glouppe ,,2228,0.768850987432675,0.053314121037463975,40056,421.1354104254044,33.32834032354703,107.9738366287198,3995,73,1560,369,travis,pprett,arjoly,false,arjoly,47,0.8723404255319149,142,29,1938,true,true,false,true,7,36,2,16,16,0,0
5255862,scikit-learn/scikit-learn,python,3877,1416859452,,1417650278,13180,,unknown,false,false,false,31,15,1,7,21,0,28,0,8,0,0,9,13,9,0,0,0,0,13,13,11,0,0,164,0,280,49,27.111028378794636,0.6133934767359569,14,s8wu@uwaterloo.ca,sklearn/svm/__init__.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/svm/src/liblinear/liblinear_helper.c,9,0.001444043321299639,0,9,false,[WIP] Initial implementation of LinearSVR using liblinear code Very initial implementation of LinearSVR using liblinears code (which was always available we just did not use it) Still lacking tests and documentation,,2227,0.7691962281095645,0.053429602888086646,40056,421.1354104254044,33.32834032354703,107.9738366287198,3994,72,1560,203,travis,fabianp,fabianp,true,,36,0.7222222222222222,220,25,1654,false,true,false,false,1,6,0,0,1,0,348
5333329,scikit-learn/scikit-learn,python,3874,1416591172,1416597865,1416597865,111,111,commit_sha_in_comments,false,false,false,14,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,8,8,8,8.926330096780898,0.20196000193501815,1,Florian.Wilhelm@gmail.com,sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_image.py,1,0.0007342143906020558,0,0,true,[MRG] FIX: Raise error when patch width/height is greater than image width/height Fixes https://githubcom/scikit-learn/scikit-learn/issues/3738,,2226,0.7690925426774483,0.04331864904552129,40039,421.0894377981468,33.29253977372063,107.91977821623918,3977,71,1557,195,travis,MechCoder,amueller,false,amueller,45,0.8666666666666667,81,41,885,true,true,false,false,14,416,50,257,107,3,111
5243196,scikit-learn/scikit-learn,python,3873,1416588978,,1417103011,8567,,unknown,false,false,false,70,10,1,39,16,0,55,0,3,0,0,6,8,4,0,0,0,0,8,8,5,0,0,48,90,78,139,27.215358468286645,0.6157529230181119,31,olivier.grisel@ensta.org,doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py,19,0.0066273932253313695,1,10,true,[MRG] ENH add coverage multilabel ranking metric This pr adds the coverage multilabel ranking metric in scikit-learnIts very useful as it carries a lot of intuition on your estimator performance This is the average number of true labels that you need to predict in order to avoid missing any true one given your y_score matrix@jnothman Do you see an easy way to make this with sparse csc matrices,,2225,0.7694382022471911,0.04197349042709867,40039,421.0894377981468,33.29253977372063,107.91977821623918,3977,71,1557,203,travis,arjoly,arjoly,true,,74,0.8243243243243243,28,26,1067,true,true,false,false,6,157,10,255,79,1,1354
5236480,scikit-learn/scikit-learn,python,3871,1416572889,1416668814,1416668814,1598,1598,commits_in_master,false,false,false,75,2,1,10,10,0,20,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,34,0,83,0,4.566336064143206,0.10342279507124308,2,dsullivan7@hotmail.com,sklearn/cluster/affinity_propagation_.py,2,0.0014781966001478197,0,4,true,Accelerate AffinityPropagation Accelerate AffinityPropagation for large inputs using standard tricks (in-place operations and out parameters) to avoid extra allocationsBasic tests (see below not in the PR) on datasets for 1000 points in 2D show ~10-20% increase inspeedimport numpy as npfrom sklearncluster import AffinityPropagationN  100M  5nprandomseed(0)data  npconcatenate(    [nprandomrandom((N 2)) + i for i in range(M)] +    [nprandomrandom((N * M 2)) * M])print(AffinityPropagation()fit_predict(data)),,2224,0.7693345323741008,0.04138950480413895,39871,419.82894835845605,33.407739961375434,107.94813272804795,3977,71,1557,197,travis,anntzer,ogrisel,false,ogrisel,0,0,15,2,1045,false,false,false,false,0,0,0,0,0,0,72
5331068,scikit-learn/scikit-learn,python,3870,1416570906,1416575624,1416575624,78,78,commits_in_master,false,false,false,19,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.744180957613624,0.10745079820405877,46,s8wu@uwaterloo.ca,doc/whats_new.rst,46,0.03399852180339985,0,0,true,DOC: Added whats_new for TheilSenRegressor Added the announcements for the TheilSenRegressor and median_absolute_error in whats_newrst for merged PR #2949,,2223,0.7692307692307693,0.04138950480413895,39871,419.82894835845605,33.407739961375434,107.94813272804795,3976,71,1557,196,travis,FlorianWilhelm,ogrisel,false,ogrisel,4,0.75,18,20,686,true,true,false,false,0,20,2,28,56,0,78
5235944,scikit-learn/scikit-learn,python,3869,1416523063,1417606497,1417606497,18057,18057,commits_in_master,false,false,false,82,3,1,6,5,0,11,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,28,31,48,8.913808487222145,0.20188855474684828,2,dsullivan7@hotmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,2,0.0014781966001478197,0,0,false,Fix RFE n_features minimum value #3812 In RFE class the step value was just rounded down step  int(selfstep * n_features)This was a problem when the number of features was small because step was 0 and it was raising an exceptionValueError: Step must be 0With the following fix we still keep the rounding down (so as not to change behavior too much) but do a minimum with 1 **step  [1 n_features-1]**Fixed issue https://githubcom/scikit-learn/scikit-learn/issues/3812,,2222,0.7691269126912691,0.04138950480413895,39871,419.82894835845605,33.407739961375434,107.94813272804795,3967,71,1556,206,travis,borjaayerdi,GaelVaroquaux,false,GaelVaroquaux,0,0,9,2,630,false,false,false,false,1,0,0,0,0,0,733
5320899,scikit-learn/scikit-learn,python,3868,1416502933,1416514320,1416514320,189,189,commits_in_master,false,false,false,22,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.522370149410332,0.10287000064998328,0,,examples/cluster/plot_dict_face_patches.py,0,0.0,0,0,false,[MRG] DOC: Typo in dict_facespy It seems to me there is a typo Im not sure if it is right or wrong,,2221,0.7690229626294462,0.04047976011994003,39599,417.56104952145256,33.18265612767999,107.65423369277002,3964,71,1556,196,travis,MechCoder,agramfort,false,agramfort,44,0.8636363636363636,81,41,884,true,true,true,false,14,406,48,257,106,3,-1
5319875,scikit-learn/scikit-learn,python,3867,1416494681,1416500344,1416500344,94,94,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.49995292156916,0.10236006532490802,10,s8wu@uwaterloo.ca,doc/modules/linear_model.rst,10,0.007501875468867217,0,0,false,Small typo in the documentation ,,2220,0.768918918918919,0.04051012753188297,39595,417.60323273140546,33.186008334385654,107.66510923096351,3964,71,1556,197,travis,snuderl,agramfort,false,agramfort,0,0,10,2,1164,false,false,false,false,0,0,0,0,0,0,-1
5297568,scikit-learn/scikit-learn,python,3862,1416357285,1416361559,1416361559,71,71,commits_in_master,false,false,false,31,2,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,9,0,3.516959946456265,0.08014521852799568,9,najera.oscar@gmail.com,doc/sphinxext/gen_rst.py,9,0.0067618332081141996,0,0,false,gen_rst informs file with missing documentation When porting this file to a new repository I realized it does not specify which script in the examples does not match the docstring layout,,2219,0.7688147814330779,0.03831705484598046,39595,417.57797701730016,33.186008334385654,107.66510923096351,3953,72,1554,199,travis,Titan-C,GaelVaroquaux,false,GaelVaroquaux,1,1.0,7,2,1322,true,false,false,false,0,1,1,1,1,0,71
5289620,scikit-learn/scikit-learn,python,3861,1416322045,1416330116,1416330116,134,134,commits_in_master,false,false,false,60,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,11,0,11,4.238747858869216,0.09659351098654194,5,s8wu@uwaterloo.ca,sklearn/utils/testing.py,5,0.003799392097264438,0,1,false,[MRG] FIX make assert_raises_regex backport for 26 consistent with 27+ Use regexsearch instead of regexmatchFurthermore silence the deprecation warning from Python 34 asassert_raises_regexp is now deprecated in favor of the shorter name:assert_raises_regexI dont want to change all occurrences of assert_raises_regexp in this PR Lets do that later (if ever as this is not a big deal),,2218,0.7687105500450857,0.0364741641337386,39595,417.55272130319486,33.186008334385654,107.6398535168582,3946,71,1554,199,travis,ogrisel,agramfort,false,agramfort,102,0.8529411764705882,1061,124,2001,true,true,true,true,20,180,26,172,71,8,133
5219444,scikit-learn/scikit-learn,python,3860,1416319268,1416404531,1416404531,1421,1421,commits_in_master,false,true,false,66,1,1,3,13,0,16,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,12,3,12,3,13.309842897081133,0.3033076037816812,7,s8wu@uwaterloo.ca,sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/utils/__init__.py,7,0.0007598784194528875,0,4,false,[MRG] ENH use specific warning class for RP This is a followup to the discussion in #3856Using a custom warning class will make it possible to use RP for usage not related to dimensionality reduction without issuing a phony warning when we expect the number of components to be larger than the number of featuresAn example of such generic usage of RP is LSH,,2217,0.7686062246278755,0.0364741641337386,39595,417.55272130319486,33.186008334385654,107.6398535168582,3946,71,1554,198,travis,ogrisel,ogrisel,true,ogrisel,101,0.8514851485148515,1061,124,2001,true,true,false,false,20,177,25,172,70,8,12
5236666,scikit-learn/scikit-learn,python,3858,1416267278,1416779438,1416779438,8536,8536,commits_in_master,false,false,false,10,1,0,10,10,0,20,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,3,11,0,0.0,0,,,0,0.0,0,1,false,k-means: add n_init argument check Attempts to address #3842 ,,2215,0.7688487584650113,0.036418816388467376,39595,417.55272130319486,33.186008334385654,107.6398535168582,3938,70,1553,201,travis,banilo,ogrisel,false,ogrisel,1,0.0,5,0,608,true,false,false,false,0,2,1,0,1,0,134
5278866,scikit-learn/scikit-learn,python,3857,1416260066,1416495077,1416495077,3916,3916,commits_in_master,false,false,false,57,1,1,0,11,0,11,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,11,8,11,8.531587655169865,0.19441975855632904,3,larsmans@gmail.com,sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,3,0.0022813688212927757,0,2,false,[MRG] Warn when sparse matrix is supplied to predict in KNeighborsClassifier In https://githubcom/scikit-learn/scikit-learn/pull/3802There might be cases when I predict on sparse data after fitting on dense data (the centroids) Is it okay to densify the sparse matrix supplied to the predict method (with a warning) or should I just resort to using algorithmbrute over here https://githubcom/scikit-learn/scikit-learn/pull/3802/files#diff-1741ad6b05f1eb0fd71af8bad0e001c7R484,,2214,0.7687443541102078,0.03574144486692015,39595,417.55272130319486,33.186008334385654,107.6398535168582,3937,69,1553,200,travis,MechCoder,agramfort,false,agramfort,43,0.8604651162790697,81,41,881,true,true,true,false,14,390,49,255,101,3,671
5275318,scikit-learn/scikit-learn,python,3856,1416244135,,1416318731,1243,,unknown,false,false,false,91,1,1,0,5,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,10,8,10,8.744003608556891,0.19926034158028624,1,dsullivan7@hotmail.com,sklearn/random_projection.py|sklearn/tests/test_random_projection.py,1,0.0007662835249042146,0,0,false,[MRG] remove useless warning in Random Projections While RP is often used for dimensionality reduction its not necessarily the case For instance the LSH pull request #3304 is using it internally to bucket the dataFurthermore if the user calls it with a fixed number of components (instead of using the auto mode that computes n_components based on the number of samples and the eps of the JL lemma) the users probably know what he/she is is doing so the warning is uselessTherefore I think we should remove this warning,,2213,0.7690917306823317,0.03524904214559387,39595,417.55272130319486,33.186008334385654,107.6398535168582,3936,69,1553,199,travis,ogrisel,ogrisel,true,,100,0.86,1059,124,2000,true,true,false,false,20,174,23,172,69,8,11
5251368,scikit-learn/scikit-learn,python,3854,1416014242,1416014418,1416014418,2,2,commits_in_master,false,false,false,47,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.513752718746604,0.10296871698747838,11,manojkumarsivaraj334@gmail.com,doc/modules/model_evaluation.rst,11,0.008416220351951033,0,0,true,DOC FIX: typo and minor update The documentation repeated mean_absolute_error twice when (I assume) the second instance was supposed to refer to mean_squared_error so I fixed thatAlso it appears that median_absolute_error fits the criteria described in the sentence so I added that function to the sentence,,2212,0.7689873417721519,0.03442999234889059,39590,417.4791614043951,33.19019954533974,107.65344784036373,3925,69,1550,198,travis,justmarkham,GaelVaroquaux,false,GaelVaroquaux,0,0,533,0,292,false,false,false,false,0,0,0,0,0,0,2
5244619,scikit-learn/scikit-learn,python,3853,1415981011,1416083627,1416083627,1710,1710,commits_in_master,false,false,false,55,1,1,0,4,0,4,0,3,1,0,3,4,4,0,0,1,0,3,4,4,0,0,544,19,544,19,18.123454357976634,0.41343621569508066,16,s8wu@uwaterloo.ca,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py|sklearn/utils/_scipy_sparse_lsqr_backport.py|sklearn/utils/fixes.py,15,0.0038255547054322878,0,2,true,[MRG] Fix ZeroDivisionError in LinearRegression on sparse data This is a fix for #2986 by backporting the fix for scipy/scipy#4142 To do so I embedded a copy of the scipy/sparse/linalg/isolve/lsqrpy file from scipy as _scipy_sparse_lsqr_backportpy in sklearn/utilsI extended the test in scikit-learn to trigger the issue consistently by using a range of random seeds,,2211,0.7688828584350973,0.03442999234889059,39590,417.4791614043951,33.19019954533974,107.65344784036373,3919,68,1550,197,travis,ogrisel,ogrisel,true,ogrisel,99,0.8585858585858586,1058,124,1997,true,true,false,false,20,185,21,190,66,7,0
5205067,scikit-learn/scikit-learn,python,3852,1415970125,1416363275,1416363275,6552,6552,commits_in_master,false,false,false,23,2,0,3,13,0,16,0,7,0,0,0,1,0,0,0,1,0,1,2,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,1,true,[MRG] DOC: add funding from Paris-Saclay Centre for Data Science to the website Also completed INRIA funding description while I was at it,,2210,0.7687782805429865,0.03450920245398773,39590,417.4791614043951,33.19019954533974,107.65344784036373,3919,68,1550,200,travis,lesteve,GaelVaroquaux,false,GaelVaroquaux,3,1.0,4,0,933,false,false,false,false,0,0,0,0,0,0,7
5233021,scikit-learn/scikit-learn,python,3850,1415909791,1421591405,1421591405,94693,94693,commits_in_master,false,false,false,213,13,4,21,16,0,37,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,114,199,258,217,43.91026486263121,1.0016862711295267,47,s8wu@uwaterloo.ca,sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_metaestimators.py,43,0.006051437216338881,0,11,false,MRG Make ParameterSampler sample without replacement Fixes #3792I think this issue came up before so maybe it is worth addressingThis makes the ParametersSampler sample without replacement if all parameters are listsThis has a couple of gotchas:* It changes current behavior in particular I raise an error if there are less possible settings than n_iter* I dont know if there is a way to detect finite support in a scipy distribution so we can not do it there That means that bla : bernoulli(05) is not equivalent any more to bla: [0 1] (the first does sampling with replacement the second without)Possibilities to resolve these would be:* give a warning instead of a value error when n_iter  grid_size and produce a smaller grid (or sample with replacement in this case) Might invalidate the __len__ implementation* add a parameter with_replacement set it to None and go to a deprecation cycle to set it to False The parameter would only do something in the case all parameters are lists though* Hack into the scipy dists check if something is discrete and use lower and upper to get the support If a user tries to implement his own distribution with finite support we would still be screwed,,2209,0.768673607967406,0.03479576399394856,39592,417.45807233784603,33.188522933926045,107.64800969892907,3910,69,1549,243,travis,amueller,jnothman,false,jnothman,214,0.8551401869158879,1026,40,1483,true,true,false,false,8,88,10,18,11,0,15651
5195583,scikit-learn/scikit-learn,python,3847,1415821120,1415844416,1415844416,388,388,merged_in_comments,false,false,false,15,1,1,2,9,0,11,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.009069508835268,0.11426782799958178,0,,doc/about.rst,0,0.0,0,2,false,Update about page I found the GSoC list was outdated I just updated the list,,2208,0.7685688405797102,0.03303303303303303,39592,417.45807233784603,33.188522933926045,107.64800969892907,3898,70,1548,197,travis,MechCoder,MechCoder,true,MechCoder,42,0.8571428571428571,81,41,876,true,true,false,false,13,385,47,228,95,3,2
5190447,scikit-learn/scikit-learn,python,3842,1415539628,,1416607613,17799,,unknown,false,true,false,4,2,1,0,10,0,10,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,7,0,4.331243775681022,0.09880585671553309,14,s8wu@uwaterloo.ca,sklearn/cluster/k_means_.py,14,0.010486891385767791,0,0,false,k-means: missing variable initialization ,,2207,0.7689170820117807,0.03146067415730337,39585,417.5318933939624,33.194391815081474,107.66704559808008,3875,70,1545,203,travis,banilo,ogrisel,false,,0,0,5,0,600,false,false,false,false,0,0,0,0,0,0,1358
5231368,scikit-learn/scikit-learn,python,3838,1415407016,,1415903220,8270,,unknown,false,false,false,56,4,3,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,76,0,78,0,11.908818975827476,0.2719523123519487,1,amueller@ais.uni-bonn.de,sklearn/tree/export.py|sklearn/tree/export.py|sklearn/tree/export.py,1,0.0007358351729212656,0,4,true,additional options for decision tree export functions Modified export functions for decision trees to include options to (1) show sample values for all nodes and (2) options to convert to probabilities Default behavior computes this only for the leaves of the decision tree This is done through additional options: verbose  True/False and probabilities  True/False,,2206,0.7692656391659112,0.03090507726269316,39562,417.7746322228401,33.213689904453766,107.72963955310652,3875,70,1543,197,travis,xbhsu,xbhsu,true,,0,0,2,4,193,true,false,false,false,0,0,0,0,2,0,2
5282610,scikit-learn/scikit-learn,python,3837,1415382331,1416276121,1416276121,14896,14896,commits_in_master,false,true,false,17,1,1,0,1,0,1,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,7,4,7,8.770181086972272,0.20027767379512582,9,mathieu@mblondel.org,sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py,9,0.0066469719350073855,0,2,true,Fix for issue 3815 Discrete AdaBoostClassifier now fails early if the base classifier if worse than random,,2205,0.7691609977324263,0.02880354505169867,39565,417.7429546316189,33.211171489953244,107.72147099709339,3875,69,1543,201,travis,mattilyra,larsmans,false,larsmans,4,0.75,6,1,1148,false,true,false,false,1,1,0,0,0,0,308
5954320,scikit-learn/scikit-learn,python,3836,1415380611,1421891730,1421891730,108518,108518,commits_in_master,false,false,false,111,12,2,14,17,0,31,0,5,0,0,3,3,2,0,0,0,0,3,3,2,0,0,110,72,186,124,28.34848335257172,0.6466955750594903,9,larsmans@gmail.com,doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,7,0.0036900369003690036,0,3,true,[MRG+1] ENH: cross-validate over predefined folds For some datasets a pre-defined split of the data into several folds already exists This is the case eg if one wants to build upon results produced by others or even for famous datasets such as NORB Currently there is no way to use such predefined folds in sklearn when using eg GridSearchCVThis PR adds PredefinedSplit as new CV-Split class that takes a predefined split and can be fed into GridSearchCV Example usage:    # folds is an array that contains 0 for samples that are part of testfold 0 1 for testfold 1     cv  PredefinedSplit(folds)    gs  GridSearchCV(clf param_gridparam_grid cvcv)    gsfit(X y),,2204,0.7690562613430127,0.028782287822878228,39585,417.5318933939624,33.194391815081474,107.66704559808008,3875,69,1543,249,travis,untom,amueller,false,amueller,9,0.6666666666666666,10,0,627,true,false,false,false,0,12,4,9,1,0,16
5180734,scikit-learn/scikit-learn,python,3833,1415328829,1416063264,1416063264,12240,12240,commits_in_master,false,false,false,27,2,1,0,7,0,7,0,4,0,0,3,4,3,0,0,0,0,4,4,4,0,0,39,4,84,8,13.364393694091824,0.3051920654991854,32,s8wu@uwaterloo.ca,sklearn/linear_model/stochastic_gradient.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,23,0.005865102639296188,0,2,false,WIP raise error when number of features changes in partial fit Fixes an issue where partial fit silently ignores additional featuresThe NB classifiers are still todo,,2202,0.7693006357856494,0.028592375366568914,39565,417.7429546316189,33.211171489953244,107.72147099709339,3873,68,1542,199,travis,amueller,jnothman,false,jnothman,213,0.8544600938967136,1022,40,1476,true,true,false,false,7,60,9,12,7,0,578
5170883,scikit-learn/scikit-learn,python,3829,1415237463,1415322916,1415322916,1424,1424,commits_in_master,false,false,false,11,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.355742528964161,0.09946853793684154,0,,sklearn/feature_selection/from_model.py,0,0.0,0,0,false,MRG DOC explain SelectorMixin strategy with multiple classes Inspired by [stackoverflow](http://stackoverflowcom/questions/26764249/how-does-threshold-work-for-linearsvc-transform/26764788noredirect1#comment42111988_26764788),,2201,0.769195820081781,0.028363636363636365,39565,417.7682294957665,33.211171489953244,107.72147099709339,3864,68,1541,195,travis,amueller,larsmans,false,larsmans,212,0.8537735849056604,1022,40,1475,true,true,true,true,4,30,6,6,7,0,27
5170722,scikit-learn/scikit-learn,python,3828,1415236402,1415301888,1415301888,1091,1091,commits_in_master,false,false,false,20,2,2,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,8.729596970673043,0.19935068284608273,0,,sklearn/tree/export.py|sklearn/tree/export.py,0,0.0,0,1,false,MRG DOC Add note about sample weights in export_graphviz Documentation for what export_graphviz actually does fixes #3794 do some degree,,2200,0.769090909090909,0.028363636363636365,39565,417.7682294957665,33.211171489953244,107.72147099709339,3864,68,1541,195,travis,amueller,amueller,true,amueller,211,0.8530805687203792,1022,40,1475,true,true,false,false,4,28,5,6,7,0,658
5167389,scikit-learn/scikit-learn,python,3826,1415219356,1416584494,1416584494,22752,22752,commits_in_master,false,false,false,19,2,1,0,3,0,3,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,8,17,4.379830629879214,0.1000186744931061,4,larsmans@users.noreply.github.com,sklearn/neighbors/kde.py,4,0.002902757619738752,0,1,false,WIP FIX in KDE fit and score allow yNone Fix for using pipeline / grid-search with KDE Tests a-coming,,2198,0.7693357597816196,0.02830188679245283,39565,417.7682294957665,33.211171489953244,107.72147099709339,3863,68,1541,201,travis,amueller,ogrisel,false,ogrisel,209,0.8564593301435407,1022,40,1475,true,true,true,false,1,18,3,6,6,0,9928
5159792,scikit-learn/scikit-learn,python,3824,1415158922,1418645201,1418645201,58104,58104,commits_in_master,false,false,false,35,9,5,16,20,0,36,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,37,37,92,52,31.08832192206468,0.7099390522636592,2,joel.nothman@gmail.com,sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py,2,0.0014388489208633094,0,6,false,[MRG] FIX: Bug in RFECV when step is not 1 HiThis patch fixes the bug in RFECV class when step is not equal to one Everything should be clear from the codePlease review,,2197,0.7692307692307693,0.02805755395683453,39565,417.7682294957665,33.211171489953244,107.72147099709339,3853,68,1540,225,travis,nmayorov,agramfort,false,agramfort,4,0.0,5,0,223,true,false,false,false,0,22,1,13,10,0,59
5152836,scikit-learn/scikit-learn,python,3823,1415114769,1417011746,1417011746,31616,31616,commits_in_master,false,false,false,14,3,0,6,7,0,13,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,9,0,0.0,0,,,0,0.0,2,3,false,Fixes #3644 Added return_path parameter while calling orthogonal_mp_gram to propagate the same@agramfort @MechCoder,,2196,0.7691256830601093,0.028077753779697623,39565,417.7682294957665,33.211171489953244,107.72147099709339,3849,68,1540,206,travis,eientuni,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,58,false,false,false,false,0,1,0,0,0,0,74
5151205,scikit-learn/scikit-learn,python,3821,1415106121,1415242685,1415242685,2276,2276,merged_in_comments,false,false,false,20,2,2,0,13,0,13,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,9.381023375774706,0.214226902484053,2,manojkumarsivaraj334@gmail.com,sklearn/utils/stats.py|sklearn/utils/stats.py,2,0.0014388489208633094,0,9,false,[MRG] COSMIT: Use searchsorted in weighted_percentile Im sure this does not change speed or anything but it definitely looks better,,2194,0.7693710118505014,0.02805755395683453,39565,417.7682294957665,33.211171489953244,107.72147099709339,3847,68,1540,196,travis,MechCoder,amueller,false,amueller,41,0.8536585365853658,81,41,868,true,true,false,false,11,382,49,226,105,3,0
5141866,scikit-learn/scikit-learn,python,3820,1415040920,1415217567,1415217567,2944,2944,merged_in_comments,false,false,false,41,1,1,0,7,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.27729308583335,0.0976801929848461,1,dsullivan7@hotmail.com,sklearn/cross_decomposition/pls_.py,1,0.0007097232079489,0,7,false,[MRG] Fix unstable CCA test under 32 bit Python  FIX #3503: use linalgpinv to better deal with singular input dataI will run the AppVeyor tests on this branch as a sanity check The failure is random and quite rare though,,2193,0.769265845873233,0.027679205110007096,39810,414.87063551871387,32.98166289876915,106.98317005777443,3840,68,1539,193,travis,ogrisel,amueller,false,amueller,98,0.8571428571428571,1052,124,1986,true,true,false,true,19,199,20,240,59,7,1014
5123904,scikit-learn/scikit-learn,python,3817,1414810801,,1420528861,95301,,unknown,false,false,false,230,2,1,0,8,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,50,46,100,9.098684757419566,0.20801529310980624,26,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,23,0.015851137146795313,0,3,true,Show estimated end time for GBM verbose output I have often checked in on bigger GBM models (eg 2000 trees) to find the verbose1 output telling me that there was still several hundred minutes remaining from the 1000 tree marker Without an anchoring time stamp this left me with little idea of whether the update posted hours ago and I was nearly done or if I was possibly only half way throughThis PR adds an estimated end time to the output if the user enters a negative verbosity -1 gives the occasional update and -2 every tree as before but with the end time shown as well Old output still stands if using positive numbers or zero Not sure how people will feel about the negative verbosity implementation but I think this will be helpful for large datasets and/or big ensemblesQuick example output for 20 trees:    clf  GradientBoostingClassifier(n_estimators20 learning_rate005 max_depth6 verbose-1 random_state415)    clffit(train_x train_y)      Iter       Train Loss   Remaining Time         End Time          1           12427            595m     31 Oct 15:36         2           12053            534m     31 Oct 15:35         3           11716            458m     31 Oct 15:35         4           11409            375m     31 Oct 15:34         5           11131            318m     31 Oct 15:34         6           10873            275m     31 Oct 15:33         7           10640            242m     31 Oct 15:33         8           10423            213m     31 Oct 15:33         9           10227            188m     31 Oct 15:33        10           10041            165m     31 Oct 15:33        20           08781            000s     31 Oct 15:32,,2192,0.7696167883211679,0.026878015161957272,39740,413.63865123301457,32.8887770508304,106.86965274282838,3825,68,1536,233,travis,trevorstephens,trevorstephens,true,,2,1.0,102,51,445,true,false,false,false,1,18,2,4,8,0,316
5104237,scikit-learn/scikit-learn,python,3814,1414680883,,1441984903,455067,,unknown,false,false,false,9,65,0,20,55,0,75,0,7,0,0,0,23,0,0,0,6,2,21,29,21,0,0,0,0,10755,2919,0,0.0,0,,,0,0.0,0,10,false,[WIP] Adding Implementation of SAG An implementation of SAG,,2190,0.7703196347031963,0.025885558583106268,39740,413.63865123301457,32.8887770508304,106.86965274282838,3812,68,1535,362,travis,dsullivan7,agramfort,false,,14,0.6428571428571429,8,19,447,true,true,true,false,1,135,10,26,39,1,1
5093496,scikit-learn/scikit-learn,python,3811,1414605254,1414615643,1414615643,173,173,commits_in_master,false,false,false,37,5,5,0,7,0,7,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,165,10,165,10,36.363836171653226,0.8313539365866903,48,s8wu@uwaterloo.ca,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|doc/whats_new.rst|sklearn/metrics/pairwise.py|doc/whats_new.rst|sklearn/metrics/pairwise.py,45,0.003419972640218878,0,6,false,[MRG] Fix repeated calls of check_pairwise and type casting in pairwise_distances_argmin_min Fixes https://githubcom/scikit-learn/scikit-learn/issues/3807I am able to bring the time down from 313s as seen in this gist (https://gistgithubcom/MechCoder/8342cec755d167cc7cad) to 249s as seen in this gist (https://gistgithubcom/MechCoder/6c29efb33917c5d97857),,2189,0.7702147099132024,0.025991792065663474,39728,413.7635924285139,32.89871123640757,106.90193314538864,3807,68,1534,188,travis,MechCoder,agramfort,false,agramfort,40,0.85,81,41,862,true,true,true,false,11,385,45,228,106,3,2
5084468,scikit-learn/scikit-learn,python,3809,1414533425,1414542902,1414542902,157,157,commits_in_master,false,false,false,12,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.326826654525228,0.09892041262509871,4,perimosocordiae@gmail.com,sklearn/metrics/pairwise.py,4,0.002704530087897228,0,3,false,COSMIT: Minor opt in pairwise_distances_argmin_min I removed npwhere since it was unnecessary,,2188,0.770109689213894,0.025693035835023664,39730,413.742763654669,32.897055122074,106.89655172413794,3802,68,1533,188,travis,MechCoder,larsmans,false,larsmans,39,0.8461538461538461,81,41,861,true,true,false,false,11,391,44,229,106,3,0
5050696,scikit-learn/scikit-learn,python,3804,1414193516,1450993256,1450993256,613329,613329,merged_in_comments,false,true,false,59,7,5,0,9,0,9,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,53,36,59,43,27.105254767947528,0.6196840660489246,8,larsmans@users.noreply.github.com,sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py,8,0.005198180636777128,1,3,true,Store KL divergence after optimization in t-SNE This PR is based on #3422 from @joker0x5F5F and fixes [this issue](http://datasciencestackexchangecom/questions/762) which unfortunately has not been reported here )Additionally I renamed the error_ to kl_divergence_ because error is a term that might confuse users (maybe cost would be a better name) and added a test that requires the attribute TSNEkl_divergence_,,2187,0.7700045724737082,0.02599090318388564,39727,413.7740076018829,32.89953935610542,106.90462405920407,3778,69,1529,474,travis,AlexanderFabisch,ogrisel,false,ogrisel,9,0.7777777777777778,32,27,1219,true,true,true,false,1,9,1,1,2,0,8959
5048297,scikit-learn/scikit-learn,python,3802,1414179740,1418350244,1418350244,69508,69508,commits_in_master,false,true,false,207,61,9,256,230,5,491,0,7,3,0,3,17,5,0,0,4,1,13,18,10,0,0,2607,75,6233,606,95.34844841015753,2.1798693540461658,8,s8wu@uwaterloo.ca,sklearn/cluster/birch.py|sklearn/cluster/birch.py|examples/cluster/plot_birch.py|sklearn/cluster/__init__.py|sklearn/cluster/birch.py|sklearn/cluster/tests/test_birch.py|sklearn/utils/testing.py|sklearn/cluster/birch.py|sklearn/cluster/birch.py|examples/cluster/plot_birch.py|sklearn/cluster/__init__.py|sklearn/cluster/birch.py|sklearn/cluster/tests/test_birch.py|sklearn/utils/testing.py|sklearn/cluster/birch.py|sklearn/cluster/birch.py|examples/cluster/plot_birch.py|sklearn/cluster/__init__.py|sklearn/cluster/birch.py|sklearn/cluster/tests/test_birch.py|sklearn/utils/testing.py,8,0.0,0,81,true,[WIP] Clustering algorithm - BIRCH (Proof of Concept) Fixes https://githubcom/scikit-learn/scikit-learn/issues/2690The design is similar to the Java code written here https://codegooglecom/p/jbirch/I am pretty much sure it works (If the JAVA implementation is correct ofc) since I get the same clusters for both cases I opened this as a Proof of ConceptThis example has been modified http://scikit-learnorg/stable/auto_examples/cluster/plot_mini_batch_kmeanshtml When threshold is set to 30[figure_1](https://cloudgithubusercontentcom/assets/1867024/4772737/1dd8604a-5b9b-11e4-800d-aea031410a30png)When threshold is set to 10[figure_2](https://cloudgithubusercontentcom/assets/1867024/4772748/2e46daec-5b9b-11e4-8f49-672b0ada17b7png)TODO: A LOT- [ ] This is just a PoC and is really slow as compared to kmeans It is obvious that one should use Cython for insertion into the tree- [ ] There is a merging refinement that is described in the paper which I have not implemented yet- [ ] After a certain number of insertions there is an option to rebuild the tree if it goes beyond a certain memory- [ ] Only the Euclidean metric has been added there should be support for other metrics- [ ] Support for sparse matrices but I doubt this can be done given the concept of Linear Sum and Squared Sum in this model- [ ] More tests I have just added a sanity testAwating some initial feedback ,,2186,0.7698993595608418,0.025957170668397145,39727,413.7740076018829,32.89953935610542,106.90462405920407,3778,70,1529,226,travis,MechCoder,agramfort,false,agramfort,38,0.8421052631578947,81,41,857,true,true,true,false,11,391,43,218,105,4,212
5047996,scikit-learn/scikit-learn,python,3801,1414178279,1414203111,1414203111,413,413,commits_in_master,false,false,false,27,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.30548015676311,0.0984324800736029,3,joel.nothman@gmail.com,sklearn/mixture/dpgmm.py,3,0.0019442644199611147,0,0,true,DOC: Remove extra # from url in fit The extra # messes with the rst parser which produces an invalid URL where the html suffix is missing,,2185,0.7697940503432494,0.02592352559948153,39727,413.7740076018829,32.89953935610542,106.90462405920407,3778,70,1529,190,travis,cmd-ntrf,MechCoder,false,MechCoder,4,1.0,13,3,1213,false,true,false,false,0,0,0,0,0,0,413
5044746,scikit-learn/scikit-learn,python,3800,1414158696,1414350043,1414350043,3189,3189,commits_in_master,false,false,false,33,4,3,0,5,0,5,0,3,0,0,4,4,2,0,0,0,0,4,4,2,0,0,88,72,95,72,45.86901847561892,1.0486645982703284,55,s8wu@uwaterloo.ca,doc/whats_new.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/whats_new.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,48,0.0032216494845360823,0,2,true,Handle_unknown option to OneHotEncoder OneHotEncoder fails with a error that is not helpful if a missing categorical feature is present during transformation Add handle_unknown with an error and an ignore optionFixes https://githubcom/scikit-learn/scikit-learn/issues/2169,,2184,0.7696886446886447,0.027706185567010308,39727,415.2843154529665,32.89953935610542,107.40806000956528,3777,70,1529,190,travis,MechCoder,larsmans,false,larsmans,37,0.8378378378378378,81,41,857,true,true,false,false,12,397,42,218,104,4,0
5040746,scikit-learn/scikit-learn,python,3798,1414117611,,1445264041,519107,,unknown,false,false,false,29,1,1,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,349,284,349,284,8.737066471274213,0.19974816566066272,10,mathieu@mblondel.org,sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py,9,0.005813953488372093,0,0,false,Issue-3449: Add multioutput support to bagging Also edited the tests to evaluate the multioutput changes currently passes all tests but let me know if I missed an obvious case,,2183,0.7700412276683463,0.029069767441860465,39727,415.2843154529665,32.89953935610542,107.40806000956528,3773,70,1528,381,travis,tliu30,glouppe,false,,0,0,0,0,1028,false,false,false,false,0,2,0,0,0,0,7
5037067,scikit-learn/scikit-learn,python,3797,1414096931,1414201683,1414201683,1745,1745,commits_in_master,false,false,false,48,2,1,3,4,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,34,0,38,0,4.556719032149199,0.10417644348001873,4,joel.nothman@gmail.com,doc/sphinxext/gen_rst.py,4,0.0025889967637540453,0,1,false,A short cleanup After reviewing differences between this file and the one in the nilearn repository it seems to my understanding this version is the most up to date and all changes seem to be merged in hereDuring the revision I would suggest this minor aesthetic changes,,2182,0.76993583868011,0.02912621359223301,39727,415.2843154529665,32.89953935610542,107.40806000956528,3772,70,1528,192,travis,Titan-C,GaelVaroquaux,false,GaelVaroquaux,0,0,7,2,1296,false,false,false,false,0,0,0,0,0,0,32
5034986,scikit-learn/scikit-learn,python,3795,1414086312,1414099761,1414099761,224,224,commits_in_master,false,false,false,32,1,0,0,2,0,2,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,15,0,0,0.0,0,,,0,0.0,0,3,false,[MRG] adding example for memory wrapper for load_svm_light The doc string recommends caching the call to load_svm_light using joblib so I thought it would be good to have an example for this ,,2181,0.7698303530490601,0.02912621359223301,39727,415.2843154529665,32.89953935610542,107.40806000956528,3772,70,1528,187,travis,dsullivan7,agramfort,false,agramfort,13,0.6153846153846154,8,19,440,true,true,true,false,2,120,10,37,50,1,7
5020125,scikit-learn/scikit-learn,python,3793,1413985947,,1414159159,2886,,unknown,false,false,false,29,1,1,0,5,0,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,19,8,19,8.830602402921041,0.20188665257434732,29,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,25,0.01592356687898089,0,0,false,[MRG] MAINT remove deprecated loss in gradient boosting There is no mention to the version where this will be removed However I still think that it should be removed  ,,2180,0.7701834862385321,0.028662420382165606,39727,415.2843154529665,32.89953935610542,107.40806000956528,3761,71,1527,188,travis,arjoly,arjoly,true,,72,0.8472222222222222,28,26,1037,true,true,false,false,10,291,20,703,88,2,10
5011013,scikit-learn/scikit-learn,python,3790,1413917796,1416947018,1416947018,50487,50487,merged_in_comments,false,false,false,11,7,3,8,5,0,13,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,238,0,382,0,12.712393472112836,0.2906329429943158,25,trev.stephens@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py,25,0.015693659761456372,0,6,false,[MRG] Avoid duplicated methods whenever possible + more robust parameter passing ,,2179,0.7700780174391922,0.029504080351537978,39724,415.214983385359,32.90202396536099,107.4161715839291,3754,72,1526,206,travis,arjoly,arjoly,true,arjoly,71,0.8450704225352113,28,26,1036,true,true,false,false,10,303,19,719,87,2,2
5004653,scikit-learn/scikit-learn,python,3788,1413872131,1415237848,1415237848,22761,22761,commit_sha_in_comments,false,false,false,30,5,0,3,9,0,12,0,5,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,27,85,0,0.0,0,,,0,0.0,0,5,false,Adds commented lines which will add support for sample weight in hinge loss This is dependent on #3607 I will uncomment the added lines after its merge in main tree,,2177,0.7703261368856225,0.03150092649783817,39691,414.19969262553224,32.90418482779471,106.97639263309063,3741,73,1526,202,travis,SaurabhJha,MechCoder,false,MechCoder,2,0.5,11,2,937,true,true,false,false,0,41,2,41,134,0,12
5005704,scikit-learn/scikit-learn,python,3787,1413870664,,1413885059,239,,unknown,false,false,false,2,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,1,0,4.373140679513132,0.09997812635025528,0,,README.md,0,0.0,0,0,false,Create READMEmd ,,2176,0.7706801470588235,0.03150092649783817,39691,414.19969262553224,32.90418482779471,106.97639263309063,3741,73,1526,186,travis,origingod,agramfort,false,,0,0,7,12,682,false,false,false,false,0,0,0,0,0,0,-1
4998860,scikit-learn/scikit-learn,python,3786,1413833834,1413920375,1413920375,1442,1442,commits_in_master,false,false,false,18,7,4,2,6,0,8,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,12,9,24,18,17.30922359194096,0.39572103211931353,3,joel.nothman@gmail.com,sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/svm/libsvm_sparse.c|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py,3,0.0,0,4,false,[MRG] Fix t-SNE with non-squarable metric Fixes #3526 based on #3532 * removed trailing \* added test,,2175,0.7705747126436782,0.03196066379840197,39691,414.19969262553224,32.90418482779471,106.97639263309063,3738,73,1525,187,travis,AlexanderFabisch,larsmans,false,larsmans,8,0.75,31,27,1215,true,true,true,false,1,7,0,0,2,0,2
4985992,scikit-learn/scikit-learn,python,3784,1413717573,,1413723340,96,,unknown,false,false,false,15,2,0,3,22,0,25,0,6,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,68,18,0,0.0,0,,,0,0.0,0,3,false,Adds support for sample weights in median_absolute_error Adds support for sample_weight in median_absolute_error Issue #3450 ,,2174,0.7709291628334867,0.03140096618357488,39691,414.19969262553224,32.90418482779471,106.97639263309063,3726,73,1524,479,travis,SaurabhJha,MechCoder,false,,1,1.0,11,2,935,true,true,false,false,0,38,1,39,134,0,1
4983450,scikit-learn/scikit-learn,python,3783,1413672391,1413713499,1413713499,685,685,commits_in_master,false,false,false,6,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.341137397392128,0.09924651311781466,4,larsmans@gmail.com,sklearn/mixture/gmm.py,4,0.002403846153846154,0,1,false,DOC: fix doc inconsistency in GMM ,,2173,0.7708237459733088,0.03125,39691,414.19969262553224,32.90418482779471,106.97639263309063,3724,73,1523,183,travis,jakevdp,agramfort,false,agramfort,42,0.8809523809523809,1542,0,1256,false,true,false,true,0,8,0,1,0,0,12
4965801,scikit-learn/scikit-learn,python,3780,1413496002,,1417962559,74442,,unknown,false,true,false,96,3,3,5,19,0,24,0,7,2,0,1,3,2,0,0,2,0,1,3,2,0,0,207,0,207,0,13.97785672351874,0.3195685611200606,0,,sklearn/kernel_regression.py|examples/plot_kernel_regression.py|sklearn/kernel_regression.py,0,0.0,0,17,false,[WIP] Kernel regression Implementation of Nadaraya-Watson kernel regression with automatic bandwidth selectionThis simple algorithm is suited as a baseline for more complex algorithms particularly in cases where the size of the training set is large compared to number of dimensions An advantage of this algorithm is that it allows efficient bandwidth selection via leave-one-out cross-validation This PR includes an example comparing kernel regression with SVR on a simple 1D toy dataset:[figure_1](https://cloudgithubusercontentcom/assets/1116263/4668304/37ff234c-5564-11e4-99f9-9e526e1cd2c5png)[figure_2](https://cloudgithubusercontentcom/assets/1116263/4668310/401355a8-5564-11e4-85cd-410788904d08png)Potential future extensions of this could include metric learning for kernel regression (http://citeseerxistpsuedu/viewdoc/summarydoi10111177080)Any opinions on whether this is interesting for sklearn,,2172,0.7711786372007366,0.030388294879009566,39691,414.19969262553224,32.90418482779471,106.97639263309063,3716,75,1521,215,travis,jmetzen,jmetzen,true,,10,0.5,11,2,1102,true,true,false,false,0,19,3,4,104,0,50
4963758,scikit-learn/scikit-learn,python,3779,1413480310,1415105391,1415105391,27084,27084,commits_in_master,false,false,false,7,2,1,25,10,0,35,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,63,19,68,19,18.370041232002944,0.4199846201477898,68,trev.stephens@gmail.com,sklearn/dummy.py|sklearn/ensemble/gradient_boosting.py|sklearn/tests/test_dummy.py|sklearn/utils/stats.py,39,0.015041782729805013,0,9,false,[MRG] Add sample_weight support to Dummy Regressor ,,2171,0.7710732381391064,0.029526462395543174,39691,414.17449799702706,32.90418482779471,106.97639263309063,3714,75,1521,196,travis,arjoly,MechCoder,false,MechCoder,70,0.8428571428571429,28,25,1031,true,true,true,true,24,440,41,901,95,2,0
4962968,scikit-learn/scikit-learn,python,3778,1413475445,1413495536,1413495536,334,334,commits_in_master,false,false,false,16,2,1,4,7,0,11,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,14,0,14,13,4.453238765249989,0.10181205668110416,17,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py,17,0.009407858328721638,0,3,false,[MRG] Expose positive option in elasticnet and lasso path Fixes https://githubcom/scikit-learn/scikit-learn/issues/1052Should be a quick merge,,2170,0.7709677419354839,0.02877697841726619,39690,414.1849332325523,32.90501385739481,106.97908793146888,3713,75,1521,183,travis,MechCoder,agramfort,false,agramfort,36,0.8333333333333334,81,41,849,true,true,true,false,15,428,42,194,107,4,0
4963226,scikit-learn/scikit-learn,python,3777,1413423439,,1425432020,200143,,unknown,false,false,false,58,2,1,0,26,0,26,0,9,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,27,0,4.620155046805969,0.10562907551541188,13,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py,13,0.007103825136612022,0,11,false,DOC FIX: Explicit encoding for opened files in gen_rstpy The encoding (if not specified) used by open in Python is platform dependent On my Windows machine it is cp1251 so I had troubles building docs with the example gallery because of that (some examples contain non-ASCII characters) I think setting it explicitly to utf-8 is a good thing,,2169,0.7713231904103274,0.02841530054644809,39639,412.2455157799137,32.821211433184494,106.6878579177073,3713,75,1520,270,travis,nmayorov,ogrisel,false,,3,0.0,5,0,203,true,false,false,false,0,6,0,5,4,0,518
4956409,scikit-learn/scikit-learn,python,3775,1413413429,1413505639,1413505639,1536,1536,commit_sha_in_comments,false,false,false,70,2,2,0,4,0,4,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.728506242798158,0.22241961799162943,2,gael.varoquaux@normalesup.org,doc/faq.rst|doc/faq.rst,2,0.001095290251916758,0,0,false,[MRG] FAQ: what to do with strings (and graphs and trees and ) Quick write-up to prevent more questions like [this one](http://stackoverflowcom/q/26370056/166749) amounting to I have strings what do I do It looks like non-stats-savvy users can really not find the answer to this question in our docsAlso includes the trick I proposed at #3737 for string metric It doesnt really solve the issue but it presents the workaround,,2167,0.7715736040609137,0.028477546549835708,39639,412.2455157799137,32.821211433184494,106.6878579177073,3711,75,1520,185,travis,larsmans,larsmans,true,larsmans,122,0.7540983606557377,145,38,1550,true,true,false,false,27,157,28,43,216,9,135
4950909,scikit-learn/scikit-learn,python,3773,1413367886,1413369014,1413369014,18,18,commits_in_master,false,false,false,7,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,4,0,0.0,0,,,0,0.0,0,0,false,[MRG] fixing duplicated classifier in asgd test ,,2166,0.7714681440443213,0.028342245989304814,39639,412.2455157799137,32.821211433184494,106.6878579177073,3707,76,1520,181,travis,dsullivan7,agramfort,false,agramfort,12,0.5833333333333334,8,19,432,true,true,true,false,1,88,9,31,50,1,11
4945368,scikit-learn/scikit-learn,python,3772,1413308250,1413888616,1413888616,9672,9672,commits_in_master,false,false,false,41,16,2,34,26,0,60,0,7,0,0,4,6,4,0,0,0,0,6,6,5,0,0,184,86,610,116,35.65589247969577,0.8151895766455525,33,t3kcit@gmail.com,sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py,19,0.005690636316606311,0,17,false,[MRG] ENH: Patches Nearest Centroid for metricmanhattan for sparse and dense data Fixes https://githubcom/scikit-learn/scikit-learn/issues/743I have also added a utility for calculating the median of csc sparse matrices since NumPy does not handle it and it is not a trivial one-liner,,2165,0.7713625866050808,0.027418520434557683,39639,412.34642649915486,32.821211433184494,106.73831327732789,3706,76,1519,189,travis,MechCoder,agramfort,false,agramfort,35,0.8285714285714286,81,41,847,true,true,true,false,14,424,40,197,120,4,1
4947053,scikit-learn/scikit-learn,python,3770,1413306415,1413326932,1413326932,341,341,github,false,false,false,12,1,1,0,1,1,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,31,3,31,8.375200069018788,0.19147959351944627,50,t3kcit@gmail.com,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,42,0.021683014971605574,0,1,false,[MRG] fix to a minor bug with intercept This will fix #3769,,2163,0.7716134997688395,0.027361899845121322,39639,412.34642649915486,32.821211433184494,106.73831327732789,3706,76,1519,181,travis,dsullivan7,agramfort,false,agramfort,10,0.6,8,19,431,true,true,true,false,1,86,7,31,50,0,341
4937511,scikit-learn/scikit-learn,python,3767,1413243380,,1413243494,1,,unknown,false,false,false,1,41,41,0,1,0,1,0,1,10,2,14,26,11,0,1,10,2,14,26,11,0,1,6566,1157,6566,1157,342.08356753915086,7.821016630871623,16,olivier.grisel@ensta.org,sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/fixes.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_network/README.txt|examples/neural_network/plot_mlp_alpha.py|examples/neural_network/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/neural_networks_supervised.rst|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/base.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|examples/neural_network/README.txt|examples/neural_networks/plot_mlp_alpha.py|examples/neural_networks/plot_mlp_nonlinear.py|sklearn/neural_network/base.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|doc/modules/neural_networks_supervised.rst|examples/neural_networks/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|benchmarks/bench_mnist.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py,10,0.0,0,0,false,Mlp ,,2161,0.7723276260990283,0.02717948717948718,39639,412.39688185877543,32.821211433184494,106.76354095713818,3704,76,1518,180,travis,spitz-dan-l,spitz-dan-l,true,,0,0,1,0,614,false,true,false,false,0,0,0,0,0,0,1
5821649,scikit-learn/scikit-learn,python,3765,1413239635,,1413241566,32,,unknown,false,false,false,28,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,3,4.5033603091919066,0.1029597955704229,1,larsmans@gmail.com,sklearn/cluster/tests/test_spectral.py,1,0.000513083632632119,0,0,false,TST fix test failure due to warning registry Partially reverts cd7b43ccf97c2333380d0b3f5701d579ee8837c8 I must admit Ive no clue whats going on the patch looked good and only 26 fails,,2160,0.7726851851851851,0.02719343252950231,39639,412.39688185877543,32.821211433184494,106.76354095713818,3704,76,1518,180,travis,larsmans,larsmans,true,,121,0.7603305785123967,145,38,1548,true,true,false,false,27,152,27,51,230,12,-1
4936631,scikit-learn/scikit-learn,python,3764,1413236656,1416510723,1416510723,54567,54567,commit_sha_in_comments,false,true,false,28,9,1,4,16,0,20,0,6,0,0,2,3,1,0,0,0,0,3,3,2,0,0,4,0,12,3,9.165387153397642,0.20954716541589255,28,t3kcit@gmail.com,doc/modules/model_evaluation.rst|sklearn/metrics/regression.py,23,0.011800923550538737,0,10,false,ENH: median_absolute_error consistent with other regression metrics As discussed in #3761 this PR should make metricsmedian_absolute_error consistent to the other regression metrics like mean_absolute_error in case of multi-output,,2159,0.7725798981009727,0.02719343252950231,39639,412.39688185877543,32.821211433184494,106.76354095713818,3704,76,1518,206,travis,FlorianWilhelm,arjoly,false,arjoly,3,0.6666666666666666,18,20,647,true,true,false,false,0,17,1,24,48,0,52
4934815,scikit-learn/scikit-learn,python,3763,1413222907,1413241612,1413241612,311,311,commits_in_master,false,true,false,38,1,1,0,9,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,16,0,16,4.540077204433747,0.10379924961551994,1,larsmans@gmail.com,sklearn/cluster/tests/test_spectral.py,1,0.0005128205128205128,0,3,false,FIX warning check in test_affinities Tentative PR to see if this fixes the travis failures on test_affinities such as this one:https://travis-ciorg/scikit-learn/scikit-learn/jobs/37830488I cannot reproduce it on my local Python 27 so its a shot in the dark,,2158,0.7724745134383688,0.027692307692307693,39639,412.39688185877543,32.821211433184494,106.76354095713818,3704,76,1518,179,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,97,0.8556701030927835,1042,123,1965,true,true,true,true,36,473,63,361,150,9,14
4927235,scikit-learn/scikit-learn,python,3761,1413125304,1413141988,1413141988,278,278,commits_in_master,false,true,false,34,6,4,1,1,0,2,0,4,0,0,9,9,7,0,0,0,0,9,9,7,0,0,56,22,56,22,53.516556248313755,1.2235504085931217,52,vlad@vene.ro,sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/regression.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py,22,0.0036344755970924196,2,4,false,Median absolute error The median absolute error is a robust regression metric as suggested by @GaelVaroquaux in PR #2949 for the Theil-Sen regressor @arjoly suggested to make this an own PR for clearer distinction ,,2157,0.7723690310616597,0.028037383177570093,39636,417.1460288626501,33.00030275507115,109.64779493389848,3701,76,1517,182,travis,FlorianWilhelm,larsmans,false,larsmans,2,0.5,18,20,646,true,true,false,false,0,11,0,17,41,0,14
4927016,scikit-learn/scikit-learn,python,3760,1413117808,1424831375,1424831375,195226,195226,commit_sha_in_comments,false,false,false,182,13,5,9,36,0,45,0,7,0,0,4,4,4,0,0,0,0,4,4,4,0,0,94,96,219,138,51.24596272629736,1.1716377702177854,7,t3kcit@gmail.com,sklearn/neighbors/regression.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py,6,0.003116883116883117,0,8,false,Fix KNeighborsRegressor and RadiusNeighborsRegressor returning NaN predi A  KNeighborsRegressor with distance weighting will predict NaN if the data to predict on happens to match a data point from the training set  (This actually happened to me with some data from a Kaggle competition)For example    from sklearn import neighbors    import numpy as np        X  nparray([[10][20][30]])    y  nparray([10 20 30])        clf  neighborsKNeighborsRegressor(n_neighbors3 weightsdistance)        clffit(Xy)    print(clfpredict([[10]]))The output is NaN  This seems like undesirable behavior  I would have expected pred to be 10  In addition RadiusNeighborsRegressor suffers from this problem  Note that the infinities are already handled properly by KNeighborsClassifier and RadiusNeighborsClassifiers  For example    from sklearn import neighbors    import numpy as np        X  nparray([[10][20][30]])    y  nparray([1 0 1])        clf  neighborsKNeighborsClassifier(n_neighbors3 weightsdistance)        clffit(Xy)    print(clfpredict([[10]]))This outputs 1 as expectedThis pull request is my proposed solution  Note that if there are two points with distance0 it chooses one of them which is consistent with the warning given in the [KNeighborsRegressor docs](http://scikit-learnorg/dev/modules/generated/sklearnneighborsKNeighborsRegressorhtml)This is my first pull request for any project so let me know if Im doing something wrong,,2156,0.7722634508348795,0.028051948051948054,39636,417.1460288626501,33.00030275507115,109.64779493389848,3700,77,1517,276,travis,Garrett-R,amueller,false,amueller,0,0,6,2,247,true,true,false,false,0,0,0,0,2,0,55
4924089,scikit-learn/scikit-learn,python,3759,1413057398,1413167800,1413167800,1840,1840,commit_sha_in_comments,false,false,false,39,5,3,2,5,0,7,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,10,43,13,52,8.893498732945815,0.20333229137592593,14,t3kcit@gmail.com,sklearn/metrics/tests/test_classification.py|sklearn/metrics/classification.py,14,0.007276507276507277,0,0,false,Classification report digits test Added digits as optional argument for classification report (per issue #3749) and added unit test to test_classificationpy Also cleaned up so that merge is on top of master Changes pass clean through nosetests --with-coverage etc,,2155,0.7721577726218097,0.028066528066528068,39636,417.1460288626501,33.00030275507115,109.64779493389848,3700,77,1516,181,travis,agileminor,jnothman,false,jnothman,1,0.0,0,0,811,true,false,false,false,1,1,1,0,2,0,132
4930675,scikit-learn/scikit-learn,python,3757,1413005016,,1413175190,2836,,unknown,false,false,false,84,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.528133650747427,0.10352683668263594,14,t3kcit@gmail.com,sklearn/metrics/classification.py,14,0.007261410788381743,0,0,false,code for issue #3749 - change the format of displayed results in classification_report Made a simple enhancement for issue #3749 adding digits to the optional arguments to the classification_report function with default value of 2 (matches previous hard coded value) Allows user to print out the floating values with whatever precision they want in the formatNo unit test added at this time I will work on that once Ive figured out the testing setup using Nose Any suggestions for test would be welcome,,2153,0.772875058058523,0.028008298755186723,39636,417.1460288626501,33.00030275507115,109.64779493389848,3698,77,1516,182,travis,agileminor,agileminor,true,,0,0,0,0,811,true,false,false,false,1,1,0,0,2,0,-1
4920986,scikit-learn/scikit-learn,python,3756,1412997962,1413456779,1413456779,7646,7646,commit_sha_in_comments,false,false,false,8,11,3,21,17,0,38,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,99,282,190,310,40.603406627448074,0.9283167350823722,61,t3kcit@gmail.com,doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py,38,0.009865005192107996,0,5,true,[MRG] Add quantile strategy to DummyRegressor Fixes #3421 ,,2152,0.7727695167286245,0.028037383177570093,39636,417.1460288626501,33.00030275507115,109.64779493389848,3697,76,1515,188,travis,staple,arjoly,false,arjoly,1,0.0,3,0,1577,true,false,false,false,0,7,1,0,7,0,364
4920454,scikit-learn/scikit-learn,python,3755,1412991395,1419782149,1419782149,113179,113179,merged_in_comments,false,false,false,6,7,2,10,7,0,17,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,20,0,53,32,8.479072132212846,0.19385724529288018,5,t3kcit@gmail.com,sklearn/qda.py|sklearn/qda.py,5,0.0026001040041601664,0,3,true,Bug fix in QDA Fixes #3721,,2151,0.7726638772663877,0.028081123244929798,39636,417.1460288626501,33.00030275507115,109.64779493389848,3697,76,1515,223,travis,x0l,larsmans,false,larsmans,2,1.0,4,25,599,true,false,false,false,0,2,2,0,3,0,105
4917188,scikit-learn/scikit-learn,python,3752,1412965109,1413299325,1413299325,5570,5570,commits_in_master,false,false,false,66,5,1,10,20,0,30,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,7,0,30,4.311192123649597,0.09856717208953689,0,,sklearn/utils/tests/test_testing.py,0,0.0,0,12,true,[MRG] FIX: Remove test that tests state of warnings before and after assert_warns Fixes https://githubcom/scikit-learn/scikit-learn/issues/3626The warnings registry is cleared before and after every call to assert_warns (https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/utils/testingpy#L146) This is because there might be cases when the warning is logged and does not raise a warning in the subsequent callSo I suggest we remove this test and there is no use testing this any more,,2150,0.7725581395348837,0.027979274611398965,39634,416.9400010092345,32.976737144875614,109.62809708835849,3696,75,1515,184,travis,MechCoder,MechCoder,true,MechCoder,34,0.8235294117647058,81,41,843,true,true,false,false,11,414,38,190,129,4,0
4923012,scikit-learn/scikit-learn,python,3751,1412961459,,1413047633,1436,,unknown,false,false,false,37,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.3148744549525055,0.0986513615602945,8,t3kcit@gmail.com,sklearn/semi_supervised/label_propagation.py,8,0.00411522633744856,0,0,true,[FIX] Fixing Issue #3550 - hard clamping Fixing the issue #3550 Clamping seems not to have been implemented correctly For alpha  1 no clamping should actually take place Currently it seems to be completely the opposite,,2149,0.7729176361098186,0.027777777777777776,39634,416.9400010092345,32.976737144875614,109.62809708835849,3696,75,1515,181,travis,kpysniak,kpysniak,true,,4,0.25,4,0,1225,true,false,false,false,0,0,0,0,1,0,-1
4915937,scikit-learn/scikit-learn,python,3750,1412953409,1413166073,1413166073,3544,3544,commits_in_master,false,false,false,62,1,1,0,14,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,17,17,17,8.843399932356272,0.20218744560408425,8,t3kcit@gmail.com,sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/tests/test_nearest_centroid.py,8,0.004092071611253197,0,7,true,[MRG] : Bug fix in computing the dataset_centroid in NearestCentroid Continuation of work in https://githubcom/scikit-learn/scikit-learn/pull/3746 There is a bug in L111 of the current code where the mean of the first feature is taken as the centroidA bit more of minor numpy vectorization and code optimizations were added I also added a test to verify the behavior that fails in master ,,2148,0.7728119180633147,0.02762148337595908,39634,416.9400010092345,32.976737144875614,109.62809708835849,3696,74,1515,183,travis,MechCoder,MechCoder,true,MechCoder,33,0.8181818181818182,81,41,843,true,true,false,false,11,413,38,190,142,4,1
4911025,scikit-learn/scikit-learn,python,3747,1412890175,1427141578,1427141578,237523,237523,merged_in_comments,false,true,false,13,32,6,18,42,0,60,0,6,0,0,3,16,3,0,0,0,0,16,16,13,0,0,96,55,323,211,39.23285197095663,0.8969874749910945,13,tejeshpapineni95@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/gaussian_process/gaussian_process.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,9,0.004547751389590703,0,4,false,Fix preprocessingscale for arrays with near zero ratio variance/max Fix #3722 prolongating #3725,,2147,0.7727061015370285,0.02829711975745326,39632,416.9610415825595,32.97840129188535,109.6336293903916,3692,75,1514,276,travis,ngoix,amueller,false,amueller,0,0,4,1,360,true,false,false,false,1,2,0,0,2,0,16
4908682,scikit-learn/scikit-learn,python,3746,1412878994,1412945208,1412945208,1103,1103,commits_in_master,false,false,false,48,2,2,0,2,0,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,19,28,19,17.740867490807478,0.40558254083646206,5,t3kcit@gmail.com,sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/tests/test_nearest_centroid.py|sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/tests/test_nearest_centroid.py,5,0.0025303643724696357,0,2,false,[MRG] FIX: Thresholded Nearest Centroid fails with non-encoded y Minimal code:    X  [[-2 -1] [-1 -1] [-1 -2] [1 1] [1 2] [2 1]]    y  [r r r b b b]    clf  NearestCentroid(shrink_threshold001)    IndexError: arrays used as indices must be of integer (or boolean) type,,2146,0.7726001863932899,0.02834008097165992,39556,414.8801698857316,32.94064111639195,109.54090403478612,3692,73,1514,177,travis,MechCoder,larsmans,false,larsmans,32,0.8125,81,41,842,true,true,false,false,11,413,38,190,146,4,0
4900541,scikit-learn/scikit-learn,python,3744,1412796493,1412805951,1412805951,157,157,commits_in_master,false,false,false,36,3,2,4,5,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,22,21,22,18.130862582149753,0.4144980876589724,7,t3kcit@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,5,0.002531645569620253,0,3,false,FIX: Raise warnings in f_classif a given feature is constant across each class Fixes https://githubcom/scikit-learn/scikit-learn/issues/2359This is because for each class the variation within the samples is zero which technically gives a F score of inf,,2145,0.7724941724941725,0.028354430379746835,39557,414.81912177364313,32.91452840205274,109.51285486765933,3690,73,1513,176,travis,MechCoder,agramfort,false,agramfort,31,0.8064516129032258,81,41,841,true,true,true,false,11,409,37,189,157,4,0
4898928,scikit-learn/scikit-learn,python,3742,1412782801,1413040421,1413040421,4293,4293,commits_in_master,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.737713997952296,0.10831108465626732,0,,examples/feature_stacker.py,0,0.0,0,0,false,fix svmfit in feature_stackerpy ,,2144,0.7723880597014925,0.02832574607991907,39557,414.81912177364313,32.91452840205274,109.51285486765933,3690,73,1513,182,travis,abhishekkrthakur,jnothman,false,jnothman,3,0.6666666666666666,105,47,1064,false,true,false,false,0,0,1,0,0,0,-1
4887649,scikit-learn/scikit-learn,python,3740,1412694602,1412705671,1412705671,184,184,commits_in_master,false,false,false,9,3,3,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,13.080496339141,0.2990395507790878,17,t3kcit@gmail.com,sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py,17,0.008555611474584801,0,3,false,[MRG] DOC: Document criterion_ attribute in LassoLarsIC Fixes https://githubcom/scikit-learn/scikit-learn/issues/3635,,2143,0.7722818478768082,0.028183190739808756,39554,414.4966375082166,32.89174293371087,109.44531526520706,3685,73,1512,178,travis,MechCoder,agramfort,false,agramfort,30,0.8,81,41,840,true,true,true,false,11,407,36,172,165,4,9
4862867,scikit-learn/scikit-learn,python,3735,1412412119,1431280240,1431280240,314468,314468,commits_in_master,false,true,false,376,25,2,26,49,0,75,0,7,0,0,3,3,2,0,0,0,0,3,3,2,0,0,527,227,1733,616,21.54784450287456,0.4926159070733716,53,vlad@vene.ro,sklearn/tree/export.py|sklearn/tree/tests/test_export.py|doc/whats_new.rst|sklearn/tree/export.py|sklearn/tree/tests/test_export.py,52,0.0005089058524173028,0,43,false,[WIP] Pretty decision trees - ref #3643 As mentioned in #3643 this enhancement is intended to make the output of treeexport_graphviz more aesthetically pleasing as inspired by [fancyRpartPlot](http://blogrevolutionanalyticscom/2013/06/plotting-classification-and-regression-trees-with-plotrparthtml) in R Colour brewing is done manually so the only additional import required is numpy and output is still pure dot format Enhancements include:- Drawing leaf nodes at the base of the tree for clarity- Colours indicating how the node classification is leaning including multi-class- Percentage samples shown in node instead of raw number of samples- Percentage per class shown for all nodes not just leavesStandard output omitting the new pretty and simple flags should result in the exact same graphical output from Graphviz as the incumbent version though the dot code has been altered slightly for simplicity given the new more complicated node representation for prettyA few examples using the Titanic survivor dataset:no flags:[full_new](https://cloudgithubusercontentcom/assets/5210848/4514716/0a8c122a-4b88-11e4-92dd-78f78bbfcee9png)pretty:[full_pretty](https://cloudgithubusercontentcom/assets/5210848/4514717/154df71e-4b88-11e4-9d09-b763ebe63763png)pretty & simple:[full_pretty_simple](https://cloudgithubusercontentcom/assets/5210848/4514718/20512c9e-4b88-11e4-88f7-6eb78fabec4dpng)pretty & max_depth:[cropped_pretty](https://cloudgithubusercontentcom/assets/5210848/4514721/3957a330-4b88-11e4-9544-78f68cab3da5png)It has been moderately tested on pure treeDecisionTreeClassifier objects though I am well aware that regression needs work and will fail if attempted with the new flags (though the standard output still appears to be functional)To be discussed:- How to represent colours for regression Im thinking a diverging colour scheme with white for the median or mean of the two most extreme values in the leaves should work well- Should the text and graphical options be broken out further Something like show_major_class use_colours use_percentages etc or some perhaps discarded I based the options off the R package thats not meant to imply that this is the state of the art just the best Ive stumbled across- Should the base implementation even be maintained The extra sub-tree recursion per node is a little expensive but its just a single tree And seeing what each node is voting towards gives more intuitive information than the error coefficients alone It even appears that this used to be standard output from the examples [here](http://scikit-learnorg/stable/modules/treehtml#tree)Other options that might be useful:- Left to right orientation for use in reports- Anything else you can think of Though Graphviz is a little limited in what you can pull off- Welcome any other suggestions / gotchas,,2141,0.7725361980382999,0.030534351145038167,39546,414.55520153745005,32.89839680372224,109.46745562130178,3668,73,1509,308,travis,trevorstephens,glouppe,false,glouppe,1,1.0,97,51,418,true,false,true,false,1,0,1,0,1,0,10
4850212,scikit-learn/scikit-learn,python,3733,1412294535,,1412444241,2495,,unknown,false,false,false,90,15,10,0,7,0,7,0,4,182,0,6,194,33,6,56,182,0,12,194,34,6,56,2650,0,2651,0,481.8155244798441,11.015057098103322,4,olivier.grisel@ensta.org,.gitignore|doc_sc/Makefile|doc_sc/README|doc_sc/about.rst|doc_sc/conf.py|doc_sc/data_transforms.rst|doc_sc/datasets/covtype.rst|doc_sc/datasets/index.rst|doc_sc/datasets/labeled_faces.rst|doc_sc/datasets/labeled_faces_fixture.py|doc_sc/datasets/mldata.rst|doc_sc/datasets/mldata_fixture.py|doc_sc/datasets/olivetti_faces.rst|doc_sc/datasets/twenty_newsgroups.rst|doc_sc/datasets/twenty_newsgroups_fixture.py|doc_sc/developers/debugging.rst|doc_sc/developers/index.rst|doc_sc/developers/maintainer.rst|doc_sc/developers/performance.rst|doc_sc/developers/utilities.rst|doc_sc/documentation.rst|doc_sc/faq.rst|doc_sc/images/inria-logo.jpg|doc_sc/images/iris.svg|doc_sc/images/last_digit.png|doc_sc/images/ml_map.png|doc_sc/images/no_image.png|doc_sc/images/plot_digits_classification.png|doc_sc/images/plot_face_recognition_1.png|doc_sc/images/plot_face_recognition_2.png|doc_sc/images/rbm_graph.png|doc_sc/images/scikit-learn-logo-notext.png|doc_sc/includes/big_toc_css.rst|doc_sc/includes/bigger_toc_css.rst|doc_sc/index.rst|doc_sc/install.rst|doc_sc/logos/favicon.ico|doc_sc/logos/scikit-learn-logo-notext.png|doc_sc/logos/scikit-learn-logo-small.png|doc_sc/logos/scikit-learn-logo-thumb.png|doc_sc/logos/scikit-learn-logo.bmp|doc_sc/logos/scikit-learn-logo.png|doc_sc/logos/scikit-learn-logo.svg|doc_sc/make.bat|doc_sc/model_selection.rst|doc_sc/modules/biclustering.rst|doc_sc/modules/classes.rst|doc_sc/modules/clustering.rst|doc_sc/modules/computational_performance.rst|doc_sc/modules/covariance.rst|doc_sc/modules/cross_decomposition.rst|doc_sc/modules/cross_validation.rst|doc_sc/modules/decomposition.rst|doc_sc/modules/density.rst|doc_sc/modules/dp-derivation.rst|doc_sc/modules/ensemble.rst|doc_sc/modules/feature_extraction.rst|doc_sc/modules/feature_selection.rst|doc_sc/modules/gaussian_process.rst|doc_sc/modules/glm_data/lasso_enet_coordinate_descent.png|doc_sc/modules/grid_search.rst|doc_sc/modules/hmm.rst|doc_sc/modules/isotonic.rst|doc_sc/modules/kernel_approximation.rst|doc_sc/modules/label_propagation.rst|doc_sc/modules/lda_qda.rst|doc_sc/modules/learning_curve.rst|doc_sc/modules/linear_model.rst|doc_sc/modules/manifold.rst|doc_sc/modules/metrics.rst|doc_sc/modules/mixture.rst|doc_sc/modules/model_evaluation.rst|doc_sc/modules/model_persistence.rst|doc_sc/modules/multiclass.rst|doc_sc/modules/naive_bayes.rst|doc_sc/modules/neighbors.rst|doc_sc/modules/neural_networks.rst|doc_sc/modules/outlier_detection.rst|doc_sc/modules/pipeline.rst|doc_sc/modules/preprocessing.rst|doc_sc/modules/random_projection.rst|doc_sc/modules/scaling_strategies.rst|doc_sc/modules/sgd.rst|doc_sc/modules/svm.rst|doc_sc/modules/tree.rst|doc_sc/presentations.rst|doc_sc/related_projects.rst|doc_sc/sphinxext/LICENSE.txt|doc_sc/sphinxext/MANIFEST.in|doc_sc/sphinxext/README.txt|doc_sc/sphinxext/gen_rst.py|doc_sc/sphinxext/github_link.py|doc_sc/sphinxext/numpy_ext/__init__.py|doc_sc/sphinxext/numpy_ext/docscrape.py|doc_sc/sphinxext/numpy_ext/docscrape_sphinx.py|doc_sc/sphinxext/numpy_ext/numpydoc.py|doc_sc/supervised_learning.rst|doc_sc/support.rst|doc_sc/templates/class.rst|doc_sc/templates/class_with_call.rst|doc_sc/templates/function.rst|doc_sc/testimonials/README.txt|doc_sc/testimonials/images/Makefile|doc_sc/testimonials/images/aweber.png|doc_sc/testimonials/images/bestofmedia-logo.gif|doc_sc/testimonials/images/birchbox.jpg|doc_sc/testimonials/images/change-logo.png|doc_sc/testimonials/images/datapublica.png|doc_sc/testimonials/images/datarobot.png|doc_sc/testimonials/images/evernote.png|doc_sc/testimonials/images/howaboutwe.png|doc_sc/testimonials/images/inria.png|doc_sc/testimonials/images/lovely.png|doc_sc/testimonials/images/machinalis.png|doc_sc/testimonials/images/okcupid.png|doc_sc/testimonials/images/peerindex.png|doc_sc/testimonials/images/phimeca.png|doc_sc/testimonials/images/rangespan.png|doc_sc/testimonials/images/spotify.png|doc_sc/testimonials/images/telecomparistech.jpg|doc_sc/testimonials/images/yhat.png|doc_sc/testimonials/testimonials.rst|doc_sc/themes/scikit-learn/layout.html|doc_sc/themes/scikit-learn/static/ML_MAPS_README.rst|doc_sc/themes/scikit-learn/static/css/bootstrap-responsive.css|doc_sc/themes/scikit-learn/static/css/bootstrap-responsive.min.css|doc_sc/themes/scikit-learn/static/css/bootstrap.css|doc_sc/themes/scikit-learn/static/css/bootstrap.min.css|doc_sc/themes/scikit-learn/static/css/examples.css|doc_sc/themes/scikit-learn/static/img/FNRS-logo.png|doc_sc/themes/scikit-learn/static/img/forkme.png|doc_sc/themes/scikit-learn/static/img/glyphicons-halflings-white.png|doc_sc/themes/scikit-learn/static/img/glyphicons-halflings.png|doc_sc/themes/scikit-learn/static/img/google.png|doc_sc/themes/scikit-learn/static/img/inria-small.jpg|doc_sc/themes/scikit-learn/static/img/inria-small.png|doc_sc/themes/scikit-learn/static/img/plot_classifier_comparison_1.png|doc_sc/themes/scikit-learn/static/img/plot_manifold_sphere_1.png|doc_sc/themes/scikit-learn/static/img/scikit-learn-logo-notext.png|doc_sc/themes/scikit-learn/static/img/scikit-learn-logo-small.png|doc_sc/themes/scikit-learn/static/img/scikit-learn-logo.png|doc_sc/themes/scikit-learn/static/img/scikit-learn-logo.svg|doc_sc/themes/scikit-learn/static/img/telecom.png|doc_sc/themes/scikit-learn/static/jquery.js|doc_sc/themes/scikit-learn/static/jquery.maphilight.js|doc_sc/themes/scikit-learn/static/jquery.maphilight.min.js|doc_sc/themes/scikit-learn/static/js/bootstrap.js|doc_sc/themes/scikit-learn/static/js/bootstrap.min.js|doc_sc/themes/scikit-learn/static/js/copybutton.js|doc_sc/themes/scikit-learn/static/js/examples.js|doc_sc/themes/scikit-learn/static/nature.css_t|doc_sc/themes/scikit-learn/static/sidebar.js|doc_sc/themes/scikit-learn/theme.conf|doc_sc/tune_toc.rst|doc_sc/tutorial/basic/tutorial.rst|doc_sc/tutorial/common_includes/info.txt|doc_sc/tutorial/index.rst|doc_sc/tutorial/machine_learning_map/ML_MAPS_README.txt|doc_sc/tutorial/machine_learning_map/index.rst|doc_sc/tutorial/machine_learning_map/parse_path.py|doc_sc/tutorial/machine_learning_map/pyparsing.py|doc_sc/tutorial/machine_learning_map/svg2imagemap.py|doc_sc/tutorial/statistical_inference/finding_help.rst|doc_sc/tutorial/statistical_inference/index.rst|doc_sc/tutorial/statistical_inference/model_selection.rst|doc_sc/tutorial/statistical_inference/putting_together.rst|doc_sc/tutorial/statistical_inference/settings.rst|doc_sc/tutorial/statistical_inference/supervised_learning.rst|doc_sc/tutorial/statistical_inference/unsupervised_learning.rst|doc_sc/tutorial/text_analytics/.gitignore|doc_sc/tutorial/text_analytics/data/languages/fetch_data.py|doc_sc/tutorial/text_analytics/data/movie_reviews/fetch_data.py|doc_sc/tutorial/text_analytics/data/twenty_newsgroups/fetch_data.py|doc_sc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc_sc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py|doc_sc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc_sc/tutorial/text_analytics/solutions/exercise_02_sentiment.py|doc_sc/tutorial/text_analytics/solutions/generate_skeletons.py|doc_sc/tutorial/text_analytics/working_with_text_data.rst|doc_sc/tutorial/text_analytics/working_with_text_data_fixture.py|doc_sc/unsupervised_learning.rst|doc_sc/user_guide.rst|doc_sc/whats_new.rst|doc_sc/modules/neighbors.rst|doc_sc/modules/linear_model.rst|doc/modules/svm.rst|doc_sc/modules/linear_model.rst|doc_sc/modules/svm.rst|doc_sc/modules/linear_model.rst|doc_sc/modules/sgd.rst|doc_sc/modules/svm.rst,3,0.0,0,2,false,Initiate a Chinese translation project of scikit-learn Hi I am starting a project to translate the scikit-learn documentation into Chinese  I am not sure whether it is necessary to add it into the current package  However it would be nice to help other people who want the translation or draw people to help the translation through your platform :)Also I edited two files for minor corrections as listed below:doc/modules/svmrst  - typo change lower indices from 1 to 2 examples/cross_decomposition/plot_compare_cross_decompositionpy - add yshape(n1) to fix the scriptJiayi Liu,,2140,0.7728971962616823,0.03048780487804878,39542,414.5971372211825,32.90172474836883,109.47852915886905,3664,72,1507,181,travis,jiayiliu,mblondel,false,,0,0,5,6,939,true,false,false,false,0,0,0,0,1,0,1383
4849111,scikit-learn/scikit-learn,python,3732,1412288095,1412332444,1412332444,739,739,commits_in_master,false,false,false,7,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.3304170250350325,0.09900011179767422,9,t3kcit@gmail.com,sklearn/ensemble/weight_boosting.py,9,0.004573170731707317,0,0,false,Fix a typo Change BaseAdaBoost to BaseWeightBoosting,,2139,0.7727910238429172,0.03048780487804878,39542,414.5971372211825,32.90172474836883,109.47852915886905,3662,72,1507,180,travis,queqichao,arjoly,false,arjoly,1,1.0,0,0,97,true,true,false,false,0,5,1,0,2,0,739
4841889,scikit-learn/scikit-learn,python,3731,1412231666,1412328062,1412328062,1606,1606,commits_in_master,false,false,false,7,2,2,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,9.854224749323937,0.22528129081266193,0,,examples/ensemble/plot_adaboost_twoclass.py|examples/ensemble/plot_adaboost_twoclass.py,0,0.0,0,1,false,plot_adaboost_twoclasspy: minor improvements Before:[figure_1a](https://cloudgithubusercontentcom/assets/202816/4486532/c0900c02-49e4-11e4-8b70-35193f04ae63png)After:[figure_1](https://cloudgithubusercontentcom/assets/202816/4486533/c546513e-49e4-11e4-849c-ddeca326bc61png),,2138,0.7726847521047708,0.032093734080489045,39542,416.94906681503215,33.20519953467199,110.13605786252592,3661,74,1507,181,travis,ndawe,glouppe,false,glouppe,15,0.7333333333333333,38,63,1693,true,true,true,false,0,4,0,0,1,0,169
4835508,scikit-learn/scikit-learn,python,3730,1412184609,1412249756,1412249756,1085,1085,commits_in_master,false,false,false,20,1,1,0,5,0,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,13,4,13,8.938333618434749,0.20434219190981975,6,olivier.grisel@ensta.org,sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/tests/test_svm.py,6,0.0030303030303030303,0,3,false,[MRG] FIX call srand whenever random_seed  0 in libsvm This bug was discovered by #3691 (failing test under windows),,2137,0.7725783809078147,0.03333333333333333,39508,416.70041510580137,33.157841449832944,110.12959400627722,3655,75,1506,180,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,96,0.8541666666666666,1030,123,1953,true,true,true,true,35,537,70,319,170,6,13
4833761,scikit-learn/scikit-learn,python,3729,1412171579,,1450224969,634223,,unknown,false,true,false,14,36,6,13,86,0,99,0,7,3,0,7,17,7,0,0,4,0,14,18,12,0,0,4960,3,14714,470,114.50024529747263,2.617640040516622,6,olivier.grisel@ensta.org,sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pxd|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pxd|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pxd|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pxd|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/learning_rates.c|sklearn/linear_model/learning_rates.pxd|sklearn/linear_model/learning_rates.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx,4,0.0,0,20,false,[WIP] extracting learning rates to learning_ratespxd and learning_ratespyx This pull request is for #3647,,2136,0.7729400749063671,0.033855482566953005,39508,416.70041510580137,33.157841449832944,110.12959400627722,3655,75,1506,465,travis,dsullivan7,dsullivan7,true,,9,0.6666666666666666,8,19,418,true,true,false,false,1,65,5,29,39,0,1
4827821,scikit-learn/scikit-learn,python,3725,1412115304,1425388234,1425388234,221215,221215,commit_sha_in_comments,false,true,false,35,10,3,0,14,0,14,0,7,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,4,128,4,13.645179761829304,0.31194840510441996,10,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,9,0.004591836734693878,0,4,false,Fix nearly zero division fixes #3722- separate reset zero to 10 function from _mean_and_std()- add  _replace_nearly_value() function    - with isclose() nealy zero values also replaced to 10- add floating point error testcase,,2134,0.7731958762886598,0.0336734693877551,39508,416.70041510580137,33.157841449832944,110.12959400627722,3647,74,1505,275,travis,Cocu,ogrisel,false,ogrisel,0,0,15,18,868,true,true,false,false,1,1,0,0,1,0,12
4825924,scikit-learn/scikit-learn,python,3724,1412093389,1412104004,1412104004,176,176,github,false,false,false,6,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.324009654165851,0.09885307034303863,38,t3kcit@gmail.com,sklearn/cluster/k_means_.py,38,0.019437340153452685,0,0,false,Rename k to n_clusters in docs ,,2133,0.773089545241444,0.03324808184143223,39508,416.70041510580137,33.157841449832944,110.12959400627722,3646,73,1505,176,travis,mlopezantequera,agramfort,false,agramfort,0,0,4,1,1119,true,false,false,false,0,0,0,0,1,0,14
4861575,scikit-learn/scikit-learn,python,3719,1412007148,,1412781783,12910,,unknown,false,false,false,266,8,2,1,11,0,12,0,5,0,0,4,5,4,0,0,0,0,5,5,5,0,0,248,4,1314,60,26.316314118478722,0.601628733233096,22,t3kcit@gmail.com,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py,17,0.003686150605581885,0,8,false,[WIP] ENH: Faster stopping criteria based on glmnet for coordinate descent The current cd algorithm breaks when the dual gap if the biggest coordinate update is less than tolerance Glmnet however checks if the max change in the objective is less than tol and then breaks This surprisingly leads to huge changes in speedup with almost no noticeable regression in predictionIt states that Each inner coordinate-descent loop continues until the maximum change in the objective after any coefficient update is less than thresh times the null devianceIt should be noted that the default tolerance in this case is 1e-7Some initial benchmarks using LassoCV and precomputeFalseFor the newsgroup dataset (using two categories) 5 random splits (1174 X 130107) test_size  2/3 of total size    In this branch (using tol1e-7)    mean_time  166982872009    mean_accuracy_score  089587628866    In master (using tol1e-4)    mean_time  236406961584    mean_accuracy_score  0889175257732For the haxby dataset with the mask using 5 random splits (216 X 577)    In this branch (using tol1e-7)    mean_time  0495861053467    mean_accuracy_score  0958333333333    In master (using tol1e-4)    mean_time  305996584892    mean_accuracy_score  0930555555556For the arcene dataset (100 * 10000)    In this branch (using tol1e-7)    mean_time  469407510757    accuracy_score on test data  068    In master (using tol1e-7)    mean_time  573407440186068    accuracy_score on test data  068For the duke dataset 5 random splits    In master    mean_accuracy_score  084000000000000008    mean_time  3316893196105957    In this branch    mean_accuracy_score  082666666666666655    mean_time  12530781269073485Since the default tolerances are different how do we accomodate this change in terms of API Do we need a new stopping criteria called glmnet,,2130,0.7741784037558685,0.03580832016850974,39508,416.70041510580137,33.157841449832944,110.12959400627722,3642,71,1504,183,travis,MechCoder,MechCoder,true,,29,0.8275862068965517,80,41,832,true,true,false,false,10,400,36,176,181,4,2
4813102,scikit-learn/scikit-learn,python,3718,1412003562,1412070088,1412070088,1108,1108,commit_sha_in_comments,false,false,false,16,4,1,11,5,0,16,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,6,9,15,8.710969622970808,0.19950762377586614,11,t3kcit@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,8,0.00421274354923644,1,3,false,Raise exception for sparse inputs in the case of svd solver Fix for #3709 Ping @agramfort ,,2129,0.7740723344293096,0.03580832016850974,39505,416.6054929755727,33.13504619668396,110.11264396911784,3642,71,1504,178,travis,akshayah3,larsmans,false,larsmans,3,0.6666666666666666,7,5,525,true,false,false,false,0,35,5,15,22,1,11
4808018,scikit-learn/scikit-learn,python,3715,1411948364,1411984156,1411984156,596,596,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.373145013527842,0.10015949222570632,22,t3kcit@gmail.com,sklearn/ensemble/gradient_boosting.py,22,0.011615628299894404,0,0,false,GradientBoostingClassifier docstring correction Incorrectly specified default for max_features as auto when it is None,,2128,0.7739661654135338,0.03590285110876452,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,71,1503,176,travis,trevorstephens,glouppe,false,glouppe,0,0,93,51,412,false,false,true,false,0,0,0,0,0,0,10
4800288,scikit-learn/scikit-learn,python,3713,1411837783,1412001869,1412001869,2734,2734,commits_in_master,false,false,false,7,4,1,17,7,0,24,0,4,0,0,3,5,3,0,0,0,0,5,5,5,0,0,24,6,107,40,13.719207820858374,0.31421525808667655,17,t3kcit@gmail.com,sklearn/ensemble/bagging.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,12,0.004219409282700422,1,5,false,Added has_fit_parameter method Fixes #3711 Ping @mblondel ,,2127,0.7738598965679361,0.037447257383966245,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,72,1502,175,travis,akshayah3,mblondel,false,mblondel,2,0.5,7,5,523,true,false,false,false,0,29,4,13,18,1,923
4785972,scikit-learn/scikit-learn,python,3710,1411792944,1412034753,1412034753,4030,4030,commits_in_master,false,false,false,42,1,1,1,1,0,2,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,20,12,20,8.967270337541418,0.20538016481969754,11,vlad@vene.ro,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,9,0.004694835680751174,0,0,true,Fixes ovr in the binary classifier case Fixes https://githubcom/scikit-learn/scikit-learn/issues/3700 Also allows feature arrays (rather than just numpy arrays) in the multiclass case (already supported in the binary case) and adds a test for the number of probabilities returned from predict_proba in test_ovr_binary,,2126,0.7737535277516463,0.03808033385498174,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,72,1501,177,travis,wlamond,larsmans,false,larsmans,1,1.0,3,1,632,true,false,false,false,0,4,1,0,4,0,10
4742371,scikit-learn/scikit-learn,python,3706,1411722237,1412524011,1412524011,13362,13362,commit_sha_in_comments,false,false,false,44,15,5,40,18,0,58,0,5,2,0,3,7,0,0,0,2,2,3,7,0,0,0,0,0,0,0,38.27196222956681,0.8765545780916321,16,olivier.grisel@ensta.org,issue-3661/model_evaluation_copy_edited.rst|issue-3661/model_evaluation_edit_notes.txt|doc/modules/model_evaluation.rst|issue-3661/model_evaluation_copy_edited.rst|issue-3661/model_evaluation_edit_notes.txt|doc/modules/model_evaluation.rst|issue-3661/model_evaluation_copy_edited.rst|issue-3661/model_evaluation_edit_notes.txt,16,0.0,0,7,true,Issue 3661 * doc/modules/model_evaluationrst in the MattpSoftware fork contains my changes for issue #3661* issue-3661/model_evaluation_copy_editedrst also fixes typos I found when reading the document* issue-3661/model_evaluation_edit_notestxt has the explanation for the changes in model_evaluation_copy_editedrst and flags a few other other items for questions,,2124,0.7740112994350282,0.03876375065479309,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,72,1501,182,travis,MattpSoftware,jnothman,false,jnothman,0,0,0,0,106,true,false,false,false,1,3,0,0,4,0,63
4788826,scikit-learn/scikit-learn,python,3705,1411713518,1411726085,1411726085,209,209,github,false,false,false,5,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.600069887867193,0.10535682219701772,3,larsmans@gmail.com,doc/modules/feature_extraction.rst,3,0.0015748031496062992,0,0,true,fix the link of Out-of-core_algorithm ,,2123,0.7739048516250588,0.03884514435695538,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,71,1501,174,travis,floydsoft,floydsoft,true,floydsoft,0,0,19,163,892,true,false,false,false,0,0,0,0,1,0,9
4785602,scikit-learn/scikit-learn,python,3704,1411684084,,1411686307,37,,unknown,false,false,false,40,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,80,0,80,0,4.695097300009741,0.10753326438354875,13,t3kcit@gmail.com,sklearn/naive_bayes.py,13,0.006806282722513089,0,0,false,Added Poisson naive Bayes classifier (PoissonNB) Added PoissonNB class to naive_bayespy to implement a Poisson naieve Bayes classifier  Poisson classifiers have important applications in systems neuroscience  See: Ma et Al *Bayesian inference with probabilistic population codes*  Nat Neuroscience 9:1432 (2006),,2122,0.7742695570216777,0.03926701570680628,39497,416.56328328733827,33.09112084462111,109.9830366863306,3642,70,1500,173,travis,BrianLondon,BrianLondon,true,,0,0,0,0,6,false,false,false,false,0,0,0,0,0,0,-1
4761403,scikit-learn/scikit-learn,python,3703,1411675469,1415023300,1415023300,55797,55797,merged_in_comments,false,false,false,62,29,3,21,23,4,48,0,6,0,0,2,5,2,0,0,1,0,5,6,3,0,0,396,69,594,206,27.544167564449594,0.6308524128167528,15,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,12,0.0063025210084033615,0,1,false,ENH Add cross_val_predict function This function returns the cross-validated prediction for each element inthe input data (ie the prediction that is made when that object is inthe test set)This is a loop that I keep writing over and over again so I feel it shouldbe part of scikit-learn Additionally this uses joblib to take advantage ofmultiple CPUs,,2121,0.7741631305987742,0.03939075630252101,39497,416.335417879839,33.065802466010076,109.93239992910854,3642,70,1500,204,travis,luispedro,jnothman,false,jnothman,3,1.0,316,43,1975,true,false,false,false,0,0,3,0,4,0,18
4769922,scikit-learn/scikit-learn,python,3697,1411561931,1411565226,1411565226,54,54,github,false,false,false,30,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.37144334297255,0.1001206201850008,15,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py,15,0.007869884575026232,0,0,false,DOC Fix n_jobs documentation You can take advantage of multiple CPUS for the different folds not just different values of l1_ratio The documentation was incorrect (at least for current version),,2120,0.7740566037735849,0.04459601259181532,39514,416.156299033254,33.051576656374955,109.88510401376728,3642,70,1499,176,travis,luispedro,luispedro,true,luispedro,2,1.0,316,43,1974,true,false,false,false,0,0,0,0,1,0,11
4654715,scikit-learn/scikit-learn,python,3696,1411540947,1411677511,1411677511,2276,2276,commit_sha_in_comments,false,false,false,53,5,1,5,7,0,12,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,13,0,30,18,4.578197500612349,0.10485597939353351,11,t3kcit@gmail.com,sklearn/utils/__init__.py,11,0.005747126436781609,0,8,false,Fix shuffle for arrays with ndim  2: added a allow_nd as an acceptable parameter to resample Fixes https://githubcom/scikit-learn/scikit-learn/issues/3694 by passing allow_ndTrue into check_array within resample and shuffle allowing shuffle and resample to work on arrays with ndim  2 Also added a short test for shuffling an array with ndim  3,,2119,0.7739499764039641,0.04597701149425287,39514,416.156299033254,33.051576656374955,109.88510401376728,3642,69,1499,176,travis,wlamond,larsmans,false,larsmans,0,0,3,1,630,true,false,false,false,0,0,0,0,2,0,10
4765807,scikit-learn/scikit-learn,python,3695,1411517607,1411677705,1411677705,2668,2668,github,false,false,false,26,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.198809764312031,0.09616673594033066,3,t3kcit@gmail.com,sklearn/neighbors/kde.py,3,0.0015657620041753654,0,0,false,clarify KernelDensityscore{_samples} docstrings The current docs for KernelDensitys score and score_samples methods are confusingly worded and scores return shape is wrong This clarifies them I think,,2118,0.7738432483474976,0.04592901878914405,39514,416.156299033254,33.051576656374955,109.88510401376728,3642,68,1498,175,travis,dougalsutherland,dougalsutherland,true,dougalsutherland,5,1.0,30,27,2128,true,true,false,false,0,6,4,0,2,0,2667
4607815,scikit-learn/scikit-learn,python,3690,1411435330,1416883054,1416883054,90795,90795,commits_in_master,false,false,false,65,3,2,1,6,0,7,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,23,9,23,12.536601389589363,0.28865771233263376,15,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,12,0.0062402496099844,0,7,false,+ Fix the bug of calling len when using sparse matrix for fit_params in  When using sparse matrix in fit_params calling len(v) will raise error because the __len__ of sparse matrix is defined as    def __len__(self):        # return selfgetnnz()        raise TypeError(sparse matrix length is ambiguous use getnnz()                          or shape[0])So I added extra cases to deal with sparse matrix in fit_params,,2117,0.7737364194615022,0.0483619344773791,39175,415.4690491384811,33.13337587747288,110.12125079770261,3642,68,1497,213,travis,queqichao,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,87,true,true,false,false,0,1,0,0,1,0,191
4752351,scikit-learn/scikit-learn,python,3687,1411413763,1411656862,1411656862,4051,4051,commits_in_master,false,false,false,28,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,35,0,35,0,4.48715936606997,0.1033177268089031,6,t3kcit@gmail.com,sklearn/metrics/pairwise.py,6,0.0031185031185031187,0,1,false,ENH: Use the scipy C-based L1 distance if possible This also removes the need for the size_threshold parameter as scipys implementation doesnt allocate large temporary arraysFixes #3682,,2116,0.7736294896030246,0.04833679833679834,39175,415.4690491384811,33.13337587747288,110.12125079770261,3642,67,1497,177,travis,perimosocordiae,larsmans,false,larsmans,5,0.6,45,48,1962,true,true,false,false,0,5,2,0,3,0,1
4750805,scikit-learn/scikit-learn,python,3686,1411404112,1411408774,1411408774,77,77,commit_sha_in_comments,false,false,false,49,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,17,0,17,4.778598887780561,0.11002266014904127,5,larsmans@gmail.com,sklearn/tree/tests/test_tree.py,5,0.0026041666666666665,0,0,false,[WIP] more robust test for MemoryError from Tree_resize It seems Travis doesnt like my new test for MemoryError Im not sure whats going on but the worker might be suffering from Linuxs overallocation feature and filling all the RAM+swap before receiving a timeout Lets see if this works better,,2115,0.7735224586288416,0.0484375,39172,415.2966404574696,33.135913407535995,110.10415602981722,3642,67,1497,176,travis,larsmans,larsmans,true,larsmans,120,0.7583333333333333,144,38,1527,true,true,false,false,30,157,31,52,235,14,17
4750148,scikit-learn/scikit-learn,python,3685,1411399093,,1411483823,1412,,unknown,false,false,false,48,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,4.557487124468431,0.1049317736860545,6,t3kcit@gmail.com,sklearn/metrics/pairwise.py,6,0.003129890453834116,1,0,false,[WIP] Use manhattan_distances implemented by SciPy This PR is for #3682 I remove the sum_over_features as @larsmans suggested Im not sure if we should use SciPys version for sparse matrix directly Anyway I reserve the original version for spare matrix now I can remove it anytimeThanks,,2114,0.7738883632923368,0.048513302034428794,39172,415.2966404574696,33.135913407535995,110.10415602981722,3642,67,1497,177,travis,lazywei,lazywei,true,,1,0.0,78,72,671,false,false,false,false,0,6,1,0,0,0,7
4599139,scikit-learn/scikit-learn,python,3683,1411275654,1411408773,1411408773,2218,2218,commit_sha_in_comments,false,false,false,103,8,1,11,11,0,22,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,120,0,365,41,4.302399220978743,0.09905915133552841,5,t3kcit@gmail.com,sklearn/feature_extraction/dict_vectorizer.py,5,0.002638522427440633,0,6,false,[MRG+1] Add a more memory efficient version of DictVectorizer When loading really large files for SKLL I found that temporarily storing a list of dictionaries to pass to DictVectorizer was frequently using up huge amounts of memory  With UnsortedDictVectorizer you can call fit_transform with a generator and not have to waste the temporary spaceThis sub-class also overrides fit so that the feature names are not sorted in that case either for consistencyI would have just modified the DictVectorizer class to no longer sort the list of feature names but I didnt want to break anyones code that somehow relied on that,,2113,0.7737813535257927,0.04907651715039578,38902,416.40532620430827,33.23736568813943,110.63698524497455,3636,67,1496,177,travis,dan-blanchard,dan-blanchard,true,dan-blanchard,0,0,49,23,1136,true,true,false,false,1,4,0,0,2,0,155
4730965,scikit-learn/scikit-learn,python,3679,1411152733,,1411228618,1264,,unknown,false,false,false,7,1,1,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,Merge pull request #1 from scikit-learn/master update,,2110,0.7748815165876777,0.0502092050209205,38902,416.40532620430827,33.23736568813943,110.63698524497455,3632,65,1494,175,travis,cct479,cct479,true,,0,0,1,0,6,false,false,false,false,0,0,0,0,0,0,290
4591013,scikit-learn/scikit-learn,python,3678,1411108749,1411149887,1411149887,685,685,github,false,false,false,78,8,1,14,16,3,33,0,5,0,0,1,5,1,0,0,0,0,5,5,2,0,0,12,0,59,16,4.4196677249197736,0.10175946666093924,7,t3kcit@gmail.com,sklearn/linear_model/base.py,7,0.0036706869428421605,1,13,true,[MRG+1] Removed n_jobs parameter from the fit method and added it to constructor Fixes #3672 @arjoly I have deprecated the n_jobs parameter from the fit method and added it to the constructor I was not certain as to how to add the deprecation warning though as i am new to this community could you please tell me how to add that I will patch that up in the new commit along with the tests if needed Thank you,,2109,0.7747747747747747,0.051389617199790245,38892,416.3066954643628,33.194487298159004,110.61400802221537,3630,65,1494,174,travis,akshayah3,akshayah3,true,akshayah3,0,0,7,5,515,true,false,false,false,0,1,0,0,1,0,235
4725391,scikit-learn/scikit-learn,python,3677,1411101353,,1416881259,96331,,unknown,false,false,false,15,1,1,0,7,0,7,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.452523783737904,0.1025159558096162,12,vlad@vene.ro,sklearn/multiclass.py,12,0.006289308176100629,0,1,false,Fix deprecation warning in sklearnmulticlassOneVsRestClassifier Updated multiclasspy per DeprecationWarning caused by use of multiclass_ property,,2108,0.7751423149905123,0.051362683438155136,38892,416.3066954643628,33.194487298159004,110.61400802221537,3627,64,1493,252,travis,hamiltonkibbe,amueller,false,,0,0,4,4,884,false,true,false,false,0,0,0,0,0,0,9
4725336,scikit-learn/scikit-learn,python,3676,1411100547,1411105373,1411105373,80,80,github,false,false,false,4,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.309361487766293,0.09921975340390823,11,vlad@vene.ro,sklearn/feature_extraction/text.py,11,0.005765199161425576,0,0,false,Correct documentation for TfidfVectorizer ,,2107,0.7750355956336022,0.051362683438155136,38892,416.3066954643628,33.194487298159004,110.61400802221537,3627,64,1493,174,travis,danfrankj,danfrankj,true,danfrankj,0,0,2,1,1313,true,false,false,false,0,0,0,0,1,0,10
4725119,scikit-learn/scikit-learn,python,3675,1411098069,1411408774,1411408774,5178,5178,commit_sha_in_comments,false,false,false,5,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,3.97150793024533,0.09144093355808491,1,olivier.grisel@ensta.org,sklearn/svm/src/liblinear/liblinear_helper.c,1,0.0005243838489774515,0,0,false,Fix error handling in liblinear_helperc ,,2106,0.7749287749287749,0.051389617199790245,38892,416.3066954643628,33.194487298159004,110.61400802221537,3626,63,1493,176,travis,x0l,larsmans,false,larsmans,1,1.0,4,25,577,false,false,false,false,0,0,1,0,1,0,32
4721382,scikit-learn/scikit-learn,python,3674,1411070331,1411122791,1411122791,874,874,github,false,false,false,39,1,1,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,10,4,10,9.287572027586736,0.21383924484270778,75,olivier.grisel@ensta.org,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,67,0.034895833333333334,0,0,false,[MRG+1] FIX: Make sure LogRegCV with solverliblinear works with sparse matrices The function csr_set_problem assumes X to be of csr format More specifically accessing Xindptr[0] - 1 to get n_samples in the function creates huge problems This fixes it,,2105,0.7748218527315914,0.052083333333333336,38892,416.3066954643628,33.194487298159004,110.61400802221537,3621,63,1493,174,travis,MechCoder,MechCoder,true,MechCoder,28,0.8214285714285714,79,41,821,true,true,false,false,9,401,34,152,200,2,13
4710676,scikit-learn/scikit-learn,python,3670,1410971131,1422376354,1422376354,190087,190087,commit_sha_in_comments,false,false,false,88,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.293949428843838,0.09886492749959397,13,t3kcit@gmail.com,sklearn/metrics/classification.py,13,0.006656426011264721,0,0,false,Altering precision_recall_fscore_support docstring If the labels are unsorted and precision_recall_fscore_support is used with multilabel classifications it will raise an exception Eg:sklearnmetricsprecision_recall_fscore_support(    [(12)]    [(34)]    labels[4 1 2 3])raises/usr/local/lib/python27/dist-packages/sklearn/metrics/classificationpyc in precision_recall_fscore_support(y_true y_pred beta labels pos_label average warn_for sample_weight)    945     elif label_order is not None:    946         indices  npsearchsorted(labels label_order)-- 947         precision  precision[indices]    948         recall  recall[indices]    949         f_score  f_score[indices]IndexError: index 4 is out of bounds for size 4Also AFAICT the labels do not need to be integers,,2104,0.7747148288973384,0.051715309779825906,38892,416.3066954643628,33.194487298159004,110.61400802221537,3609,63,1492,254,travis,nathanathan,larsmans,false,larsmans,0,0,15,39,1261,false,false,false,false,0,0,0,0,0,0,11
4709152,scikit-learn/scikit-learn,python,3669,1410958133,1412969295,1412969295,33519,33519,commits_in_master,false,false,false,20,2,1,7,9,0,16,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,16,13,16,14,9.080181113448864,0.2090642804114292,5,t3kcit@gmail.com,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,5,0.002560163850486431,2,4,false,Make IsotonicRegression pickleable fixes https://githubcom/scikit-learn/scikit-learn/issues/3666implements __getstate__ and __setstate__ by removing the interpolation function and rebuilding itcc @amueller @jnothman ,,2103,0.7746077032810271,0.05222734254992319,38892,416.3066954643628,33.194487298159004,110.61400802221537,3608,63,1492,188,travis,pprett,pprett,true,pprett,46,0.8695652173913043,140,29,1870,true,true,false,false,3,12,0,15,9,0,11
4708585,scikit-learn/scikit-learn,python,3668,1410953056,1412033511,1412033511,18007,18007,merged_in_comments,false,false,false,39,1,1,0,3,0,3,0,4,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.886612576515665,0.11251054678254693,1,olivier.grisel@ensta.org,doc/testimonials/images/solido_logo.gif|doc/testimonials/testimonials.rst,1,0.0005122950819672131,0,0,false,[MRG] Add solido testimonial This is a company that a friend of mine founded They use scikit-learn extensivelyWhile he cant give names officially he assures me that your pocket likely contains a device with chips designed using scikit-learn,,2102,0.774500475737393,0.05225409836065574,38892,416.3066954643628,33.194487298159004,110.61400802221537,3606,63,1492,181,travis,amueller,amueller,true,amueller,208,0.8557692307692307,977,39,1426,true,true,false,false,48,128,23,78,29,3,11
4581072,scikit-learn/scikit-learn,python,3667,1410906727,1410952620,1410952620,764,764,github,false,false,false,6,2,1,4,2,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,6,0,4.1892979869598905,0.09645539137073358,0,,sklearn/pipeline.py,0,0.0,0,0,false,typos: remove extra ss grammatical agreement,,2101,0.7743931461208948,0.05404014410705095,38892,416.3066954643628,33.194487298159004,110.61400802221537,3603,62,1491,168,travis,ajschumacher,ajschumacher,true,ajschumacher,5,1.0,167,333,1090,true,false,false,false,0,0,0,0,1,0,1
4685287,scikit-learn/scikit-learn,python,3663,1410722467,,1410774261,863,,unknown,false,false,false,11,2,2,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,8.378361348898906,0.1929053806792877,9,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py,9,0.004702194357366771,0,1,false,Add support for single features (1D arrays) in OneHotEncoder fixes #3662,,2099,0.7751310147689376,0.05747126436781609,38892,416.3066954643628,33.194487298159004,110.61400802221537,3589,59,1489,169,travis,mkneierV,mkneierV,true,,0,0,5,1,159,true,false,false,false,0,2,0,0,2,0,9
4677412,scikit-learn/scikit-learn,python,3659,1410586128,1439151181,1439151181,476084,476084,commits_in_master,false,false,false,151,57,2,137,126,5,268,0,13,2,0,1,17,3,0,0,8,2,13,23,12,0,1,1028,408,4396,1351,27.067284919041164,0.6232000464954719,1,larsmans@gmail.com,sklearn/decomposition/__init__.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py|sklearn/decomposition/__init__.py|sklearn/decomposition/online_lda.py|sklearn/decomposition/tests/test_online_lda.py,1,0.0,0,48,false,[WIP] implementing LDA(Latent Dirichlet Allocation) with online variational Bayes This PR is an implementation of Matt Hoffmans topic modeling algorithm LDA with online variational BayesBased on previous discussion in [this email thread](http://sourceforgenet/p/scikit-learn/mailman/message/32748341/) I asked Matt if he could relicense his onlineldavb code to BSD And now his code is relicensed so I create a PR for itI use the name OnlineLDA for this model and put it in decomposition folder And since the model can run both online and batch update I implemented both fit and partial_fit methodThe algorithm part and unit test is done and ready for review Will work on an example next###### Check List:- [X] algorithm implementation- [X] unit test - [ ] example- [ ] documentation###### Reference:[1] [Online Learning for Latent Dirichlet Allocation Matthew D Hoffman David M Blei Francis Bach](https://wwwcsprincetonedu/~blei/papers/HoffmanBleiBach2010bpdf)[2] [original onlineldavb code (with BSD license)](https://wwwdropboxcom/s/wnkro3xtqjm7bli/onlineldavb_bsdtar),,2098,0.775023832221163,0.05888594164456233,38892,417.61801912989813,33.4001851280469,110.66543247968734,3574,58,1488,361,travis,chyikwei,larsmans,false,larsmans,2,0.5,16,1,718,true,true,false,false,0,0,1,0,1,0,41
4570031,scikit-learn/scikit-learn,python,3657,1410544351,1440950409,1440950409,506767,506767,commit_sha_in_comments,false,false,false,10,6,1,11,21,0,32,0,5,0,0,11,11,11,0,0,0,0,11,11,11,0,0,25,36,45,40,41.408897525296325,0.9534028603266742,41,vlad@vene.ro,benchmarks/bench_multilabel_metrics.py|examples/datasets/plot_random_multilabel_dataset.py|examples/plot_multilabel.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_multiclass.py,10,0.0015923566878980893,0,16,true,make_multilabel_classification sparse target for issue #3554closing pull request #3583,,2097,0.7749165474487363,0.058386411889596604,38892,417.61801912989813,33.4001851280469,110.66543247968734,3573,57,1487,364,travis,kashif,jnothman,false,jnothman,1,0.0,90,44,2335,true,false,false,false,7,7,1,0,71,0,2
4577971,scikit-learn/scikit-learn,python,3656,1410430399,1411678816,1411678816,20806,20806,commits_in_master,false,false,false,261,1,1,3,4,0,7,0,5,0,0,3,3,2,0,0,0,0,3,3,2,0,0,2,7,2,7,13.896755062689929,0.31996118148154074,51,vlad@vene.ro,doc/whats_new.rst|sklearn/kernel_approximation.py|sklearn/tests/test_kernel_approximation.py,45,0.003188097768331562,0,3,false,[MRG+1] fix RBFSamplers incorrect bandwidth RBFSampler is missing a factor of 2 in variance of the Gaussian to sample from The original paper talks about the kernel exp(1/(2 σ^2) ||x-y||^2) Fourier-transforming into a Gaussian with variance 1/σ^2 the current code has exp(ɣ ||x-y||^2) (incorrectly) Fourier-transforming to a Gaussian with variance ɣDemonstration: heres the value of the RBF kernel between 0 and 1 for varying values of gamma via rbf_kernel RBFSampler(gamma) and RBFSampler(2 * gamma) (using lots of components):[](https://iimgurcom/1z4Fk4upng)The reason test_rbf_sampler works is because the test is too loose and it happens to be somewhat close at the values tested:[](https://iimgurcom/97fsDz5png)(x is true value y is the approximated value) I just upped the number of Monte Carlo samples and increased it to 2 decimals giving this new scatterplot:[](http://iimgurcom/4KnURsYpng)This demonstrates that assert_array_almost_equal isnt really the right tool for testing Monte Carlo approximators like this (we can still only use 2 decimals in this test with 10k samples) Instead there should probably be a helper function to test that the mean absolute difference is fairly small and also a looser bound on the max differenceor somethingHeres [a notebook](http://nbvieweripythonorg/gist/dougalsutherland/11fd9895924c3c1aed19) that makes these pictures(This probably wont dramatically change anyones practical results with RBFSampler since effectively youre just cross-validating over a set of gamma values that are double the ones you thought you were cross-validating over But if people were picking gamma based on an exact evaluation on a subset or from some other external source their theyd be using a substantially different kernel than they thought they were),,2096,0.7748091603053435,0.05844845908607864,38891,417.6287572960325,33.40104394332879,110.6682780077653,3561,57,1486,180,travis,dougalsutherland,larsmans,false,larsmans,4,1.0,30,27,2116,true,true,false,false,0,5,3,0,1,0,7136
4657577,scikit-learn/scikit-learn,python,3655,1410404805,,1421462004,184286,,unknown,false,true,false,19,2,1,0,7,0,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,3,0,3,26,4.377862294615104,0.10102168254149452,5,t3kcit@gmail.com,sklearn/neighbors/regression.py,5,0.002652519893899204,0,0,false,BUG: avoid infinite weights in kNN regression Same issue as #2941 but in the regressor instead of the classifier,,2095,0.7751789976133652,0.05888594164456233,38891,417.6287572960325,33.40104394332879,110.6682780077653,3557,56,1485,246,travis,perimosocordiae,perimosocordiae,true,,4,0.75,45,48,1950,true,true,false,false,0,4,1,0,2,0,10
4654745,scikit-learn/scikit-learn,python,3654,1410384486,1410479961,1410479961,1591,1591,merged_in_comments,false,false,false,81,2,1,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,32,13,32,8.797141668796822,0.2029990879409055,18,t3kcit@gmail.com,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,15,0.007974481658692184,2,2,false,[MRG+1] Fix LARS on 32 bit Python that failed on Windows FIX #3370: better LARS alpha path inequality checks for 32 bit supportFinally fixed the failure reported by AppVeyor (ignore the upload failure its because I did not configure the secret key on my personal appveyor account):  https://ciappveyorcom/project/ogrisel/scikit-learn/build/job/3enuragixu8bh5r8The drop for good strategy is no longer triggered by any test as a result of this fix I did not repove the code for drop for good thoughPing @fabianp @GaelVaroquaux,,2094,0.7750716332378224,0.05901116427432217,38891,417.6287572960325,33.40104394332879,110.6682780077653,3556,56,1485,170,travis,ogrisel,ogrisel,true,ogrisel,95,0.8526315789473684,1015,123,1932,true,true,false,false,37,620,74,309,184,4,11
4649806,scikit-learn/scikit-learn,python,3653,1410348334,1410350730,1410350730,39,39,github,false,false,false,20,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,0,3,0,3.686111392697686,0.08505913355000147,7,olivier.grisel@ensta.org,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx,7,0.0037333333333333333,0,2,false,[MRG] OPT: Prevent iterating across n_features twice In the cd_fastpyx code to find X*residual we iterate twice which is unnecessary,,2093,0.774964166268514,0.0592,38891,417.6287572960325,33.40104394332879,110.6682780077653,3553,56,1485,168,travis,MechCoder,MechCoder,true,MechCoder,27,0.8148148148148148,79,41,813,true,true,false,false,8,391,30,135,197,2,6
4570082,scikit-learn/scikit-learn,python,3651,1410294139,1440938970,1440938970,510747,510747,commits_in_master,false,false,false,23,6,2,2,7,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,27,0,39,0,9.589281532217154,0.22127816947102638,3,olivier.grisel@ensta.org,examples/model_selection/plot_roc.py|examples/plot_roc.py,3,0.0016129032258064516,0,0,false,[MRG+1] Added macro-average ROC to plot_rocpy I added macro-average ROC to plot_rocpyMacro-averaging is also an important performance measure used in multi-label classification,,2092,0.7748565965583174,0.05806451612903226,38891,417.6287572960325,33.40104394332879,110.6682780077653,3552,56,1484,366,travis,wangz10,GaelVaroquaux,false,GaelVaroquaux,0,0,6,12,424,false,false,true,false,0,0,0,0,0,0,401
4547723,scikit-learn/scikit-learn,python,3646,1410143668,1413421310,1413421310,54627,54627,commits_in_master,false,false,false,97,16,6,61,36,0,97,0,6,0,0,1,4,1,0,0,0,0,4,4,2,0,0,228,0,456,124,22.626219431999353,0.5221129844883974,67,olivier.grisel@ensta.org,sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py,67,0.03563829787234043,0,21,false,[WIP] Multinomial newtoncg This adds newton-cg solver to the linear_modelLogisticRegression class when multi_classmultinomial Benchmarks on a few sparse datasets against L-BFGS solver are included below Convergence times tended to be slightly lower for newton-cg and the scores were pretty much identical on these datasetsBenchmark on 20NewsGroup dataset (11314 x 130107 sparse matrix):[20news_bench](https://cloudgithubusercontentcom/assets/4610553/4179948/d35bf1d0-36dd-11e4-8bb5-b02299024725jpeg)Benchmark on Brown Corpus using TFIDF feature extraction (335 x 42453 sparse matrix):[brown_bench](https://cloudgithubusercontentcom/assets/4610553/4179957/57b56b32-36de-11e4-93d9-70ba5f551238jpeg)Benchmark on Movie Review Corpus using TFIDF feature extraction (1340 x 39659 sparse matrix)[moviereview_bench](https://cloudgithubusercontentcom/assets/4610553/4179803/13e0a806-36d7-11e4-9775-22426e48189ajpeg)Benchmark on Reuters Corpus using TFIDF feature extraction (7227 x 30916 sparse matrix)[reuters_bench](https://cloudgithubusercontentcom/assets/4610553/4180034/40167f08-36e2-11e4-907f-90f7bbe29b7ejpeg),,2091,0.7747489239598279,0.06702127659574468,38891,417.6287572960325,33.40104394332879,110.6682780077653,3529,57,1482,201,travis,s8wu,MechCoder,false,MechCoder,0,0,0,0,460,true,false,false,false,0,0,0,0,6,0,8
4548371,scikit-learn/scikit-learn,python,3645,1410089508,,1445530937,590690,,unknown,false,true,false,79,9,4,9,40,2,51,0,10,1,0,6,8,7,0,0,1,0,7,8,7,0,0,212,20,334,20,32.05999936470532,0.7398028646062366,29,tejeshpapineni95@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.py|examples/gaussian_process/plot_gp_regression.py|sklearn/gaussian_process/tests/test_gaussian_process.py|examples/plot_predictive_standard_deviation.py,17,0.002127659574468085,0,12,false,[MRG] Prediction variance in bagging-based regressors This addresses issue  #3271 The main points are * Option with_std added to method predict of BaggingRegressor and RandomForestRegressor If True the standard deviation of the predictions is returned in addition to the mean * Added option with_std to GaussianProcesspredict() and deprecated eval_MSE which returned the predictive variance * Added one example comparing the predictive distributions of the three methodsPlease also check the planned deprecation path of the parameter eval_MSE in GaussianProcess,,2090,0.7751196172248804,0.06648936170212766,38891,417.6287572960325,33.40104394332879,110.6682780077653,3525,57,1482,381,travis,jmetzen,glouppe,false,,9,0.5555555555555556,11,2,1063,true,true,false,false,0,16,4,1,108,0,71
4611890,scikit-learn/scikit-learn,python,3640,1409918682,1421207078,1421207078,188139,188139,commits_in_master,false,false,false,21,2,1,0,12,0,12,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,16,4.589774690055682,0.10591181944968517,5,t3kcit@gmail.com,sklearn/metrics/cluster/bicluster.py,5,0.0026469031233456856,0,5,true,FIX consensus score on non-square similarity matrices This PR fixes #2445: When similarity matrices are non-square the biclusterconsensus_score gave wrong results ,,2089,0.77501196744854,0.07199576495500265,38891,417.4487670669306,33.3753310534571,110.6425651178936,3510,57,1480,253,travis,untom,amueller,false,amueller,8,0.625,9,0,564,true,false,false,false,0,19,3,18,2,0,13
4611589,scikit-learn/scikit-learn,python,3639,1409915904,,1438172265,470939,,unknown,false,true,false,402,4,3,9,26,0,35,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,1392,15,1409,15,54.66132695404622,1.2613827380014233,17,t3kcit@gmail.com,doc/modules/preprocessing.rst|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/pipeline.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,11,0.0026329647182727752,1,10,true,[WIP] Refactor scaler code This PR refactors the scalers in preprocessing to use a common baseclass (BaseScaler) This is done to share some common functionality / avoid code duplication This PR is part of a series of PR that split up #2514  and other PRs in this series intend to introduce new scalers that will make use of BaseScaler (eg a scaler that uses robust statistics)There is one issues with this PR that needs to be discussed:All scalers (both the existing ones and the ones Im going to introduce) do center the data and then scale it to fall into some range So in essence all layers do    (x - some_centering_statistic) / some_scaling_statisticfor each feature-column x  The existing scalers already have attributes that expose their centering/scaling statistic however those are not uniformly named  I propose that the  StandardScalermean_/StandardScalerstd_/MinMaxScalerscale_ and MinMaxScalermin_ properties are being deprecated and replaced with *center_/*scale_ and the with_mean/with_std constructer arguments be renamed to with_centering/with_scaling@larsmans already said back in #2514 that he is against renaming the mean_/std_ attributes since the scaler API should be stable An additional problem is that my proposal would change the meaning of MinMaxScalerscale_ (as the previous implementation used a slightly different math to arrive at the same scaling result)   This will of course break user code that uses MinMaxScalerscale_ and relies on its current implementation  (NOTE: I had overlooked this when proposing #2514 and just noticed it when preparing this PR)There are several ways to deal with this:* dont expose the new scale_/center_ arguments in StandardScaler/MinMaxScaler and just keep the old mean_/std_/scale_ arguments* deprecate mean_/std_ and remove them after a few releases Print a warning   whenever a user uses scale_ on MinMaxScaler that the value has changed* I would really appreciate any input on what the right thing to do here is*NOTE:* As side-effect of this PR StandardScaler and MinMaxScaler gain the ability to scale on other rows/samples instead of just columns/features (ie it is possible to use axis1) However no tests for this new functionality is included in this PR This is because I intend to send another PR tomorrow that refactors the tests in this module (and adds the tests for axis  1) I just thought that splitting up the tests makes it easier to review the changes (but I can add that other PR to this one if you wish),,2088,0.7753831417624522,0.07161664033701948,38900,417.3521850899743,33.36760925449871,110.61696658097686,3510,57,1480,360,travis,untom,untom,true,,7,0.7142857142857143,9,0,564,true,false,false,false,0,19,2,18,2,0,10
4607844,scikit-learn/scikit-learn,python,3638,1409874173,1421201074,1421201074,188781,188781,commit_sha_in_comments,false,false,false,34,3,2,0,6,0,6,0,3,0,0,4,4,2,0,0,0,0,4,4,2,0,0,208,54,228,54,37.260097081319635,0.8598262408475021,30,t3kcit@gmail.com,doc/modules/classes.rst|doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,15,0.00676730869338886,0,0,false,[WIP] RollingWindow cross-validation A cross-validation strategy for timeseries see http://robjhyndmancom/hyndsight/tscvexampleInitial commit tests and unfinished docsI dont really like the name of the class Hopefully someone will find a better name for this,,2087,0.7752755150934355,0.07183758459135867,38900,417.3521850899743,33.36760925449871,110.61696658097686,3508,57,1479,254,travis,x0l,x0l,true,x0l,0,0,4,25,563,false,false,false,false,0,0,0,0,0,0,14
4548611,scikit-learn/scikit-learn,python,3636,1409852427,1410627844,1410627844,12923,12923,merged_in_comments,false,false,false,8,6,0,5,13,0,18,0,5,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,15,22,0,0.0,0,,,0,0.0,0,5,false,[MRG+1] adding value error for partial_fit with class_weightsauto ,,2086,0.7751677852348994,0.070076726342711,38891,417.6287572960325,33.40104394332879,110.6682780077653,3508,58,1479,177,travis,dsullivan7,dsullivan7,true,dsullivan7,8,0.625,8,19,391,true,true,false,false,1,51,3,26,18,0,10
4537124,scikit-learn/scikit-learn,python,3634,1409843659,1409850824,1409850824,119,119,github,false,false,false,87,1,1,1,6,0,7,0,3,0,4,2,6,8,0,0,0,4,2,6,8,0,0,87,95,87,95,9.107193708315911,0.20950636190764046,7,olivier.grisel@ensta.org,sklearn/base.py|sklearn/cluster/bicluster.py|sklearn/cluster/bicluster/__init__.py|sklearn/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/tests/test_bicluster.py|sklearn/setup.py,5,0.0,0,1,false,[MRG+1] big refactor of sklearnclusterbicluster * Removed dead and useless code in the utils submodule* decoupled sklearnbase from the bicluster module (no more deep import)* rewrote tests to test the API not the implementation* added input validation to the get_submatrix method (but it still needs  documentation because Ive no idea what to pass for data)* got rid of the subdirectory when it turned out only one file was leftIm still trying to run the examples they take an awful amount of time,,2085,0.7750599520383693,0.07011258955987718,38694,422.8045691838528,33.338502093347806,110.76652711014627,3508,58,1479,168,travis,larsmans,larsmans,true,larsmans,119,0.7563025210084033,144,38,1509,true,true,false,false,36,178,46,63,235,14,33
4536933,scikit-learn/scikit-learn,python,3633,1409835087,1409849915,1409849915,247,247,commit_sha_in_comments,false,false,false,7,2,1,1,4,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,21,0,25,0,4.542845384243827,0.10416590752926895,7,t3kcit@gmail.com,sklearn/utils/extmath.py,7,0.003575076608784474,0,0,false,ENH Speed up and simplify cartesian product ,,2084,0.7749520153550864,0.07048008171603677,38702,422.51046457547415,33.33161076946928,110.691953904191,3506,58,1479,170,travis,stefanv,stefanv,true,stefanv,1,1.0,176,3,2065,false,false,false,false,0,1,1,0,3,0,45
4592158,scikit-learn/scikit-learn,python,3631,1409765936,1409842119,1409842112,1269,1269,github,false,false,true,114,1,1,0,1,0,1,0,2,0,0,2,2,0,0,1,0,0,2,2,0,0,1,0,0,0,0,9.591899542491388,0.22066168899149627,6,olivier.grisel@ensta.org,appveyor.yml|continuous_integration/appveyor/requirements.txt,5,0.00254323499491353,0,0,false,[MRG] Appveyor wheelhouse uploader Here is an update to the appveyor configuration to upload the generated artifacts to a rackspace container I needed to create the branch in the main sklearn to be able to use traditional sklearn-ci account:https://ciappveyorcom/project/sklearn-ci/scikit-learnI encrypted therackspace secret API key on that AppVeyor account (see the secure: prefix)Once I get a successful build on that branch I will post the link to the resulting containersThis tool should help use automate the release process for windows build artifacts in particular by making it easier to test those artifacts prior to a release The final upload to PyPI when we do a release will still be manual though,,2083,0.7748439750360058,0.07171922685656154,38696,422.57597684515196,33.33677899524499,110.70911722141824,3500,58,1478,170,travis,ogrisel,ogrisel,true,ogrisel,94,0.851063829787234,1013,123,1925,true,true,false,false,37,687,82,347,189,7,12
4591878,scikit-learn/scikit-learn,python,3629,1409764852,1409778054,1409778054,220,220,github,false,false,false,61,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.127096794639281,0.09463332851093388,10,t3kcit@gmail.com,sklearn/linear_model/least_angle.py,10,0.005083884087442806,0,2,false,[MRG+1] FIX heisenfailure in test_lasso_lars_path_length Tentative fix for #3370: random failure under Python 32 bit and WindowsThe failure is very rare so I am not 100% sure that this fixes it Appveyor will tell over time In the mean time I do a PR here to have travis check that this does not cause a failure in some other test,,2082,0.7747358309317963,0.07219115404168785,38702,422.51046457547415,33.33161076946928,110.66611544623018,3500,58,1478,169,travis,ogrisel,ogrisel,true,ogrisel,93,0.8494623655913979,1013,123,1925,true,true,false,false,36,686,82,347,190,7,0
4589449,scikit-learn/scikit-learn,python,3625,1409749985,1409838470,1409838470,1474,1474,merged_in_comments,false,false,false,26,1,1,0,2,0,2,0,3,0,3,3,6,6,0,0,0,3,3,6,6,0,0,195,65,195,65,12.705096352573733,0.29132627793940125,10,t3kcit@gmail.com,sklearn/utils/__init__.py|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/_min_spanning_tree.c|sklearn/utils/sparsetools/_min_spanning_tree.pyx|sklearn/utils/sparsetools/setup.py|sklearn/utils/sparsetools/tests/test_spanning_tree.py,10,0.0,0,0,false,[MRG] remove unused minimum spanning tree code This trims away 7000 LODC (lines of dead code) Nothing in scikit-learn seems to be interested in computing MSTs,,2081,0.77462758289284,0.07211782630777044,38696,421.43890841430635,33.25925160223279,110.29563779201985,3499,58,1478,170,travis,larsmans,larsmans,true,larsmans,118,0.7542372881355932,144,38,1508,true,true,false,false,39,175,45,64,235,14,11
4532127,scikit-learn/scikit-learn,python,3623,1409685765,1409928452,1409928452,4044,4044,commit_sha_in_comments,false,false,false,35,1,0,1,37,0,38,0,9,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,11,18,0,0.0,0,,,0,0.0,0,21,false,[MRG+3] DOC: Explain difference between Logistic Regression and Liblinear when df is zero Fixes https://githubcom/scikit-learn/scikit-learn/issues/3600Im not sure that this is the right thing to do but it is best to do what liblinear does,,2080,0.7745192307692308,0.07236842105263158,38702,422.51046457547415,33.33161076946928,110.66611544623018,3493,58,1477,171,travis,MechCoder,MechCoder,true,MechCoder,26,0.8076923076923077,79,41,805,true,true,false,false,7,367,32,106,279,2,1
4527411,scikit-learn/scikit-learn,python,3622,1409664327,1409755714,1409755714,1523,1523,commit_sha_in_comments,false,false,false,106,3,2,19,9,0,28,0,5,0,0,9,9,8,0,0,0,0,9,9,8,0,0,59,69,61,69,40.91492626416448,0.9381740981777286,72,t3kcit@gmail.com,sklearn/feature_selection/variance_threshold.py|doc/developers/utilities.rst|sklearn/cluster/k_means_.py|sklearn/decomposition/truncated_svd.py|sklearn/linear_model/base.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py,33,0.004056795131845842,0,0,false,[MRG+1] Add axis argument to sparsefuncsmean_variance_axis This PR adds an axis argument to sparsefuncsmean_variance_axis making it easier to calculate the columnwise mean/variance of sparse matrices While switching the codebase over to the new functionality I noticed that  VarianceThreshold needlessly converted CSC matrices to CSR This PR also includes a commit to fix this as the change fits naturally with the other changes(If the immediate need for the new functionality is not obvious: This PR is the first in a series that tries to split up #2514 into a few smaller easy-to-digest PRs I plan to commit other PRs that will make use of this one),,2079,0.7744107744107744,0.07150101419878296,38696,421.43890841430635,33.25925160223279,110.29563779201985,3490,58,1477,168,travis,untom,untom,true,untom,6,0.6666666666666666,9,0,561,true,false,false,false,0,12,0,9,2,0,13
4576580,scikit-learn/scikit-learn,python,3621,1409660625,1409661383,1409661383,12,12,commit_sha_in_comments,false,false,false,19,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.959673047128844,0.1137238231449306,9,olivier.grisel@ensta.org,continuous_integration/install.sh,9,0.004563894523326572,0,0,false,[MRG] FIX define CC and CXX for travis To be merged if this fixes the most recent travis failures,,2078,0.7743022136669875,0.07150101419878296,38692,421.4307867259382,33.262689961749196,110.28119507908612,3490,58,1477,167,travis,ogrisel,ogrisel,true,ogrisel,92,0.8478260869565217,1011,123,1924,true,true,false,false,37,697,83,351,185,6,-1
4575805,scikit-learn/scikit-learn,python,3620,1409655506,1409660661,1409660661,85,85,commit_sha_in_comments,false,false,false,79,1,1,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,19,15,19,15,8.670496012201927,0.19881180078512037,38,olivier.grisel@ensta.org,sklearn/__init__.py|sklearn/tests/test_common.py,32,0.01627670396744659,0,0,false,[MRG] FIX update sklearn__all__ to include all end-user submodules sklearn__all__ appears to have been populated with something like the following in mind:pythonfrom sklearn import *iris  datasetsload_iris()clf  linear_modelLogisticRegression()fit(irisdata iristarget)This PR fixes #3615 which notes that __all__ is not up-to-date This PR adds a test to ensure that the list is maintained while making exceptions explicit test_import_all_consistency is extended to ensure that the entire contents of sklearn__all__ is (not only complete but) valid,,2077,0.7741935483870968,0.07171922685656154,38692,421.4307867259382,33.262689961749196,110.28119507908612,3488,58,1477,167,travis,jnothman,jnothman,true,jnothman,93,0.6989247311827957,28,1,1953,true,true,false,false,29,462,47,331,122,7,-1
4568746,scikit-learn/scikit-learn,python,3619,1409588271,1409597364,1409597364,151,151,github,false,false,false,22,1,0,0,5,1,6,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,[MRG] FIX bump up the miniconda installation script To fix the recent travis failures I will merge if travis is green again,,2076,0.7740847784200385,0.07331975560081466,38692,421.4307867259382,33.262689961749196,110.28119507908612,3474,58,1476,168,travis,ogrisel,ogrisel,true,ogrisel,91,0.8461538461538461,1011,123,1923,true,true,false,false,37,692,78,350,178,6,1
4523369,scikit-learn/scikit-learn,python,3613,1409503842,1409873411,1409873411,6159,6159,commit_sha_in_comments,false,false,false,56,6,2,12,8,2,22,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,38,0,98,7,8.348185235599445,0.19161875049427388,34,t3kcit@gmail.com,sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,34,0.017258883248730966,0,0,false,[MRG] Kmeans memory problem ENH: Precompute distances only if overhead is below 100MBPrecomputing distances can consume a lot of memory if several jobs areused With 8 jobs a dataset of shape (1e6 2) and n_clusters100 theprecomputed distances will consume 3GB of memory while the dataset onlyconsume 15MB (Assuming double precision is used),,2074,0.7743490838958534,0.07309644670050762,38684,420.7165753283011,33.243718333161,109.99379588460346,3454,57,1475,172,travis,kain88-de,kain88-de,true,kain88-de,0,0,9,13,488,true,false,false,false,0,1,0,0,8,0,87
4523381,scikit-learn/scikit-learn,python,3612,1409503622,1409530436,1409530436,446,446,github,false,false,false,26,2,1,2,6,0,8,0,5,0,0,5,5,5,0,0,0,0,5,5,5,0,0,49,21,49,33,22.62169310482972,0.5192434576472528,34,olivier.grisel@ensta.org,setup.py|sklearn/datasets/base.py|sklearn/datasets/tests/test_svmlight_format.py|sklearn/datasets/twenty_newsgroups.py|sklearn/tests/test_common.py,30,0.0005076142131979696,0,1,false,[MRG+1] Fixed ResourceWarnings from inside scikit-learn Fixes #3410 Remaining ResourceWarnings are due to a bug in Pillow (a PR is being created at the same time),,2073,0.7742402315484804,0.07309644670050762,38684,420.7165753283011,33.243718333161,109.99379588460346,3454,57,1475,170,travis,calvingiles,calvingiles,true,calvingiles,0,0,4,3,730,true,false,false,false,0,1,0,0,3,0,89
4561725,scikit-learn/scikit-learn,python,3611,1409496967,1409589249,1409589249,1538,1538,github,false,true,false,48,1,1,0,14,0,14,0,5,1,0,2,3,3,0,0,1,0,2,3,3,0,0,17,0,17,0,9.099192566063763,0.20885687856813895,0,,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast_helpers.h,0,0.0,0,5,false,[MRG+1] FIX #3566: redefine isfinite alias in sklearn npy_isfinite from npy_mathh depends on the compiler used to build numpy If we build scikit-learn with a different compiler (eg MSVC vs MinGW) this can cause link errorsTherefore we redefine the isfinite macro in a scikit-learn specific header file,,2072,0.7741312741312741,0.07318321392016376,38684,420.7165753283011,33.243718333161,109.99379588460346,3454,55,1475,170,travis,ogrisel,ogrisel,true,ogrisel,90,0.8444444444444444,1010,123,1922,true,true,false,false,36,677,77,349,173,6,1
4560928,scikit-learn/scikit-learn,python,3610,1409484092,1410194418,1410194418,11838,11838,commit_sha_in_comments,false,false,false,63,3,2,0,4,0,4,0,3,1,0,1,3,0,0,0,1,0,2,3,0,0,0,0,0,0,0,9.353378928762154,0.21469131172558312,3,joel.nothman@gmail.com,doc/datasets/sparse_data_loading.rst|doc/datasets/index.rst,3,0.0015608740894901144,1,0,false,[WIP] DOC Sparse dataset loading This PR adds a small section to the datasets loading doc showing how to load data directly to a CSR matrixThe original snippet comes from Lars - [x] test links after make doc - [ ] review by eg @larsmans- [ ] add an item to whatsnewrst- [ ] review by one more core dev,,2071,0.7740222114920329,0.07492195629552549,38686,420.6948250012925,33.241999689810264,109.98810939357907,3453,55,1475,173,travis,oddskool,oddskool,true,oddskool,6,0.6666666666666666,4,3,1196,true,true,false,false,0,0,0,0,1,0,7
4555847,scikit-learn/scikit-learn,python,3608,1409403704,1409510733,1409510733,1783,1783,github,false,false,false,25,1,1,0,6,2,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.214501446285498,0.0967387633128544,14,t3kcit@gmail.com,sklearn/naive_bayes.py,14,0.007257646448937273,0,7,false,[MRG+2] fix_gnb_proba a 2 was missingit does not affect results as the probas are normalized to sum to 1 afterwards but its more correct,,2070,0.7739130434782608,0.07413167444271643,38704,420.49917321207107,33.226539892517565,109.93695742042166,3450,54,1474,170,travis,agramfort,agramfort,true,agramfort,40,0.9,159,186,1732,true,true,false,false,11,233,19,299,98,1,11
4521184,scikit-learn/scikit-learn,python,3607,1409359622,1415039010,1415039010,94656,94656,commit_sha_in_comments,false,false,false,11,137,28,162,111,23,296,0,6,0,0,13,19,11,0,0,0,0,19,19,17,0,0,439,81,1914,2004,163.54134460383102,3.765414742078494,223,vlad@vene.ro,sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|doc/modules/model_evaluation.rst|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/random.py|sklearn/utils/random.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py|doc/whats_new.rst|doc/whats_new.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/tests/test_common.py|sklearn/linear_model/logistic.py|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/linear_model/logistic.py|sklearn/tests/test_common.py|sklearn/tests/test_lda.py|sklearn/tests/test_lda.py|doc/whats_new.rst|sklearn/svm/base.py,67,0.009307135470527405,0,44,true,[MRG] Implements Multiclass hinge loss Implements multiclass hinge loss Fixes #3451 ,,2069,0.7738037699371677,0.0734229576008273,38892,416.3066954643628,33.194487298159004,110.61400802221537,3448,54,1473,209,travis,SaurabhJha,MechCoder,false,MechCoder,0,0,10,2,884,true,true,false,false,0,2,0,0,27,0,1
4548787,scikit-learn/scikit-learn,python,3604,1409334920,1409335826,1409335826,15,15,github,false,false,false,9,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.238636997309915,0.09729269091825993,3,larsmans@gmail.com,doc/modules/clustering.rst,3,0.0015471892728210418,0,0,true,Repeated word: the the ~ the Removed one the,,2068,0.7736943907156673,0.07323362558019597,38704,420.34415047540307,33.226539892517565,109.93695742042166,3441,54,1473,167,travis,rahiel,rahiel,true,rahiel,0,0,2,0,179,true,false,false,false,0,0,0,0,3,0,14
4547346,scikit-learn/scikit-learn,python,3603,1409326022,1409358284,1409358284,537,537,github,false,false,false,12,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.7427487454714,0.10886395510843869,0,,examples/imputation.py,0,0.0,0,1,true,[MRG] FIX: Fix the imputation example The imputation example would not run,,2067,0.7735849056603774,0.07327141382868937,38704,420.34415047540307,33.226539892517565,109.93695742042166,3439,53,1473,166,travis,MechCoder,MechCoder,true,MechCoder,25,0.8,79,41,801,true,true,false,false,7,364,27,75,306,3,11
4537079,scikit-learn/scikit-learn,python,3598,1409250632,1409335750,1409335750,1418,1418,github,false,false,false,19,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,10,0,10,4.278773266277952,0.09821396953065292,11,vlad@vene.ro,sklearn/utils/testing.py,11,0.005705394190871369,0,3,false,[MRG+1] MAINT: be robust to numpys DeprecationWarning numpy 19 raises a lot of VisibleDeprecationWarning which breaks ourtest suite,,2065,0.7738498789346246,0.07468879668049792,38704,420.34415047540307,33.226539892517565,109.93695742042166,3434,52,1472,167,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,52,0.7884615384615384,534,3,1648,true,true,false,false,22,248,27,115,95,7,1
4509264,scikit-learn/scikit-learn,python,3593,1409071479,1409073725,1409073725,37,37,github,false,false,false,15,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.643450103033964,0.10658468996053805,5,t3kcit@gmail.com,sklearn/utils/extmath.py,5,0.0024925224327018943,0,0,false,[MRG] DOC: Added npsqrt since default is squaredFalse An innocuous doc string fix in sklearnutilsextmathrow_norms,,2064,0.7737403100775194,0.07427716849451645,38704,420.34415047540307,33.226539892517565,109.93695742042166,3411,53,1470,164,travis,MechCoder,MechCoder,true,MechCoder,24,0.7916666666666666,79,41,798,true,true,false,false,7,351,25,59,306,3,15
4505762,scikit-learn/scikit-learn,python,3591,1409063775,1409408527,1409408527,5745,5745,github,false,false,false,33,3,1,23,20,0,43,0,5,0,0,5,6,4,0,0,0,0,6,6,5,0,0,584,18,849,22,21.886882615974443,0.5023868146751014,73,t3kcit@gmail.com,doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/linear_model/logistic.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/tests/test_svm.py,62,0.004466501240694789,0,12,false,[MRG+1] MAINT: Remove BaseLibLinear for LogisticRegression BaseLibLinear has very little to do with liblinear in masterThis replaces it with a private _fit_liblinear function and simplifies the recursive code paths to an extent,,2062,0.7740058195926285,0.07394540942928039,38704,420.49917321207107,33.226539892517565,109.93695742042166,3410,53,1470,166,travis,MechCoder,MechCoder,true,MechCoder,23,0.782608695652174,79,41,798,true,true,false,false,7,350,24,57,302,3,0
4499322,scikit-learn/scikit-learn,python,3587,1408863725,1410430098,1410430098,26106,26106,commits_in_master,false,false,false,44,2,2,19,8,0,27,0,4,2,0,7,9,0,0,0,2,0,7,9,0,0,0,0,0,0,0,73.34243024590737,1.685307688102424,17,valentin.haenel@gmx.de,doc/data_transforms.rst|doc/model_selection.rst|doc/modules/kernel_approximation.rst|doc/modules/pipeline.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing_targets.rst|doc/modules/unsupervised_reduction.rst|doc/data_transforms.rst|doc/model_selection.rst|doc/modules/cross_validation.rst|doc/modules/grid_search.rst|doc/modules/kernel_approximation.rst|doc/modules/pipeline.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing_targets.rst|doc/modules/unsupervised_reduction.rst,8,0.0009779951100244498,0,5,false,[MRG] DOC restructure data transformation user guide This PR restructures the data transformations narrative documentation most notably including Pipeline and FeatureUnion as per #1383 It also separates discussion of preprocessing into more appropriate sections and gives an introduction to the chapter as a whole,,2061,0.7738961669092673,0.07775061124694377,38683,419.8743634154538,33.21872657239614,109.71227671069978,3400,54,1468,176,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,92,0.6956521739130435,28,1,1944,true,true,false,false,26,454,52,327,112,6,9245
4498753,scikit-learn/scikit-learn,python,3586,1408850324,1408881482,1408881482,519,519,commits_in_master,false,false,false,18,3,3,0,8,1,9,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,13.215525372738732,0.30367450926173895,11,olivier.grisel@ensta.org,doc/install.rst|doc/install.rst|doc/install.rst,11,0.005376344086021506,0,1,false,DOC: Explicit instructions for Python3 A minor installation change that I guess would be helpful for Python3 newbies,,2060,0.7737864077669903,0.07771260997067449,38683,419.8743634154538,33.21872657239614,109.71227671069978,3399,54,1467,163,travis,MechCoder,jnothman,false,jnothman,22,0.7727272727272727,79,41,795,true,true,false,false,7,352,23,56,311,3,10
4488409,scikit-learn/scikit-learn,python,3583,1408708608,,1410544376,30596,,unknown,false,true,false,10,76,13,0,15,2,17,0,4,1,0,16,90,15,0,0,4,8,81,93,75,0,3,366,154,1891,620,107.49115753074904,2.469998247702038,95,vlad@vene.ro,sklearn/datasets/samples_generator.py|sklearn/svm/bounds.py|sklearn/svm/bounds.py|sklearn/tests/test_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|sklearn/tests/test_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_lda.py|doc/whats_new.rst|doc/datasets/index.rst|doc/sphinxext/gen_rst.py|examples/datasets/plot_random_dataset.py|examples/datasets/plot_random_multilabel_dataset.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py,53,0.0019038553069966682,0,4,true,make_multilabel_classification sparse target MultiLabelBinarizer supports sparse outputFor issue #3554,,2059,0.7741622146673143,0.08138981437410757,38683,419.8743634154538,33.21872657239614,109.71227671069978,3392,55,1466,176,travis,kashif,kashif,true,,0,0,88,44,2314,true,false,false,false,1,1,0,0,10,0,13
4487945,scikit-learn/scikit-learn,python,3582,1408703943,,1412763038,67651,,unknown,false,true,false,18,3,1,14,7,0,21,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,4,19,13,9.105286740992806,0.20922690583775044,0,,sklearn/utils/class_weight.py|sklearn/utils/tests/test_class_weight.py,0,0.0,0,2,true,[MRG] FIX for exception thrown when y does not have all classes This is a fix for #3581,,2058,0.7745383867832848,0.0811965811965812,38683,419.8743634154538,33.21872657239614,109.71227671069978,3392,55,1466,191,travis,dsullivan7,dsullivan7,true,,7,0.7142857142857143,8,19,378,true,true,false,false,1,45,2,22,16,0,14
4469077,scikit-learn/scikit-learn,python,3580,1408551067,1409575395,1409575395,17072,17072,commits_in_master,false,false,false,16,3,1,2,6,2,10,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,40,30,40,8.89393261204593,0.2043677909343709,15,t3kcit@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py,12,0.005512172714745062,1,3,false,[MRG] more intuitive behaviour for *SearchCVscore Ill be very happy to see this changed Ping @GaelVaroquaux,,2057,0.7744287797763734,0.07900780891134589,38535,418.3729077462047,33.2425068119891,109.61463604515376,3385,56,1464,171,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,91,0.6923076923076923,28,1,1940,true,true,false,false,26,463,51,340,110,4,15801
4469025,scikit-learn/scikit-learn,python,3579,1408550713,,1410805187,37574,,unknown,false,false,false,6,13,3,70,70,0,140,0,5,0,0,6,6,6,0,0,0,0,6,6,6,0,0,801,36,1521,54,51.44243263048521,1.1820616116141276,44,t3kcit@gmail.com,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py,31,0.006893382352941176,0,37,false,[WIP] Strong rules for coordinate descent ,,2056,0.7748054474708171,0.07904411764705882,38535,418.3729077462047,33.2425068119891,109.61463604515376,3385,56,1464,175,travis,MechCoder,MechCoder,true,,21,0.8095238095238095,78,41,792,true,true,false,false,8,360,25,55,313,3,3
4465308,scikit-learn/scikit-learn,python,3578,1408513899,1417319729,1417319729,146763,146763,commits_in_master,false,false,false,63,1,1,2,6,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,13,10,13,9.181098546665122,0.2109674043306898,6,t3kcit@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py,6,0.002748511223087494,0,2,false,[MRG] PLSRegression again supports 1d target plot_compare_cross_decompositionpy was broken by the change in validation helper functions: _PLS calls Y  check_array(Y) which returns Y[None :] if Y is 1d It should be Y[: None]Note that the 1d input while used in an example was not covered by the docstring (nor tested obviously) I would like to check that this is correct behaviour,,2055,0.7746958637469586,0.07924874026568941,38532,418.4054811585176,33.24509498598567,109.62317035191529,3383,56,1464,223,travis,jnothman,jnothman,true,jnothman,90,0.6888888888888889,28,1,1940,true,true,false,false,26,464,50,338,103,4,137622
4450457,scikit-learn/scikit-learn,python,3574,1408395790,,1408798182,6706,,unknown,false,false,false,73,2,2,1,5,0,6,0,4,1,0,3,4,4,0,0,1,0,3,4,4,0,0,257,0,257,0,17.909899926576237,0.4115453173535199,44,t3kcit@gmail.com,examples/classification/plot_reliability_diagram.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/ranking.py,38,0.004539264639128461,0,1,false,Reliability curves for calibration of predict_proba This PR adds the reliability_curve metric to metricsrankingpy Reliability diagrams allow checking if the predicted probabilities of a binary classifier are well calibrated The PR also contains an example comparing how well the predicted probabilities of different classifiers are calibrated A notebook giving the same example can be found under http://jmetzengithubio/2014-08-16/reliability-diagramhtmlFor some backgrounds on reliability-diagrams please refer to the paper Predicting Good Probabilities with Supervised Learning,,2054,0.7750730282375852,0.07898320472083523,38532,418.4054811585176,33.24509498598567,109.62317035191529,3377,58,1462,163,travis,jmetzen,jnothman,false,,8,0.625,11,2,1043,true,true,false,false,0,12,3,0,46,0,453
4448387,scikit-learn/scikit-learn,python,3573,1408383300,1408383696,1408383696,6,6,commits_in_master,false,false,false,10,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.039312305277818,0.09281793422086528,29,t3kcit@gmail.com,sklearn/linear_model/coordinate_descent.py,29,0.013163867453472538,0,2,false,[MRG] FIX: Memory crashes for _alpha_grid in ENetCV Fixes https://githubcom/scikit-learn/scikit-learn/issues/3563,,2053,0.77496346809547,0.07943713118474807,38532,418.4054811585176,33.24509498598567,109.62317035191529,3377,58,1462,158,travis,MechCoder,larsmans,false,larsmans,20,0.8,78,41,790,true,true,false,false,8,357,24,53,309,3,0
4446458,scikit-learn/scikit-learn,python,3572,1408369130,1408371425,1408371425,38,38,commits_in_master,false,false,false,8,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.284650169509021,0.0984554967317836,0,,examples/plot_kernel_approximation.py,0,0.0,0,0,false,Update plot_kernel_approximationpy Correct documentation typo not to note,,2052,0.7748538011695907,0.07992733878292461,38532,418.4054811585176,33.24509498598567,109.62317035191529,3376,58,1462,158,travis,AndrewWalker,jnothman,false,jnothman,0,0,20,38,1379,false,false,false,false,0,0,0,0,1,0,-1
4444520,scikit-learn/scikit-learn,python,3571,1408346286,1415750918,1415750918,123410,123410,commits_in_master,false,false,false,52,3,3,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,338,0,338,0,11.998593276757255,0.275711447095424,21,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py,21,0.009528130671506351,0,0,false,[MRG] DOC Fixes for link embedding 1 Docsscipyorg appears to be down The dev documentation therefore cannot resolve embedded documentation links in example code This PR allows at least scikit-learn-internal links to still be embedded2 The link embedding regular expression was not capturing all contexts The current expressions is more expansive,,2051,0.7747440273037542,0.07985480943738657,38532,418.4054811585176,33.24509498598567,109.62317035191529,3375,58,1462,213,travis,jnothman,jnothman,true,jnothman,89,0.6853932584269663,28,1,1938,true,true,false,false,25,458,50,326,94,5,12
4433639,scikit-learn/scikit-learn,python,3568,1408151943,1413141695,1413141695,83162,83162,commit_sha_in_comments,false,false,false,10,6,1,9,7,1,17,0,5,0,0,8,8,8,0,0,0,0,8,8,8,0,0,0,57,0,110,35.88122397495108,0.8244857257148177,65,vlad@vene.ro,sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_spectral.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/feature_extraction/tests/test_text.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_ranking.py|sklearn/preprocessing/tests/test_data.py|sklearn/tests/test_common.py,29,0.0027223230490018148,0,1,true,catch_warnings blocks more robust Explicitely calling clean_warning_registry before with block,,2050,0.7746341463414634,0.08030852994555354,38424,414.89693941286697,33.15636060795337,108.89027691026442,3362,60,1459,198,travis,Tafkas,larsmans,false,larsmans,0,0,20,24,2102,false,false,false,false,0,0,0,0,1,0,12
4425152,scikit-learn/scikit-learn,python,3564,1408074402,1409917572,1409917572,30719,30719,commits_in_master,false,false,false,33,2,1,9,11,0,20,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,43,0,46,0,4.667012743868719,0.10723952426302788,0,,sklearn/svm/bounds.py,0,0.0,0,3,false,[MRG] update sklearnsvmbounds to use recent helpers Here was some code with stale conventions If it is useful code perhaps it should be available in linear_model (and for alpha) not just in svm,,2049,0.7745241581259151,0.08001808318264014,38424,414.89693941286697,33.15636060795337,108.89027691026442,3352,60,1458,173,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,88,0.6818181818181818,28,1,1934,true,true,false,false,24,457,49,328,94,5,10
4417214,scikit-learn/scikit-learn,python,3562,1408017908,1421265755,1421265755,220797,220797,commits_in_master,false,true,false,36,6,3,9,20,0,29,0,6,1,0,1,3,2,0,0,2,0,2,4,3,0,0,496,0,562,0,23.008051202876658,0.5286834799019712,1,arnaud.v.joly@gmail.com,benchmarks/bench_mnist.py|benchmarks/bench_covertype.py|benchmarks/bench_mnist.py|benchmarks/bench_covertype.py|benchmarks/bench_mnist.py,1,0.0004526935264825713,0,5,false,[MRG] ENH add a benchmark on mnist I have extracted and improved the benchmark of https://githubcom/scikit-learn/scikit-learn/pull/3204 on mnistThis will be helpful whenever you want to bench and compare new algorithms (ELM multilayer neural network ),,2048,0.7744140625,0.08012675418741512,38424,414.89693941286697,33.15636060795337,108.89027691026442,3345,60,1458,242,travis,arjoly,amueller,false,amueller,69,0.8405797101449275,27,25,968,true,true,true,false,34,487,54,920,145,2,1
4413065,scikit-learn/scikit-learn,python,3558,1407974783,1407984933,1407984933,169,169,merged_in_comments,false,false,false,9,1,1,0,12,0,12,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,3.274993419846315,0.07525415231165872,8,t3kcit@gmail.com,sklearn/gaussian_process/gaussian_process.py,8,0.0036330608537693005,0,2,false,[FIX] G is upper-triangular so GT should be lower-triangular ,,2047,0.7743038593063019,0.08038147138964577,38556,413.47650171179583,33.042846768336965,108.51748106650068,3339,59,1457,253,travis,kaicheng,kaicheng,true,kaicheng,0,0,18,22,672,false,true,false,false,0,0,0,0,1,0,1264
4409709,scikit-learn/scikit-learn,python,3557,1407955393,1408897785,1408897785,15706,15706,merged_in_comments,false,false,false,37,7,0,5,31,0,36,0,7,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,9,84,0,0.0,0,,,0,0.0,0,25,false,[FIX] orthogonal LDA transform (conform with R) Removed an unnecessary dot product from the LDAtransform() Now it gives the same results as R on the setosa data set See discussion in #3500 for detailsRegression test included,,2046,0.7741935483870968,0.08043875685557587,38556,413.47650171179583,33.042846768336965,108.51748106650068,3332,58,1457,166,travis,kazemakase,mblondel,false,mblondel,0,0,3,0,691,false,true,false,false,1,15,0,4,2,0,5
4406516,scikit-learn/scikit-learn,python,3555,1407931918,1408013851,1408013851,1365,1365,commits_in_master,false,false,false,12,7,5,2,14,0,16,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1391,0,1401,0,23.985809974278073,0.5511558545994053,0,,benchmarks/bench_covertype.py|benchmarks/bench_covertype.py|benchmarks/bench_covertype.py|benchmarks/bench_covertype.py|benchmarks/bench_covertype.py,0,0.0,0,1,false,MAINT simplify covertype benchmark Greatly simplify the overall logic of covertype benchmark,,2044,0.7744618395303327,0.08077099586966498,38556,413.47650171179583,33.042846768336965,108.51748106650068,3328,58,1457,156,travis,arjoly,arjoly,true,arjoly,68,0.8382352941176471,27,25,967,true,true,false,false,34,478,52,913,143,2,10
4397891,scikit-learn/scikit-learn,python,3552,1407858576,1407931045,1407931045,1207,1207,commit_sha_in_comments,false,false,false,93,3,2,4,19,0,23,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,108,39,126,45,17.97564121516667,0.41305554813208456,21,olivier.grisel@ensta.org,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,14,0.006430868167202572,0,8,false,[MRG] Bugfix: Clone-safe vectorizers with custom vocabulary I ran into the issue summarized by [this gist](https://gistgithubcom/vene/b8a6e199ee764289c65e): Basically vectorizers throw away the vocabulary parameter in grid search This is because the __init__ function of vectorizers doesnt obey the rules required to make it clone-safe  I added a minimal test and fixed it but it needed some more changes to the tests: many of the tests were relying to vectvocabulary_ to exist straight after initIt still seems like the __init__ does a bit more computation than it should with respect to other parameters though,,2043,0.7743514439549681,0.08130454754248967,38548,413.01753657777317,33.0237625817163,108.46217702604545,3321,59,1456,155,travis,vene,vene,true,vene,48,0.7916666666666666,66,33,1584,true,true,false,false,2,67,3,130,8,3,12
4381818,scikit-learn/scikit-learn,python,3545,1407695811,1407766726,1407766726,1181,1181,commits_in_master,false,false,false,125,4,4,0,1,0,1,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,12,23,12,23,16.957384092918446,0.3896506412798802,6,t3kcit@gmail.com,examples/gaussian_process/gp_diabetes_dataset.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/gaussian_process.py,6,0.0004621072088724584,4,0,false,Bugfixes in gaussian_process subpackage (continued) This is a continuation of the inactive PR #2632 which fixed two issues in the gaussian_process package: * Optimum theta over multiple random-starts was not determined correctly * gp_diabetes_datasetpy crashed because it tried to access gptheta rather than gptheta_Credit for these fixes goes to  @dubourg The fixes have already been reviewed by @agramfort @jaquesgrobler and @GaelVaroquaux The only thing missing was a regression test which I add in this PR (sorry for a new PR but the old one is apparently inactive and I can not work on it) Furthermore I removed a call to scipyrand in GaussianProcess which resulted in inconsistent results for the same random_stateI hope that this is sufficient for getting this PR merged soon,,2042,0.7742409402546523,0.08364140480591498,38548,412.550586282038,32.997820898619906,108.43623534294905,3315,59,1454,157,travis,jmetzen,larsmans,false,larsmans,7,0.5714285714285714,11,2,1035,true,true,false,false,0,11,2,0,37,0,9
4370869,scikit-learn/scikit-learn,python,3542,1407522432,1407753176,1407753176,3845,3845,commit_sha_in_comments,false,false,false,7,1,1,0,9,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.693583444861356,0.10785023144900066,0,,sklearn/datasets/twenty_newsgroups.py,0,0.0,0,5,true,[MRG] Incomplete download of 20newsgroup_dataset Fixes https://githubcom/scikit-learn/scikit-learn/issues/2652,,2041,0.7741303282704557,0.0863109048723898,38548,412.550586282038,32.997820898619906,108.43623534294905,3306,60,1452,161,travis,MechCoder,arjoly,false,arjoly,19,0.7894736842105263,78,41,780,true,true,true,true,7,326,21,50,304,3,0
4358410,scikit-learn/scikit-learn,python,3539,1407415646,1407420836,1407420836,86,86,commits_in_master,false,false,false,13,1,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.326838068944471,0.0994229979020464,8,t3kcit@gmail.com,sklearn/utils/__init__.py,8,0.003754106053496011,0,0,false,[MRG] ENH faster safe_indexing for common case take is substantially faster than [],,2040,0.7740196078431373,0.08962928202721727,38546,412.57199190577495,32.999533025476055,108.44186167176879,3291,60,1451,163,travis,jnothman,jnothman,true,jnothman,87,0.6781609195402298,28,1,1927,true,true,false,false,18,421,41,318,81,4,4
4349084,scikit-learn/scikit-learn,python,3538,1407335526,1407421271,1407421271,1429,1429,commit_sha_in_comments,false,false,false,13,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.704880191922024,0.10810965814216317,1,amueller@ais.uni-bonn.de,examples/feature_selection/plot_rfe_digits.py,1,0.0004710315591144607,0,0,false,Update plot_rfe_digitspy Import statement have been moved to the beginning of the file,,2039,0.7739087788131437,0.09138012246820537,38543,412.6041045066549,33.00210154891939,108.45030225981371,3286,61,1450,163,travis,ugurthemaster,jnothman,false,jnothman,6,1.0,1,0,435,true,false,false,false,0,0,2,0,4,0,1429
4345682,scikit-learn/scikit-learn,python,3535,1407296201,1407439480,1407439480,2387,2387,commits_in_master,false,false,false,44,1,1,0,2,0,2,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.721776097738258,0.10849823996202321,10,olivier.grisel@ensta.org,sklearn/utils/fixes.py,10,0.004668534080298786,0,2,false,[MRG] FIX only use testingignore_warnings in tests Fix for #3530 I think its a good idea to avoid sklearnutilstesting except in testing code In this case its ignore_warnings did things to the global Python state which is fine in testing but uncool in production,,2038,0.7737978410206084,0.09243697478991597,38577,411.98123234051377,32.97301500894315,108.35471913316225,3280,63,1449,165,travis,jnothman,larsmans,false,larsmans,86,0.6744186046511628,28,1,1925,true,true,false,false,18,419,40,320,82,4,0
4340185,scikit-learn/scikit-learn,python,3533,1407259203,1407422177,1407422177,2716,2716,commit_sha_in_comments,false,false,false,11,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.8079753378535264,0.08750068055404031,10,t3kcit@gmail.com,sklearn/feature_selection/rfe.py,10,0.004662004662004662,0,0,false,[Trivial] Verbose message in RFE to count from 1 not 0 ,,2037,0.773686794305351,0.09463869463869463,38577,411.98123234051377,32.97301500894315,108.35471913316225,3278,63,1449,165,travis,SamStudio8,jnothman,false,jnothman,0,0,35,53,1035,false,true,false,false,0,0,0,0,2,0,12
4338994,scikit-learn/scikit-learn,python,3532,1407250635,,1413920408,111162,,unknown,false,true,false,27,2,1,1,29,0,30,0,7,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,10,0,4.353074257923788,0.10002586847653562,34,t3kcit@gmail.com,sklearn/manifold/t_sne.py,34,0.015843429636533086,0,12,false,bug fix for issue #3526 Some of the pairwise distances do not support the additional squared parameter I suggest using sqeuclidian and such whenever this is required,,2036,0.7740667976424361,0.0945945945945946,38577,411.98123234051377,32.97301500894315,108.35471913316225,3277,63,1449,212,travis,makokal,larsmans,false,,0,0,19,35,1595,false,false,false,false,1,1,0,0,1,0,11
4338751,scikit-learn/scikit-learn,python,3531,1407248287,1407328240,1407328240,1332,1332,commit_sha_in_comments,false,false,false,22,12,9,2,6,0,8,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,50,47,85,47,63.646671013760944,1.462486777523574,54,vlad@vene.ro,sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py|doc/whats_new.rst|doc/whats_new.rst|sklearn/metrics/tests/test_common.py|doc/whats_new.rst|doc/whats_new.rst|sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py|doc/whats_new.rst|sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py,46,0.004195804195804196,0,0,false,Add sample_weight parameter to metricslog_loss * Also modified binary output & multiclass tests in test_sample_weight_invariance to test for prediction inputs as probabilities,,2035,0.773955773955774,0.09463869463869463,38577,411.98123234051377,32.97301500894315,108.35471913316225,3277,63,1449,167,travis,jatinshah,arjoly,false,arjoly,1,1.0,23,6,1146,false,true,false,false,0,9,1,2,2,0,6
4328650,scikit-learn/scikit-learn,python,3528,1407160565,1407163264,1407163264,44,44,commits_in_master,false,false,false,13,2,2,0,9,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,8.803272698989922,0.2022834954373084,19,olivier.grisel@ensta.org,sklearn/utils/estimator_checks.py|sklearn/utils/estimator_checks.py,19,0.008928571428571428,0,1,false,Fix heisenbug due to addition of max_iter param in LSVC Discussion @ https://githubcom/scikit-learn/scikit-learn/pull/3501#issuecomment-50989399,,2033,0.7742252828332513,0.09539473684210527,38577,411.98123234051377,32.97301500894315,108.35471913316225,3266,63,1448,165,travis,MechCoder,jnothman,false,jnothman,18,0.7777777777777778,77,41,776,true,true,false,false,7,296,20,50,284,3,11
4326424,scikit-learn/scikit-learn,python,3527,1407133436,1407250890,1407250890,1957,1957,commits_in_master,false,false,false,36,1,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.7371363599426655,0.10885093919245713,17,olivier.grisel@ensta.org,doc/modules/model_evaluation.rst,17,0.007984969469234382,0,0,false,DOC A less-nested coverage of model evaluation http://scikit-learnorg/stable/modules/model_evaluationhtml has a section numbered 352162 This is badThis PR increases readability by removing a level of nesting It makes classification metrics etc top-level sections on the page,,2032,0.7741141732283464,0.09628933771723815,38577,411.98123234051377,32.97301500894315,108.35471913316225,3266,63,1448,167,travis,jnothman,jnothman,true,jnothman,85,0.6705882352941176,28,1,1924,true,true,false,false,18,410,37,319,78,4,224
4313987,scikit-learn/scikit-learn,python,3525,1406930993,1407252358,1407252358,5356,5356,commit_sha_in_comments,false,false,false,19,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.351392424765117,0.09998139652896917,12,michael@bommaritollc.com,sklearn/feature_extraction/text.py,12,0.0057306590257879654,0,1,true,Add UNICODE flag to recompile() in text tokenization So that the token_pattern can use \w etc in unicode mode,,2031,0.7740029542097489,0.09933142311365807,38579,412.01171621866814,32.971305632598046,108.32318100521009,3258,66,1445,169,travis,jonathanronen,jnothman,false,jnothman,0,0,14,16,845,false,false,false,false,0,0,0,0,0,0,10
4311375,scikit-learn/scikit-learn,python,3524,1406913882,,1406994113,1337,,unknown,false,false,false,140,6,6,3,4,0,7,0,4,0,0,7,7,7,0,0,0,0,7,7,7,0,0,558,113,558,113,96.01534200153147,2.2061323821053,32,t3kcit@gmail.com,sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_learning_curve.py|sklearn/learning_curve.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/feature_selection/rfe.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_learning_curve.py,16,0.003825920612147298,0,1,true,[WIP] scorer_params and sample_weight support This is me picking up the remaining commits from #1574 rebasing on master and adding a different API:Since I want to write a sample_group-aware scorer I needed an API to pass arbitrary params to scorers in cross-validation in the same way as fit_params This supersedes the need for an explicit sample_weights parameter at the cost of some duplication in the function call (fit_paramsdict(sample_weightsw) scorer_paramsdict(sample_weightsw)) Internally this saves us the need for special treatment though and allows scorers to be more powerful- [ ] Consider API change: fit_params and scorer_params starting with sample_ will be indexed by train-test splits- [ ] support fit_params and scorer_params in learning_curve and RFECV- [ ] test that scorer_params are getting appropriately indexed- [ ] proof of concept learning to rank grid search using this API,,2030,0.774384236453202,0.09947393591582974,38579,412.01171621866814,32.971305632598046,108.32318100521009,3256,66,1445,166,travis,vene,jnothman,false,,47,0.8085106382978723,66,33,1573,true,true,false,false,2,58,2,101,6,3,10
4309956,scikit-learn/scikit-learn,python,3523,1406902090,1419026657,1419026657,202076,202076,commits_in_master,false,false,false,96,228,11,90,102,0,192,0,8,1,0,3,9,3,0,0,3,1,7,11,7,0,0,2068,458,8391,1946,121.33263101703582,2.7878554746752586,5,t3kcit@gmail.com,sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|sklearn/lda.py|doc/modules/lda_qda.rst|examples/classification/plot_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|doc/modules/lda_qda.rst|examples/classification/plot_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|doc/modules/lda_qda.rst|examples/classification/plot_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|doc/modules/lda_qda.rst|examples/classification/plot_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py|doc/modules/lda_qda.rst|examples/classification/plot_lda.py|sklearn/lda.py|sklearn/tests/test_lda.py,3,0.0009583133684714902,0,27,true,New LDA class with shrinkage OK here is a new version of the LDA class with (optional) shrinkage The one in #3105 was already very messy so I thought Id start from scratch By default nothing has changed and the old SVD-based code is usedWhen using the shrinkage approach based on Ledoit Wolf shrinkage it is important that the data are standardized (centered and scaled to unit standard deviation) Therefore this really works only once #3521 gets merged tooConcerning #3500 I have not implemented this yet to avoid adding multiple features in one PR,,2029,0.7742730409068507,0.09966459032103497,38413,413.7401400567516,33.11378960247833,108.79129461380262,3254,66,1445,245,travis,cle1109,amueller,false,amueller,2,0.5,1,0,450,true,true,false,false,1,4,1,0,8,0,598
4309726,scikit-learn/scikit-learn,python,3522,1406899705,1406903067,1406903067,56,56,commits_in_master,false,false,false,13,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.920275337970876,0.11305298849034119,2,rajatkhanduja13@gmail.com,examples/tree/plot_tree_regression.py,2,0.0009587727708533077,0,0,true,Update plot_tree_regressionpy Import statements have been moved to the beginning of the file,,2028,0.7741617357001972,0.09971236816874401,38413,413.7401400567516,33.11378960247833,108.79129461380262,3254,66,1445,167,travis,ugurthemaster,arjoly,false,arjoly,5,1.0,1,0,430,true,false,false,false,0,0,1,0,2,0,12
4309395,scikit-learn/scikit-learn,python,3521,1406896643,,1407227646,5516,,unknown,false,false,false,24,2,1,6,1,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,27,0,31,0,4.6698083127816625,0.10729873123301624,2,t3kcit@gmail.com,sklearn/covariance/shrunk_covariance_.py,2,0.0009587727708533077,0,0,true,Added optional parameter to scale the data before shrinkage As discussed in #3508  I implemented an option to scale the data before shrinkage estimation,,2027,0.7745436605821411,0.09971236816874401,38410,413.66831554282743,33.0903410570164,108.7216870606613,3254,66,1445,170,travis,cle1109,cle1109,true,,1,1.0,1,0,450,true,true,false,false,1,4,0,0,7,0,128
4308751,scikit-learn/scikit-learn,python,3519,1406890788,1406893839,1406893839,50,50,commit_sha_in_comments,false,false,false,8,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.335000492085951,0.09960577027806211,13,vlad@vene.ro,sklearn/utils/testing.py,13,0.006238003838771593,0,0,true,fix six issue with module imports See #3518,,2026,0.7744323790720632,0.10028790786948176,38410,413.61624576933093,33.0903410570164,108.7216870606613,3253,66,1445,166,travis,andaag,larsmans,false,larsmans,0,0,12,2,1156,false,false,false,false,0,0,0,0,2,0,12
4301353,scikit-learn/scikit-learn,python,3515,1406827475,,1406910156,1378,,unknown,false,false,false,9,3,3,2,7,0,9,0,7,0,0,5,5,4,0,0,0,0,5,5,4,0,0,72,72,72,72,57.657966939869816,1.3248140216872368,87,vlad@vene.ro,doc/whats_new.rst|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|doc/whats_new.rst|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|doc/whats_new.rst|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,45,0.008683068017366137,0,8,false,[MRG] FIX class_weightauto on SGDClassifier Regression reported in #3485,,2025,0.7748148148148148,0.10130246020260492,38409,413.47080111432217,33.06516701814679,108.6724465620037,3247,67,1444,167,travis,ogrisel,ogrisel,true,,89,0.8539325842696629,995,123,1891,true,true,false,false,32,679,82,261,183,11,8
4301327,scikit-learn/scikit-learn,python,3514,1406827260,,1426658931,330527,,unknown,false,false,false,75,6,2,0,5,0,5,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,90,0,101,22,8.79273787631676,0.20203179275663863,12,t3kcit@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py,12,0.005791505791505791,0,0,false,[WIP] ENH faster low-memory PolynomialFeatures This is an alternative to #3512 for fast and low-memory PolynomialFeatures It takes use of the powers matrix sparsity and uses logarithmic space so that sparse matrix multiplication can be usedI have marked this WIP as:* in some cases (eg binary-valued features) a float-typed output as required for logarithmic calculations isnt ideal* we might now consider accepting some sparse input but that may be up for debate,,2024,0.775197628458498,0.10135135135135136,38409,413.47080111432217,33.06516701814679,108.6724465620037,3247,67,1444,283,travis,jnothman,jnothman,true,,84,0.6785714285714286,27,1,1920,true,true,false,false,19,391,32,300,69,4,224
4299834,scikit-learn/scikit-learn,python,3512,1406815640,,1426658951,330721,,unknown,false,true,false,67,6,2,5,16,0,21,0,7,1,0,2,4,2,0,0,1,1,2,4,2,0,0,80,0,243,0,14.646085498669773,0.3365247695416388,12,t3kcit@gmail.com,examples/polynomial_features_memory.py|examples/polynomial_features_memory.py|sklearn/preprocessing/data.py,12,0.0057859209257473485,0,6,false,Polynomial Features Memory Usage Hi guysAfter having a lot of memory issues with PolynomialsFeatures I modify the transform function in order to significantly decrease the memory consumption In the example file the memory consumption of the old and proposed transform functions are compared The results shown that for the example below the memory consumption decreases from 3455 MiB to 374 MiB without increasing the execution time,,2023,0.7755808205635195,0.10125361620057859,38409,413.47080111432217,33.06516701814679,108.6724465620037,3246,67,1444,283,travis,albahnsen,jnothman,false,,0,0,7,1,529,false,false,false,false,0,0,0,0,0,0,9
4297284,scikit-learn/scikit-learn,python,3511,1406786835,,1445733089,649104,,unknown,false,true,false,88,6,1,4,16,0,20,0,4,0,0,2,4,1,0,1,0,0,4,4,3,0,1,83,0,257,0,9.149701234944324,0.21023372420870717,8,t3kcit@gmail.com,.gitignore|sklearn/metrics/classification.py,8,0.0038517091959557053,0,2,false,Prototype for balanced accuracy score As mentioned in #3506 Im trying to implement balanced accuracy scoreNow it only supports binary classificationAlso it is currently a prototype which means I havent work on document and test However this is the first time I contribute to scikit-learn Therefore Id like to open the pull request first so that you can point out my mistake as early as possible I also want to make sure my understanding to this issue is correctIll keep working on this branch Thanks,,2022,0.7759643916913946,0.10158883004333173,38409,413.47080111432217,33.06516701814679,108.6724465620037,3242,67,1444,393,travis,lazywei,arjoly,false,,0,0,77,72,618,false,false,false,false,0,1,0,0,0,0,1
4292473,scikit-learn/scikit-learn,python,3510,1406749741,1412697619,1412697619,99131,99131,commits_in_master,false,false,false,16,10,4,0,21,1,22,0,5,0,0,2,4,2,0,0,0,0,4,4,3,0,0,20,73,28,99,36.16700704719156,0.8310134276269522,0,,sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py,0,0.0,1,1,false,FIX ensure that pipeline delegate classes_ to the estimator @ogrisel Should it be backported to 0151,,2021,0.7758535378525483,0.10164569215876089,38409,413.47080111432217,33.06516701814679,108.6724465620037,3237,67,1443,206,travis,arjoly,arjoly,true,arjoly,67,0.835820895522388,27,25,953,true,true,false,false,36,399,44,623,161,1,1
4292286,scikit-learn/scikit-learn,python,3509,1406748636,1406985224,1406985224,3943,3943,commits_in_master,false,false,false,12,3,3,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,65,4,65,21.27044404386962,0.48873340801961546,3,t3kcit@gmail.com,sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py,3,0.0014527845036319612,2,0,false,MAINT regression: random projection should work with sparse matrix Ping @amueller @ogrisel,,2020,0.7757425742574258,0.1016949152542373,38409,413.47080111432217,33.06516701814679,108.6724465620037,3237,67,1443,169,travis,arjoly,jnothman,false,jnothman,66,0.8333333333333334,27,25,953,true,true,true,false,36,399,43,623,160,1,2865
4286153,scikit-learn/scikit-learn,python,3505,1406695038,,1406714151,318,,unknown,false,false,true,1,83,83,0,0,0,0,0,1,7,0,88,95,48,1,2,7,0,88,95,48,1,2,904,538,904,538,721.5225459492323,16.578498434669015,346,vlad@vene.ro,doc/conf.py|sklearn/__init__.py|doc/Makefile|doc/README|sklearn/linear_model/tests/test_least_angle.py|continuous_integration/exclude_joblib_mp.txt|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/datasets/tests/test_samples_generator.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ransac.py|sklearn/tests/test_common.py|doc/modules/outlier_detection.rst|doc/install.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/covariance.rst|doc/modules/cross_decomposition.rst|doc/modules/decomposition.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/gaussian_process.rst|doc/modules/isotonic.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/lda_qda.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/modules/neural_networks.rst|doc/modules/outlier_detection.rst|doc/modules/random_projection.rst|doc/modules/scaling_strategies.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/sphinxext/gen_rst.py|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/sphinxext/gen_rst.py|sklearn/linear_model/tests/test_omp.py|sklearn/utils/testing.py|sklearn/decomposition/sparse_pca.py|sklearn/tests/test_common.py|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|doc/whats_new.rst|doc/whats_new.rst|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/_random.c|sklearn/utils/_random.pxd|sklearn/utils/_random.pyx|sklearn/utils/random.py|sklearn/utils/setup.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/tests/test_extmath.py|sklearn/cluster/k_means_.py|doc/install.rst|sklearn/cluster/k_means_.py|doc/install.rst|doc/install.rst|doc/whats_new.rst|doc/conf.py|doc/whats_new.rst|doc/index.rst|sklearn/tests/test_common.py|doc/whats_new.rst|sklearn/__init__.py|doc/documentation.rst|doc/themes/scikit-learn/layout.html|sklearn/tests/test_common.py|sklearn/cluster/k_means_.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|doc/support.rst|Makefile|sklearn/cluster/k_means_.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/utils/__init__.py|sklearn/tests/test_cross_validation.py|sklearn/utils/tests/test_utils.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/feature_extraction/tests/test_text.py|sklearn/utils/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/decomposition/pca.py|appveyor.yml|continuous_integration/appveyor/install.ps1|sklearn/feature_selection/tests/test_feature_select.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/utils/fixes.py|sklearn/utils/testing.py|continuous_integration/test_script.sh|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/utils/testing.py|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/linear_model/stochastic_gradient.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/semi_supervised/label_propagation.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/validation.py|sklearn/preprocessing/label.py|sklearn/feature_selection/tests/test_feature_select.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/whats_new.rst|doc/whats_new.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/fixes.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,43,0.0024295432458697765,0,0,false,015x ,,2019,0.7761267954432888,0.10252672497570457,38405,413.53990365837785,33.068610857961204,108.68376513474807,3230,67,1442,162,travis,jld23,ogrisel,false,,0,0,0,0,575,false,false,false,false,0,0,0,0,0,0,-1
4282809,scikit-learn/scikit-learn,python,3502,1406669071,1406827150,1406827150,2634,2634,commit_sha_in_comments,false,false,false,44,1,1,0,8,0,8,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.756307688496363,0.10928617547883161,43,vlad@vene.ro,doc/whats_new.rst,43,0.020904229460379193,2,0,false,[MRG] whats_newrst: missing backported fixes for 0151 Please @amueller and @GaelVaroquaux have a look at the summary of the fixes that were backported in the 015X since the 0150 releaseOnce merged to master I will backport this doc change to the 015X branch,,2018,0.77601585728444,0.10257656781720953,38405,413.53990365837785,33.068610857961204,108.68376513474807,3227,67,1442,165,travis,ogrisel,ogrisel,true,ogrisel,88,0.8522727272727273,994,123,1889,true,true,false,false,27,653,79,252,182,11,23
4281441,scikit-learn/scikit-learn,python,3501,1406660808,,1406899012,3970,,unknown,false,false,false,16,8,2,10,27,1,38,0,7,0,0,11,17,9,0,0,0,0,17,17,15,0,0,114,0,761,64,71.18434605063273,1.6356101083726748,35,t3kcit@gmail.com,sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h|doc/modules/model_evaluation.rst|doc/modules/svm.rst|sklearn/metrics/classification.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h,16,0.0,0,19,false,[FIX] Pass max_iter parameter to liblinear Fixes https://githubcom/scikit-learn/scikit-learn/issues/3499  The max_iter param is hardcoded to 1000,,2017,0.7764005949429846,0.10272638753651411,38405,413.53990365837785,33.068610857961204,108.68376513474807,3227,67,1442,167,travis,MechCoder,ogrisel,false,,17,0.8235294117647058,77,41,770,true,true,true,false,6,272,19,46,280,3,0
4279061,scikit-learn/scikit-learn,python,3497,1406645763,1406718999,1406718999,1220,1220,commit_sha_in_comments,false,false,false,15,6,1,8,17,0,25,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,10,2,20,11,8.959215476646632,0.2058548643040683,7,t3kcit@gmail.com,sklearn/metrics/classification.py|sklearn/metrics/tests/test_common.py,7,0.003401360544217687,0,3,false,Add sample_weight parameter to metricsjaccard_similarity_score #3450  - Include this metric in sample_weight test in tests/test_commonpy,,2016,0.7762896825396826,0.10252672497570457,38367,413.6106549899653,33.07529908515131,108.79140928402012,3226,67,1442,162,travis,jatinshah,arjoly,false,arjoly,0,0,22,5,1139,false,false,false,false,0,1,0,0,1,0,13
4278252,scikit-learn/scikit-learn,python,3496,1406638344,1406639133,1406639133,13,13,commits_in_master,false,false,false,36,1,1,0,1,1,2,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,15,49,15,49,14.307317509691636,0.32873873801132086,60,vlad@vene.ro,doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,41,0.0077632217370208634,0,1,false,[MRG+2] Implemented correct handling of multilabel y in cross_val_score This is a squashed and rebased version of #3128 with a resolved conflict + whats_new entryOnce travis is green I will merge and backport to 015X,,2015,0.7761786600496278,0.10237748665696264,38363,413.6537809868884,33.07874775173996,108.80275265229518,3225,67,1442,161,travis,ogrisel,ogrisel,true,ogrisel,87,0.8505747126436781,994,123,1889,true,true,false,false,27,645,73,245,178,11,10
4269567,scikit-learn/scikit-learn,python,3495,1406563654,1407329295,1407329295,12760,12760,merged_in_comments,false,false,false,75,6,3,5,17,0,22,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,395,90,424,300,26.302807126780046,0.605472662486816,19,t3kcit@gmail.com,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,14,0.006866110838646395,0,3,false,[MRG] ENH removed manual code for parallel task batching forests Forests now use the threading backend that has such a low overheadthat batching tasks together does not bring any measurableperformance benefitThis removes this boilerplate to simplify the code and make iteasier to understand and maintainFurthermore batching will soon be implemented in joblib So even formodels that use the multiprocessing backend manually batching taskswill be useless at some point,,2014,0.7760675273088381,0.10446297204512016,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,172,travis,ogrisel,arjoly,false,arjoly,86,0.8488372093023255,994,123,1888,true,true,false,true,26,613,70,231,173,11,3
4269024,scikit-learn/scikit-learn,python,3493,1406559057,1406559732,1406559732,11,11,commits_in_master,false,false,false,30,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.336869523187346,0.09983179634775598,17,olivier.grisel@ensta.org,sklearn/utils/estimator_checks.py,17,0.008337420304070623,0,0,false,[MRG] FIX revert changes introduced by mistake in previous commit If the dataset is too small stratified k-fold CV usedinternally by some classifiers such as LogisticRegressionCVraises a warning,,2013,0.7759562841530054,0.10446297204512016,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,164,travis,ogrisel,ogrisel,true,ogrisel,85,0.8470588235294118,994,123,1888,true,true,false,false,26,604,67,231,172,10,9
4268893,scikit-learn/scikit-learn,python,3492,1406557866,1406560323,1406560323,40,40,commits_in_master,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.405061888693216,0.10140153837233282,6,valentin.haenel@gmx.de,doc/modules/kernel_approximation.rst,6,0.0029426189308484553,0,0,false,Typo In particularly - In particular,,2012,0.775844930417495,0.10446297204512016,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,163,travis,furukama,ogrisel,false,ogrisel,0,0,25,26,1632,false,false,false,false,0,0,0,0,1,0,-1
4268838,scikit-learn/scikit-learn,python,3491,1406557224,1406620584,1406620584,1056,1056,commits_in_master,false,true,false,12,3,3,0,21,0,21,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,66,0,66,13.69150638966515,0.3151691952637989,13,ronald.phlypo@inria.fr,sklearn/tests/test_cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_cross_validation.py,13,0.006378802747791953,0,4,false,TST unstable warning test in test_kfold_valueerrors Will merge if travis is green,,2011,0.7757334659373446,0.10451422963689892,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,163,travis,ogrisel,jnothman,false,jnothman,84,0.8452380952380952,994,123,1888,true,true,false,false,26,601,66,231,172,9,3
4268368,scikit-learn/scikit-learn,python,3490,1406552607,1407854550,1407854550,21699,21699,merged_in_comments,false,false,false,10,40,8,72,103,0,175,0,9,0,0,7,12,6,0,0,0,0,12,12,8,0,0,1654,349,4424,687,166.1505503428097,3.8246730311361548,71,t3kcit@gmail.com,doc/modules/linear_model.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|doc/modules/linear_model.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,47,0.005405405405405406,1,48,false,Multinomial Logistic Regression Continuation of the work by @larsmanshttps://githubcom/scikit-learn/scikit-learn/pull/2814,,2010,0.7756218905472637,0.10466830466830467,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,181,travis,MechCoder,MechCoder,true,MechCoder,16,0.8125,77,41,769,true,true,true,false,6,251,18,44,279,2,1554
4268067,scikit-learn/scikit-learn,python,3489,1406549525,1406985294,1406985294,7262,7262,commits_in_master,false,false,false,90,3,3,0,9,0,9,0,4,0,0,70,70,70,0,0,0,0,70,70,70,0,0,1031,24,1031,24,312.5632746729219,7.194994688242757,315,vlad@vene.ro,doc/sphinxext/numpy_ext/docscrape_sphinx.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/cca_.py|sklearn/cross_decomposition/pls_.py|sklearn/datasets/mlcomp.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/isotonic.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/base.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/logistic.py|sklearn/linear_model/omp.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/mds.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/t_sne.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/nearest_centroid.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/classes.py|sklearn/tree/tree.py|doc/sphinxext/numpy_ext/docscrape_sphinx.py,47,0.0029469548133595285,1,1,false,[MRG] DOC fix formatting of Attributes sections in documentation The Attributes sections of class documentation are currently formatted poorly and [appear after](http://scikit-learnorg/dev/modules/generated/sklearnensembleExtraTreesClassifierhtml) See Also References etcThis PR uses the same formatting code for Attributes as for Parameters and places them together as they are conventionally in docstrings The inconsistent convention to surround attribute names with backticks was [critiqued](http://sourceforgenet/p/scikit-learn/mailman/message/30617924/) by @vene in March 2013 this PR gets rid of that inconsistency It also fixes up a few misformatted Attribute descriptions (missing space before :) and Returns descriptions (surrounded by backticks),,2009,0.7755102040816326,0.10461689587426326,38318,414.1917636619865,33.11759486403257,108.9827235241923,3216,68,1441,171,travis,jnothman,jnothman,true,jnothman,83,0.6746987951807228,27,1,1917,true,true,false,false,19,363,30,302,61,3,0
4256062,scikit-learn/scikit-learn,python,3488,1406338756,,1406570068,3855,,unknown,false,false,false,15,3,1,5,8,0,13,0,9,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.801626058344794,0.11053159050350492,5,olivier.grisel@ensta.org,doc/testimonials/images/machinalis.png|doc/testimonials/testimonials.rst,5,0.0024449877750611247,0,4,true,DOC: added Machinalis testimonial Added a testimonial of how we use scikit-learn at Machinalis (wwwmachinaliscom),,2008,0.7758964143426295,0.10562347188264058,38298,414.4080630842342,33.13488955036817,109.03963653454488,3204,67,1438,165,travis,rafacarrascosa,ogrisel,false,,2,0.5,31,3,784,false,false,false,false,0,0,0,0,1,0,608
4244086,scikit-learn/scikit-learn,python,3486,1406314937,1406654766,1406654766,5663,5663,commit_sha_in_comments,false,false,false,29,15,2,8,22,0,30,0,5,0,0,2,3,2,0,0,0,0,3,3,3,0,0,36,36,359,62,17.935930483141977,0.41287824153165703,33,vlad@vene.ro,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,29,0.014194811551639746,0,5,true,[MRG] Support unseen labels LabelBinarizer and test (Backward compatibility) Patches the updates to LabelBinarizer for sparse support which broke the case the case with unseen labels For issue #3462,,2007,0.7757847533632287,0.10621634850709741,38298,414.4080630842342,33.13488955036817,109.03963653454488,3204,68,1438,164,travis,hamsal,ogrisel,false,ogrisel,7,0.7142857142857143,4,8,573,true,true,false,false,9,99,22,85,340,0,13
4238990,scikit-learn/scikit-learn,python,3480,1406201223,1412879345,1412879345,111302,111302,commits_in_master,false,false,false,16,48,3,83,117,5,205,0,10,0,0,5,13,5,0,0,0,0,13,13,10,0,0,505,39,5037,599,42.17237477891065,0.9707809007959988,8,t3kcit@gmail.com,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/setup.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,8,0.0,0,41,false,[WIP] Implementing Averaged SGD Working on implementing averaged SGD for both classifier and regressor for #543,,2005,0.7760598503740649,0.10505450941526263,38255,413.8282577440857,33.11985361390668,109.03149915043785,3194,67,1437,206,travis,dsullivan7,agramfort,false,agramfort,5,0.8,8,19,349,true,true,true,false,0,1,0,0,3,0,43
4232175,scikit-learn/scikit-learn,python,3479,1406140513,1406142361,1406142361,30,30,commits_in_master,false,false,false,12,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,4.375741028751368,0.10072742343892674,45,manojkumarsivaraj334@gmail.com,sklearn/linear_model/logistic.py,45,0.022299306243805748,1,0,false,Improve docstring of Logistic Regression CV model @larsmans Please have a look,,2004,0.7759481037924152,0.10604558969276512,38255,413.8282577440857,33.11985361390668,109.03149915043785,3186,67,1436,164,travis,MechCoder,larsmans,false,larsmans,15,0.8,77,41,764,true,true,false,false,5,233,20,48,289,2,20
4231432,scikit-learn/scikit-learn,python,3477,1406136801,1406139904,1406139904,51,51,commits_in_master,false,false,false,9,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.3787189005087965,0.10079585710491834,19,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py,19,0.009419930589985127,0,0,false,Docbuild fixes #3475 Fixes bug in new repo docbuilding,,2003,0.7758362456315526,0.10609816559246406,38255,413.8282577440857,33.11985361390668,109.03149915043785,3186,67,1436,164,travis,kastnerkyle,kastnerkyle,true,kastnerkyle,9,0.7777777777777778,224,66,853,true,false,false,false,5,122,6,11,52,0,2
4231070,scikit-learn/scikit-learn,python,3476,1406134921,,1406136231,21,,unknown,false,false,false,20,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.343096215907888,0.10007684871146509,13,vlad@vene.ro,sklearn/utils/multiclass.py,13,0.006458022851465474,0,0,false,FIX When passing a multilabel-indicator to type_of_target(y) that isnt  a numpy array it doesnt type it as a multilabel-indicator,,2002,0.7762237762237763,0.10630899155489319,37920,417.87974683544303,33.46518987341772,109.88924050632912,3186,67,1436,165,travis,freud14,jnothman,false,,0,0,11,12,1028,false,false,false,false,0,0,0,0,0,0,17
4224716,scikit-learn/scikit-learn,python,3474,1406074309,1430358507,1430358507,404736,404736,merged_in_comments,false,true,false,20,26,5,81,16,0,97,0,6,0,0,4,4,3,0,0,0,0,4,4,3,0,0,1297,372,1487,381,81.09081794053688,1.8685365811302466,21,t3kcit@gmail.com,doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py|doc/modules/model_evaluation.rst|sklearn/metrics/regression.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_regression.py,16,0.002491280518186348,0,6,false,[MRG] Multi-output scoring or 2493 contd for real This time taking into account the refactoring of metricspy into several files,,2001,0.776111944027986,0.10662680617837568,37920,417.87974683544303,33.46518987341772,109.88924050632912,3180,66,1435,316,travis,eickenberg,amueller,false,amueller,12,0.5833333333333334,11,9,929,true,true,false,false,1,23,6,19,76,0,637
4219709,scikit-learn/scikit-learn,python,3473,1406044145,,1425339371,321587,,unknown,false,true,false,34,1,1,0,9,0,9,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.712718365845906,0.10847407661129821,10,arnaud4567@gmail.com,sklearn/utils/estimator_checks.py,10,0.005030181086519115,0,1,false,[MRG] MAINT more OMP skip on travis Fix the heisen failure observed in #3471 + other potential failuresMaybe we should refactor the estimator checks to handle those skips in a more centralized way,,2000,0.7765,0.10764587525150905,37963,418.93422543002396,33.558991649764245,110.02818533835575,3175,66,1435,289,travis,ogrisel,ogrisel,true,,83,0.8554216867469879,993,123,1882,true,true,false,false,24,537,61,208,157,9,2
4219539,scikit-learn/scikit-learn,python,3472,1406042915,1406045029,1406045029,35,35,commits_in_master,false,false,false,13,1,1,0,1,0,1,0,2,0,0,6,6,6,0,0,0,0,6,6,6,0,0,313,100,313,100,27.897808811855008,0.6421323778386504,41,t3kcit@gmail.com,sklearn/metrics/__init__.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_ranking.py,34,0.0030181086519114686,0,0,false,MAINT move log_loss and hinge_loss to the classification metrics Aim to fix https://githubcom/scikit-learn/scikit-learn/issues/3468,,1999,0.7763881940970485,0.10764587525150905,37963,418.93422543002396,33.558991649764245,110.02818533835575,3175,66,1435,168,travis,arjoly,agramfort,false,agramfort,65,0.8307692307692308,27,25,945,true,true,false,false,35,379,42,524,168,2,10
4218284,scikit-learn/scikit-learn,python,3471,1406033120,1406299883,1406299883,4446,4446,commit_sha_in_comments,false,false,false,19,2,2,0,7,0,7,0,4,0,0,5,5,5,0,0,0,0,5,5,5,0,0,6,24,6,24,22.05170100452631,0.5075707029371359,28,vlad@vene.ro,sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_score_objects.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/utils/fixes.py|sklearn/utils/testing.py,11,0.0025188916876574307,0,0,false,Fix np19 Some numpy 19 stuff I had forgotten to push :(Sorry working while traveling has side effects,,1998,0.7762762762762763,0.10730478589420656,37963,418.93422543002396,33.558991649764245,110.02818533835575,3175,66,1435,170,travis,GaelVaroquaux,ogrisel,false,ogrisel,51,0.7843137254901961,526,3,1611,true,true,true,true,19,268,28,133,95,9,10
4217962,scikit-learn/scikit-learn,python,3470,1406030384,1406032093,1406032093,28,28,commits_in_master,false,false,false,27,1,1,0,5,0,5,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,3.853045577829079,0.08868668846288673,7,t3kcit@gmail.com,sklearn/decomposition/pca.py,7,0.0035317860746720484,0,0,false,[MRG] better RandomizedPCA sparse deprecation Better deprecation warning for the problem reported on #3469 This fix should be backported in the 015X branch for inclusion in 0151,,1997,0.7761642463695543,0.10746720484359233,37963,418.93422543002396,33.558991649764245,110.02818533835575,3175,66,1435,166,travis,ogrisel,ogrisel,true,ogrisel,82,0.8536585365853658,993,123,1882,true,true,false,false,24,533,59,208,154,9,1
4214430,scikit-learn/scikit-learn,python,3465,1405989946,1406201625,1406201625,3527,3527,commits_in_master,false,false,false,10,1,1,0,3,0,3,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.793054471705823,0.11032317136382658,6,larsmans@gmail.com,doc/modules/outlier_detection.rst,6,0.003022670025188917,0,0,false,Update outlier_detectionrst Add reference for the original One-class SVM paper,,1996,0.7760521042084169,0.10680100755667506,37963,418.93422543002396,33.558991649764245,110.02818533835575,3174,66,1434,169,travis,pvnguyen,agramfort,false,agramfort,0,0,4,4,1303,false,true,false,false,0,0,0,0,1,0,589
4212030,scikit-learn/scikit-learn,python,3463,1405968279,,1407328369,22668,,unknown,false,false,false,4,1,1,4,0,0,4,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.60530858336064,0.1060017679800282,6,t3kcit@gmail.com,sklearn/metrics/ranking.py,6,0.0030472320975114273,0,0,false,add sample_weights to log_loss ,,1994,0.7768304914744233,0.10766886744540376,37963,418.93422543002396,33.558991649764245,110.02818533835575,3174,66,1434,179,travis,abhishekkrthakur,arjoly,false,,2,1.0,97,47,985,false,true,false,false,0,3,1,0,0,0,11
4208760,scikit-learn/scikit-learn,python,3460,1405941861,1405961961,1405961961,335,335,commits_in_master,false,false,false,28,1,1,0,5,0,5,0,5,1,0,1,2,1,0,0,1,0,1,2,1,0,0,8,0,8,0,8.815228915287822,0.20331958728349508,17,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py|examples/neural_networks/README.txt,17,0.00874485596707819,1,4,false,DOC make neural networks example appear Also make the absence of a README fail doc compilation@gaelvaroquaux are you still sure you dont want to backport doc fixes,,1993,0.7767185148018063,0.10905349794238683,37915,419.4645918501912,33.57510220229461,110.14110510352103,3171,67,1434,164,travis,jnothman,ogrisel,false,ogrisel,82,0.6707317073170732,27,1,1910,true,true,false,false,18,351,28,308,55,3,178
4208678,scikit-learn/scikit-learn,python,3459,1405941040,,1406149372,3472,,unknown,false,false,false,39,2,1,1,18,0,19,0,7,1,0,2,3,3,0,0,1,0,2,3,3,0,0,244,0,254,0,14.695481449309629,0.33894516545386116,8,olivier.grisel@ensta.org,sklearn/externals/copy_joblib.sh|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/embedded_joblib_init.py,6,0.00102880658436214,0,3,false,[MRG] make it possible to use the system joblib Set export SKLEARN_SYSTEM_JOBLIB1 in your env to use the development version of joblib installed in your python modules (for instance using pip install -e joblib_source_folder) instead of the embedded copy,,1992,0.7771084337349398,0.10905349794238683,37915,419.4645918501912,33.57510220229461,110.14110510352103,3171,67,1434,169,travis,ogrisel,ogrisel,true,,81,0.8641975308641975,993,123,1881,true,true,false,false,23,505,56,199,151,9,3
4205667,scikit-learn/scikit-learn,python,3456,1405892938,,1406074352,3023,,unknown,false,false,false,47,2,1,0,3,0,3,0,2,0,0,2,3,2,0,0,0,0,3,3,2,0,0,249,71,249,71,9.36152079266432,0.2161991885150262,45,olivier.grisel@ensta.org,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,34,0.01752577319587629,1,1,false,[WIP] PR2493 contd (multiple output scoring) As discussed here are @MechCoder s changed from PR 2493 played onto master with necessary adjustments made (sample weights werent present at the time)https://githubcom/scikit-learn/scikit-learn/pull/2493This PR is WIP because I havent added the doc changes yet Otherwise everything is done,,1991,0.7774987443495731,0.10927835051546392,37915,419.4645918501912,33.57510220229461,110.14110510352103,3168,68,1433,170,travis,eickenberg,eickenberg,true,,11,0.6363636363636364,11,9,927,true,true,false,false,1,22,5,19,67,0,229
4204032,scikit-learn/scikit-learn,python,3454,1405870903,1406493801,1406493801,10381,10381,commits_in_master,false,false,false,6,10,3,8,23,0,31,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,127,0,193,0,14.366029000638363,0.33178403745045926,1,amueller@ais.uni-bonn.de,examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_confusion_matrix.py,1,0.0005192107995846313,0,4,false,[MRG] Modified the confusion matrix example ,,1990,0.7773869346733668,0.11007268951194185,37943,420.07748464802467,33.603036133147086,110.63964367604038,3167,68,1433,172,travis,ldirer,ogrisel,false,ogrisel,4,0.75,0,3,450,true,true,false,false,2,12,4,3,28,0,2
4203435,scikit-learn/scikit-learn,python,3447,1405859140,1405875110,1405875110,266,266,commits_in_master,false,false,false,16,7,1,21,12,0,33,0,4,3,10,93,112,103,0,0,3,10,99,112,104,0,0,6526,3362,10856,4112,414.76323380857195,9.578974140515932,360,vlad@vene.ro,doc/developers/index.rst|doc/developers/utilities.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/base.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_validation.py|sklearn/datasets/samples_generator.py|sklearn/datasets/svmlight_format.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/partial_dependence.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_selection/base.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_base.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/variance_threshold.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/isotonic.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/learning_curve.py|sklearn/linear_model/base.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ransac.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/mds.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/t_sne.py|sklearn/metrics/__init__.py|sklearn/metrics/base.py|sklearn/metrics/classification.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/__init__.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/pairwise.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_regression.py|sklearn/metrics/tests/test_score_objects.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/base.py|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/classification.py|sklearn/neighbors/kd_tree.c|sklearn/neighbors/kde.py|sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/regression.py|sklearn/neural_network/rbm.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/setup.py|sklearn/svm/base.py|sklearn/tests/test_multiclass.py|sklearn/tree/tree.py|sklearn/utils/__init__.py|sklearn/utils/estimator_checks.py|sklearn/utils/extmath.py|sklearn/utils/mocking.py|sklearn/utils/multiclass.py|sklearn/utils/stats.py|sklearn/utils/tests/test_stats.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,37,0.0015568240788790867,0,0,false,[WIP] remove check_arrays stuff and old input validation Follow-up on #3443 removing all the old stuff,,1989,0.7772750125691302,0.11001556824078879,37943,420.07748464802467,33.603036133147086,110.63964367604038,3167,68,1433,165,travis,amueller,amueller,true,amueller,207,0.855072463768116,936,39,1367,true,true,false,false,46,146,25,85,40,3,9
4200920,scikit-learn/scikit-learn,python,3446,1405806060,1405821465,1405821465,256,256,commit_sha_in_comments,false,false,false,11,1,1,0,11,0,11,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,18,0,18,8.394255861043835,0.19386935362309718,4,vlad@vene.ro,sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_score_objects.py,2,0.0010362694300518134,0,0,false,TST: fix tests on numpy 19b2 The name says it all,,1988,0.7771629778672032,0.11191709844559586,37920,418.3544303797469,33.59704641350211,110.12658227848101,3164,68,1432,165,travis,GaelVaroquaux,ogrisel,false,ogrisel,50,0.78,525,3,1608,true,true,true,true,20,270,27,137,94,12,11
4199861,scikit-learn/scikit-learn,python,3445,1405788307,1405804313,1405804313,266,266,commits_in_master,false,false,false,10,3,1,3,4,0,7,0,3,0,2,4,7,8,0,0,0,2,5,7,10,0,0,15,6,23,10,16.778380406867214,0.39007548281562776,4,olivier.grisel@ensta.org,sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/metrics/__init__.py|sklearn/metrics/bicluster.py|sklearn/metrics/cluster/__init__.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/tests/__init__.py|sklearn/metrics/tests/test_bicluster.py|sklearn/setup.py,2,0.0,0,0,false,[WIP] MAINT flatten metrics module and avoid nested bicluster module ,,1987,0.7770508303975843,0.11312921639854696,37866,410.68504727195904,33.486505044102884,109.5177732002324,3162,69,1432,171,travis,arjoly,arjoly,true,arjoly,64,0.828125,27,25,942,true,true,false,false,31,303,37,411,173,4,10
4199763,scikit-learn/scikit-learn,python,3444,1405786502,1405789276,1405789276,46,46,commits_in_master,false,false,false,22,2,1,4,6,0,10,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,0,21,0,8.552112108410903,0.19882546341698049,15,vlad@vene.ro,sklearn/base.py|sklearn/multiclass.py,12,0.006227296315516347,0,0,false,ENH + DOC set a default scorer in the multiclass module follow the discussion at the sprintand intend to fix https://githubcom/scikit-learn/scikit-learn/issues/1979,,1986,0.7769385699899295,0.11312921639854696,37866,410.68504727195904,33.486505044102884,109.5177732002324,3162,69,1432,171,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,63,0.8253968253968254,27,25,942,true,true,true,false,31,301,36,411,173,4,1
4199718,scikit-learn/scikit-learn,python,3443,1405785624,1405857098,1405857098,1191,1191,commits_in_master,false,false,false,21,16,4,55,14,0,69,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,468,106,762,402,30.79396007483495,0.7159194482847723,17,olivier.grisel@ensta.org,sklearn/utils/validation.py|sklearn/utils/validation.py|sklearn/utils/validation.py|sklearn/feature_extraction/image.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,9,0.002596053997923157,0,0,false,[WIP] Input validation refactoring Refactor input validation to make it more consistentTodo* [ ] Tests* [ ] docstrings,,1985,0.7768261964735517,0.11318795430944964,37866,410.68504727195904,33.486505044102884,109.5177732002324,3162,69,1432,169,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,206,0.8543689320388349,936,39,1366,true,true,true,false,45,138,24,69,40,3,36
4199649,scikit-learn/scikit-learn,python,3442,1405784543,1405804065,1405804065,325,325,commits_in_master,false,false,false,24,3,1,3,8,0,11,0,4,7,1,3,14,12,0,0,8,1,6,15,13,0,0,2868,3043,2913,3043,45.942273405346505,1.0680979954956353,33,olivier.grisel@ensta.org,sklearn/metrics/__init__.py|sklearn/metrics/base.py|sklearn/metrics/classification.py|sklearn/metrics/pairwise.py|sklearn/metrics/ranking.py|sklearn/metrics/regression.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_ranking.py|sklearn/metrics/tests/test_regression.py,28,0.0,0,7,false,MAINT split sklearn/metrics/metricspy As the title saidThis is a bit brutal but file are of more manageable sizeIt intends to fix https://githubcom/scikit-learn/scikit-learn/issues/3425,,1984,0.7767137096774194,0.11318795430944964,37866,410.68504727195904,33.486505044102884,109.5177732002324,3162,69,1432,171,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,62,0.8225806451612904,27,25,942,true,true,true,false,31,296,34,410,173,4,5
4199182,scikit-learn/scikit-learn,python,3441,1405775287,1406134587,1406134587,5988,5988,commits_in_master,false,false,false,22,5,3,0,16,0,16,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,16,0,32,0,14.379184964832586,0.3342972401119023,11,olivier.grisel@ensta.org,continuous_integration/test_script.sh|continuous_integration/test_script.sh|continuous_integration/test_script.sh,11,0.005720228809152366,0,1,false,MAINT disable verbose tests on travis Try to disable verbose tests to see if the travis build report is more user friendly,,1983,0.7766011094301564,0.11336453458138325,37807,411.1407940328511,33.53876266299891,109.68868199010765,3161,70,1432,177,travis,ogrisel,ogrisel,true,ogrisel,80,0.8625,991,123,1879,true,true,false,false,23,488,55,189,151,9,13
4198979,scikit-learn/scikit-learn,python,3439,1405770850,1405772607,1405772607,29,29,commits_in_master,false,false,false,6,3,2,3,5,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,119,0,125,0,8.831762700361551,0.2057663602007735,6,arnaud4567@gmail.com,sklearn/utils/estimator_checks.py|sklearn/utils/estimator_checks.py,6,0.003126628452318916,2,0,false,[MRG] Simplify test commons Ping @amueller @kylekatsner,,1982,0.7764883955600403,0.11360083376758728,37747,408.9331602511458,33.53908919914166,108.32648952234615,3161,70,1432,171,travis,arjoly,arjoly,true,arjoly,61,0.819672131147541,27,25,942,true,true,false,false,30,285,29,408,170,4,0
4196850,scikit-learn/scikit-learn,python,3438,1405732480,1406656690,1406656690,15403,15403,commits_in_master,false,false,false,24,98,15,154,69,0,223,0,5,0,0,7,13,7,0,0,0,0,13,13,13,0,0,391,143,2616,939,86.28570376108578,2.0103218247169483,13,olivier.grisel@ensta.org,sklearn/neighbors/tests/test_neighbors.py|sklearn/utils/sparsefuncs_fast.c|sklearn/neighbors/classification.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/utils/sparsefuncs_fast.pyx|sklearn/neighbors/classification.py|sklearn/utils/sparsefuncs_fast.pyx|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/classification.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/utils/sparsefuncs_fast.pyx,5,0.0010465724751439038,0,13,true,[WIP] Sparse Target Dummy Classifier - [x] Test a sparse multioutput case- [x] Fit with sparse target- [ ] Predict sparse target,,1981,0.7763755678950025,0.1140763997906855,37747,408.9331602511458,33.53908919914166,108.32648952234615,3160,70,1431,194,travis,hamsal,hamsal,true,hamsal,6,0.6666666666666666,4,8,566,true,true,false,false,5,88,21,81,308,0,924
4194098,scikit-learn/scikit-learn,python,3434,1405712330,1405768994,1405768994,944,944,commits_in_master,false,false,false,7,5,3,2,4,0,6,0,4,0,0,6,7,6,0,0,0,0,7,7,7,0,0,33,1,79,13,41.58019221325863,0.9687403913408006,9,olivier.grisel@ensta.org,sklearn/cluster/affinity_propagation_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/utils/estimator_checks.py|sklearn/ensemble/gradient_boosting.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py|sklearn/ensemble/gradient_boosting.py|sklearn/svm/base.py|sklearn/utils/estimator_checks.py,4,0.0005224660397074191,0,0,true,[MRG] Stricter decision function test Closes #1490,,1979,0.7766548762001011,0.11389759665621735,37745,408.76937342694396,33.540866339912576,108.27924228374619,3159,70,1431,173,travis,amueller,amueller,true,amueller,205,0.8536585365853658,936,39,1365,true,true,false,false,44,126,21,67,38,3,4
4193946,scikit-learn/scikit-learn,python,3433,1405711536,1405712777,1405712777,20,20,commits_in_master,false,false,false,47,2,2,0,3,0,3,0,3,0,0,7,7,7,0,0,0,0,7,7,7,0,0,8,18,8,18,35.1631193041963,0.8192346437650296,14,olivier.grisel@ensta.org,sklearn/feature_extraction/image.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/feature_extraction/image.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/locally_linear.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_shortest_path.py,6,0.0010449320794148381,0,0,true,[MRG] Get rid of DeprecationWarning: using a non-integer number instead of an integer when running the tests This addresses #3398 As far as I could see I removed all of them except a couple in hmmpy which I thought wasnt worth touching since it is going away,,1978,0.7765419615773509,0.11389759665621735,37745,408.76937342694396,33.540866339912576,108.27924228374619,3159,70,1431,171,travis,lesteve,arjoly,false,arjoly,2,1.0,4,0,814,true,false,false,false,0,1,2,1,6,0,11
4192986,scikit-learn/scikit-learn,python,3432,1405705791,1405707489,1405707489,28,28,commits_in_master,false,false,false,11,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,8,4,8,8.378009710438159,0.19519259629574584,15,vlad@vene.ro,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,10,0.005227391531625719,0,1,true,[MRG] Fix tiebreaking in OvO that I messed up Closes #2360,,1977,0.776428932726353,0.11447987454260324,37743,408.92350899504544,33.54264366902472,108.2849799962907,3158,70,1431,173,travis,amueller,amueller,true,amueller,204,0.8529411764705882,936,39,1365,true,true,false,false,12,106,17,67,37,3,3
4191917,scikit-learn/scikit-learn,python,3430,1405698529,1405712668,1405712668,235,235,commits_in_master,false,false,false,10,5,4,1,3,0,4,0,3,0,0,6,6,6,0,0,0,0,6,6,6,0,0,174,38,183,40,38.9058213403315,0.906435841077651,26,olivier.grisel@ensta.org,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/decomposition/nmf.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/utils/estimator_checks.py,21,0.0010465724751439038,0,1,true,[MRG] Test list input Adds test for list input everywhere,,1976,0.7763157894736842,0.11459968602825746,37743,408.92350899504544,33.54264366902472,108.2849799962907,3157,70,1431,174,travis,amueller,arjoly,false,arjoly,203,0.8522167487684729,936,39,1365,true,true,false,true,8,100,16,64,36,2,18
4191632,scikit-learn/scikit-learn,python,3428,1405696065,1405775894,1405775894,1330,1330,commit_sha_in_comments,false,false,false,21,5,5,1,25,0,26,0,5,0,0,4,4,2,0,0,0,0,4,4,2,0,0,1311,155,1311,155,91.68980424974501,2.1384441204935594,55,vlad@vene.ro,doc/modules/classes.rst|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/modules/classes.rst|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/modules/classes.rst|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/modules/classes.rst|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|doc/modules/classes.rst|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,34,0.006813417190775681,0,7,true,MAINT deprecate fit_ovr fit_ovo fit_ecoc predict_ovr predict_ovo pr edict_ecoc and predict_proba_ovrAs the title said :-)It will fix issue https://githubcom/scikit-learn/scikit-learn/issues/3424,,1974,0.776595744680851,0.11477987421383648,37695,414.35203607905555,33.58535614803024,109.00649953574745,3156,70,1431,175,travis,arjoly,ogrisel,false,ogrisel,60,0.8166666666666667,27,25,941,true,true,true,false,23,260,25,331,174,4,11
4191055,scikit-learn/scikit-learn,python,3427,1405690640,1405707774,1405707774,285,285,commit_sha_in_comments,false,false,false,11,8,7,0,5,0,5,0,4,1,0,4,5,4,0,0,1,0,4,5,4,0,0,1212,1219,1268,1232,53.27449411350498,1.242498019029477,32,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/utils/testing.py|sklearn/utils/estimator_checks.py|sklearn/decomposition/nmf.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/decomposition/nmf.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,20,0.005263157894736842,0,0,true,[MRG] Speed up common tests Faster tests on top of #3415,,1973,0.7764825139381653,0.11526315789473685,37697,414.33005278934667,33.58357428973128,109.00071623736638,3154,70,1431,174,travis,amueller,amueller,true,amueller,202,0.8514851485148515,936,39,1365,true,true,false,false,8,97,14,63,34,2,10
4190875,scikit-learn/scikit-learn,python,3426,1405689364,1410174607,1410174607,74754,74754,commits_in_master,false,false,false,15,48,6,15,12,0,27,0,6,0,0,2,115,2,0,0,15,1,114,130,119,1,1,97,140,8738,5041,40.40042858901218,0.9422417486093881,6,olivier.grisel@ensta.org,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py,5,0.0026301946344029457,0,3,true,[MRG] Quantile dummy Added quantile strategy in dummy classifierAdded quantile strategy tests in test_dummy,,1972,0.776369168356998,0.1162546028406102,37697,414.33005278934667,33.58357428973128,109.00071623736638,3154,70,1431,205,travis,RolT,RolT,true,RolT,0,0,1,0,80,true,false,false,false,0,0,0,0,1,0,0
4190476,scikit-learn/scikit-learn,python,3423,1405686147,1405715246,1405715246,484,484,merged_in_comments,false,false,false,46,8,2,2,15,0,17,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,11,27,47,12.79120997988596,0.29832051371133766,0,,sklearn/ensemble/base.py|sklearn/ensemble/base.py|sklearn/ensemble/tests/test_base.py,0,0.0,0,0,true,FIX: Raise error when n_estimators  0 in ensemble models It is possible to do:clf  ensembleRandomForestClassifier(n_estimators0)then clffit(X y) raises a division by zero error0141 raised a ValueError behavior changed with PR: ENH bagging meta-estimator74d395215bbd2009dc922967b41c43f5469942ceThis PR just resets the 0141 behavior,,1971,0.776255707762557,0.11656118143459916,37663,419.43021002044446,34.11836550460664,110.0549611024082,3154,69,1431,174,travis,ldirer,amueller,false,amueller,3,0.6666666666666666,0,3,448,true,true,false,false,1,5,3,3,26,0,1
4190308,scikit-learn/scikit-learn,python,3422,1405684753,1451335292,1451335292,760842,760842,merged_in_comments,false,true,false,52,2,1,3,8,0,11,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,10,0,4.467452617133646,0.10419129713452263,29,alexandre.gramfort@m4x.org,sklearn/manifold/t_sne.py,29,0.015335801163405606,0,3,true,Kullback-Leibler divergence as a parameter of TSNE As in [1] I think it may be useful for the user to have access to the variable error (Kullback-Leibler divergence AFAIC) In fact one can choose between many t-SNE embedding on the same dataset by looking at the error [2][1] http://datasciencestackexchangecom/questions/762/t-sne-python-implementation-kullback-leibler-divergence/[2] http://homepagetudelftnl/19j49/t-SNEhtml,,1970,0.7761421319796954,0.11686938127974617,37663,419.43021002044446,34.11836550460664,110.0549611024082,3154,69,1431,480,travis,joker0x5F5F,ogrisel,false,ogrisel,0,0,1,4,682,false,false,false,false,0,0,0,0,2,0,14
4189863,scikit-learn/scikit-learn,python,3418,1405680834,1405682642,1405682642,30,30,commits_in_master,false,false,false,14,1,1,0,4,0,4,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,25,6,25,9.217688745034149,0.21520244628977966,3,olivier.grisel@ensta.org,sklearn/dummy.py|sklearn/tests/test_dummy.py,2,0.0010615711252653928,0,0,true,ENH add sample_weight support to dummy classifier Add sample_weight support in the Dummy classifier,,1969,0.7760284408329101,0.1173036093418259,37620,419.2450824029771,34.0776182881446,110.07442849548113,3154,69,1431,171,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,59,0.8135593220338984,27,25,941,true,true,true,false,21,246,23,320,171,4,11
4184025,scikit-learn/scikit-learn,python,3416,1405625261,1405688838,1405688838,1059,1059,merged_in_comments,false,false,false,33,2,2,3,9,0,12,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,44,15,44,15,25.356514254660162,0.5919904704879814,4,olivier.grisel@ensta.org,sklearn/linear_model/logistic.py|sklearn/svm/base.py|sklearn/svm/tests/test_sparse.py|sklearn/linear_model/logistic.py|sklearn/svm/base.py|sklearn/svm/tests/test_sparse.py,3,0.0005288207297726071,0,5,false,FIX: Remove raw_coef_ attribute that eats up memory I reproduce the same behaviour It is even more visible in larger datasets like 20_newsgroupWith raw_coef : 40 MBWithout: 20 MBFixes https://githubcom/scikit-learn/scikit-learn/issues/3413,,1967,0.7763091001525165,0.12004230565838181,37620,419.2450824029771,34.0776182881446,110.07442849548113,3149,72,1430,172,travis,MechCoder,agramfort,false,agramfort,14,0.7857142857142857,77,41,758,true,true,true,false,2,192,19,47,283,2,0
4183550,scikit-learn/scikit-learn,python,3415,1405622698,1405697708,1405697708,1250,1250,merged_in_comments,false,false,false,23,9,5,11,8,0,19,0,3,1,0,2,4,2,0,0,1,0,3,4,3,0,0,935,946,3043,3594,45.554935792298195,1.0491138929745376,18,olivier.grisel@ensta.org,sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py|sklearn/tests/test_common.py|sklearn/utils/estimator_checks.py,18,0.009544008483563097,0,1,false,[WIP] Test common refactor This actually makes the tests faster and hopefully it will make the actual testing file way better to maintain,,1966,0.7761953204476093,0.12036055143160128,37621,447.83498577921904,35.565242816512054,113.97889476622099,3149,72,1430,177,travis,amueller,amueller,true,amueller,201,0.8507462686567164,935,39,1364,true,true,false,false,8,94,12,63,28,2,1
4183514,scikit-learn/scikit-learn,python,3414,1405622466,1405623457,1405623457,16,16,commits_in_master,false,false,false,24,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,3.9653658217814165,0.09132095791605999,7,olivier.grisel@ensta.org,sklearn/linear_model/omp.py,7,0.003711558854718982,0,0,false,[MRG] Tackle remaining comments in #3411 A few comments were not tackled before #3411 was merged Now is the pull request to do that,,1965,0.7760814249363868,0.12036055143160128,37621,447.83498577921904,35.565242816512054,113.97889476622099,3149,72,1430,170,travis,lesteve,GaelVaroquaux,false,GaelVaroquaux,1,1.0,4,0,813,true,false,false,false,0,1,1,1,5,0,16
4182605,scikit-learn/scikit-learn,python,3412,1405617179,1405619686,1405619686,41,41,commits_in_master,false,false,false,155,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.841650517559936,0.11150396608442892,2,rajatkhanduja13@gmail.com,examples/linear_model/plot_logistic_l1_l2_sparsity.py,2,0.0010655301012253596,0,0,false,use more interesting range for C in logistic l1 l2 example Before this change output is (http://scikit-learnorg/dev/auto_examples/linear_model/plot_logistic_l1_l2_sparsityhtml#example-linear-model-plot-logistic-l1-l2-sparsity-py):C10Sparsity with L1 penalty: 625%score with L1 penalty: 09104Sparsity with L2 penalty: 469%score with L2 penalty: 09093C100Sparsity with L1 penalty: 625%score with L1 penalty: 09098Sparsity with L2 penalty: 469%score with L2 penalty: 09098C1000Sparsity with L1 penalty: 469%score with L1 penalty: 09098Sparsity with L2 penalty: 469%score with L2 penalty: 09098With this change output is:C10000Sparsity with L1 penalty: 625%score with L1 penalty: 09110Sparsity with L2 penalty: 469%score with L2 penalty: 09098C100Sparsity with L1 penalty: 938%score with L1 penalty: 09104Sparsity with L2 penalty: 469%score with L2 penalty: 09093C001Sparsity with L1 penalty: 8594%score with L1 penalty: 08625Sparsity with L2 penalty: 469%score with L2 penalty: 08915And here is the new image:[l1_l2](https://cloudgithubusercontentcom/assets/1739/3613943/6d27c668-0dbc-11e4-95d1-0c508266e130png),,1964,0.7759674134419552,0.12093766648907832,37682,447.34886683297066,35.534207313836845,113.87399819542487,3146,72,1430,170,travis,brentp,arjoly,false,arjoly,0,0,258,75,2330,false,false,false,false,0,0,0,0,0,0,13
4182204,scikit-learn/scikit-learn,python,3411,1405614535,1405619850,1405619850,88,88,commits_in_master,false,false,false,53,8,7,3,6,1,10,1,5,0,0,12,12,12,0,0,0,0,12,12,12,0,0,199,96,216,96,66.42880403010871,1.5298657110293425,36,ronald.phlypo@inria.fr,sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/grid_search.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/scorer.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/linear_model/tests/test_omp.py|sklearn/manifold/spectral_embedding_.py|sklearn/linear_model/ridge.py|sklearn/linear_model/omp.py|sklearn/linear_model/omp.py|sklearn/linear_model/omp.py,10,0.002663825253063399,0,0,false,[MRG] Remove deprecated parameters whose removal was scheduled for 015 Some deprecated parameters were scheduled for removal for 015 but managed to escape it The one with most impact was score_func and loss_func from sklearnmetricsscorercheck_scoring which was still used in quite a few different places eg cross_validationpy grid_searchpy a few different tests ,,1963,0.7758532857870606,0.12093766648907832,37682,447.34886683297066,35.534207313836845,113.87399819542487,3146,72,1430,169,travis,lesteve,arjoly,false,arjoly,0,0,4,0,813,true,false,false,false,0,0,0,0,5,0,11
4181246,scikit-learn/scikit-learn,python,3409,1405606947,,1406301305,11572,,unknown,false,false,false,10,56,20,57,27,4,88,0,6,0,0,5,5,5,0,0,0,0,5,5,5,0,0,835,406,1341,965,167.76640742443908,3.8679861839403618,19,olivier.grisel@ensta.org,sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py,9,0.0021470746108427268,0,13,false,#3364 warm start in random forests Related to issue #3364,,1962,0.7762487257900101,0.12184648416532474,37681,446.85650593137126,35.50861176720363,113.71778880602956,3145,71,1430,182,travis,ldirer,ogrisel,false,,2,1.0,0,3,447,true,false,false,false,1,2,2,0,22,0,9
4180798,scikit-learn/scikit-learn,python,3408,1405602843,1405611360,1405611360,141,141,merged_in_comments,false,false,false,6,2,1,7,5,0,12,0,4,1,0,1,2,0,0,0,1,0,1,2,0,0,0,0,0,0,0,9.443844892546538,0.21773525539728827,2,olivier.grisel@ensta.org,doc/index.rst|doc/related_projects.rst,2,0.0010804970286331713,0,0,false,MRG Add related project to website ,,1961,0.776134625191229,0.12263641274986493,37681,446.85650593137126,35.50861176720363,113.71778880602956,3144,71,1430,170,travis,amueller,amueller,true,amueller,200,0.85,935,39,1364,true,true,false,false,8,88,10,55,24,0,1
4180180,scikit-learn/scikit-learn,python,3407,1405597742,,1405683447,1428,,unknown,false,false,false,40,6,1,22,10,0,32,0,4,0,0,2,12,2,0,0,1,0,12,13,12,0,0,6,0,310,565,7.9254178181605885,0.18272659094940324,3,larsmans@gmail.com,sklearn/utils/__init__.py|sklearn/utils/validation.py,2,0.001095890410958904,0,1,false,WIP better Pandas support Dont convert dataframes in grid search and cross-validationIf we can agree on this Ill* [ ] add tests* [ ] rename allow_lists to force_array (and invert the meaning) Its an internal parameter anyhow ,,1960,0.7765306122448979,0.12438356164383561,37675,446.74187126741873,35.3550099535501,113.41738553417386,3142,71,1430,172,travis,amueller,amueller,true,,199,0.8542713567839196,934,39,1364,true,true,false,false,7,83,9,49,24,0,6
4178399,scikit-learn/scikit-learn,python,3406,1405575541,1405591655,1405591655,268,268,commits_in_master,false,false,false,5,2,2,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,151,0,151,9.108353667084423,0.2110822171393873,21,olivier.grisel@ensta.org,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,21,0.011513157894736841,0,0,false,[MRG] Minor improvements to test_metrics  ,,1959,0.776416539050536,0.125,37595,447.4797180476127,35.43024338342865,113.6587312142572,3140,71,1430,169,travis,jnothman,arjoly,false,arjoly,81,0.6666666666666666,26,1,1906,true,true,false,true,15,317,26,293,41,3,12
4173077,scikit-learn/scikit-learn,python,3403,1405536070,1405625247,1405625247,1486,1486,commits_in_master,false,false,false,10,14,2,0,6,1,7,0,4,0,0,1,23,1,0,0,0,13,10,23,20,0,1,0,12,66,7799,7.714383868680666,0.17877782025209507,12,olivier.grisel@ensta.org,sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py,12,0.006648199445983379,0,1,false,[WIP] Make tests faster to run Ongoing fix for #3402,,1958,0.7763023493360572,0.12797783933518006,37583,450.12372615278184,35.521379347045205,114.04092275762979,3134,70,1429,172,travis,ogrisel,ogrisel,true,ogrisel,79,0.8607594936708861,988,123,1876,true,true,false,false,23,480,53,190,149,9,1228
4172538,scikit-learn/scikit-learn,python,3401,1405532957,1405798323,1405798323,4422,4422,commits_in_master,false,false,false,39,8,3,16,7,0,23,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,29,108,53,144,13.156580889778581,0.30489899680282934,0,,sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py,0,0.0,1,1,false, [MRG] scorer: add sample_weight support (+test)  Wraps up #3098 (a part of #1574) ready for reviewInitial description by @ndawe: This is part of the larger #1574 and adds support for sample weights in the scorer interface,,1957,0.776188042922841,0.13007226236798222,37578,450.1836180744052,35.5261057001437,114.05609665229656,3134,70,1429,182,travis,vene,arjoly,false,arjoly,46,0.8043478260869565,65,33,1557,true,true,false,true,2,34,0,72,6,3,0
4172012,scikit-learn/scikit-learn,python,3400,1405530040,1405532891,1405532891,47,47,commits_in_master,false,false,false,23,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,155,2,155,9.222499008039982,0.21372960373214844,10,olivier.grisel@ensta.org,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,7,0.00390625,4,0,false,ENH improve forest testing + avoid *args Pull out forest improvement from #3173 for testing and oob_score function interfacePing @vene @amueller @ogrisel @glouppe,,1956,0.7760736196319018,0.13058035714285715,37578,449.14577678428867,35.47288306988131,113.81659481611581,3134,70,1429,171,travis,arjoly,ogrisel,false,ogrisel,58,0.8103448275862069,26,25,939,true,true,true,false,20,217,19,272,165,4,1
4171932,scikit-learn/scikit-learn,python,3399,1405529567,1405685720,1405685720,2602,2602,merged_in_comments,false,false,false,26,11,6,17,5,5,27,0,3,0,0,9,16,9,0,0,0,0,16,16,16,0,0,88,154,108,217,101.66473784689398,2.356060338375393,48,vlad@vene.ro,sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/semi_supervised/label_propagation.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/validation.py|sklearn/utils/multiclass.py|sklearn/utils/multiclass.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/semi_supervised/label_propagation.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/validation.py|sklearn/utils/multiclass.py,22,0.00446677833612507,0,1,false,[WIP] Supporting non array subclasses The support (and test) for y being not an array subclass is implemented but I still need to work on X,,1955,0.7759590792838875,0.1306532663316583,37578,449.14577678428867,35.47288306988131,113.81659481611581,3134,70,1429,176,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,49,0.7755102040816326,522,3,1605,true,true,false,false,18,206,20,95,79,12,17
4171704,scikit-learn/scikit-learn,python,3397,1405527973,1405533381,1405533381,90,90,commits_in_master,false,false,false,16,3,3,1,3,0,4,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,90,0,90,0,12.477848505708266,0.2891722256314064,6,larsmans@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,6,0.0033651149747616375,3,2,false,Improve random number generator helper in the tree module Taken from #3173Ping @vene @jnothman @amueller,,1954,0.7758444216990789,0.13123948401570387,37570,448.9486292254458,35.480436518498806,113.84083044982698,3134,70,1429,172,travis,arjoly,ogrisel,false,ogrisel,57,0.8070175438596491,26,25,939,true,true,true,false,20,215,18,268,165,4,8
4171550,scikit-learn/scikit-learn,python,3396,1405527027,1405533029,1405533029,100,100,commits_in_master,false,false,false,40,2,1,0,12,0,12,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,105,0,128,0,13.053804456417316,0.3025199164657674,7,rajatkhanduja13@gmail.com,examples/linear_model/plot_sparse_recovery.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py,3,0.001122334455667789,0,0,false,ENH add convergence_warning option to lars_path Previously plot_sparse_recoverypy had 10k lines output I dont think you want to know about some of your randomized designs being ill-conditionedAlso: this was a pain the class-structure might need some cleaning up ),,1953,0.7757296466973886,0.13131313131313133,37570,448.9486292254458,35.480436518498806,113.84083044982698,3134,70,1429,172,travis,amueller,ogrisel,false,ogrisel,198,0.8535353535353535,933,39,1363,true,true,true,false,7,54,6,31,20,0,6
4171476,scikit-learn/scikit-learn,python,3395,1405526538,1407879700,1407879700,39219,39219,commits_in_master,false,false,false,38,35,8,38,51,0,89,0,5,0,0,9,10,9,0,0,0,0,10,10,9,0,0,1946,1133,3349,1576,137.29414487429017,3.181770753291069,48,olivier.grisel@ensta.org,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|benchmarks/bench_multilabel_metrics.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|benchmarks/bench_multilabel_metrics.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|benchmarks/bench_multilabel_metrics.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|benchmarks/bench_multilabel_metrics.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/ranking.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py,28,0.0,0,11,false,[MRG] Sparse multilabel target support This introduces a series of helper classes that abstract away aggregation over multilabel structures This enables efficient calculation in sparse and dense binary indicator matrices while maintaining support for the deprecated sequences format,,1952,0.7756147540983607,0.13131313131313133,37570,448.9486292254458,35.480436518498806,113.84083044982698,3133,70,1429,188,travis,jnothman,larsmans,false,larsmans,80,0.6625,26,1,1905,true,true,false,false,15,312,25,284,41,3,32
4170489,scikit-learn/scikit-learn,python,3394,1405518058,1405528259,1405528259,170,170,merged_in_comments,false,false,false,8,1,1,4,6,0,10,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,24,23,24,8.976345804681333,0.20802447410864866,7,vlad@vene.ro,sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py,7,0.003963759909399773,0,2,false,BUG: Support array interface This should fix #3390,,1951,0.7754997437211686,0.13250283125707815,37557,449.05077615357993,35.49271773570839,113.88023537556248,3132,70,1429,172,travis,GaelVaroquaux,amueller,false,amueller,48,0.7708333333333334,522,3,1605,true,true,false,true,18,200,19,93,75,12,0
4170214,scikit-learn/scikit-learn,python,3393,1405515592,1405526446,1405526446,180,180,commit_sha_in_comments,false,false,false,19,2,2,0,1,0,1,0,2,0,0,2,2,1,0,1,0,0,2,2,1,0,1,27,0,27,0,20.184590270720584,0.4677726178926169,6,olivier.grisel@ensta.org,.travis.yml|continuous_integration/install.sh|.travis.yml|continuous_integration/install.sh,4,0.0022675736961451248,0,0,false,[MRG] add new 32 bit travis build Title should says it all Should be merge-able if travis is green,,1950,0.7753846153846153,0.1326530612244898,37557,449.05077615357993,35.49271773570839,113.88023537556248,3132,70,1429,169,travis,ogrisel,ogrisel,true,ogrisel,78,0.8589743589743589,988,123,1876,true,true,false,false,22,465,51,188,145,9,169
4161425,scikit-learn/scikit-learn,python,3388,1405442562,,1421536119,268225,,unknown,false,true,false,491,21,21,1,18,1,20,0,5,3,0,4,7,7,0,0,3,0,4,7,7,0,0,1932,11,1932,11,131.12817744758834,3.038853614674893,2,larsmans@gmail.com,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|examples/gaussian_process/plot_matern_kernel.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_learning_curve.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_nonstationary.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/gp_diabetes_dataset.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/correlation_models.py,2,0.0,0,10,false,[WIP] Gaussian Process revisited (refactoring of correlation models + some new features/bugfixes) This PR contains a proposal for a refactoring of the gaussian_process subpackage (mainly the correlation_models module) and some novel features which become possible because of this refactoring Moreover some subtle bugs are fixed In general I tried to keep the external interface of GaussianProcess fixed and to change only internalsThe main purpose of this PR is to begin a discussion on the future of the GP subpackage and revive its further developmentCHANGES: * Correlation models are now classes instead of functions and inherit from abstract base class StationaryCorrelation This reduces code-redundancy (DRY) and separates GPs and their correlation models more strictly Moreover non-stationary correlation models become possible (see examples/gaussian_process/plot_gp_nonstationarypy) This was not possible formerly since only the differences of the datapoints were passed to the correlation models but not the datapoints themselves * The hyperparameters theta are estimated as maximum-a-posterior estimates rather than maximum-likelihood estimates At the moment the prior on the hyperparameters theta is uniform such that both result in the same value However a non-uniform prior could be used in the future in cases where the risk of overfitting theta to the data is serious (eg forcing the factor analysis distance to be close to diagonal etc) * Hyperparameters theta are represented as 1D ndarrays instead of 2D 1xn arrays in GaussianProcess NEW FEATURES: * Matern correlation models for nu15 and nu25 have been added (see https://enwikipediaorg/wiki/Mat%C3%A9rn_covariance_function) An example script showing the potential benefit of the Matern correlation model compared to squared-exponential and absolute-exponential was added under examples/gaussian_process/plot_matern_kernelpy (see attached image) * squared_exponential absolute_exponential and Matern correlation models support factor analysis distance This can be seen as an extension of learning dimension-specific length scales in which also correlations of feature dimensions can be taken into account See Rasmussen and Williams 2006 p107 for details An example script showing the potential benefit of this extension was added under examples/gaussian_process/plot_gp_learning_curvepy (see attached image)  * GP supports additional optimizers for ML estimation passed as callables In addition to fmin_cobyla and Welch GaussianProcess allows now to pass other optimizers directly as callables via the parameter optimizerBUGFIXES* Setting optimal_rlf_value correctly in GPs and actually maximizing likelihood over several random starts The sign of the optimal_rlf_value was set wrongly in _arg_max_reduced_likelihood_function() since it was set to the negative of the return of reduced_likelihood_function() This error was probably caused by confusing reduced_likelihood_function(theta)  with minus_reduced_likelihood_function(log10t) It resulted however in _arg_max_reduced_likelihood_function() returning the worst local maxima instead of the best one when performing several random_starts* A typo in gp_diabetes_datasetpy was fixed which caused a crash of the example (gptheta_ versus gptheta)To be discussed: * The factor analysis distance has hyperparameters that can can take on arbitrary real values (not just positive ones) Since sklearn enforces the hyperparameters theta to be positive this is internally handled by taking the log of the corresponding components of theta Are there better ideas[plot_gp_learning_curve](https://fcloudgithubcom/assets/1116263/2310367/a7499dde-a2e2-11e3-80f5-6e98a23f0ee8png)[plot_matern_kernel](https://fcloudgithubcom/assets/1116263/2310372/adf004fc-a2e2-11e3-8d25-85e69e09fd11png),,1949,0.775782452539764,0.13571017826336976,37558,448.87906704297353,35.46514723893711,113.85057777304436,3126,72,1428,270,travis,jmetzen,jmetzen,true,,6,0.6666666666666666,11,2,1009,true,true,false,false,0,1,0,0,8,0,8
4159444,scikit-learn/scikit-learn,python,3387,1405426901,1405872830,1405872830,7432,7432,commit_sha_in_comments,false,false,false,55,2,2,0,23,0,23,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,14,0,9.397924187732622,0.21779257267555302,16,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py,16,0.009313154831199068,0,6,false,FIX: some python3 errors while building the doc Some fixes I had to make to compile sklearn with latests sphinx and python 3 (it still does not fully compile but I think this is a step forward)Those are encoding errors that are fixed for both python 2 3 by using ioopen instead of open,,1948,0.7756673511293635,0.13678696158323633,37558,448.90569252888866,35.46514723893711,113.85057777304436,3120,72,1428,184,travis,fabianp,fabianp,true,fabianp,35,0.7142857142857143,216,25,1522,true,true,false,true,2,13,4,14,12,0,1
4154975,scikit-learn/scikit-learn,python,3385,1405380829,1405386308,1405386308,91,91,commits_in_master,false,false,false,10,3,2,5,2,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,11,0,8.391153144136773,0.19446104631834826,19,larsmans@gmail.com,sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,19,0.011189634864546525,0,0,false,BUG: fix windows pointer size problem Fix windows test failure,,1947,0.7755521314843349,0.1413427561837456,37556,448.9295984662903,35.46703589306635,113.85664074981361,3111,74,1427,172,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,47,0.7659574468085106,522,3,1603,true,true,false,false,16,186,17,82,65,9,8
4152768,scikit-learn/scikit-learn,python,3382,1405366135,1405378295,1405378295,202,202,commits_in_master,false,false,false,14,3,1,3,2,0,5,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.000279426389717,0.11655250199421824,8,yoshiki89@gmail.com,doc/install.rst,8,0.0047875523638539795,0,0,false,[MRG] Install doc update Refactored and update the installation documentation to give per-OS instructions,,1944,0.7762345679012346,0.1448234590065829,37474,460.3725249506324,35.598014623472274,114.47937236483963,3108,75,1427,177,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,77,0.8571428571428571,987,123,1874,true,true,true,true,21,457,50,194,119,9,53
4151915,scikit-learn/scikit-learn,python,3381,1405360974,1405374359,1405374359,223,223,commits_in_master,false,false,false,36,3,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,18,4.493371131417785,0.10473646941314334,6,olivier.grisel@ensta.org,sklearn/tests/test_common.py,6,0.0036275695284159614,0,0,false,[WIP] Skip common regressor test for OrthogonalMatchingPursuitCV on Travis omp_cv and related tests have issues on Travis Should be fixed long term but for now this will skip the test_regressors_int when running common tests on Travis,,1943,0.7761194029850746,0.14691656590084642,37474,460.3725249506324,35.598014623472274,114.47937236483963,3107,75,1427,179,travis,kastnerkyle,GaelVaroquaux,false,GaelVaroquaux,8,0.75,224,66,844,true,false,true,false,1,54,5,1,42,0,75
4151543,scikit-learn/scikit-learn,python,3380,1405358978,1405367191,1405367191,136,136,merged_in_comments,false,false,false,16,2,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,4,0,4.05809818447487,0.09459063968374785,0,,sklearn/linear_model/least_angle.py,0,0.0,0,0,false,FIX: check with tolerance on lars_path Hopefully this will fix issue #3370 but I havent checked,,1942,0.7760041194644696,0.14763061968408261,37474,460.3725249506324,35.598014623472274,114.47937236483963,3106,75,1427,178,travis,fabianp,GaelVaroquaux,false,GaelVaroquaux,34,0.7058823529411765,216,25,1521,true,true,true,true,2,7,2,12,6,0,14
4151364,scikit-learn/scikit-learn,python,3378,1405356499,1405359298,1405359298,46,46,commits_in_master,false,false,false,36,1,1,0,0,1,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.112439924125022,0.09585729703932898,0,,sklearn/decomposition/sparse_pca.py,0,0.0,0,0,false,[MRG] Eliminate dense cholesky warning during tests Sparse PCA was still using the dense_cholesky name for the internal ridge solver Switching this to cholesky eliminates a DeprecationWarning during tests and should have the exact same functionality,,1941,0.7758887171561051,0.14799025578562727,37474,460.3725249506324,35.598014623472274,114.47937236483963,3106,74,1427,177,travis,kastnerkyle,fabianp,false,fabianp,7,0.7142857142857143,224,66,844,true,false,false,false,1,53,4,1,41,0,-1
4151227,scikit-learn/scikit-learn,python,3377,1405355491,1405421818,1405421818,1105,1105,commits_in_master,false,false,false,2,4,2,3,9,0,12,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,8,0,23,9.235865359810273,0.21528493573125893,3,michael@bommaritollc.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py,3,0.0018303843807199512,0,2,false,Hashing fix3356 ,,1940,0.7757731958762887,0.14826113483831604,37474,460.3725249506324,35.598014623472274,114.47937236483963,3106,74,1427,177,travis,ldirer,ogrisel,false,ogrisel,1,1.0,0,3,444,false,false,false,false,1,0,1,0,3,0,5
4151163,scikit-learn/scikit-learn,python,3376,1405354978,1405377444,1405377444,374,374,commits_in_master,false,false,false,53,47,42,12,3,0,15,0,5,1,0,5,6,8,0,0,1,0,5,6,8,0,0,842,208,857,208,225.87629722466622,5.265101020509673,4,sdenton4@gmail.com,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/_random.c|sklearn/utils/_random.pxd|sklearn/utils/_random.pyx|sklearn/utils/random.py|sklearn/utils/setup.py|sklearn/utils/random.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/tests/test_extmath.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/utils/_random.c|sklearn/utils/_random.pxd|sklearn/utils/_random.pyx|sklearn/utils/random.py|sklearn/utils/setup.py|sklearn/utils/random.py|sklearn/utils/random.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/utils/random.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/tests/test_extmath.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,2,0.0,3,0,false,[MRG] Fix MiniBatchKMeans  This is a follow up on @amuellers PR:https://githubcom/scikit-learn/scikit-learn/pull/2638The code was rebased on masterA minor test failure on my box was fixedThe strategy has been switched to uniform drawing of centers in the population@ogrisel @amueller please review I believe that this should go in for 015,,1939,0.7756575554409489,0.14871481028151776,37474,460.3725249506324,35.598014623472274,114.47937236483963,3106,74,1427,178,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,46,0.7608695652173914,522,3,1603,true,true,false,false,13,164,9,79,41,9,15
4150305,scikit-learn/scikit-learn,python,3375,1405346530,1405353411,1405353411,114,114,commits_in_master,false,false,false,5,3,1,1,1,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,11,0,4.206070971409351,0.09804207288353319,7,michael@bommaritollc.com,sklearn/feature_extraction/text.py,7,0.004310344827586207,0,1,false,Corrected two typos in docstring ,,1938,0.7755417956656346,0.14963054187192118,37474,460.3725249506324,35.598014623472274,114.47937236483963,3105,74,1427,174,travis,ldirer,agramfort,false,agramfort,0,0,0,3,444,false,false,false,false,1,0,0,0,1,0,12
4140655,scikit-learn/scikit-learn,python,3369,1405184807,1405185097,1405185097,4,4,commits_in_master,false,false,false,27,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.769622204443873,0.1111773949377639,8,yoshiki89@gmail.com,doc/install.rst,8,0.0050062578222778474,0,0,false,DOC updated installation documentation Our documentation is way out of date This is a small fix (release critical for 015 IMO)Added Canopy and AnacondaRemoved EPD,,1937,0.7754259163655137,0.15456821026282855,37439,464.5690322925292,35.81826437671946,115.60137824194022,3057,73,1425,180,travis,NelleV,GaelVaroquaux,false,GaelVaroquaux,34,0.8529411764705882,48,13,1636,true,true,true,false,0,10,0,3,2,0,4
4137411,scikit-learn/scikit-learn,python,3368,1405124660,1405124833,1405124833,2,2,github,false,false,false,5,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.314759124125467,0.10057477587498555,1,larsmans@gmail.com,doc/modules/mixture.rst,1,0.000625,0,0,true,fixed typo: diriclet - dirichlet ,,1936,0.7753099173553719,0.15625,37439,464.5690322925292,35.81826437671946,115.60137824194022,3037,72,1424,180,travis,stevetjoa,agramfort,false,agramfort,0,0,31,17,842,true,true,false,false,0,0,0,0,2,0,-1
4133882,scikit-learn/scikit-learn,python,3366,1405100041,1405100082,1405100082,0,0,commits_in_master,false,false,false,27,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.912105770683917,0.11449860734649052,25,ronald.phlypo@inria.fr,doc/whats_new.rst,25,0.01567398119122257,0,2,true,FIX: My name was linked to the wrong github account My name was linked to my old and wrong github account This Pull Request fixes this issue,,1935,0.7751937984496124,0.15673981191222572,37439,464.5690322925292,35.81826437671946,115.60137824194022,3030,71,1424,180,travis,MechCoder,arjoly,false,arjoly,13,0.7692307692307693,76,41,752,true,true,true,true,1,167,18,43,252,2,0
4132789,scikit-learn/scikit-learn,python,3365,1405092563,1405093116,1405093116,9,9,commits_in_master,false,false,false,102,2,2,0,0,1,1,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.601461347557116,0.22380531038703788,3,larsmans@users.noreply.github.com,doc/modules/linear_model.rst|doc/modules/linear_model.rst,3,0.0018796992481203006,0,1,true,DOC: write cost function of logistic regression Improve documentation of logistic regression by displaying the exact form of the cost function (as is done for other models)I feel it important to state clearly what is the cost function of themodel because:    * Some might want to compare their implementation against this one    in which case it is important to know exactly what is the form of the    cost function    * Furthermore since the models are quite inconsistent in terms of    notation for the regularization parameter: some models in this    module penalize by 1/C others by alpha and others by alpha/2,,1934,0.7750775594622544,0.15726817042606517,37439,464.5690322925292,35.81826437671946,115.60137824194022,3028,71,1424,180,travis,fabianp,mblondel,false,mblondel,33,0.696969696969697,216,25,1518,false,true,true,true,0,3,1,9,3,0,-1
4133035,scikit-learn/scikit-learn,python,3363,1405078364,1405376090,1405376090,4962,4962,commits_in_master,false,false,false,189,16,12,18,20,0,38,0,5,4,0,2,6,4,0,1,4,0,2,6,4,0,1,1408,18,1408,80,236.10921841918085,5.503588985127802,12,olivier.grisel@ensta.org,sklearn/tests/test_common.py|sklearn/utils/testing.py|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd|appveyor.yml|continuous_integration/appveyor/install.ps1|continuous_integration/appveyor/requirements.txt|continuous_integration/appveyor/run_with_env.cmd,7,0.0,0,1,true,[MRG] Add Windows continuous integration with AppVeyor CI AppVeyor is a system similar to Travis CI that provides a Windows based build environment with MSVC++ Express 2008 and 2010 pre-installed along with the matching Windows SDKs for 64 bit builds This PR configures how to build scikit-learn generate whl and exe packages for 4 supported target platforms: Python 34 & 27 each with 32 and 64 bitThis PR also has a workaround for #3255:  some common tests that call fit_transform twice (with a fixed random state) do not deterministic output on 32 bit Python (both 2 and 3) for some estimators that use LAPACK routines like SVD or eigen solvers Note the numpy / scipy libraries tested here come from [Christoph Gohlkes distribution](wwwlfduciedu/~gohlke/pythonlibs/) and are therefore linked with MKL However I also had non-deterministic fit_transform for the same bunch of estimators when I manually built numpy and scipy with OpenBLAS on a Rackspace windows VM a while ago so the problem is not tied to the use of MKL I therefore decided to catch AssertionError and raise SkipTest instead for this specific test_common check under 32 bit platforms,,1933,0.7749612002069323,0.15756434400502198,37439,464.5690322925292,35.81826437671946,115.60137824194022,3024,71,1424,184,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,76,0.8552631578947368,984,123,1871,true,true,true,true,20,445,49,192,113,9,12
4125733,scikit-learn/scikit-learn,python,3362,1405015307,1405186428,1405186428,2852,2852,commit_sha_in_comments,false,false,false,30,2,2,1,6,0,7,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,8.771941625859021,0.20465858360446254,3,larsmans@gmail.com,sklearn/tree/tree.py|sklearn/tree/tree.py,3,0.001893939393939394,0,0,false,[MRG] MAINT Re-order argument to put deprecated one at the end Small pull request to follow the convention: - fit(X y sample_weightNone ) - put deprecated argument at the end,,1932,0.7748447204968945,0.1590909090909091,37439,464.5690322925292,35.81826437671946,115.60137824194022,3012,71,1423,180,travis,arjoly,larsmans,false,larsmans,56,0.8035714285714286,26,25,933,true,true,true,true,19,202,16,250,169,4,15
4124221,scikit-learn/scikit-learn,python,3361,1405005212,1405016888,1405016888,194,194,commits_in_master,false,false,false,169,2,2,0,7,0,7,0,4,1,0,2,3,2,0,0,1,0,2,3,2,0,0,17,0,17,0,18.829497432797627,0.439311862156784,7,olivier.grisel@ensta.org,continuous_integration/test_script.sh|continuous_integration/exclude_joblib_mp.txt|continuous_integration/install.sh|continuous_integration/test_script.sh,7,0.0,0,3,false,[MRG] skip joblib multiprocessing tests on travis The travis environment seems to be quite heavily loaded sometimes to the point that process forks cause random test errors such as the following:ERROR: sklearnexternalsjoblibtesttest_paralleltest_simple_parallel(None)----------------------------------------------------------------------Traceback (most recent call last):File /home/travis/virtualenv/python27_with_system_site_packages/local/lib/python27/site-packages/nose/casepy line 197 in runTest  selftest(*selfarg)File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/test/test_parallelpy line 84 in check_simple_parallel  Parallel(n_jobsn_jobs)(delayed(square)(x) for x in X))File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallelpy line 604 in __call__  self_pool  MemmapingPool(n_jobs **poolargs)File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/poolpy line 561 in __init__  super(MemmapingPool self)__init__(**poolargs)File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/poolpy line 400 in __init__  super(PicklingPool self)__init__(**poolargs)File /usr/lib/python27/multiprocessing/poolpy line 136 in __init__  self_repopulate_pool()File /usr/lib/python27/multiprocessing/poolpy line 199 in _repopulate_pool  wstart()File /usr/lib/python27/multiprocessing/processpy line 130 in start  self_popen  Popen(self)File /usr/lib/python27/multiprocessing/forkingpy line 120 in __init__  selfpid  osfork()OSError: [Errno 12] Cannot allocate memoryThe following skips the numerous embedded joblib tests that call osfork() when run on travis Note that upstream joblib tests are also running on travis but as there are fewer pull requests on the joblib github project this is a much less frequent issue there,,1931,0.7747281201450026,0.1619718309859155,37439,464.5690322925292,35.81826437671946,115.60137824194022,3011,71,1423,180,travis,ogrisel,ogrisel,true,ogrisel,75,0.8533333333333334,983,123,1870,true,true,false,false,21,433,47,191,106,9,5
4122727,scikit-learn/scikit-learn,python,3360,1404992019,1406045605,1406045605,17559,17559,merged_in_comments,false,false,false,21,12,3,19,22,0,41,0,9,0,0,21,26,21,0,0,0,0,26,26,26,0,0,832,249,2103,681,259.0385316285355,6.043639778822449,38,olivier.grisel@ensta.org,sklearn/cluster/affinity_propagation_.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_k_means.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/tests/test_fastica.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/semi_supervised/label_propagation.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_k_means.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/tests/test_fastica.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/semi_supervised/label_propagation.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_k_means.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/tests/test_fastica.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/semi_supervised/label_propagation.py,18,0.0006397952655150352,0,13,false,ENH: Added n_iter_ parameters across all iterative solvers Added a n_iter parameter across all iterative solvers that have a max_iter parameter,,1930,0.7746113989637305,0.1618682021753039,37439,464.5690322925292,35.81826437671946,115.60137824194022,3008,71,1423,198,travis,MechCoder,agramfort,false,agramfort,12,0.75,75,40,751,true,true,true,false,1,152,16,38,239,2,1732
4122475,scikit-learn/scikit-learn,python,3359,1404989659,1445529265,1445529265,675660,675660,commit_sha_in_comments,false,false,false,59,57,10,45,45,0,90,0,7,0,0,2,14,2,0,0,1,0,14,15,11,0,0,510,126,2140,1062,77.51941412842434,1.8086089814985145,10,larsmans@gmail.com,sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,9,0.005772931366260423,0,9,false,Use float as percentage in min_samples_split and min_samples_leaf of DecisionTree A float number can be used as percentage in these two parameters in case the sample size varies greatlyI found a new parameter min_weight_fraction_leaf is added recently It works in a similar way But it is still convenient to use float in min_samples_split for those sample without weighting,,1929,0.7744945567651633,0.16228351507376523,37439,464.5690322925292,35.81826437671946,115.60137824194022,3007,71,1423,404,travis,yelite,arjoly,false,arjoly,0,0,4,4,516,true,true,false,false,0,0,0,0,10,0,12
4119379,scikit-learn/scikit-learn,python,3358,1404953058,1405185678,1405185678,3877,3877,commits_in_master,false,false,false,34,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.3034729132706335,0.10040452253267111,2,jdlabal@stanford.edu,doc/modules/outlier_detection.rst,2,0.0012944983818770227,0,0,false,Fix labels of inliers vs outliers At least based on what svmOneClassSVM does in the example linked from this document the description was wrong labels actually assigned are 1/-1 for good/bad respectively not 0/1,,1928,0.7743775933609959,0.16375404530744336,37439,464.5690322925292,35.81826437671946,115.60137824194022,2998,70,1422,180,travis,houbysoft,larsmans,false,larsmans,1,1.0,12,1,1593,true,false,false,false,0,0,3,0,3,0,16
4118703,scikit-learn/scikit-learn,python,3357,1404947864,1404947975,1404947975,1,1,github,false,false,false,6,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.356097395940534,0.1016322830752273,0,,doc/modules/outlier_detection.rst,0,0.0,0,0,false,Fix grammar its should be its,,1927,0.7742605085625325,0.16396629941672067,37439,464.5690322925292,35.81826437671946,115.60137824194022,2995,70,1422,176,travis,houbysoft,houbysoft,true,houbysoft,0,0,12,1,1593,true,false,false,false,0,0,0,0,2,0,1
4116449,scikit-learn/scikit-learn,python,3355,1404932455,1404946446,1404946446,233,233,commits_in_master,false,false,false,27,1,1,0,7,0,7,0,4,0,0,2,2,1,0,1,0,0,2,2,1,0,1,8,0,8,0,9.864717639837227,0.23015563024665636,2,olivier.grisel@ensta.org,doc/Makefile|doc/README,2,0.0013020833333333333,0,8,false,DOC run optipng before uploading website Uses GNU find and xargs to run in parallel because OptiPNG is very expensive on the sh*itload of PNGs we generate,,1926,0.7741433021806854,0.1640625,37439,464.5690322925292,35.81826437671946,115.60137824194022,2988,69,1422,176,travis,larsmans,ogrisel,false,ogrisel,117,0.7521367521367521,142,38,1452,true,true,true,true,42,196,43,67,97,7,25
4116049,scikit-learn/scikit-learn,python,3354,1404929869,,1405433856,8399,,unknown,false,false,false,95,2,1,0,3,0,3,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,34,0,34,0,8.91365006274423,0.20796306612377913,5,olivier.grisel@ensta.org,sklearn/decomposition/pca.py|sklearn/utils/validation.py,3,0.001955671447196871,0,2,false,Allow PCA of complex data rather than only keeping the real part I want to be able to do PCA of NMR spectra which are complexIve rolled my own code borrowing heavily from scikit-learn - see https://githubcom/chatcannon/nmrpcaHowever it doesnt look like it will be too hard to add support for complex numbers upstream - heres my attemptNB Trying to run the scikit-learn test suite gave all sorts of spurious errors (lots of cannot import name inplace_column_scale among others) I think there is something wrong with my setup Therefore this code is untested,,1925,0.7745454545454545,0.1636245110821382,37439,464.5690322925292,35.81826437671946,115.60137824194022,2987,69,1422,180,travis,chatcannon,chatcannon,true,,0,0,1,5,596,false,true,false,false,0,0,0,0,0,0,5308
4113383,scikit-learn/scikit-learn,python,3353,1404909178,1405341433,1405341433,7204,7204,commits_in_master,false,false,false,94,2,1,1,10,0,11,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,8,0,9,9.306548016430359,0.21712971082568816,7,olivier.grisel@ensta.org,sklearn/linear_model/tests/test_omp.py|sklearn/utils/testing.py,7,0.0045871559633027525,0,3,false,[MRG] Skip test for OrthogonalMatchingPursuitCV if running on Travis OrthogonalMatchingPursuitCV has been having heisen-failures on Travis for quite some time See #3190 for more detailsI have been unable to replicate this test on a local box using varying Python numpy scipy and BLAS implementations I have also been unable to replicate this using a CoreOS VM on Rackspace using Docker (along with the above veriations) and severely limiting memory available to the container One way to avoid this failure is simply skip this particular test on Travis using a similar technique as #3333,,1924,0.7744282744282744,0.16448230668414154,37439,464.5690322925292,35.81826437671946,115.60137824194022,2977,69,1422,178,travis,kastnerkyle,GaelVaroquaux,false,GaelVaroquaux,6,0.6666666666666666,223,66,839,true,false,true,false,1,43,3,1,32,0,12
4112663,scikit-learn/scikit-learn,python,3352,1404902777,,1405364024,7687,,unknown,false,false,false,5,2,2,1,5,0,6,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,60,0,60,0,13.248643096443256,0.3091021546640941,0,,sklearn/_isotonic.c|sklearn/_isotonic.pyx|sklearn/_isotonic.c|sklearn/_isotonic.pyx,0,0.0,0,1,false,Performance improvements to isotonic regression ,,1923,0.7748309932397296,0.1636245110821382,37439,464.5690322925292,35.81826437671946,115.60137824194022,2976,69,1422,181,travis,robertwb,larsmans,false,,0,0,12,0,1330,false,false,false,false,0,0,0,0,2,0,4383
4083057,scikit-learn/scikit-learn,python,3349,1404771633,1405522730,1405522730,12518,12518,commits_in_master,false,true,false,35,15,2,17,69,0,86,0,9,0,0,6,23,6,0,0,0,0,23,23,23,0,0,162,28,678,322,38.741643497286674,0.9038763262154824,28,olivier.grisel@ensta.org,sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py,18,0.007042253521126761,2,31,false,ENH: Return attribute n_iter_ for linear models dependent on the enet solver This should be quick review and will help to finish or close the other pull requests @agramfort @ogrisel please have a quick look,,1921,0.7751171264966163,0.16389244558258642,37423,464.7676562541753,35.83357828073645,115.6508029821233,2946,69,1420,181,travis,MechCoder,ogrisel,false,ogrisel,11,0.7272727272727273,74,40,748,true,true,true,false,1,144,15,38,214,2,32
4073651,scikit-learn/scikit-learn,python,3348,1404667757,,1405445750,12966,,unknown,false,false,false,15,2,1,4,6,0,10,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,4,8,4.176172501267959,0.09743374615360792,2,jnothman@student.usyd.edu.au,sklearn/feature_selection/rfe.py,2,0.0012406947890818859,0,2,false,Quick fix on grid_scores updating final grid_scores value with len(cv) selfgrid_scores_  scores / len(cv),,1920,0.7755208333333333,0.1588089330024814,37423,464.7676562541753,35.83357828073645,115.6508029821233,2921,69,1419,185,travis,lmatthieu,agramfort,false,,0,0,1,2,1109,false,false,false,false,0,0,0,0,2,0,81
4066210,scikit-learn/scikit-learn,python,3344,1404513490,1404556527,1404556527,717,717,commits_in_master,false,false,false,57,3,1,3,3,0,6,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,30,0,73,0,4.391549445199488,0.10246093920448608,4,olivier.grisel@ensta.org,sklearn/naive_bayes.py,4,0.0024783147459727386,0,0,true,Remove duplicate GaussianNBfit() code Follow-on to #3324 GaussianNBfit is substantially duplicated by partial_fit and the latter is more flexible so just have the former call the latter There should be no significant degradation in performance or numerical stability since the mean/variance update will just call numpymean and numpyvar when n_past  0 rather than trying anything fancy,,1918,0.7758081334723671,0.15861214374225527,37405,464.85763935302765,35.85082208260928,115.65298756850687,2885,68,1417,172,travis,ihaque,agramfort,false,agramfort,4,0.75,23,5,948,true,false,false,false,0,5,2,17,2,0,1
4066151,scikit-learn/scikit-learn,python,3343,1404512855,1404513030,1404513030,2,2,commits_in_master,false,false,false,23,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.598978650619331,0.10730059779410397,0,,doc/modules/naive_bayes.rst,0,0.0,0,0,true,Update Sphinx docs for GaussianNB partial_fit Follow-on to #3324 Forgot to advertise in the user docs on Naive Bayes that GaussianNB supports partial_fit,,1917,0.7756911841418883,0.15809051456912585,37405,464.85763935302765,35.85082208260928,115.65298756850687,2885,68,1417,172,travis,ihaque,agramfort,false,agramfort,3,0.6666666666666666,23,5,948,true,false,false,false,0,5,1,17,2,0,-1
4059513,scikit-learn/scikit-learn,python,3342,1404437791,1407187411,1407187411,45827,45827,commit_sha_in_comments,false,false,false,154,7,7,0,9,0,9,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,132,260,132,260,49.353245087999795,1.1527106500310191,13,olivier.grisel@ensta.org,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/testing.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/testing.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/testing.py,6,0.003026634382566586,0,2,false,Truncating in MinMaxScaler The output of MinMaxScaler doesnt always lie within the passed feature range if data that you transform() has values outside the range of the values that you fit() on If youre using it just to make the scale of the data nicer this probably doesnt matter but if your algorithm actually relies on the data lying in a certain range ([example](https://githubcom/dougalsutherland/skl-groups/blob/2c7874e824b6f4c78d15c5e1c3f963b444a98b6e/skl_groups/summaries/l2_densitypy)) this is no goodSo this PR adds optional support for truncation so that values that would be transformed outside of feature_range are clipped to the ends of it It also adds a fit_feature_range to make truncation less likely (eg if you need your data to lie in [0 1] you can make your training data like in [1 9] and then test values have more of a range to avoid clipping)Incidentally I also add assert_array_{less_equalgreatergreater_equal} because my tests wanted them and its silly that numpy only provides assert_array_less,,1916,0.7755741127348643,0.15254237288135594,37345,465.2028383987147,35.82808943633685,115.59780425759807,2875,67,1416,204,travis,dougalsutherland,dougalsutherland,true,dougalsutherland,3,1.0,28,26,2046,false,true,false,false,0,0,0,0,0,0,12
4056429,scikit-learn/scikit-learn,python,3341,1404416216,1405356500,1405356500,15671,15671,merged_in_comments,false,false,false,48,4,4,4,12,0,16,0,5,0,0,37,37,1,0,0,0,0,37,37,1,0,0,105,0,105,0,351.2024845543188,8.20280092101893,57,valentin.haenel@gmx.de,doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/covariance.rst|doc/modules/cross_decomposition.rst|doc/modules/decomposition.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/gaussian_process.rst|doc/modules/isotonic.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/lda_qda.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/modules/neural_networks.rst|doc/modules/outlier_detection.rst|doc/modules/random_projection.rst|doc/modules/scaling_strategies.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/sphinxext/gen_rst.py|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/modules/clustering.rst|doc/modules/computational_performance.rst|doc/modules/covariance.rst|doc/modules/cross_decomposition.rst|doc/modules/decomposition.rst|doc/modules/density.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/feature_selection.rst|doc/modules/gaussian_process.rst|doc/modules/isotonic.rst|doc/modules/kernel_approximation.rst|doc/modules/label_propagation.rst|doc/modules/lda_qda.rst|doc/modules/learning_curve.rst|doc/modules/linear_model.rst|doc/modules/manifold.rst|doc/modules/mixture.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/neighbors.rst|doc/modules/neural_networks.rst|doc/modules/outlier_detection.rst|doc/modules/random_projection.rst|doc/modules/scaling_strategies.rst|doc/modules/sgd.rst|doc/modules/svm.rst|doc/modules/tree.rst|doc/sphinxext/gen_rst.py|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst,17,0.0,0,5,false,[MRG] ensure that examples figures are displayed in the correct order New attempt at fixing the order of figures in examples This time I also switched to a new format %03d format to make it more user friendly for users doing an ls -l on their filesystem folders,,1915,0.7754569190600522,0.15,37345,465.2028383987147,35.82808943633685,115.59780425759807,2874,69,1416,186,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,74,0.8513513513513513,978,123,1863,true,true,true,true,20,390,45,203,97,9,0
4056283,scikit-learn/scikit-learn,python,3340,1404415297,1446769508,1446769508,705903,705903,merged_in_comments,false,true,false,45,15,13,8,21,0,29,0,8,6,0,14,20,15,0,0,6,0,14,20,15,0,0,10243,18,10369,18,188.45445825287646,4.40160440689516,38,sdenton4@gmail.com,sklearn/model_selection/partition.py|sklearn/grid_search.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/cross_validation.py|sklearn/model_selection/__init__.py|sklearn/model_selection/partition.py|sklearn/model_selection/scoring.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/model_selection/validate.py|sklearn/feature_selection/rfe.py|sklearn/linear_model/least_angle.py|sklearn/model_selection/validate.py|sklearn/covariance/graph_lasso_.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/omp.py|sklearn/cross_validation.py|sklearn/model_selection/partition.py|sklearn/model_selection/validate.py|sklearn/tests/test_cross_validation.py|sklearn/learning_curve.py|sklearn/model_selection/validate.py|sklearn/covariance/graph_lasso_.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/model_selection/__init__.py|sklearn/model_selection/partition.py|sklearn/model_selection/scoring.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/model_selection/validate.py|sklearn/tests/test_cross_validation.py|sklearn/learning_curve.py|sklearn/model_selection/__init__.py|sklearn/model_selection/validate.py|sklearn/grid_search.py|sklearn/model_selection/__init__.py|sklearn/model_selection/search.py|sklearn/model_selection/utils.py|sklearn/model_selection/partition.py,18,0.0011904761904761906,0,18,false,Data independent CV and model_selection module Hi guysIm working on #1848 and #2904 and would like to get some feedback on my changesHeres a writeup about the issues/changes: http://pignaciogithubio/jwoc-blog/stories/stuff-about-scikit-learn18482904htmlAny thoughts / suggestions / improvements / rants on the changes are greatly appreciated,,1914,0.7753396029258098,0.15,37345,465.2028383987147,35.82808943633685,115.59780425759807,2874,69,1416,423,travis,pignacio,MechCoder,false,MechCoder,1,1.0,5,4,390,true,false,false,false,0,4,1,0,19,0,471
4054738,scikit-learn/scikit-learn,python,3339,1404406800,1404416086,1404416086,154,154,merged_in_comments,false,false,true,26,1,1,2,5,0,7,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,33,0,33,0,4.493798985175593,0.10495864942769521,17,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py,17,0.010035419126328217,0,0,false,MAINT ensure that examples figures are displayed in the correct order For instance the ordering of the figures generated in this example is currently random:http://scikit-learnorg/dev/auto_examples/plot_johnson_lindenstrauss_boundhtml,,1913,0.775222164140094,0.1487603305785124,37345,465.2028383987147,35.82808943633685,115.59780425759807,2874,69,1416,173,travis,ogrisel,ogrisel,true,ogrisel,73,0.8493150684931506,977,123,1863,true,true,false,false,21,387,46,203,99,9,7
4054510,scikit-learn/scikit-learn,python,3338,1404405382,1405280188,1405280188,14580,14580,commit_sha_in_comments,false,false,false,27,3,1,0,9,0,9,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,5,0,5,32,4.179034361146226,0.09760690318031659,6,olivier.grisel@ensta.org,sklearn/preprocessing/data.py,6,0.0035419126328217238,0,5,false,Fix MinMaxScaler for 1D inputs Now MinMaxScaler chokes on 1D input arrays (only one feature)This patches it the same way as already done for the StandardScaler,,1912,0.7751046025104602,0.1487603305785124,37345,465.2028383987147,35.82808943633685,115.59780425759807,2874,69,1416,184,travis,gatagat,larsmans,false,larsmans,1,0.0,0,0,800,false,false,false,false,0,0,0,0,2,0,47
4049045,scikit-learn/scikit-learn,python,3335,1404329202,1406633155,1406633155,38399,38399,commit_sha_in_comments,false,false,false,19,16,3,20,132,0,152,0,7,0,0,4,8,4,0,0,0,0,8,8,5,0,0,352,0,2141,62,38.09960623878385,0.8898669539384146,25,olivier.grisel@ensta.org,sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py,18,0.006444053895723492,0,72,false,WIP: Random with replacement Testing if random with replacement has the same effect on the duke and arcene datasets,,1910,0.775392670157068,0.14997070884592853,37345,465.2028383987147,35.82808943633685,115.59780425759807,2868,69,1415,196,travis,MechCoder,ogrisel,false,ogrisel,10,0.7,73,40,743,true,true,true,false,1,126,14,29,192,2,76
4046870,scikit-learn/scikit-learn,python,3333,1404311277,1404490136,1404490136,2980,2980,commits_in_master,false,false,false,56,3,2,2,9,0,11,0,6,1,0,2,4,3,0,0,1,0,3,4,3,0,0,22,18,32,18,28.68639287799182,0.67000883372112,10,olivier.grisel@ensta.org,continuous_integration/test_script.sh|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/utils/testing.py|continuous_integration/test_script.sh|doc/tutorial/text_analytics/working_with_text_data_fixture.py|sklearn/utils/testing.py,6,0.002925687536571094,0,4,false,MAINT skip tests that require large datadownload under travis The text tutorial requires the 20 newsgroups data that gets downloaded over and over by stateless travis workers wasting a significant amount of bandwithFurthermore this should reduce memory pressure that caused some either failures when calling osfork() on a large Python process on loaded travis workers,,1909,0.7752750130958617,0.1492100643651258,37345,465.2028383987147,35.82808943633685,115.59780425759807,2867,67,1415,175,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,72,0.8472222222222222,976,123,1862,true,true,true,true,23,398,50,206,105,9,1
4043004,scikit-learn/scikit-learn,python,3331,1404260933,1404491581,1404491581,3844,3844,commit_sha_in_comments,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.6077489786505454,0.10761823718109366,0,,MANIFEST.in,0,0.0,0,0,false,Include binary_treepxi in source distribution ,,1908,0.7751572327044025,0.1489111241907004,37292,464.79137616647006,35.71811648610962,115.11852408023168,2864,66,1414,175,travis,cgohlke,larsmans,false,larsmans,3,1.0,31,0,1323,false,false,false,false,1,0,0,0,2,0,14
4047391,scikit-learn/scikit-learn,python,3329,1404233826,,1404655078,7020,,unknown,false,false,false,5,2,1,5,11,0,16,0,7,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,58,0,4.187772229928874,0.09780924457817369,7,michael@bommaritollc.com,sklearn/feature_extraction/text.py,7,0.004134672179562906,0,0,false,skipgram analyzer added to feature_extraction/textpy ,,1907,0.7755637126376508,0.1494388659184879,37292,464.79137616647006,35.71811648610962,115.11852408023168,2862,66,1414,176,travis,afshinrahimi,larsmans,false,,0,0,0,2,290,false,false,false,false,0,0,0,0,0,0,13
4039040,scikit-learn/scikit-learn,python,3328,1404226727,1404232936,1404232936,103,103,commits_in_master,false,false,false,16,1,1,0,4,0,4,0,3,0,0,6,6,6,0,0,0,0,6,6,6,0,0,32,125,32,125,26.62711064377954,0.6218985610037612,4,olivier.grisel@ensta.org,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_logger.py|sklearn/externals/joblib/test/test_pool.py,4,0.0011806375442739079,0,0,false,MAINT bump joblib to 082 This new version of joblib notably includes a fix for #3313,,1906,0.7754459601259182,0.1487603305785124,37292,465.3544996245844,35.71811648610962,115.41349351067252,2862,66,1414,169,travis,ogrisel,ogrisel,true,ogrisel,71,0.8450704225352113,975,123,1861,true,true,false,false,23,413,49,204,104,9,12
4038449,scikit-learn/scikit-learn,python,3327,1404220349,1405937108,1405937108,28612,28612,commits_in_master,false,false,false,110,8,6,9,8,8,25,0,5,2,0,7,9,3,1,1,2,0,7,9,3,1,1,656,0,680,0,112.84722820014412,2.6356419128534916,17,olivier.grisel@ensta.org,doc/conf.py|doc/sphinxext/gen_rst.py|doc/templates/class.rst|doc/templates/class_with_call.rst|doc/templates/function.rst|doc/themes/scikit-learn/static/css/examples.css|doc/themes/scikit-learn/static/nature.css_t|doc/sphinxext/gen_rst.py|doc/conf.py|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/css/examples.css|doc/themes/scikit-learn/static/js/examples.js|doc/conf.py|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/css/examples.css|doc/themes/scikit-learn/static/js/examples.js|doc/sphinxext/gen_rst.py|doc/conf.py|doc/sphinxext/gen_rst.py|doc/templates/class.rst|doc/templates/class_with_call.rst|doc/templates/function.rst|doc/themes/scikit-learn/static/css/examples.css|doc/themes/scikit-learn/static/nature.css_t,17,0.0,0,6,false,[WIP] DOC show referring examples on API reference pages This places an example gallery on each API reference (class function) page Currently this is at the end of the page It might be nicer for this to appear under the examples section but that is much harder to implement (except perhaps by Javascript/DOM games) The intention is that this should provide a manual of examples for a particular class in many cases such that for example the use of PCAcomponents_ should be more obvious (cf #3322)The CSS/Javascript/Python that makes the example galleries work is a bit of a mess but Im only cleaning it up minimally in this PR,,1905,0.7753280839895013,0.1488481984642646,37292,465.3544996245844,35.71811648610962,115.41349351067252,2862,66,1414,190,travis,jnothman,jnothman,true,jnothman,79,0.6582278481012658,26,1,1890,true,true,false,false,16,285,26,258,43,2,13
4038363,scikit-learn/scikit-learn,python,3326,1404219640,1404248336,1404248336,478,478,commit_sha_in_comments,false,false,false,94,1,1,0,3,0,3,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.765544413950539,0.1113032973454711,2,joel.nothman@gmail.com,Makefile,2,0.0011813349084465446,0,1,false,MAINT remove residual sparsefuncs*so when compiling The sparsefuncs - sparsefuncs_fast rename in bbb8f1d has caused a lot of errors being reported (ImportError: cannot import name inplace_column_scale or similar) due to dirty working copies This attempts to avoid that by removing the old file explicitly Ideally this deletion would only remain in the codebase for a short while while 014 users pull in the file renameThe current solution may be too simple:* many users may be using python setuppy build --inplace directly* I do not think the current solution suffices in Windows,,1904,0.7752100840336135,0.1488481984642646,37292,465.3544996245844,35.71811648610962,115.41349351067252,2862,66,1414,169,travis,jnothman,larsmans,false,larsmans,78,0.6538461538461539,26,1,1890,true,true,false,false,16,285,25,258,43,2,12
4035849,scikit-learn/scikit-learn,python,3324,1404187823,1404492169,1404492169,5072,5072,commit_sha_in_comments,false,false,false,78,6,1,43,13,0,56,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,129,14,292,14,9.192394721762902,0.2146950152531062,3,olivier.grisel@ensta.org,sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,3,0.0017584994138335288,0,4,false,Add partial_fit to GaussianNB Uses a [method due to Chan Golub and LeVeque](http://istanfordedu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773pdf) to perform online update of the model parameters in GaussianNB Does not implement sample weightingAdded tests to test_naive_bayes to ensure that partial_fit called on the entire toy set produces the same result as fit and that it produces the same result even if the toy set is fitted in two partsTODO: fit could be a thin wrapper around partial_fit rather than duplicating code,,1903,0.7750919600630584,0.14712778429073858,37264,465.3821382567626,35.71811936453413,115.47337913267496,2857,66,1413,176,travis,ihaque,larsmans,false,larsmans,2,0.5,23,5,944,false,false,false,false,0,0,0,0,1,0,10
4010413,scikit-learn/scikit-learn,python,3322,1403869534,,1435793595,532067,,unknown,false,false,false,25,2,1,5,13,0,18,0,7,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,8,0,4.479979303001659,0.10487635414087185,3,olivier.grisel@ensta.org,sklearn/decomposition/pca.py,3,0.0017605633802816902,0,3,true,DOC Mention that loadings are found in the components_ attribute Changed the docstring for the components_ attribute to mention that the loadings are found here  ,,1902,0.7754994742376445,0.15023474178403756,37264,465.3821382567626,35.71811936453413,115.47337913267496,2831,66,1410,358,travis,FedericoV,amueller,false,,4,0.75,3,1,1222,false,false,false,false,0,0,0,0,0,0,12
4001282,scikit-learn/scikit-learn,python,3319,1403787233,1403804179,1403804179,282,282,commit_sha_in_comments,false,false,false,31,2,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,8,0,4.370107239260464,0.10230424818060783,4,michael@bommaritollc.com,sklearn/grid_search.py,4,0.0023515579071134627,0,1,false,Added a note about the sample_weight parameter to GridSearchCV docs Support for passing sample_weights to GridSearchCV was introduced in 5d8b429f Added documentation to GridSearchCV on how to use the added functionality,,1901,0.7753813782219884,0.15167548500881833,37264,465.3821382567626,35.71811936453413,115.47337913267496,2816,67,1409,171,travis,mattilyra,mattilyra,true,mattilyra,3,0.6666666666666666,6,1,1014,false,true,false,false,0,0,0,0,0,0,16
4001154,scikit-learn/scikit-learn,python,3318,1403786225,1403786451,1403786451,3,3,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.403300219567801,0.10308129612849044,2,olivier.grisel@ensta.org,sklearn/linear_model/stochastic_gradient.py,2,0.001176470588235294,0,0,false,small spelling error in doc ,,1900,0.7752631578947369,0.1511764705882353,37264,465.3821382567626,35.71811936453413,115.47337913267496,2816,67,1409,171,travis,abhishekkrthakur,arjoly,false,arjoly,1,1.0,92,47,960,false,true,false,false,0,3,0,0,0,0,-1
3999200,scikit-learn/scikit-learn,python,3317,1403761263,1403885015,1403885015,2062,2062,commits_in_master,false,false,false,44,15,10,3,13,0,16,0,4,1,0,5,6,0,0,0,1,0,5,6,0,0,0,0,0,0,0,77.1015838565447,1.8049491664935007,7,jnothman@student.usyd.edu.au,doc/modules/model_persistence.rst|doc/modules/model_persistence.rst|doc/model_persistence.rst|doc/tutorial/basic/tutorial.rst|doc/user_guide.rst|doc/model_persistence.rst|doc/tutorial/basic/tutorial.rst|doc/model_selection.rst|doc/modules/model_persistence.rst|doc/model_persistence.rst|doc/tutorial/basic/tutorial.rst|doc/user_guide.rst|doc/model_persistence.rst|doc/tutorial/basic/tutorial.rst|doc/model_selection.rst|doc/modules/model_persistence.rst|doc/user_guide.rst|doc/modules/model_persistence.rst|doc/modules/model_persistence.rst,7,0.0,0,8,false,Model persistence doc Taking over #3084 Fixes #1332Comments on #3084 should be addressed in this patch I took the liberty and simplified the Security and maintenance limitations section as it felt overly verbose for meFeel free to comment any improvements/corrections on this,,1899,0.775144813059505,0.15144372421921037,37264,465.3821382567626,35.71811936453413,115.47337913267496,2811,67,1409,172,travis,pignacio,jnothman,false,jnothman,0,0,5,4,383,true,false,false,false,0,0,0,0,7,0,44
3997983,scikit-learn/scikit-learn,python,3316,1403747442,1405273910,1405273910,25441,25441,merged_in_comments,false,false,false,11,8,3,11,8,0,19,0,6,0,0,4,5,4,0,0,0,0,5,5,5,0,0,75,36,143,50,23.259730382742774,0.5445106166081886,5,olivier.grisel@ensta.org,sklearn/datasets/lfw.py|sklearn/datasets/lfw.py|sklearn/datasets/california_housing.py|sklearn/datasets/covtype.py|sklearn/datasets/species_distributions.py,2,0.0011813349084465446,0,1,false,DOC dataset doc add return and improve doc #3315 Fixes #3315,,1898,0.7750263435194942,0.151801535735381,37264,465.3821382567626,35.71811936453413,115.47337913267496,2810,67,1408,188,travis,ilam,larsmans,false,larsmans,3,0.6666666666666666,4,6,1281,true,true,false,false,3,13,3,1,7,0,12
3996961,scikit-learn/scikit-learn,python,3314,1403739959,1403744769,1403744769,80,80,commit_sha_in_comments,false,false,false,124,7,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,18,0,4.425058579901158,0.1035907036204566,21,ronald.phlypo@inria.fr,sklearn/preprocessing/label.py,21,0.012433392539964476,0,2,false,Patch for errors introduced in the sparse label binarizer The travis error:ERROR: Test that ovr works with classes that are always present or absent----------------------------------------------------------------------Traceback (most recent call last):  File /home/travis/anaconda/envs/testenv/lib/python26/site-packages/nose/casepy line 197 in runTest    selftest(*selfarg)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multiclasspy line 73 in test_ovr_always_present    assert_warns(UserWarning ovrfit X y)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/utils/testingpy line 135 in assert_warns    result  func(*args **kw)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/multiclasspy line 202 in fit    n_jobsselfn_jobs)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/multiclasspy line 92 in fit_ovr    for i in range(Yshape[1]))  File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallelpy line 644 in __call__    selfdispatch(function args kwargs)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallelpy line 391 in dispatch    job  ImmediateApply(func args kwargs)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallelpy line 129 in __init__    selfresults  func(*args **kwargs)  File /home/travis/build/scikit-learn/scikit-learn/sklearn/multiclasspy line 57 in _fit_binary    str(classes[c]))TypeError: list indices must be integers not numpyfloat64----------------------------------------------------------------------,,1897,0.7749077490774908,0.1521610420367081,37262,465.4071171703075,35.72003649830927,115.47957704900435,2810,67,1408,170,travis,hamsal,jnothman,false,jnothman,4,0.75,4,8,543,true,true,false,false,4,52,15,46,240,0,30
3990373,scikit-learn/scikit-learn,python,3311,1403690070,1403783312,1403783312,1554,1554,merged_in_comments,false,false,false,4,2,1,4,2,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,42,0,54,0,4.382105858771359,0.10257990575501691,7,michael@bommaritollc.com,sklearn/feature_extraction/text.py,7,0.004166666666666667,0,0,false,DOC typos in feature_extractiontext ,,1896,0.7747890295358649,0.1494047619047619,37143,460.3020757612471,35.538324852596716,114.31494494251945,2801,67,1408,176,travis,ryw90,larsmans,false,larsmans,0,0,7,10,918,false,false,false,false,0,0,0,0,1,0,11
3982427,scikit-learn/scikit-learn,python,3310,1403624126,1431295112,1431295112,461183,461183,commit_sha_in_comments,false,true,false,216,2,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,3,0,4.2927127235943425,0.10048728982784513,1,larsmans@gmail.com,sklearn/tree/export.py,1,0.0005991611743559018,0,0,false,[bug]Fixed bugs when rendering dot file into graphics which is introduced by treeexport_graphviz When exporting a tree into a dot file using treeexport_graphviz  the dot file is usually like this     digraph Tree {        0 [labelA  15000\nerror  0451898202175\nsamples  19396\nvalue  [  6690  12706] shapebox]         1 [labelB  15000\nerror  0484204229212\nsamples  6690\nvalue  [ 2667  4023] shapebox]         0 - 1         2 [labelC  15000\nerror  0429043311043\nsamples  12706\nvalue  [ 4023  8683] shapebox]         0 - 2     }However when sample number is large treeexport_graphviz will create a dot file like this    digraph Tree {        0 [labelA  15000\nerror  0451898202175\nsamples  248325 e6\nvalue  [  2470544 e6   12706 e4 ]        1 [labelB  15000\nerror  0484204229212\nsamples  2470544 e6\nvalue  [ 2667e3  2467877e6] shapebox]         0 - 1         2 [labelC  15000\nerror  0429043311043\nsamples  12706 e4\nvalue  [ 4023 e3  8683  e3] shapebox]         0 - 2     }The scientific notation(like value  [  2470544 e6   12706 e4 ])  will broke the dot file rendering process like follows[1][2]:    dot -Tpdf filedot -o filepdfThe patch will transform the scientific notation(like value  [  2470544 e6   12706 e4 ]) into normal one(like  value  [  2470544   12706])[1] http://scikit-learnorg/stable/modules/treehtml#classification[2] http://scikit-learnorg/stable/modules/generated/sklearntreeexport_graphvizhtml#sklearntreeexport_graphviz,,1895,0.7746701846965699,0.14979029358897544,37144,460.2896833943571,35.53736808098213,114.31186732715916,2794,67,1407,336,travis,logicmd,glouppe,false,glouppe,0,0,23,11,1304,false,false,false,false,0,0,0,0,0,0,1272
3975394,scikit-learn/scikit-learn,python,3309,1403554926,1403625770,1403625770,1180,1180,commits_in_master,false,false,false,9,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,6,0,6,4.329791602398889,0.10135622308569191,5,olivier.grisel@ensta.org,sklearn/manifold/spectral_embedding_.py,5,0.003003003003003003,0,0,false,MAINT: symeig-eigh Use eigh instead of the deprecated symeig,,1894,0.7745512143611405,0.15075075075075076,37134,460.5752140895136,35.546938116012285,114.3426509398395,2784,68,1406,174,travis,argriffing,jnothman,false,jnothman,0,0,14,9,1816,false,false,false,false,0,1,0,0,0,0,1044
3974054,scikit-learn/scikit-learn,python,3308,1403546045,1403691369,1403691369,2422,2422,commits_in_master,false,false,false,12,4,1,12,8,0,20,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,8,6,56,9.454898359832379,0.22133062242254387,6,olivier.grisel@ensta.org,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,5,0.0030048076923076925,0,3,false,FIX OvR with constant label for non-predict methods bug mentioned on ML,,1893,0.774432118330692,0.1502403846153846,37134,460.5752140895136,35.546938116012285,114.3426509398395,2782,68,1406,174,travis,jnothman,arjoly,false,arjoly,77,0.6493506493506493,26,1,1882,true,true,false,true,16,306,26,267,48,3,56
3966350,scikit-learn/scikit-learn,python,3306,1403436861,1407235509,1407235509,63310,63310,commits_in_master,false,false,false,6,136,4,501,190,7,698,0,19,1,0,11,54,10,0,0,16,5,48,69,40,0,5,2343,415,16724,5166,54.604813586673764,1.2782796267701513,42,olivier.grisel@ensta.org,sklearn/neural_network/extreme_learning_machines.py|sklearn/neural_network/extreme_learning_machines.py|sklearn/neural_network/extreme_learning_machines.py|doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/weight_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,15,0.003656307129798903,0,61,false,GSoC 2014: Standard Extreme Learning Machines ,,1892,0.7743128964059197,0.15112736136502133,37110,460.8730800323363,35.51603341417408,114.38965238480193,2766,68,1405,277,travis,IssamLaradji,arjoly,false,arjoly,5,0.6,19,1,513,true,false,false,false,1,31,3,2,27,0,587
3961895,scikit-learn/scikit-learn,python,3304,1403347082,1417077361,1417077361,228837,228837,commit_sha_in_comments,false,true,false,9,120,2,361,135,32,528,0,11,2,0,2,20,4,0,0,7,2,16,25,15,0,0,394,0,3698,1960,18.143163252139253,0.4247251190329945,0,,sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/lshashing.py,0,0.0,0,28,false,[WIP] Locality Sensitive Hashing for approximate nearest neighbor search(GSoC) ,,1891,0.7741935483870968,0.15379939209726443,37110,460.8730800323363,35.51603341417408,114.38965238480193,2761,68,1404,258,travis,maheshakya,maheshakya,true,maheshakya,12,0.5,2,0,884,true,false,false,false,1,14,3,1,79,0,1302
3961807,scikit-learn/scikit-learn,python,3303,1403344469,,1403345168,11,,unknown,false,false,false,25,54,54,0,0,0,0,0,0,4,0,73,77,64,0,1,4,0,73,77,64,0,1,1926,344,1926,344,464.4053905231381,10.87156809583616,193,yoshiki89@gmail.com,sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/lshashing.py|sklearn/neighbors/__init__.py|sklearn/neighbors/lsh_forest.py|sklearn/neighbors/lsh_forest.py|sklearn/metrics/metrics.py|sklearn/cluster/tests/test_spectral.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|doc/modules/linear_model.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/logistic.py|sklearn/svm/classes.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/linear_model/tests/test_bayes.py|sklearn/svm/base.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/metrics/metrics.py|sklearn/linear_model/tests/test_sgd.py|.travis.yml|continuous_integration/install.sh|continuous_integration/test_script.sh|continuous_integration/install.sh|continuous_integration/test_script.sh|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_sgd.py|sklearn/cluster/hierarchical.py|sklearn/tree/tests/test_tree.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/ensemble/tests/test_bagging.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/neural_network/tests/test_rbm.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py|sklearn/cluster/affinity_propagation_.py|sklearn/neighbors/base.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|doc/index.rst|examples/linear_model/plot_sgd_loss_functions.py|doc/modules/clustering.rst|doc/Makefile|sklearn/cluster/mean_shift_.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/nmf.py|sklearn/mixture/dpgmm.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|doc/tutorial/basic/tutorial.rst|examples/applications/face_recognition.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|doc/tutorial/text_analytics/working_with_text_data.rst|sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py|doc/developers/index.rst|doc/tutorial/statistical_inference/index.rst|doc/tutorial/text_analytics/working_with_text_data.rst|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/utils/validation.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|.travis.yml|README.rst|setup.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/supervised.py,26,0.0018270401948842874,0,0,false,[WIP] Locality Sensitive Hashing for approximate nearest neighbor search(GSoC) LSH forest has been implemented as the LSH data structure Test cases need to be written,,1890,0.7746031746031746,0.1540803897685749,37110,460.8730800323363,35.51603341417408,114.38965238480193,2761,68,1404,173,travis,maheshakya,maheshakya,true,,11,0.5454545454545454,2,0,884,true,false,false,false,1,14,1,1,76,0,-1
3950135,scikit-learn/scikit-learn,python,3302,1403221689,,1403889679,11133,,unknown,false,false,false,194,1,1,0,10,0,10,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,20,5,20,8.732923804749088,0.20443490909098516,10,olivier.grisel@ensta.org,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,8,0.004898958971218616,0,1,false,[BUG]  _BaseRidgeCV doesnt pass scoring if cv is not None / RidgeClassifierCV shouldnt use continuous setting in inner CV I could not create a failing test for the failure to pass scoring through to the GridSearchCV which is involved since these objects are not persisted in the _BaseRidgeCVPassing the the scoring to the GridSearchCV within _BaseRidgeCV creates a new problem since the internal CV of this object seems to be exclusively working in the continuous setting thus making eg f1_score raise an exception due to discrepancy of y_true and y_pred typesWhile there may exist consistency proofs for accuracy score wrt continous Ridge I am unsure if this is the case for f1_score () So the solution would be to have RidgeClassifierCV use an inner CV evaluated in a classifier setting not a regression approximation to this setting This looks like a slightly bigger change than I thoughtThe diff I am pushing shows the situation where RidgeClassifierCV breaks due to scoringf1 once it is passed through to the GridSearchCV within _BaseRidgeCVI have some ideas for a general refactoring of this code but will first collect those in a gist for discussion,,1889,0.7750132345156168,0.15554194733619106,37110,460.8730800323363,35.46213958501752,114.3088116410671,2739,67,1402,178,travis,eickenberg,eickenberg,true,,9,0.7777777777777778,11,9,896,true,true,false,false,0,23,7,21,44,0,323
3949626,scikit-learn/scikit-learn,python,3301,1403218305,,1403228516,170,,unknown,false,false,false,49,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,27,0,27,0,4.6453845326794925,0.1087469427037199,17,olivier.grisel@ensta.org,sklearn/preprocessing/label.py,17,0.010416666666666666,0,0,false,DOC Binary input case doc in LabelBinarizer and label_binarizer #2704 I added examples to illustrate the difference in using two classes and more than two classesThe changes were made in Label_Binarizer label_binarize and also inverse_transform (since it can accept parameter input of shape [n_samples1])What do you think,,1888,0.7754237288135594,0.15502450980392157,37110,460.8730800323363,35.46213958501752,114.3088116410671,2736,67,1402,173,travis,ilam,jnothman,false,,2,1.0,4,6,1275,true,true,false,false,3,4,2,0,2,0,169
3949391,scikit-learn/scikit-learn,python,3300,1403216826,1405365227,1405365227,35806,35806,commit_sha_in_comments,false,false,false,52,6,2,15,33,0,48,0,6,0,0,4,7,4,0,0,0,0,7,7,4,0,0,177,0,359,0,25.675466168917687,0.6010543214927505,29,olivier.grisel@ensta.org,sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/covariance/graph_lasso_.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py,21,0.007352941176470588,2,25,false,[WIP] Random coordinate descent This is just an attempt to check if shuffling of features helps in random convergence for now it is just a quick hack if there is really a decrease in the number of iterations we can shift to the Cython version to reduce the overheadping @ogrisel @agramfort,,1887,0.775304716481187,0.15502450980392157,37110,460.8730800323363,35.46213958501752,114.3088116410671,2736,67,1402,192,travis,MechCoder,MechCoder,true,MechCoder,9,0.6666666666666666,72,40,730,true,true,false,false,3,116,13,25,175,2,675
3946264,scikit-learn/scikit-learn,python,3297,1403197247,1403197405,1403197405,2,2,commits_in_master,false,false,false,33,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.677867450918995,0.10950670196041931,2,jnothman@student.usyd.edu.au,doc/modules/kernel_approximation.rst,2,0.0012330456226880395,0,0,false,DOC add the Random Kitchen Sink synonym to the RBFSampler section This should make it easier for people to find this code if they search for theterms random kitchen sink and python,,1886,0.7751855779427359,0.15536374845869297,37110,460.8730800323363,35.46213958501752,114.3088116410671,2730,67,1402,172,travis,esc,GaelVaroquaux,false,GaelVaroquaux,0,0,152,246,2008,false,false,true,false,0,0,0,0,0,0,2
3945667,scikit-learn/scikit-learn,python,3296,1403193432,1403212535,1403212535,318,318,commit_sha_in_comments,false,false,false,28,4,2,7,7,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,94,0,562,0,9.293488657734809,0.21755624828898695,3,g.louppe@gmail.com,sklearn/tree/_utils.c|sklearn/tree/_utils.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pyx,3,0.0018484288354898336,1,2,false,ENH reduce amount of boiler code using standard operation in tree/_utils Inspired from the last @jnothman pull request I simplify a bit sklearn/tree/_utilspyx using standard tools (memcpy one-liner swap),,1885,0.7750663129973475,0.15526802218114602,37110,460.8730800323363,35.46213958501752,114.3088116410671,2729,67,1402,171,travis,arjoly,larsmans,false,larsmans,55,0.8,25,25,912,true,true,true,true,21,197,14,214,335,4,18
3944375,scikit-learn/scikit-learn,python,3295,1403181182,1403523109,1403523109,5698,5698,commits_in_master,false,false,false,36,15,1,5,28,2,35,0,7,0,0,1,8,1,0,0,0,0,8,8,5,0,0,6,0,221,60,4.174975633457363,0.0977342383429892,8,rmcgibbo@gmail.com,sklearn/cross_validation.py,8,0.004932182490752158,0,12,false,[BUG] exception in bootstrap must be evoked when n_train + n_test  n (resolves issue #3290) Since selftest_size + selftrain_size may not exceed n the test selftest_size  n is insufficient for throwing a ValueError exception,,1884,0.7749469214437368,0.15474722564734894,37110,460.8730800323363,35.46213958501752,114.3088116410671,2724,67,1402,174,travis,rphlypo,ogrisel,false,ogrisel,1,1.0,2,0,288,false,false,false,false,0,0,0,0,0,0,11
3943560,scikit-learn/scikit-learn,python,3293,1403173074,1403506576,1403506576,5558,5558,commit_sha_in_comments,false,false,false,113,5,1,9,15,0,24,0,6,0,0,3,3,3,0,0,0,0,3,3,3,0,0,320,0,746,0,8.031887464439105,0.18802275095998483,21,larsmans@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,17,0.009242144177449169,0,6,false,[MRG] Factor out split data as struct in Tree splitters and builders This uses a struct to store 6 C variables that get passed by tree builders into splitters where they are duplicated for the current and best candidate Copying current to best is then a one-liner The refactor hence saves 300 lines of C codeThis changes the not-quite-public Splitternode_split interface so we may want to be conservative about itI originally made part of this patch as a contribution to #3173 where the code was duplicated another two times but this version is a bit more ambitious in modifying the node_split interface and the tree builder and thought warranted separate review,,1883,0.7748274030801912,0.15465187923598275,37110,460.8730800323363,35.46213958501752,114.3088116410671,2723,67,1402,174,travis,jnothman,jnothman,true,jnothman,76,0.6447368421052632,26,1,1878,true,true,false,false,16,283,23,216,43,3,13
3940581,scikit-learn/scikit-learn,python,3292,1403139185,1403139871,1403139871,11,11,commits_in_master,false,false,false,20,1,1,0,2,1,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.527992628623407,0.10599821738589879,7,olivier.grisel@ensta.org,sklearn/linear_model/ridge.py,7,0.004331683168316832,0,1,false,[MRG] RidgeClassifierCV didnt have scoring parameter  but is advertised as having one Reported [here](http://stackoverflowcom/q/24294940/166749) Please merge if tests pass,,1882,0.7747077577045696,0.15408415841584158,37110,460.8730800323363,35.46213958501752,114.3088116410671,2719,67,1401,169,travis,larsmans,larsmans,true,larsmans,116,0.75,141,38,1431,true,true,false,false,48,255,46,61,112,16,1
3939752,scikit-learn/scikit-learn,python,3291,1403133163,1403137782,1403137782,76,76,commits_in_master,false,false,false,93,1,1,0,3,0,3,0,3,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.628974276937335,0.22541007464684085,8,olivier.grisel@ensta.org,README.rst|doc/modules/computational_performance.rst,6,0.0037151702786377707,0,0,false,[MRG] big fat warning about multithreaded BLAS #3288 is yet another case where someone sees Python crash apparently because multiprocessing and BLAS (here Apples Accelerate) dont like each other Ive decided to document the situation better and put a link in the READMEI wasnt aware of a better way to link to the docs from the README so I just put in the full URLAlso changed the description of non-BLAS-using parts: LinearSVC and LogisticRegression actually do use it since bb5f643d0cc9c9e197bed1b082a913f333d419dc (and their predict method has used npdot for quite a while),,1881,0.7745879851143009,0.15356037151702787,37110,460.8730800323363,35.46213958501752,114.3088116410671,2719,67,1401,169,travis,larsmans,larsmans,true,larsmans,115,0.7478260869565218,141,38,1431,true,true,false,false,48,256,44,61,111,16,42
3929102,scikit-learn/scikit-learn,python,3286,1403036234,1403091716,1403091716,924,924,commits_in_master,false,false,false,8,2,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,14,0,4.010027338703743,0.09387289921792576,1,larsmans@gmail.com,sklearn/ensemble/bagging.py,1,0.0006293266205160479,0,0,false,Sparse input support in BaggingClassfier and BaggingRegressor #3241 ,,1880,0.774468085106383,0.1554436752674638,37105,460.93518393747473,35.46691820509365,114.32421506535508,2710,66,1400,170,travis,ilam,arjoly,false,arjoly,1,1.0,4,6,1273,false,false,false,false,2,3,1,0,1,0,881
3928988,scikit-learn/scikit-learn,python,3285,1403035346,1411498816,1411498816,141057,141057,commits_in_master,false,true,false,96,98,8,222,115,0,337,0,8,5,0,8,19,10,0,0,5,0,14,19,10,0,0,3502,1343,8149,2614,209.89422183205843,4.913527382293351,20,sdenton4@gmail.com,examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/__init__.py|examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/tests/test_incremental_pca.py|benchmarks/bench_plot_incremental_pca.py|examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/base.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|benchmarks/bench_plot_incremental_pca.py|examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/base.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|benchmarks/bench_plot_incremental_pca.py|examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/base.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|benchmarks/bench_plot_incremental_pca.py|doc/modules/classes.rst|doc/modules/decomposition.rst|doc/modules/scaling_strategies.rst|examples/decomposition/plot_incremental_pca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/base.py|sklearn/decomposition/incremental_pca.py|sklearn/decomposition/tests/test_incremental_pca.py|sklearn/utils/estimator_checks.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,10,0.0,0,17,false,[WIP] Incremental PCA Work in progress for Incremental PCA It seems pretty close but the SVD is still flipping around dimensions Adding svd_flip seems not to help Currently the computation is on columns because the original paper is done this way and converting to row based was very difficult See [Incremental Learning for Visual Tracking J Lim D Ross R Lin M Yang 2004](http://wwwcstorontoedu/~dross/ivt/LimRossLinYang_nips04pdf) for more detailsThe key idea behind this PR is to have an out-of-core PCA that gives equivalent results while using less memory (on the order of the batch_size rather than n_samples),,1879,0.7743480574773816,0.15554156171284636,37105,460.93518393747473,35.46691820509365,114.32421506535508,2710,66,1400,227,travis,kastnerkyle,ogrisel,false,ogrisel,5,0.6,219,66,817,true,false,true,false,1,24,2,1,5,0,3724
3921692,scikit-learn/scikit-learn,python,3284,1402967677,1403736431,1403736431,12812,12812,commit_sha_in_comments,false,false,false,24,7,1,9,11,0,20,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,122,0,4.34973280981345,0.10182513323436719,0,,sklearn/datasets/olivetti_faces.py,0,0.0,0,1,false,Documentation for Return #2898 Continuing the issue #2898If the changes are acceptable the same pattern will be followed for the other example datasets,,1878,0.7742279020234292,0.15467171717171718,37105,460.93518393747473,35.46691820509365,114.32421506535508,2700,66,1399,177,travis,ilam,arjoly,false,arjoly,0,0,4,6,1272,false,false,false,false,1,0,0,0,1,0,14
3921125,scikit-learn/scikit-learn,python,3283,1402963396,1403092943,1403092943,2159,2159,commit_sha_in_comments,false,false,false,27,1,1,4,2,0,6,0,3,0,0,14,14,13,0,1,0,0,14,14,13,0,1,57,0,57,0,52.174178119808815,1.2213721786444591,26,sdenton4@gmail.com,.gitattributes|sklearn/covariance/robust_covariance.py|sklearn/datasets/samples_generator.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/feature_extraction/image.py|sklearn/gaussian_process/gaussian_process.py|sklearn/learning_curve.py|sklearn/linear_model/randomized_l1.py|sklearn/naive_bayes.py|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/classification.py|sklearn/neighbors/kd_tree.c|sklearn/utils/linear_assignment_.py,6,0.0012634238787113076,0,0,false,[MRG] fix astype usage to prevent copying Removed were possible using copyFalse (from utilsfixes) were neededAlso some C integer type fixes to gradient boostingFixes #2206,,1877,0.7741076185402238,0.15413771320277952,37105,460.93518393747473,35.46691820509365,114.32421506535508,2700,66,1399,171,travis,larsmans,larsmans,true,larsmans,114,0.7456140350877193,141,38,1429,true,true,false,false,47,262,42,58,111,16,1
3916324,scikit-learn/scikit-learn,python,3282,1402931534,1403018167,1403018167,1443,1443,commit_sha_in_comments,false,false,false,26,2,2,0,4,0,4,0,2,0,1,7,8,7,0,0,0,1,7,8,7,0,0,3273,32,3273,32,58.52769270264807,1.3701048703271577,14,olivier.grisel@ensta.org,doc/developers/utilities.rst|sklearn/cluster/bicluster/spectral.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/arpack.py|doc/developers/utilities.rst|sklearn/cluster/bicluster/spectral.py|sklearn/cross_decomposition/pls_.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/arpack.py,6,0.0012642225031605564,0,1,false,[MRG] remove ARPACK backport from SciPy 010 Shouldnt be necessary now that we require SciPy 011 If this passes all tests we can also close #984,,1876,0.7739872068230277,0.15360303413400758,37105,460.93518393747473,35.46691820509365,114.32421506535508,2695,66,1399,170,travis,larsmans,ogrisel,false,ogrisel,113,0.7433628318584071,141,38,1429,true,true,true,true,45,257,41,58,110,16,34
3912858,scikit-learn/scikit-learn,python,3281,1402882263,1408229753,1408229753,89124,89124,commit_sha_in_comments,false,false,false,39,35,22,2,19,4,25,4,4,10,1,12,25,9,0,1,12,1,14,27,10,0,2,4342,623,7057,1521,209.6381200218289,4.907264943167594,13,olivier.grisel@ensta.org,sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_network/README.txt|examples/neural_network/plot_mlp_alpha.py|examples/neural_network/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/neural_networks_supervised.rst|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_supervised.rst|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/utils/fixes.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/multilayer_perceptron.py|doc/modules/neural_networks_supervised.rst|sklearn/neural_network/multilayer_perceptron.py|sklearn/utils/fixes.py|examples/neural_network/mlp_with_pretraining.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py,8,0.0,0,4,false,mlp with pretraining This extends the generic multi-layer perceptron  (mlp) https://githubcom/scikit-learn/scikit-learn/pull/3204 with a pre-training capabilityAn example is added to show how pre-training is done with an RBM and how it could improve mlps performance on the digits dataset,,1875,0.7738666666666667,0.15306767868437698,37101,460.9848791137705,35.47074202851675,114.33654079404867,2691,66,1398,207,travis,IssamLaradji,IssamLaradji,true,IssamLaradji,4,0.5,19,1,506,true,false,false,false,1,26,2,1,23,0,11
3910686,scikit-learn/scikit-learn,python,3280,1402849564,1402996279,1402996279,2445,2445,commits_in_master,false,false,false,6,2,2,0,4,0,4,0,3,0,1,2,3,0,0,0,0,1,2,3,0,0,0,0,0,0,0,17.601280426036183,0.4120156609536769,9,olivier.grisel@ensta.org,doc/modules/classes.rst|doc/modules/hmm.rst|doc/unsupervised_learning.rst|doc/modules/classes.rst|doc/unsupervised_learning.rst,8,0.0006337135614702154,0,0,false,[MRG] remove HMM documentation Fixes #3265,,1874,0.7737459978655283,0.1514575411913815,37101,460.9848791137705,35.47074202851675,114.33654079404867,2688,66,1398,173,travis,larsmans,GaelVaroquaux,false,GaelVaroquaux,112,0.7410714285714286,141,38,1428,true,true,true,false,42,262,38,61,109,16,37
3910475,scikit-learn/scikit-learn,python,3279,1402846184,1402852030,1402852030,97,97,commit_sha_in_comments,false,false,false,37,2,2,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.808641664460774,0.2296034084237748,3,olivier.grisel@ensta.org,doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst,3,0.0019011406844106464,0,0,false,Correct definition of multiclass log loss Someone who is more familiar with this concept may also want to clarify why (unlike in the example right above) there is no (1-t_{ik}) term involved due to the normalization N,,1873,0.7736252002135612,0.1514575411913815,37101,460.9848791137705,35.47074202851675,114.33654079404867,2687,66,1398,169,travis,stefanv,larsmans,false,larsmans,0,0,170,3,1984,false,false,false,false,0,0,0,0,1,0,-1
3901978,scikit-learn/scikit-learn,python,3277,1402698850,1402846066,1402846066,2453,2453,commit_sha_in_comments,false,false,false,27,2,1,0,7,0,7,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,8,17,12,17,13.171423904221257,0.3083205136510344,10,rmcgibbo@gmail.com,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/validation.py,7,0.0025412960609911056,1,4,true,FIX : allow nd X for cross_val_score (was working in 014) cc @ogriselwould be great to get this in the releasebreaks cross val in mne-python,,1871,0.7739176910742919,0.15120711562897077,37099,460.8210463893905,35.47265424944068,114.342704655112,2681,66,1396,168,travis,agramfort,larsmans,false,larsmans,39,0.8974358974358975,158,185,1654,true,true,true,false,6,98,11,74,23,2,5
3899693,scikit-learn/scikit-learn,python,3276,1402682784,,1404356489,27895,,unknown,false,false,false,4,129,30,95,96,3,194,0,6,0,0,8,10,7,0,0,0,0,10,10,8,0,0,1210,722,2294,1781,216.49626044637952,5.067807300846444,27,olivier.grisel@ensta.org,doc/modules/preprocessing.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/linear_model/tests/test_ridge.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|doc/modules/preprocessing.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/linear_model/tests/test_ridge.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/utils/multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,17,0.0031766200762388818,0,34,true,Sparse One vs Rest ,,1870,0.774331550802139,0.150571791613723,37099,460.8210463893905,35.47265424944068,114.342704655112,2680,66,1396,206,travis,hamsal,hamsal,true,,3,1.0,4,8,531,true,true,false,false,2,38,8,24,168,0,415
3876368,scikit-learn/scikit-learn,python,3268,1402474684,1404211791,1404211791,28951,28951,commits_in_master,false,false,false,87,7,1,10,13,0,23,0,5,0,0,2,4,2,0,0,0,0,4,4,3,0,0,3,24,67,38,9.36464950953302,0.21921043373418306,32,olivier.grisel@ensta.org,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,25,0.015792798483891344,0,6,false,Nonrepeating ROC thresholds Added a unit test to ensure that there are no spurious repeating values in the thresholds returned by roc_curve because of machine precision and a quick stab at a fixThe thresholds array produced in the unit test starts with the values [1 099 098 098 ] There should be only one 098 The undesirable behavior stems from machine epsilon differences between predicted probabilities that are mathematically equivalentI am not sure that my suggested fix is the best approach so I welcome comments,,1869,0.7742108079186731,0.14971572962728996,37099,460.8210463893905,35.47265424944068,114.342704655112,2665,68,1394,176,travis,jblackburne,ogrisel,false,ogrisel,1,1.0,1,0,49,true,false,false,false,0,2,1,0,2,0,13
3861006,scikit-learn/scikit-learn,python,3264,1402335478,1402352856,1402352856,289,289,merged_in_comments,false,false,false,4,4,3,2,5,0,7,0,4,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,14.141538013040378,0.33163190137730664,9,simonfrid@gmail.com,doc/testimonials/images/datapublica.png|doc/testimonials/testimonials.rst|doc/testimonials/images/datapublica.png|doc/testimonials/testimonials.rst|doc/testimonials/images/datapublica.png|doc/testimonials/testimonials.rst,9,0.005649717514124294,0,0,false,add Data Publica testimonial ,,1868,0.7740899357601713,0.14500941619585686,37113,460.2430415218387,35.378438821976125,114.19179263330909,2653,70,1392,166,travis,scharron,larsmans,false,larsmans,0,0,16,4,1363,true,false,false,false,0,0,0,0,4,0,4
3854110,scikit-learn/scikit-learn,python,3262,1402240155,,1406134689,64908,,unknown,false,false,false,11,2,2,0,0,0,0,0,1,0,1,32,33,32,0,0,0,1,32,33,32,0,0,755,204,755,204,184.9180710084347,4.336496171323667,131,salahi.murad@gmail.com,sklearn/__init__.py|sklearn/cluster/hierarchical.py|sklearn/datasets/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/pls.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/label.py|sklearn/tree/export.py|sklearn/tree/tree.py|sklearn/utils/extmath.py|doc/modules/feature_extraction.rst|sklearn/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/decomposition/__init__.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/truncated_svd.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/isotonic.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/pls.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/label.py|sklearn/tests/test_isotonic.py|sklearn/tree/export.py|sklearn/tree/tree.py|sklearn/utils/extmath.py,24,0.0025429116338207248,0,0,false,[WIP] remove deprecated code for 016 To be merged after 015,,1866,0.7743837084673098,0.14049586776859505,37116,460.259726263606,35.375579265007005,114.18256277616122,2644,69,1391,199,travis,larsmans,larsmans,true,,111,0.7477477477477478,141,38,1421,true,true,false,false,51,292,41,82,112,15,-1
3854025,scikit-learn/scikit-learn,python,3261,1402238419,1402266700,1402266700,471,471,commits_in_master,false,false,false,20,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.708628577123488,0.11042160285108013,1,larsmans@gmail.com,sklearn/preprocessing/tests/test_data.py,1,0.0006361323155216285,0,0,false,TST test interaction features outside doctest Uncontroversial change but I want to see if the Py26 tests on Travis succeed,,1865,0.7742627345844504,0.1405852417302799,37116,460.259726263606,35.375579265007005,114.18256277616122,2644,69,1391,166,travis,larsmans,ogrisel,false,ogrisel,110,0.7454545454545455,141,38,1421,true,true,true,true,51,292,40,82,111,15,-1
3853518,scikit-learn/scikit-learn,python,3260,1402226800,1402229310,1402229310,41,41,commits_in_master,false,false,false,51,3,3,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,16,4,16,4,13.772006390845558,0.322966329185874,13,olivier.grisel@ensta.org,sklearn/linear_model/coordinate_descent.py|sklearn/preprocessing/tests/test_data.py|sklearn/linear_model/coordinate_descent.py,12,0.0076481835564053535,1,0,false,[MRG] fix faulty deprecation in CD fit_intercept and normalize on enet_path and lars_path are marked with    WARNING : will be deprecated in 016Given the point at which this message was introduced I take it this means they will be *removed* in 016 not two releases later @agramfort can you confirm,,1864,0.7741416309012875,0.1383046526449968,37115,460.2721271723023,35.37653239929948,114.18563922942207,2644,69,1391,165,travis,larsmans,agramfort,false,agramfort,109,0.7431192660550459,141,38,1421,true,true,false,true,51,291,39,82,109,15,-1
3842788,scikit-learn/scikit-learn,python,3256,1402071963,1403877080,1403877080,30085,30085,commits_in_master,false,false,false,183,14,1,6,35,0,41,0,5,1,0,0,2,1,0,0,1,0,1,2,1,0,0,94,0,345,0,5.200517646904689,0.12195696448439165,0,,continuous_integration/windows/windows_testing_downloader.ps1,0,0.0,0,10,true,[MRG] Windows Powershell script to automatically download Windows packages This is a stopgap solution for easier testing of Windows packages until the Windows CI stuff is ready It will automatically download the core files needed to run nosetests for sklearn on Windows as well as downloading the get-pippy installer to test pip installing the wheel packagesCurrently to support new Python versions one would need to write a new function to create a dictionary with the right links It would also be necessary to change/update the URLs to point to the appropriate place if the package versions need modificationSome of the files are coming from my public Dropbox because figuring out how to execute Javascript to get the download links from Gholkes packages and the Windows VC++ download in Windows was non-trivial We could go so far as install and download PyQt run a Python script that headlessly parses the js then pulls the links we want out but that would need to be done differently for each URLMaybe there is an easier Windows centric solution but this works right now,,1863,0.774020397208803,0.14027149321266968,37115,460.2721271723023,35.37653239929948,114.18563922942207,2638,70,1389,178,travis,kastnerkyle,ogrisel,false,ogrisel,4,0.5,217,66,806,false,false,true,false,1,14,1,0,0,0,14
3834724,scikit-learn/scikit-learn,python,3253,1401992379,1402058700,1402058700,1105,1105,commit_sha_in_comments,false,false,false,52,1,1,3,1,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,18,0,18,0,9.331553384831532,0.21883542243280302,5,rajatkhanduja13@gmail.com,examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_robust_vs_empirical_covariance.py,4,0.002600780234070221,0,0,false,DOC: fix 2 covariance examples rst math markup Some formulas in plot_mahalanobis_distances andplot_robust_vs_empirical_covariance were not correctlyrendered by sphinx as some characters were not properlyescaped This is fixed by setting the docstrings asraw string using rThere was also missing braces for the number of featuresand samples subscript,,1861,0.7743148844707146,0.13849154746423928,37141,459.9229961498075,35.32484316523519,114.02493201583157,2632,70,1388,166,travis,cmd-ntrf,larsmans,false,larsmans,3,1.0,13,3,1072,false,true,false,false,0,0,0,0,1,0,39
3834218,scikit-learn/scikit-learn,python,3251,1401989520,1401991438,1401991438,31,31,commits_in_master,false,false,false,22,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,0,10,0,4.197446471307702,0.09843640254474346,2,alexandre.gramfort@m4x.org,sklearn/manifold/_utils.c|sklearn/manifold/_utils.pyx,2,0.0013080444735120995,0,0,false,[MRG] FIX use NPY_INFINITY instead of C99 INFINITY for MSVC I tested it on windows If travis is green I will merge,,1860,0.7741935483870968,0.13865271419228253,37141,459.9229961498075,35.32484316523519,114.02493201583157,2631,70,1388,165,travis,ogrisel,ogrisel,true,ogrisel,70,0.8428571428571429,961,123,1835,true,true,false,false,25,359,51,187,102,12,-1
3833259,scikit-learn/scikit-learn,python,3250,1401982712,1402412370,1402412370,7160,7160,commits_in_master,false,false,false,60,9,1,16,20,0,36,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,143,3,269,54,9.110329129106347,0.2136508503353274,30,olivier.grisel@ensta.org,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,19,0.012459016393442624,1,9,false,Refactor and clean IsotonicRegression Refactor and clean IsotonicRegression:* Rename X to x to properly reflect that input is a vector not matrix* Refactor the yisotonic_regression() logic into _build_y* Refactor the interp1d logic into _build_f* Store finterp1d() so that it is not re-calculated on each call to transformSorry about the last one @agramfort  Reset and syncd,,1859,0.7740720817643895,0.13639344262295083,37141,459.9229961498075,35.32484316523519,114.02493201583157,2631,70,1388,169,travis,mjbommar,ogrisel,false,ogrisel,9,1.0,32,13,1847,true,false,false,false,1,23,12,20,134,0,2
3832488,scikit-learn/scikit-learn,python,3249,1401975818,1412462310,1412462310,174774,174774,commits_in_master,false,false,false,38,6,4,6,27,0,33,0,4,0,0,5,6,1,0,0,0,0,6,6,2,0,0,36,0,40,24,51.73609004196287,1.213251431722639,24,virgile.fritsch@gmail.com,doc/modules/linear_model.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/linear_model/coordinate_descent.py|doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|doc/modules/linear_model.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py,16,0.001973684210526316,0,10,false,Change the default of precompute from auto to False  The Gram variant has been found to be slower then the normal update rules See the discussion in https://githubcom/scikit-learn/scikit-learn/pull/3220  So the default is changed from auto to False,,1858,0.7739504843918191,0.13421052631578947,37068,462.06970972267186,35.82604942268264,114.9239235998705,2630,70,1388,237,travis,MechCoder,agramfort,false,agramfort,8,0.625,71,40,716,true,true,true,false,3,82,11,13,140,2,58
3832473,scikit-learn/scikit-learn,python,3248,1401975715,1401978869,1401978869,52,52,commits_in_master,false,false,false,34,2,2,0,2,0,2,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,60,7,60,7,14.074550524529243,0.33005912431510576,9,olivier.grisel@ensta.org,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,7,0.004605263157894736,0,1,false,Remove unused param precompute from MultiTask models The param precompute is unused in MultiTaskElasticNet and MultiTaskLassoCV since there is no gram variant of the update rulesThis PR removes it from the public API,,1857,0.7738287560581584,0.13421052631578947,37068,462.06970972267186,35.82604942268264,114.9239235998705,2630,70,1388,170,travis,MechCoder,ogrisel,false,ogrisel,7,0.5714285714285714,71,40,716,true,true,true,false,3,82,10,13,140,2,51
3832465,scikit-learn/scikit-learn,python,3247,1401975611,1401981109,1401981109,91,91,commits_in_master,false,false,false,32,3,3,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,14,22,14,21.792135166218213,0.5110424689856291,9,olivier.grisel@ensta.org,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,7,0.004605263157894736,0,3,false,Use the Gram Variant when precompute is True The algorithm cd_fastenet_coordinate_descent_gram is unused even whena] precompute is Trueb] precompute is auto and n_samples  n_featuresThis PR fixes this issue,,1856,0.7737068965517241,0.13421052631578947,37068,462.06970972267186,35.82604942268264,114.9239235998705,2630,70,1388,169,travis,MechCoder,ogrisel,false,ogrisel,6,0.5,71,40,716,true,true,true,false,3,82,9,13,140,2,46
3832167,scikit-learn/scikit-learn,python,3246,1401973578,1401979467,1401979467,98,98,commits_in_master,false,false,false,21,19,18,0,1,0,1,0,1,0,0,19,19,13,0,0,0,0,19,19,13,0,0,466,362,471,362,205.5012416000507,4.81916347736784,56,olivier.grisel@ensta.org,sklearn/preprocessing/label.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/modules/multiclass.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|examples/plot_multilabel.py|sklearn/datasets/samples_generator.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|doc/modules/multiclass.rst|sklearn/metrics/metrics.py|sklearn/multiclass.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|doc/developers/utilities.rst|doc/modules/model_evaluation.rst|examples/plot_multilabel.py|sklearn/metrics/metrics.py|sklearn/multiclass.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/metrics/tests/test_metrics.py|sklearn/preprocessing/tests/test_label.py|sklearn/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/metrics/tests/test_metrics.py|sklearn/preprocessing/label.py,20,0.0,0,0,false,[MRG+1] deprecate sequences of sequences multilabel support subsumes  #2657 to add the last commit on integer dtype for the classes_ attribute,,1855,0.7735849056603774,0.13429888084265965,37068,462.06970972267186,35.82604942268264,114.9239235998705,2630,70,1388,169,travis,ogrisel,ogrisel,true,ogrisel,69,0.8405797101449275,961,123,1835,true,true,false,false,25,349,46,186,95,12,13
3829854,scikit-learn/scikit-learn,python,3244,1401943829,1401984684,1401984684,680,680,commit_sha_in_comments,false,false,false,51,98,98,0,3,0,3,0,3,7,2,69,78,64,0,2,7,2,69,78,64,0,2,4374,1444,4374,1444,841.1883148317909,19.72651830647123,209,ugurthemaster@gmail.com,sklearn/isotonic.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst|sklearn/tests/test_isotonic.py|sklearn/isotonic.py|sklearn/tests/test_isotonic.py|sklearn/isotonic.py|sklearn/isotonic.py|sklearn/tests/test_common.py|examples/ensemble/plot_adaboost_regression.py|sklearn/isotonic.py|sklearn/feature_selection/rfe.py|sklearn/cluster/dbscan_.py|sklearn/feature_extraction/text.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_multiprocessing_helpers.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/logger.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/my_exceptions.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/__init__.py|sklearn/externals/joblib/test/test_func_inspect.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_my_exceptions.py|sklearn/externals/joblib/test/test_numpy_pickle.py|sklearn/externals/joblib/test/test_parallel.py|sklearn/externals/joblib/test/test_pool.py|examples/manifold/plot_tsne_digits.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/_binary_search.c|examples/manifold/plot_compare_methods.py|examples/manifold/plot_tsne_digits.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/_binary_search.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/_binary_search.c|sklearn/manifold/tests/test_t_sne.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/tests/test_t_sne.py|examples/manifold/plot_tsne_digits.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/t_sne.py|doc/modules/classes.rst|sklearn/manifold/t_sne.py|sklearn/manifold/_binary_search.pyx|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|doc/modules/manifold.rst|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|sklearn/manifold/_utils.c|sklearn/manifold/_utils.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|doc/modules/kernel_approximation.rst|sklearn/ensemble/tests/test_weight_boosting.py|setup.cfg|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|examples/manifold/plot_lle_digits.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|doc/modules/manifold.rst|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|examples/manifold/plot_lle_digits.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/neighbors/base.py|sklearn/utils/fixes.py|sklearn/manifold/t_sne.py|sklearn/manifold/tests/test_t_sne.py|sklearn/semi_supervised/tests/test_label_propagation.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/covariance/plot_lw_vs_oas.py|examples/covariance/plot_mahalanobis_distances.py|examples/decomposition/plot_image_denoising.py|examples/decomposition/plot_sparse_coding.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_logistic.py|examples/mixture/plot_gmm.py|examples/mixture/plot_gmm_pdf.py|examples/mixture/plot_gmm_selection.py|examples/mixture/plot_gmm_sin.py|examples/neighbors/plot_kde_1d.py|examples/plot_lda_qda.py|examples/decomposition/plot_sparse_coding.py|examples/mixture/plot_gmm.py|examples/mixture/plot_gmm_sin.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|.travis.yml|sklearn/cluster/k_means_.py|doc/whats_new.rst|README.rst|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/test/test_memory.py|sklearn/isotonic.py|sklearn/tests/test_isotonic.py|sklearn/isotonic.py|sklearn/tests/test_isotonic.py,29,0.0026507620941020544,0,0,false,Refactor and clean IsotonicRegression Refactor and clean IsotonicRegression:* Rename X to x to properly reflect that input is a vector not matrix* Refactor the yisotonic_regression() logic into _build_y* Refactor the interp1d logic into _build_f* Store finterp1d() so that it is not re-calculated on each call to transform,,1854,0.7734627831715211,0.13518886679920478,37068,462.06970972267186,35.82604942268264,114.9239235998705,2629,70,1387,168,travis,mjbommar,mjbommar,true,mjbommar,8,1.0,32,13,1846,true,false,false,false,1,23,11,20,134,0,14
3826475,scikit-learn/scikit-learn,python,3243,1401914569,1406239655,1406239655,72084,72084,commits_in_master,false,true,false,80,59,18,20,30,0,50,0,7,0,0,3,9,2,0,0,0,0,9,9,6,0,1,200,88,702,236,88.94673690395314,2.0858699567023518,0,,sklearn/preprocessing/label.py|doc/modules/preprocessing.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|doc/modules/preprocessing.rst|sklearn/preprocessing/label.py|sklearn/preprocessing/label.py,0,0.0,0,15,false,Make LabelEncoder more friendly to new labels This PR intends to make preprocessingLabelEncoder more friendly for production/pipeline usage by adding a new_labels constructor argumentInstead of always raising ValueError for unseen/new labels in transform LabelEncoder may be initialized with new_labels as:  * raise: current behavior ie raise ValueError _to remain default behavior_  * nan: return npnan for unseen/new labels  * update: update classes_ with new IDs [N  N+m-1] for m new labels and assignTests and documentation updates included,,1853,0.7733405288720993,0.13741496598639455,37068,462.06970972267186,35.82604942268264,114.9239235998705,2627,70,1387,200,travis,mjbommar,mjbommar,true,mjbommar,7,1.0,32,13,1846,true,false,false,false,1,23,10,20,131,0,1145
3824046,scikit-learn/scikit-learn,python,3242,1401899118,1401900575,1401900575,24,24,commits_in_master,false,false,false,11,1,1,0,2,0,2,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,19,11,19,11,17.40978018654855,0.4082732995962258,4,olivier.grisel@ensta.org,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/test/test_memory.py,4,0.0013745704467353953,0,0,false,MAINT: joblib 081 Will merge as soon as travis is green,,1852,0.7732181425485961,0.13814432989690723,37067,462.00124099603426,35.82701594410123,114.92702403755362,2625,69,1387,165,travis,ogrisel,ogrisel,true,ogrisel,68,0.8382352941176471,960,123,1834,true,true,false,false,25,343,44,180,93,12,15
3820698,scikit-learn/scikit-learn,python,3240,1401860325,1401983714,1401983714,2056,2056,commits_in_master,false,false,false,36,1,1,0,6,0,6,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.113547035267518,0.09699335245180517,6,michael@bommaritollc.com,sklearn/feature_extraction/text.py,6,0.004166666666666667,0,1,false,fix defaultdict call In Python 27 the current version fails with:$ python -c from collections import defaultdict defaultdict(None)Traceback (most recent call last):  File string line 1 in moduleTypeError: first argument must be callable,,1851,0.773095623987034,0.1375,36860,462.88659793814435,35.81117742810635,115.11123168746609,2620,69,1387,169,travis,JelleZijlstra,jnothman,false,jnothman,0,0,3,1,1060,false,false,false,false,0,0,0,0,0,0,13
3819237,scikit-learn/scikit-learn,python,3239,1401843057,1401994858,1401994858,2530,2530,commits_in_master,false,false,false,52,6,4,5,9,0,14,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,123,0,182,0,27.358830199209372,0.6450940362247326,8,olivier.grisel@ensta.org,sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/data.py|sklearn/utils/fixes.py|sklearn/preprocessing/data.py|sklearn/utils/fixes.py,5,0.003479471120389701,0,5,false,[MRG] compute poly features directly Heres a much simpler alternative to #3194 for fixing #3191 itertools and NumPy have all the required functionalityThe great thing about this approach is that we can turn the actual polynomials (x² y²) off and get only the interaction terms (xy) by substituting combinations for combinations_with_replacement,,1850,0.772972972972973,0.13778705636743216,36860,462.88659793814435,35.81117742810635,115.11123168746609,2615,69,1386,168,travis,larsmans,larsmans,true,larsmans,108,0.7407407407407407,141,38,1416,true,true,false,false,51,278,36,79,107,14,50
3818289,scikit-learn/scikit-learn,python,3238,1401836773,1402026446,1402026446,3161,3161,commit_sha_in_comments,false,false,false,22,2,1,0,5,0,5,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,1,0,1,0,3.8718102459578185,0.09129122361703247,0,,sklearn/neighbors/dist_metrics.pyx,0,0.0,0,2,false,remove duplicate key hamming in METRIC_MAPPING A minor issue:There are 2 codehamming/code keys in METRIC_MAPPING so I removed the 2nd one,,1849,0.772850189291509,0.1374738311235171,36842,461.04988871396773,35.801530861516746,114.40746973562781,2614,69,1386,169,travis,chyikwei,jnothman,false,jnothman,1,0.0,16,1,616,false,true,false,false,0,0,0,0,1,0,38
3815004,scikit-learn/scikit-learn,python,3237,1401815294,1401816274,1401816274,16,16,commits_in_master,false,false,false,19,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,5.187940763713514,0.12232350526257386,6,olivier.grisel@ensta.org,.travis.yml,6,0.004204625087596356,0,0,false,MAINT: bump up to scipy 0140 in travis CI config I will merge as soon as travis is green,,1848,0.7727272727272727,0.13665031534688157,36842,461.04988871396773,35.801530861516746,114.40746973562781,2614,69,1386,164,travis,ogrisel,ogrisel,true,ogrisel,67,0.835820895522388,958,123,1833,true,true,false,false,27,334,41,182,90,12,16
3813898,scikit-learn/scikit-learn,python,3236,1401806808,,1405707627,65013,,unknown,false,true,false,21,1,1,0,8,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,3.823063449836628,0.09014183918524575,2,michael@bommaritollc.com,sklearn/multiclass.py,2,0.0014124293785310734,0,5,false,[WIP] Fixes #2360 changes to correct behavior of OVO in case of tie See #2360 for discussion - currently fails tests,,1847,0.7731456415809421,0.13771186440677965,36842,461.04988871396773,35.801530861516746,114.40746973562781,2612,69,1386,198,travis,kastnerkyle,amueller,false,,3,0.6666666666666666,217,66,803,false,false,false,false,0,6,0,0,0,0,23
3807603,scikit-learn/scikit-learn,python,3234,1401741863,1401754505,1401754505,210,210,commits_in_master,false,false,false,47,2,2,0,3,0,3,0,3,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,8.942889993251931,0.21085377646757236,0,,setup.cfg|setup.cfg,0,0.0,0,0,false,MAINT: run tests on files with the exec bit nosetests skips test files with the executable bit by default which can silently hide failing tests There are no executable scripts within the scikit-learn project so lets turn the --exe flag on to avoid skipping tests by mistake,,1846,0.7730227518959913,0.13749114103472715,36842,461.04988871396773,35.801530861516746,114.40746973562781,2609,69,1385,163,travis,ogrisel,ogrisel,true,ogrisel,66,0.8333333333333334,958,123,1832,true,true,false,false,26,336,41,178,89,12,15
3801073,scikit-learn/scikit-learn,python,3231,1401670267,1401671166,1401671166,14,14,commits_in_master,false,false,false,8,1,1,0,1,0,1,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,14,0,14,0,13.661094782968169,0.32209927009387707,4,rajatkhanduja13@gmail.com,examples/decomposition/plot_sparse_coding.py|examples/mixture/plot_gmm.py|examples/mixture/plot_gmm_sin.py,4,0.0028530670470756064,0,0,false,CLN: Capitalize Dirichlet and Mexican in example docstrings ,,1845,0.77289972899729,0.1362339514978602,36842,461.04988871396773,35.801530861516746,114.40746973562781,2606,69,1384,163,travis,bwignall,agramfort,false,agramfort,8,0.875,9,17,244,false,true,false,false,0,0,3,0,4,0,12
3800875,scikit-learn/scikit-learn,python,3230,1401667195,1401668797,1401668797,26,26,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,0,0,14,14,14,0,0,0,0,14,14,14,0,0,42,0,42,0,63.5609974548545,1.4986412198125039,6,rajatkhanduja13@gmail.com,examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/covariance/plot_lw_vs_oas.py|examples/covariance/plot_mahalanobis_distances.py|examples/decomposition/plot_image_denoising.py|examples/decomposition/plot_sparse_coding.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_logistic.py|examples/mixture/plot_gmm.py|examples/mixture/plot_gmm_pdf.py|examples/mixture/plot_gmm_selection.py|examples/mixture/plot_gmm_sin.py|examples/neighbors/plot_kde_1d.py|examples/plot_lda_qda.py,5,0.002144388849177984,0,0,false,CLN: Capitalize Gaussian in example docstrings ,,1844,0.7727765726681128,0.135096497498213,36842,460.832745236415,35.77438792682265,114.38032680093372,2606,69,1384,164,travis,bwignall,agramfort,false,agramfort,7,0.8571428571428571,9,17,244,false,true,false,false,0,0,2,0,3,0,12
3800169,scikit-learn/scikit-learn,python,3229,1401657604,,1404405060,45790,,unknown,false,false,false,14,3,3,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,12.839652898511993,0.3027333404492623,0,,sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/_svmlight_format.pyx,0,0.0,0,1,false,fix Cythonize on Python 34 * byte-value 35 is the ascii-code for # (hash),,1843,0.7731958762886598,0.13558106169296988,36842,460.832745236415,35.77438792682265,114.38032680093372,2606,69,1384,185,travis,besser82,larsmans,false,,0,0,13,1,896,true,false,false,false,0,0,0,0,2,0,706
3799168,scikit-learn/scikit-learn,python,3228,1401643337,1401644124,1401644124,13,13,commit_sha_in_comments,false,false,false,13,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,6,0,6,4.386280366365226,0.10341960019445466,0,,sklearn/semi_supervised/tests/test_label_propagation.py,0,0.0,0,0,false,TST stabilize flaky label propagation test Quick PR to see how Travis responds,,1842,0.7730727470141151,0.13548387096774195,36839,460.78883791633865,35.777301229675075,114.36249626754255,2602,68,1384,164,travis,larsmans,larsmans,true,larsmans,107,0.7383177570093458,139,38,1414,true,true,false,false,47,285,37,77,111,14,13
3790617,scikit-learn/scikit-learn,python,3225,1401505932,1401547855,1401547855,698,698,commits_in_master,false,false,false,3,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.415473834704319,0.10410791357941017,3,rajatkhanduja13@gmail.com,examples/plot_johnson_lindenstrauss_bound.py,3,0.002197802197802198,0,0,true,CLN: Fix typo ,,1841,0.7729494839761,0.136996336996337,36839,460.78883791633865,35.777301229675075,114.36249626754255,2597,68,1382,164,travis,bwignall,jnothman,false,jnothman,6,0.8333333333333334,9,17,242,false,true,false,false,0,0,1,0,2,0,-1
3789002,scikit-learn/scikit-learn,python,3224,1401492195,1411382003,1411382003,164830,164830,commits_in_master,false,false,false,56,25,6,42,51,0,93,0,8,0,0,4,5,3,0,0,0,0,5,5,3,0,0,720,332,895,515,54.42525123803285,1.2832369898543945,7,maheshakya@wso2.com,doc/modules/ensemble.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py|doc/modules/ensemble.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,6,0.0021929824561403508,1,26,true,Sample weights for gradient boosting and piggy-packed exponential loss for binary classification ( AdaBoost)I didnt implement sample weights for lad huber and quantile since it would require weighted median / percentiles I will either raise a warning or exception in this case: what do you prefercc @glouppeTODO: benchmark benchmark and some regression tests,,1840,0.7728260869565218,0.13669590643274854,36839,460.78883791633865,35.777301229675075,114.36249626754255,2597,68,1382,225,travis,pprett,ogrisel,false,ogrisel,45,0.8666666666666667,132,29,1760,true,true,true,true,3,8,1,2,4,0,103
3779042,scikit-learn/scikit-learn,python,3220,1401360822,,1401976070,10254,,unknown,false,false,false,25,8,2,1,39,0,40,0,6,0,0,1,6,1,0,0,0,0,6,6,2,0,0,24,0,77,7,8.275397385982776,0.1951177085644507,9,olivier.grisel@ensta.org,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,9,0.006823351023502654,3,25,false,FIX: Use coordinate_descent_gram when precompute is True | auto I found out that enet_coordinate_descent_gram is unused in enet_path@agramfort @GaelVaroquaux @ogrisel Please have a look,,1839,0.7732463295269169,0.1417740712661107,36833,460.86389922080747,35.78312925908832,114.38112562104634,2592,67,1381,175,travis,MechCoder,MechCoder,true,,5,0.6,71,40,709,true,true,false,false,3,57,8,12,51,0,4
3777293,scikit-learn/scikit-learn,python,3219,1401324760,1401487929,1401487929,2719,2719,commit_sha_in_comments,false,false,false,72,3,1,3,6,0,9,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,7,0,31,0,3.9481726757950373,0.09308992388966154,6,michael@bommaritollc.com,sklearn/neighbors/base.py,6,0.0045627376425855515,0,4,false,ENH: use partial sort for kneighbors selection This produces a considerable speedup with Numpy  18 Faster implementations of argpartition exist (see [bottlenecks argpartsort](http://berkeleyanalyticscom/bottleneck/referencehtml#bottleneckargpartsort) but that would involve adding dependenciesIs this sort of on-demand feature checking acceptable or should I make a shim for argpartitionAlso note that this has the side-effect of leaving the selected indices unsorted I dont think any downstream code relies on this but I havent checked,,1838,0.7731229597388466,0.1414448669201521,36830,460.5213141460766,35.7317404289981,114.33613901710562,2591,67,1380,165,travis,perimosocordiae,larsmans,false,larsmans,2,1.0,45,48,1845,true,true,false,false,0,5,1,0,3,0,49
3775642,scikit-learn/scikit-learn,python,3217,1401313477,1401329960,1401329960,274,274,commits_in_master,false,false,false,9,1,1,0,3,0,3,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.651905268348832,0.10968251465478436,0,,doc/modules/kernel_approximation.rst,0,0.0,0,1,false,DOC: Replace GT/LT with angle brackets for inner product ,,1837,0.7729994556341862,0.14154552410099464,36830,460.5213141460766,35.7317404289981,114.33613901710562,2591,67,1380,165,travis,bwignall,jnothman,false,jnothman,5,0.8,9,17,240,false,true,false,false,0,0,0,0,1,0,15
3773603,scikit-learn/scikit-learn,python,3215,1401300814,1401644143,1401644143,5722,5722,commit_sha_in_comments,false,false,false,23,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,4,5,4,8.233822860417263,0.19413688466471446,13,larsmans@gmail.com,sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,8,0.006144393241167435,0,0,false,ENH: issue #3171 Trees do no longer fail if int(max_features*n_features)  0 but usemax_features1 if the max_features paramater was larger than 00,,1836,0.7728758169934641,0.14208909370199693,36830,460.5213141460766,35.7317404289981,114.33613901710562,2590,67,1380,165,travis,mfeurer,larsmans,false,larsmans,0,0,10,1,274,false,false,false,false,1,1,0,0,2,0,19
3772408,scikit-learn/scikit-learn,python,3212,1401293608,1401295231,1401295231,27,27,github,false,false,false,16,1,1,0,0,0,0,0,0,0,0,16,16,17,0,0,0,0,16,16,17,0,0,727,355,727,355,71.68016958013847,1.6917413583706409,4,larsmans@gmail.com,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_multiprocessing_helpers.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/logger.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/my_exceptions.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/__init__.py|sklearn/externals/joblib/test/test_func_inspect.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_my_exceptions.py|sklearn/externals/joblib/test/test_numpy_pickle.py|sklearn/externals/joblib/test/test_parallel.py|sklearn/externals/joblib/test/test_pool.py,3,0.0007727975270479134,0,0,false,MAINT: joblib 080 Synchronize the embedded version of joblib with upstreamWill merge if travis pass,,1835,0.7727520435967302,0.14142194744976816,36838,455.4264618057441,35.561105380313805,113.63266192518596,2589,67,1380,163,travis,ogrisel,ogrisel,true,ogrisel,65,0.8307692307692308,950,123,1827,true,true,false,false,25,326,38,178,87,12,-1
3766244,scikit-learn/scikit-learn,python,3208,1401216605,1401238557,1401238557,365,365,commits_in_master,false,false,false,66,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.492142515548369,0.10602032394601604,1,dsullivan7@hotmail.com,sklearn/feature_selection/rfe.py,1,0.0007776049766718507,0,0,false,Improved documentation of the estimator_params argument for RFE and RF ECVChanged the documentation for the estimator_params argument for bothRFE and RFECV to better indicate how it could be useful for doing agrid search Previously the documentation may have indicated that RFEand RFECV would perform a grid search if estimator_params was passeda grid of parameter values which is not the actual behavior,,1834,0.7726281352235551,0.13996889580093314,36840,455.40173724212815,35.559174809989145,113.62649294245386,2579,67,1379,163,travis,staubda,jnothman,false,jnothman,0,0,0,1,89,false,false,false,false,0,0,0,0,1,0,17
3764033,scikit-learn/scikit-learn,python,3207,1401202119,,1404306005,51731,,unknown,false,false,false,54,20,5,45,56,0,101,0,7,0,0,7,13,7,0,0,0,0,13,13,12,0,0,1677,122,1881,413,117.5261030087655,2.773766742701199,69,maheshakya@wso2.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,49,0.008634222919937205,0,9,false,[MRG] tree: add min_fraction_leaf stopping criterion This PR adds a new decision tree stopping criterion: min_fraction_leaf that requires a minimum weighted fraction of the input samples to be present in all leaf nodes This criterion is more appropriate than min_samples_leaf when you have weighted samples and many more samples of one class than another,,1833,0.7730496453900709,0.141287284144427,36840,455.40173724212815,35.559174809989145,113.62649294245386,2579,67,1379,183,travis,ndawe,glouppe,false,,14,0.7857142857142857,36,62,1565,true,true,true,false,1,14,5,23,34,1,596
3760993,scikit-learn/scikit-learn,python,3204,1401166419,1408030267,1408030267,114397,114397,merged_in_comments,false,true,false,75,112,44,159,144,4,307,0,16,10,2,12,29,9,0,1,11,3,16,30,13,0,1,7052,549,20060,3222,308.4497333388228,7.279809252813128,6,michael@bommaritollc.com,benchmarks/bench_mnist.py|doc/modules/neural_networks_supervised.rst|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|examples/neural_network/mlp_example.py|sklearn/neural_network/mlp.py|sklearn/setup.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/__init__.py|sklearn/setup.py|examples/neural_network/mlp_example.py|sklearn/neural_network/mlp.py|benchmarks/bench_mnist.py|doc/images/multilayerperceptron_network.png|doc/modules/classes.rst|doc/modules/neural_networks_supervised.rst|doc/modules/neural_networks_unsupervised.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|examples/neural_network/README.txt|examples/neural_network/mlp_example.py|examples/neural_network/plot_mlp_alpha.py|examples/neural_network/plot_mlp_nonlinear.py|sklearn/neural_network/__init__.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|examples/neural_network/plot_mlp_alpha.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/__init__.py|sklearn/setup.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/tests/test_mlp.py|sklearn/neural_network/multilayer_perceptron.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py|sklearn/neural_network/mlp.py,6,0.0,0,48,false,Generic multi layer perceptron This pull request is to implement the generic Multi-layer perceptron as part of the GSoC 2014 proposal The expected time to finish this pull request is at June 15The goal is to extend Multi-layer Perceptron to support more than one hidden layer and to support having a pre-training phase (initializing weights through Restricted Boltzmann Machines) and a fine-tuning phase and write its documentationThis directly follows from this pull-request: https://githubcom/scikit-learn/scikit-learn/pull/2120,,1832,0.7729257641921398,0.14183835182250396,36840,455.40173724212815,35.559174809989145,113.62649294245386,2576,67,1378,438,travis,IssamLaradji,IssamLaradji,true,IssamLaradji,3,0.3333333333333333,19,1,486,true,false,false,false,1,8,0,0,15,0,435
3757509,scikit-learn/scikit-learn,python,3203,1401127000,1401321931,1401321931,3248,3248,merged_in_comments,false,true,false,140,243,3,178,113,1,292,0,6,0,0,2,13,2,0,0,8,1,12,21,13,0,0,114,48,4252,3866,26.84648517424698,0.6336148694754731,2,larsmans@gmail.com,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,2,0.0016168148746968471,2,20,false,[WIP] Sparse label_binarizer This is a continuation of a part of PR #2458 based on the code already written by @rsivapr and @arjoly from the branch https://githubcom/arjoly/scikit-learn/tree/sparse-label_binarizerThe first steps are to write tests to motivate correctness and then implement the appropriate changes in labelpy Validation of the input has been abstracted to a global private function to avoid duplication when using the two different APIs - [x] label validation on LabelBinarizer() - with testing- [x] label validation on label_binarizer() - with testing- [ ] y and classes consistency validation across LabelBinarizers fit() and transform() - with testing- [ ] y and classes consistency validation on  label_binarizer() - with testing- [ ] Issue a deprecation warning with the multilabel parameter - with  testing- [ ] Implement sparse parameter reutrns CSR when true - with testing,,1831,0.772801747678864,0.14308811641067098,36822,454.86394003584815,35.46792678290152,113.54624952474065,2573,67,1378,195,travis,hamsal,hamsal,true,hamsal,2,1.0,4,8,513,true,true,false,false,1,17,2,8,49,1,1860
3754297,scikit-learn/scikit-learn,python,3200,1401097746,1401097996,1401097996,4,4,commits_in_master,false,false,false,12,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.796277847553704,0.11319873523139082,4,noel.dawe@gmail.com,examples/ensemble/plot_adaboost_regression.py,4,0.0033085194375516956,0,0,false,Update plot_adaboost_regressionpy Library imports have been moved to top of the file,,1829,0.7731000546746856,0.14392059553349876,36825,454.7182620502376,35.465037338764425,113.53699932111337,2572,66,1378,164,travis,ugurthemaster,agramfort,false,agramfort,4,1.0,1,0,363,true,false,false,false,0,0,2,0,2,0,4
3753217,scikit-learn/scikit-learn,python,3199,1401079969,1401129228,1401129228,820,820,commits_in_master,false,false,false,133,7,4,7,5,0,12,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,45,44,65,58,9.326434637214792,0.22011664851514395,23,michael@bommaritollc.com,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,14,0.011715481171548118,0,2,false,Handling out-of-bounds in IsotonicRegression (clean PR #3147) Heres a clean follow-up PR based on the other Isotonic merges today and conversation from PR #3147* Added argument out_of_bounds* Added test for each value case below* Changed default behavior from raise ie ValueError to nan__Argument behavior__* nan: x-values not in training domain - y-values  NaN* clip: x-values not in training domain - y-values  y-value for nearest training x* raise: x-values not in training domain - ValueError exceptionLet me know if you want to see L296-310 differently  Id like to clean up fit vs fit_transform and some of the matrix/vector notation mismatch in the module anyway(My interpretation is also that there is no need for f to be created anew on each call to transform),,1828,0.7729759299781181,0.14560669456066946,36825,454.7182620502376,35.465037338764425,113.53699932111337,2571,66,1377,165,travis,mjbommar,GaelVaroquaux,false,GaelVaroquaux,6,1.0,32,13,1836,true,false,false,false,1,19,8,19,79,0,14
3752897,scikit-learn/scikit-learn,python,3198,1401073868,1401153498,1401153498,1327,1327,commits_in_master,false,false,false,7,4,4,0,5,0,5,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,18.135642816950444,0.42802604326480514,2,g.louppe@ulg.ac.be,doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/basic/tutorial.rst,2,0.0016792611251049538,0,0,false,Clarify shell versus Python syntax in tutorial ,,1827,0.7728516694033936,0.14609571788413098,36825,454.7182620502376,35.465037338764425,113.53699932111337,2570,66,1377,164,travis,apw,jnothman,false,jnothman,0,0,2,0,1551,true,false,false,false,0,0,0,0,6,0,12
3752393,scikit-learn/scikit-learn,python,3197,1401064813,1401070784,1401070784,99,99,commits_in_master,false,false,false,15,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.1865210415550855,0.09880764811704837,3,larsmans@gmail.com,sklearn/neighbors/base.py,3,0.002523128679562658,0,1,false,Additional fixes for todense() / issue #3167 A few extra docstring fixes here and there,,1826,0.7727272727272727,0.1455004205214466,36825,454.7182620502376,35.465037338764425,113.53699932111337,2570,66,1377,162,travis,mjbommar,jnothman,false,jnothman,5,1.0,32,13,1836,true,false,false,false,1,18,7,19,74,0,14
3749822,scikit-learn/scikit-learn,python,3196,1401022264,1401022920,1401022920,10,10,github,false,false,false,153,2,2,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,8.803840420141956,0.20778156171961676,4,olivier.grisel@ensta.org,sklearn/utils/testing.py|sklearn/utils/testing.py,4,0.003372681281618887,0,0,false,Fix for Python 3x race condition causing RuntimeError: dictionary changed size during iteration I was running the test suite on a heavily loaded server via nosetests -- it was testing a load of other packages at the same time but in completely separate processesI got a load of test failures with RuntimeError: dictionary changed size during iteration in a test helper function:https://gistgithubcom/andrewclegg/e3060f4e49e04ee66dacThe function in question (clean_warning_registry) loops through sysmodulesvalues() In Python 3 this is not a static list as before but an iterator over a dynamic view of the dict (http://bloglabixorg/2008/06/27/watch-out-for-listdictkeys-in-python-3) So another thread within sklearns nosetests process (culprit unknown) must have added/removed a module at a bad timeCalling copy() first fixes this as its atomic wrt the GIL and it wont do any harm in Python 2x The additional overhead of shallow-copying a small data structure in a test method is negligible compared to the rest of sklearn,,1825,0.7726027397260274,0.14418212478920742,36798,453.88336322626225,35.300831566932985,113.21267460188054,2564,65,1377,163,travis,andrewclegg,andrewclegg,true,andrewclegg,0,0,22,2,1219,true,false,false,false,0,0,0,0,2,0,7
3746918,scikit-learn/scikit-learn,python,3194,1400983408,1402929865,1402929865,32440,32440,commit_sha_in_comments,false,false,false,43,3,1,19,2,0,21,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,52,0,58,0,4.549939129793001,0.10738421109436012,8,olivier.grisel@ensta.org,sklearn/preprocessing/data.py,8,0.006711409395973154,0,0,false,Fix the enumeration process of polynomial features This is a fix for issue #3194 I have to admit that I could not manage running nosetests in my virtualenv of my forkI tested the code in an external way check it out [here](http://nbvieweripythonorg/gist/aboSamoor/fb015e88c5a0152abd76),,1824,0.7724780701754386,0.14345637583892618,36798,453.88336322626225,35.300831566932985,113.21267460188054,2562,64,1376,179,travis,aboSamoor,larsmans,false,larsmans,0,0,27,13,1611,false,true,false,false,0,0,0,0,0,0,77
3747534,scikit-learn/scikit-learn,python,3193,1400970728,1400981566,1400981566,180,180,github,false,false,false,28,2,2,0,2,0,2,0,2,0,0,2,2,1,0,0,0,0,2,2,1,0,0,18,0,18,0,9.000224670293619,0.21241686516983793,19,olivier.grisel@ensta.org,doc/whats_new.rst|sklearn/feature_extraction/image.py,16,0.01343408900083963,1,0,false,Minor docstring and Whats New changes for issue 3167 Sorry @ogrisel missed a handful of information commits in the whats new section and feature_extractionimage module (from issue #3167),,1823,0.7723532638507954,0.14273719563392107,36784,454.05611135276206,35.31426707264028,113.2557633753806,2562,64,1376,162,travis,mjbommar,mjbommar,true,mjbommar,4,1.0,32,13,1835,true,false,false,false,0,17,4,18,67,0,10
3745283,scikit-learn/scikit-learn,python,3192,1400933118,1400942828,1400942828,161,161,github,false,false,false,4,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.36716627661539,0.10307002375324775,1,dsullivan7@hotmail.com,doc/developers/performance.rst,1,0.0008445945945945946,0,0,false,Typo in documentation s/totime/tottime/,,1822,0.7722283205268935,0.13935810810810811,36785,453.8262878890852,35.31330705450591,113.22549952426262,2560,64,1376,163,travis,mmaker,mmaker,true,mmaker,0,0,47,62,1469,true,true,false,false,0,0,0,0,1,0,12
3736768,scikit-learn/scikit-learn,python,3189,1400847492,1400848510,1400848511,16,16,github,false,false,false,33,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,5.0215178763066906,0.1185076874612508,3,olivier.grisel@ensta.org,continuous_integration/test_script.sh,3,0.0025884383088869713,0,0,true,CI: make travis run the doctests Reuse the Makefile to avoid duplication of the list of documentation folders to test Unfortunately there is no simple way to make nose recursively introspect non-package folders,,1821,0.7721032399780341,0.13459879206212252,36766,454.0608170592395,35.33155632921721,113.28401240276342,2554,63,1375,165,travis,ogrisel,ogrisel,true,ogrisel,64,0.828125,947,123,1822,true,true,false,false,26,311,35,163,82,11,13
3729951,scikit-learn/scikit-learn,python,3184,1400782245,1400854032,1400854032,1196,1196,commits_in_master,false,false,false,15,2,2,2,2,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,21,12,21,12,18.93861968359179,0.44697708871181163,4,larsmans@gmail.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,4,0.0034542314335060447,0,0,false,FIX TfidfVectorizer exports idf_ attribute This fixes #3179 by adding an attribute idf_ to TfidfVectorizer,,1820,0.771978021978022,0.13471502590673576,36766,454.9039873796442,35.413153456998316,113.58320187129412,2545,63,1374,164,travis,YS-L,larsmans,false,larsmans,1,1.0,3,0,642,true,false,false,false,1,3,1,0,3,0,1101
3728373,scikit-learn/scikit-learn,python,3182,1400772272,1400845013,1400845013,1212,1212,commits_in_master,false,false,false,40,2,1,3,5,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,530,0,586,4.720183097261964,0.11140271752902743,4,olivier.grisel@ensta.org,sklearn/ensemble/tests/test_forest.py,4,0.003499562554680665,1,2,false,[MRG] Modernizes tests for the forest module Ping @glouppeAmong other thing:- Use sklearnutilstesting- Reduce redundant code for testing extra trees and random forest- Better debugging with yield statementThis would ease forest testing code in# 3173,,1819,0.7718526663001649,0.13560804899387577,36766,454.9039873796442,35.413153456998316,113.58320187129412,2542,62,1374,164,travis,arjoly,ogrisel,false,ogrisel,54,0.7962962962962963,25,25,884,true,true,true,false,18,165,14,120,302,5,103
3727964,scikit-learn/scikit-learn,python,3181,1400768709,,1411756697,183133,,unknown,false,false,false,13,1,1,1,2,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,24,3,24,8.765513352080958,0.2068779934670824,2,larsmans@gmail.com,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,2,0.001755926251097454,0,0,false,fix sign error in gp + unit test #3180 fix for issue #3180,,1818,0.7722772277227723,0.1360842844600527,36766,454.9039873796442,35.413153456998316,113.58320187129412,2542,62,1374,226,travis,filthysocks,filthysocks,true,,1,0.0,0,0,27,false,false,false,false,1,1,1,0,0,0,226
3727211,scikit-learn/scikit-learn,python,3178,1400761633,1400763397,1400763397,29,29,github,false,false,false,13,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,19,6,19,9.138842668502992,0.21568965487186292,7,olivier.grisel@ensta.org,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_coordinate_descent.py,7,0.00618921308576481,2,0,false,[MRG+1] ENetCV and LassoCV now accept npfloat32 input Fixes https://githubcom/scikit-learn/scikit-learn/issues/3174Ping @arjoly @agramfort,,1817,0.7721518987341772,0.1361626878868258,36766,454.9039873796442,35.413153456998316,113.58320187129412,2542,62,1374,162,travis,MechCoder,MechCoder,true,MechCoder,4,0.5,71,40,702,true,true,false,false,3,41,5,11,29,0,13
3725622,scikit-learn/scikit-learn,python,3176,1400748429,1401668706,1401668706,15337,15337,commits_in_master,false,false,false,7,2,1,0,5,0,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,7,2,16,9.026836912575291,0.21304616009730354,5,larsmans@gmail.com,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py,4,0.003561887800534283,0,0,false,FIX : omp default param could fail ,,1816,0.7720264317180616,0.1371326803205699,36766,454.9039873796442,35.413153456998316,113.58320187129412,2540,62,1374,171,travis,agramfort,agramfort,true,agramfort,38,0.8947368421052632,155,185,1632,true,true,false,false,9,92,11,63,19,2,13
3719952,scikit-learn/scikit-learn,python,3175,1400696775,,1409654532,149295,,unknown,false,false,false,115,11,5,7,9,0,16,0,4,6,2,8,20,13,0,0,6,2,12,20,17,0,0,2897,335,3304,377,56.35290589065878,1.3300085430372988,4,olivier.grisel@ensta.org,sklearn/incremental_isomap_utilities.pyx|sklearn/manifold/__init__.py|sklearn/manifold/tests/test_incremental_isomap.py|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx|sklearn/utils/tests/test_shortest_path.py|sklearn/utils/incremental_isomap_utilities.c|sklearn/incremental_isomap_utilities.pyx|sklearn/manifold/__init__.py|sklearn/manifold/incremental_isomap.py|sklearn/manifold/isomap.py|sklearn/manifold/tests/test_incremental_isomap.py|sklearn/utils/graph.py|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx|sklearn/utils/incremental_isomap_utilities.c|sklearn/utils/incremental_isomap_utilities.cpp|sklearn/utils/incremental_isomap_utilities.pyx|sklearn/utils/setup.py,4,0.0,0,4,false,Incremental Isomap Hi everybodyI implemented incremental isomap as proposed in:Law Martin HC and Anil K Jain Incremental nonlinear dimensionality reduction by manifold learning Pattern Analysis and Machine Intelligence IEEE Transactions on 283 (2006): 377-391As Im not aware of a Estimator model in sklearn that does incremental fitting like this I added fit_incremental and fit_transform_incremental This is debatableBesided incremental_isomappy the most imporant changes are in graph_shortest_pathpyx where the shortest paths are returned in addition to the distances and partial shortest path computation is possibleThe speed of incremental learning depends heavily on the speed of nearest neigbhor search I implemented only brute force search here Especially adaptive incremental learning can get slow,,1815,0.7724517906336088,0.13824057450628366,36766,454.9039873796442,35.413153456998316,113.58320187129412,2529,62,1373,214,travis,yanlend,yanlend,true,,0,0,0,1,449,true,false,false,false,0,0,0,0,2,0,4
3718174,scikit-learn/scikit-learn,python,3173,1400684880,1416575434,1416575434,264842,264842,commits_in_master,false,false,false,99,344,188,171,56,6,233,0,14,1,1,12,15,11,0,0,2,1,13,16,12,0,0,21613,9679,30362,11347,1508.4850614516674,35.60238797033237,72,olivier.grisel@ensta.org,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/forest.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|doc/modules/tree.rst|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/ensemble/forest.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|doc/modules/tree.rst|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/ensemble/forest.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_time_sparse_vs_dense.py|sklearn/tree/tests/test_tree.py|doc/modules/tree.rst|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/weight_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/weight_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/ensemble.rst|doc/modules/tree.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/weight_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,51,0.006381039197812215,0,31,false,[WIP] Sparse input support for decision tree and forest This pull request aims to finish the work in #2984TODO list:- [x] clean rebase (remove unwanted generated file github merge commit )- [x] simpler input validation for tree base method (safe_array check_array)- [ ] reduce testing times- [ ] read the whole code again- [x] use the new safe_realloc- [ ] update whats new- [x] document fit  with the sparse format support- [x] fix bug in algorithm selection binary_search- [x] raise error if int64 sparse index matrix- [ ] test oob scores,,1814,0.7723263506063948,0.14038286235186873,36766,454.9039873796442,35.413153456998316,113.58320187129412,2529,61,1373,250,travis,arjoly,ogrisel,false,ogrisel,53,0.7924528301886793,25,25,883,true,true,true,false,17,161,11,120,278,6,1520
3714925,scikit-learn/scikit-learn,python,3169,1400648340,1400968967,1400968967,5343,5343,commits_in_master,false,false,false,34,5,2,0,15,0,15,0,6,0,0,18,19,18,0,0,0,0,19,19,19,0,0,8,94,33,96,77.35280646859117,1.825635995253809,37,rajatkhanduja13@gmail.com,sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_spectral.py|sklearn/datasets/tests/test_svmlight_format.py|sklearn/decomposition/tests/test_pca.py|sklearn/feature_selection/tests/test_base.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/svm/tests/test_sparse.py|sklearn/utils/sparsetools/tests/test_spanning_tree.py|sklearn/utils/tests/test_graph.py|sklearn/utils/tests/test_validation.py|examples/applications/plot_model_complexity_influence.py|examples/linear_model/lasso_dense_vs_sparse_data.py|sklearn/feature_extraction/image.py|sklearn/manifold/spectral_embedding_.py|sklearn/metrics/metrics.py,18,0.001838235294117647,0,6,false,PR re: issue 3167 to eradicate todense() As discussed in issue 3167 (https://githubcom/scikit-learn/scikit-learn/issues/3167) replacing todense() with toarray()Need to review in additional detail for any matrix-specific notation/operators/indexing eg matrix mult or sums/means along axis,,1813,0.7722007722007722,0.14154411764705882,36766,454.9039873796442,35.413153456998316,113.58320187129412,2529,61,1372,163,travis,mjbommar,ogrisel,false,ogrisel,3,1.0,31,12,1831,true,false,false,false,0,11,3,5,44,0,10
3704073,scikit-learn/scikit-learn,python,3165,1400556752,1403506650,1403506650,49164,49164,commits_in_master,false,false,false,106,10,4,0,16,0,16,0,4,0,0,1,5,1,0,0,0,0,5,5,5,0,0,368,0,440,0,17.236722431809497,0.40681078896959366,8,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py,8,0.007729468599033816,0,8,false,[MRG] DOC more complete coverage of API linking in examples Examples in the documentation currently hyperlink to the API reference where a particular class or function is used The current implementation has a few flaws which this PR fixes:* only plot examples currently have hyperlinks added (for example [feature_selection_pipeline](http://scikit-learnorg/dev/auto_examples/feature_selection_pipelinehtml))* hyperlink determination currently involves ad-hoc regular expressions rather than AST traversal* it also runs the code and uses the resulting locals which I think is responsible for the spurious linking of estimator in [plot_learning_curve](http://scikit-learnorg/dev/auto_examples/plot_learning_curvehtml)* matching of hyperlink text was non-greedy so that eg plt rather than pltxlabel is linked in [plot_learning_curve](http://scikit-learnorg/dev/auto_examples/plot_learning_curvehtml)* etc,,1812,0.772075055187638,0.14589371980676327,36765,454.56276349789204,35.386916904664766,113.45029239766082,2529,61,1371,185,travis,jnothman,jnothman,true,jnothman,75,0.64,25,1,1847,true,true,false,false,15,288,16,199,49,3,954
3697842,scikit-learn/scikit-learn,python,3163,1400514423,,1445287745,746222,,unknown,false,true,false,28,7,1,6,12,0,18,0,7,0,0,1,2,1,0,0,0,0,2,2,2,0,0,15,0,48,202,4.023854491680812,0.09496860130662722,2,larsmans@gmail.com,sklearn/gaussian_process/gaussian_process.py,2,0.0019569471624266144,0,3,false,Allow multiple input feature with the same value for gaussian processes Fix for issue #3162 Allows to have multiple input feature with the same values for Gaussian Processes ,,1811,0.7725013804527885,0.14774951076320939,36765,454.56276349789204,35.386916904664766,113.45029239766082,2529,61,1371,394,travis,filthysocks,glouppe,false,,0,0,0,0,24,false,false,false,false,0,0,0,0,0,0,1
3694834,scikit-learn/scikit-learn,python,3161,1400464947,1401838136,1401838136,22886,22886,commits_in_master,false,true,false,113,43,6,41,43,0,84,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,63,324,601,1038,26.25531884086289,0.6196623100788222,5,larsmans@gmail.com,sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py,4,0.003933136676499509,0,11,false,AdaBoost Sparse Input Support This is a PR for issue #2581 this includes both updates to the AdaBoost classifier in ensemble/weighted_boostingpy and updates to the tests in ensemble/tests/test_weighted_boostingpyThe testing code is based off of the code written to test the sparse support in the bagging classifier (PR #3076) it was only modified slightly to work as test code for the AdaBoost classifier and regressor Like the bagging method this is showing that the results on the sparse and dense classifiers match exactlyTwo specific parts to be reviewed is the choice of parameter_sets in the testing code and also the exclusion of a npascontiguousarray in the base class for the AdaBoost code,,1810,0.7723756906077348,0.14749262536873156,36765,454.56276349789204,35.386916904664766,113.45029239766082,2529,62,1370,172,travis,hamsal,ogrisel,false,ogrisel,1,1.0,4,8,505,true,false,false,false,1,7,1,0,6,1,449
3694322,scikit-learn/scikit-learn,python,3158,1400441703,1418920985,1418920985,307988,307988,commits_in_master,false,false,false,24,16,2,16,24,0,40,0,6,0,0,1,3,1,0,0,0,0,3,3,2,0,0,49,0,168,241,9.025695092620229,0.2130190497803274,7,larsmans@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py,7,0.006910167818361303,0,5,false,ward_tree: return distances Modified ward_tree to return distances for both the structured and unstructured versionsuseful to have when plotting the dendrogram with scipy,,1809,0.7722498618021006,0.14807502467917077,36765,454.56276349789204,35.386916904664766,113.45029239766082,2529,61,1370,257,travis,mvdoc,GaelVaroquaux,false,GaelVaroquaux,0,0,8,16,159,true,false,true,false,0,0,0,0,1,0,1
3693805,scikit-learn/scikit-learn,python,3157,1400434620,1401043929,1401043929,10155,10155,commits_in_master,false,true,false,74,24,2,48,27,2,77,0,8,0,0,2,3,2,0,0,0,0,3,3,2,0,0,52,45,329,263,8.908978431928562,0.2102642822103224,0,,sklearn/tests/test_isotonic.py|sklearn/isotonic.py,0,0.0,0,11,false,Isotonic increasing auto Sorry to mix a bit of docstring fix with a real PR but:1 Adding the missing increasing docstring to IsotonicRegression  Missing here: http://scikit-learnorg/stable/modules/generated/sklearnisotonicIsotonicRegressionhtml#sklearnisotonicIsotonicRegression2 Adding the option to use either scipystatspearsonr or scipystatsspearmanr to estimate whether increasing should be True or False  Tests included and passing for isotonic though there appears to be an issue with sklearnutilsteststest_sparsefuncstest_mean_variance_axis0 for me at the moment when I merged to upstream earlier this morning,,1808,0.7721238938053098,0.14737883283877348,36765,454.56276349789204,35.386916904664766,113.45029239766082,2529,60,1370,165,travis,mjbommar,GaelVaroquaux,false,GaelVaroquaux,2,1.0,31,12,1829,true,false,false,false,0,7,2,1,19,0,9
3690291,scikit-learn/scikit-learn,python,3156,1400366909,1400437086,1400437086,1169,1169,commits_in_master,false,false,false,106,1,0,0,1,0,1,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,DOC Fix wrong confidence interval The confidence interval should be 90% instead of 95% Running the code below will give you 08997%import numpy as npfrom sklearnensemble import GradientBoostingRegressornprandomseed(1)def f(x):    return x * npsin(x)X  npatleast_2d(nprandomuniform(0 100 size10000))TX  Xastype(npfloat32)y  f(X)ravel()dy  15 + 10 * nprandomrandom(yshape)noise  nprandomnormal(0 dy)y + noisey  yastype(npfloat32)alpha  095clf  GradientBoostingRegressor(lossquantile alphaalpha                                n_estimators250 max_depth3                                learning_rate1 min_samples_leaf9                                min_samples_split9)clffit(X y)y_upper  clfpredict(X)clfset_params(alpha10 - alpha)clffit(X y)y_lower  clfpredict(X)interval  npsum((y  y_upper)  (y  y_lower)) / float(10000)print({}% confidence intervalformat(interval)),,1807,0.7719977863862756,0.14925373134328357,36780,450.27188689505164,35.26373028820011,112.04458945078848,2529,58,1369,157,travis,IvicaJovic,glouppe,false,glouppe,0,0,0,0,0,false,false,false,false,0,0,0,0,0,0,1169
3689521,scikit-learn/scikit-learn,python,3155,1400356162,1400637619,1400637619,4690,4690,commit_sha_in_comments,false,false,false,19,3,1,0,7,1,8,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,1,0,1,25,3.7771347391790044,0.08894242254032986,17,larsmans@gmail.com,sklearn/metrics/metrics.py,17,0.01691542288557214,0,0,false,Fix not used pos_label parameter in metricsprecision_recall_curve It seems like the parameter pos_label were not used in the precision_recall_curve,,1806,0.7718715393133998,0.14925373134328357,36780,450.27188689505164,35.26373028820011,112.04458945078848,2529,58,1369,160,travis,zarch,jnothman,false,jnothman,0,0,5,0,1871,false,false,false,false,0,0,0,0,1,0,589
3717396,scikit-learn/scikit-learn,python,3154,1400224927,1400246990,1400246990,367,367,github,false,false,false,30,2,2,0,1,0,1,0,5,0,0,8,8,8,0,0,0,0,8,8,8,0,0,212,0,212,0,38.0566692710787,0.8961382617655571,6,larsmans@gmail.com,examples/applications/plot_stock_market.py|examples/applications/plot_hmm_stock_analysis.py|examples/applications/plot_model_complexity_influence.py|examples/decomposition/plot_ica_vs_pca.py|examples/document_classification_20newsgroups.py|examples/ensemble/plot_forest_iris.py|examples/linear_model/plot_sparse_recovery.py|examples/tree/plot_tree_regression_multioutput.py,4,0.001953125,0,0,true,[MRG+1] Updated remaining examples to use pyplot for plotting instead of pylab These two commits fix the files that were not changed in #3151  I hope this closes #3109,,1805,0.7717451523545706,0.1455078125,36777,450.3086167985426,35.26660684667048,112.0537292329445,2529,59,1368,156,travis,rajatkhanduja,,false,,1,0.0,63,42,1310,true,false,false,false,1,1,1,0,3,0,71
3717291,scikit-learn/scikit-learn,python,3152,1400217630,1400234723,1400234723,284,284,github,false,false,false,77,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.496394232560256,0.10587870638593624,1,dsullivan7@hotmail.com,sklearn/datasets/lfw.py,1,0.0009784735812133072,0,0,true,[MRG+1] adding conditional import to lfwpy for python3 support I tried to run this command from python 34:lfw_people  datasetsfetch_lfw_people(min_faces_per_person70 resize04 data_homedatasets)I got the following error:    /usr/local/lib/python34/site-packages/sklearn/datasets/lfwpy in check_fetch_lfw(data_home funneled download_if_missing)         85                 url  BASE_URL + target_filename         86                 loggerwarn(Downloading LFW metadata: %s url)    --- 87                 urlliburlretrieve(url target_filepath)         88             else:         89                 raise IOError(%s is missing % target_filepath)    AttributeError: module object has no attribute urlretrieveUrllib in Python3 no longer supports urlliburlretrieve  It is now under urllibrequest,,1804,0.7716186252771619,0.14579256360078277,36777,450.3086167985426,35.26660684667048,112.0537292329445,2529,59,1368,156,travis,jdowns,,false,,0,0,11,28,1563,true,true,false,false,0,0,0,0,1,0,190
3677239,scikit-learn/scikit-learn,python,3147,1399953518,1401130652,1401130652,19618,19618,merged_in_comments,false,true,false,152,10,2,3,25,0,28,0,8,0,0,2,2,2,0,0,0,0,2,2,2,0,0,32,25,106,35,9.216564521283418,0.21700467043413393,0,,sklearn/isotonic.py|sklearn/tests/test_isotonic.py,0,0.0,0,15,false,Censor out-of-bounds x values for isotonic regression predict and transform in IsotonicRegression rely on interp1d which fails if an element of X is outside of [x_min x_max] for the vector that the model was fit onFor example this block below will fail with the following exception:# Load boston house price databoston_data  sklearndatasetsload_boston()# Load features/targetsfeatures  boston_datadatatarget  boston_datatargetindus_index  boston_datafeature_namestolist()index(INDUS)# Fit the modelreg  sklearnisotonicIsotonicRegression(x_censorFalse increasingFalse)regfit(features[: indus_index] target)# Now shift X up and downy_all_up  regpredict(features[: indus_index] + 1)y_all_down  regpredict(features[: indus_index] + 1)ValueError: A value in x_new is above the interpolation rangeTo make this more convenient for usage in real-world scenarios I added a simple x_censor argument to the IsotonicRegression constructor  If set to True the predict and transform methods will map out-of-bounds x values to the nearest bound  Test included,,1802,0.7719200887902331,0.1450827653359299,36777,450.1726622617397,35.26660684667048,112.02653832558393,2529,59,1364,166,travis,mjbommar,mjbommar,true,mjbommar,1,1.0,31,12,1823,true,false,false,false,0,2,1,0,17,0,465
3674166,scikit-learn/scikit-learn,python,3146,1399925836,1400418285,1400418285,8207,8207,commit_sha_in_comments,false,false,false,120,15,1,14,15,0,29,0,4,0,0,3,6,3,0,0,0,0,6,6,6,0,0,258,25,782,206,13.689906096822805,0.3223298175751923,13,olivier.grisel@ensta.org,sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs_fast.pyx|sklearn/utils/tests/test_sparsefuncs.py,11,0.009380863039399626,0,1,false,[MRG] Extend utilssparsefuncs Extends/improves  utilssparsefuncs:1 Adds functions to scale the rows of CSR/CSC matrices2 Adds functions to extract the rowwise min/max values of CSR/CSC matrices3 Adds nogil blocks in some of the critical Cython code4 Fixes some PEP8 flaws in the original codeThis commit contains changes originally written as part of #2514 as a separate commit (Or rather: those parts that havent been merged as part of the independant PR #3007 already) This is done mainly to facilitate rebasing/merging of #2514 at a later stage Plus I think it makes more sense to have this as an atomic commit as its not tightly related to the other PR (and is probably useful on its own),,1801,0.7717934480843975,0.1397748592870544,36777,450.1726622617397,35.26660684667048,112.02653832558393,2529,59,1364,156,travis,untom,jnothman,false,jnothman,5,0.6,8,0,448,false,false,false,false,0,8,0,6,0,0,308
3652827,scikit-learn/scikit-learn,python,3144,1399830632,1401331701,1401331701,25017,25017,commit_sha_in_comments,false,false,false,116,8,1,18,12,0,30,0,9,0,0,1,2,1,0,0,0,0,2,2,2,0,0,16,0,46,51,4.312861720166858,0.10154414803327785,6,larsmans@gmail.com,sklearn/ensemble/forest.py,6,0.005494505494505495,0,5,false,Adding dense pipeline support to RandomTreesEmbedding by adding a return_dense argument Hello  Currently you cant use RandomTreesEmbedding within a pipeline if the following classes expect dense input  For example this pipeline will fail:model_pipeline  sklearnpipelinePipeline([    (scale sklearnpreprocessingStandardScaler())    (feature_select sklearnfeature_selectionSelectPercentile(sklearnfeature_selectionf_classif))    (dim_red sklearnensembleRandomTreesEmbedding())    (classify sklearnensembleExtraTreesRegressor())])With this exception:TypeError: A sparse matrix was passed but dense data is required Use Xtoarray() to convert to a dense numpy array  To make this more convenient Ive added a return_dense argument to RandomTreesEmbedding that preserves default sklearn behavior but allows the pipeline to behave properly with subsequent dense-expecting classes as follows:model_pipeline  sklearnpipelinePipeline([    (scale sklearnpreprocessingStandardScaler())    (feature_select sklearnfeature_selectionSelectPercentile(sklearnfeature_selectionf_classif))    (dim_red sklearnensembleRandomTreesEmbedding(return_denseTrue))    (classify sklearnensembleExtraTreesRegressor())]),,1800,0.7716666666666666,0.1336996336996337,36779,449.9034775279371,35.237499660132144,112.02044645042007,2529,59,1363,167,travis,mjbommar,jnothman,false,jnothman,0,0,31,12,1822,true,false,false,false,0,0,0,0,23,0,76
3665909,scikit-learn/scikit-learn,python,3136,1399473250,1399745849,1399745849,4543,4543,merged_in_comments,false,false,false,28,1,1,0,15,0,15,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.211227822168391,0.09925913876249397,0,,examples/grid_search_digits.py,0,0.0,0,3,false,Update grid_search_digitspy IMHO this example does not use nested cross-validation Proper nested cv is explained here: http://scikit-learnorg/stable/tutorial/statistical_inference/model_selectionhtml#grid-search Calling the procedure nested cv (even in inverted commas) is confusing,,1798,0.7719688542825361,0.12053571428571429,36774,449.42078642519175,35.21509762332083,111.7637461249796,2528,60,1359,153,travis,t-aft,,false,,0,0,0,0,0,false,false,false,false,0,0,0,0,1,0,9
3648367,scikit-learn/scikit-learn,python,3134,1399362254,1399365419,1399365419,52,52,github,false,false,false,81,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.302697370739089,0.10141511324287814,2,larsmans@gmail.com,sklearn/cluster/affinity_propagation_.py,2,0.0018214936247723133,0,0,false,Broken string value comparison method in Affinity Propagation The old code used is instead of  to compare the user defined value of the affinity parameter for the Affinity Propagation constructor This causes problems when the input value is retrieved from an external source such as a file because the ID of the values will never be equivalent since they are stored in different locations in memory  should be used to compare the string values instead of the string IDs,,1797,0.771841958820256,0.11657559198542805,36774,449.42078642519175,35.21509762332083,111.7637461249796,2528,59,1358,152,travis,jrouly,jrouly,true,jrouly,0,0,18,22,270,true,false,false,false,0,0,0,0,1,0,52
3647171,scikit-learn/scikit-learn,python,3133,1399347562,1399347757,1399347757,3,3,github,false,false,false,72,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.618806575581806,0.10886584403151124,2,joel.nothman@gmail.com,doc/modules/cross_validation.rst,2,0.0018248175182481751,0,0,false,Mention that train_test_split returns a random split I saw https://githubcom/scikit-learn/scikit-learn/commit/17bca23b455f45c0b28d3ff04b05252317c76552 - cudos for adding this note Shuffling always bothered me and it is an issue not only for cross-validation but also for train_test_split (which shuffles by default) - maybe mention it as wellSorry for a bad diff Ive removed improper indentation What is changed is Keep in mind that :func:train_test_split still returns a random split sentence added to the second point,,1796,0.7717149220489977,0.11587591240875912,36774,449.42078642519175,35.21509762332083,111.7637461249796,2528,58,1357,152,travis,kmike,kmike,true,kmike,14,0.9285714285714286,397,0,1747,true,true,false,false,4,16,1,4,3,0,3
3641998,scikit-learn/scikit-learn,python,3131,1399312723,1399586281,1399586281,4559,4559,commit_sha_in_comments,false,false,false,52,15,2,3,16,0,19,0,6,0,0,2,4,2,0,0,0,0,4,4,3,0,0,382,34,394,216,18.332103279930745,0.4320898358384702,9,wmyers1@bloomberg.net,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,8,0.007373271889400922,0,2,false,[MRG+1] MAINT Refactor  univariate_selection module The refactoring : - simplifies the class hierachy of univariate feature selection transformer (_BaseFilter _PvalueFilter _ScoreFilter to only _BaseFilter)- improves and performs all parameter checks during the fit- fix some pep8- set the file to ascii only (instead of utf-8)- fix issue #3132,,1795,0.7715877437325905,0.11336405529953918,36774,449.42078642519175,35.21509762332083,111.7637461249796,2528,59,1357,153,travis,arjoly,larsmans,false,larsmans,52,0.7884615384615384,25,25,867,true,true,true,true,11,164,16,109,269,5,392
3633144,scikit-learn/scikit-learn,python,3129,1399201283,1399230713,1399230713,490,490,merged_in_comments,false,false,false,12,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.36823429218949,0.10295954405726462,3,olivier.grisel@ensta.org,setup.py,3,0.0027675276752767526,0,0,false,ENH: use setuptools for bdist_wheel command Enable wheel building by importing setuptools,,1794,0.7714604236343366,0.11162361623616236,36758,449.3171554491539,35.17601610533761,111.75798465640133,2522,60,1356,153,travis,matthew-brett,amueller,false,amueller,0,0,57,0,1865,false,false,false,false,0,0,0,0,1,0,428
3633334,scikit-learn/scikit-learn,python,3128,1399141828,,1406638411,124943,,unknown,false,true,false,58,10,1,3,19,0,22,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,10,0,39,69,4.247247839960822,0.1001078861274593,10,rmcgibbo@gmail.com,sklearn/cross_validation.py,10,0.009216589861751152,0,4,false,FIX: Handling of multilabel targets in cross_val_score Currently StratifiedKFold is used as a split strategy in cross_val_score But it wont work correctly if y is multilabel ie a label indicator matrix or a sequence of sequences I added a couple of lines that check the type of y and if its multilabel — simple KFold strategy is invoked,,1793,0.7718906860011154,0.11059907834101383,36758,449.3171554491539,35.17601610533761,111.75798465640133,2521,60,1355,198,travis,nmayorov,ogrisel,false,,2,0.0,5,0,38,true,false,false,false,0,7,2,3,5,0,413
3624641,scikit-learn/scikit-learn,python,3127,1399021578,1399033910,1399033910,205,205,github,false,false,false,71,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.068112421187848,0.09588565337831337,9,larsmans@gmail.com,sklearn/cross_validation.py,9,0.008341056533827619,0,0,true,[DOC] Make _fit_and_score return value docstring consistent with code The order of the return values currently listed in the cross_validation_fit_and_score docstring does not match the actual order of the return values when the kwarg return_train_scoreTrue In the code this option causes the train_score to be the first return value but in the docstring its implied that its the secondThis PR changes the docstring to be in sync with the code,,1792,0.7717633928571429,0.10843373493975904,36758,449.3171554491539,35.17601610533761,111.75798465640133,2515,59,1354,152,travis,rmcgibbo,rmcgibbo,true,rmcgibbo,3,1.0,82,90,1160,false,true,false,false,0,0,0,0,0,0,32
3621460,scikit-learn/scikit-learn,python,3125,1398983459,1398986127,1398986127,44,44,commit_sha_in_comments,false,false,false,19,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.148779612845019,0.09778698391626908,9,larsmans@gmail.com,sklearn/cross_validation.py,9,0.008372093023255815,0,0,false,cross_validation: if Y isnt a numpy array compute y_subset using a list comprehension just like its done for X ,,1790,0.7720670391061453,0.10883720930232559,36758,449.3171554491539,35.17601610533761,111.75798465640133,2514,58,1353,151,travis,mrene,mrene,true,mrene,0,0,3,2,1465,false,false,false,false,0,0,0,0,0,0,12
3596314,scikit-learn/scikit-learn,python,3119,1398746243,,1399541619,13256,,unknown,false,false,false,25,4,1,8,10,0,18,0,7,1,0,3,5,0,1,0,1,0,4,5,0,1,0,0,0,0,0,17.446749237807936,0.4112158108916462,3,larsmans@gmail.com,doc/documentation.rst|doc/faq.rst|doc/index.rst|doc/themes/scikit-learn/layout.html,3,0.0018570102135561746,0,3,false,Add FAQ to docs Do you think this is helpfulI rearranged the sections on the doc overview page not very satisfied though :-/[updated_doc_page](https://cloudgithubusercontentcom/assets/449558/2824943/cbb61d9a-cf3e-11e3-8b68-911a171924a0png),,1788,0.772930648769575,0.10677808727948004,36696,448.82275997383914,35.18094615216917,111.61979507303248,2499,56,1350,155,travis,amueller,amueller,true,,196,0.8571428571428571,875,38,1284,true,true,false,false,2,30,4,0,10,0,10
3594659,scikit-learn/scikit-learn,python,3118,1398730279,1399322693,1399322693,9873,9873,commit_sha_in_comments,false,false,false,30,2,2,2,19,0,21,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,145,5,145,5,13.568788647809178,0.31981318413946935,66,larsmans@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py,59,0.041821561338289966,2,4,false,[WIP] simpler memory management for trees This simplifies some of the realloc usage in the tree code I havent looked at the other modules besides _treepyx yet Ping @arjoly @glouppe,,1787,0.7728035814213766,0.10594795539033457,36696,448.82275997383914,35.18094615216917,111.61979507303248,2499,56,1350,154,travis,larsmans,larsmans,true,larsmans,106,0.7358490566037735,136,38,1380,true,true,false,false,47,279,53,73,118,14,52
3588505,scikit-learn/scikit-learn,python,3116,1398684455,1398687908,1398687908,57,57,commits_in_master,false,false,false,23,2,2,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,16,2,16,9.111450573381475,0.21475504366988427,17,olivier.grisel@ensta.org,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,14,0.013096351730589336,0,0,false,[BUG] Ridge doesnt raise an exception if a non-identified solver is passed  because the ValueError is only instantiated not actually raised ,,1785,0.773109243697479,0.10570626753975679,36696,448.7955090473076,35.15369522563768,111.61979507303248,2498,56,1350,148,travis,eickenberg,agramfort,false,agramfort,8,0.75,11,9,844,true,true,true,false,0,22,6,20,44,0,12
3582460,scikit-learn/scikit-learn,python,3113,1398596706,,1398623978,454,,unknown,false,false,false,47,2,2,0,7,0,7,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,94,28,94,28,9.032794462457494,0.21329311472637924,3,larsmans@gmail.com,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,3,0.0028517110266159697,0,2,false,Fix LabelBinarizer and LabelEncoder fit and transform signatures to work with Pipeline fit transform and fit_transform previously only accepted a single argument which breaks when used within a Pipeline since Pipeline always calls fit_transform with a second y argument even if it is NoneSee Issue #3112,,1784,0.773542600896861,0.10456273764258556,36696,448.7955090473076,35.15369522563768,111.61979507303248,2487,57,1349,150,travis,hxu,larsmans,false,,0,0,23,20,1969,false,true,false,false,0,0,0,0,0,0,145
3574527,scikit-learn/scikit-learn,python,3110,1398476457,,1398544381,1132,,unknown,false,false,false,123,2,2,0,7,0,7,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,135,0,135,0,9.353367204953276,0.2208628606148183,9,larsmans@gmail.com,sklearn/cross_validation.py|sklearn/cross_validation.py,9,0.008662175168431183,0,7,true,Add ShuffleKFold As per the discussion on the mailing list it would be useful to have two KFold cross validation options which are explicitly shuffling or non shuffling This commit adds a ShuffleKFold and deprecates the use of the shuffle parameter in KFold _On 23 Apr 2014 at 08:17 Mathieu Blondel mathieu@mblondelorg wrote: One solution would be to deprecate the shuffle option from KFold and add a new class ShuffleKFold The documentation should clarify the difference between ShuffleKFold and ShuffleSplit: in the latter you need to specify the split size and number of iterations while in the former this is readily specified by n_folds Also ShuffleKFold permutes the samples once whereas ShuffleSplit uses sampling and doesnt ensure that splits dont overlap,,1783,0.7739764441951766,0.10779595765158806,36696,448.7955090473076,35.15369522563768,111.61979507303248,2478,58,1347,150,travis,wcdolphin,mblondel,false,,0,0,23,22,1283,true,false,false,false,0,0,0,0,1,0,216
3560500,scikit-learn/scikit-learn,python,3108,1398388929,,1398389283,5,,unknown,false,false,false,73,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.336433329905749,0.10239702718483161,4,larsmans@gmail.com,sklearn/decomposition/pca.py,4,0.003835091083413231,0,0,false,PCA inverse_transform now computes the exact inverse operation as transform If there is no specific reason why PCAinverse_transfrom(PCAtransform(X)) should not be as close to X as possible this fixes the issue (these changes make one of the tests fail as it expects relative delta of PCAinverse_transfrom(PCAtransform(X) be greater)PS Sorry if there actually is a reason for such behavior which I just failed to find and am now bothering you for no reason,,1782,0.7744107744107744,0.10930009587727708,36696,448.7955090473076,35.15369522563768,111.61979507303248,2473,57,1346,151,travis,MikhailStartsev,MikhailStartsev,true,,0,0,0,0,704,false,false,false,false,0,0,0,0,0,0,-1
3559638,scikit-learn/scikit-learn,python,3107,1398383415,,1406655256,137864,,unknown,false,true,false,143,27,6,15,35,0,50,0,12,0,0,1,7,1,0,0,0,0,7,7,6,0,0,78,0,226,44,24.54466018320954,0.5795777416177149,4,larsmans@gmail.com,sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py,4,0.0038498556304138597,0,4,false,Feature or Bug Whitened PCA does not inverse_transform properly and is even document that way The PCAinverse_transform docstring even explicitly states that the inverse transform after whitening is inexact because the necessary rescaling to fall back into the right space is not done Is there a specific reason for this One would think that the expected behaviour of an inverse_transform should be that it maps to the closest possible point in input space given the incurred information loss Since with (full rank) PCA one can keep all the information the inverse transform should be the true inverseAny opinions on thisMy main concern for changing this is that this behaviour is documented and thus possibly expected by some usersMaking the PCA object do a true inverse is as easy as adding 2 lines to the inverse_transform as visible in the diff,,1781,0.7748455923638405,0.109720885466795,36696,448.7955090473076,35.15369522563768,111.61979507303248,2473,57,1346,209,travis,eickenberg,ogrisel,false,,7,0.8571428571428571,11,9,840,true,true,true,false,0,22,5,16,40,0,9
3557361,scikit-learn/scikit-learn,python,3106,1398371020,1399269207,1399269207,14969,14969,commit_sha_in_comments,false,true,false,31,6,2,8,14,0,22,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,17,0,69,51,8.749337364920125,0.2065995526852805,9,larsmans@gmail.com,sklearn/cross_validation.py|sklearn/cross_validation.py,9,0.008662175168431183,0,3,false,Randomized stratified k fold Added shuffle and random_state kwargs to StratifiedKFold so that the individual stratifications can be randomized It is analogous to the same kwarg in KFold Please review thanks,,1780,0.7747191011235955,0.109720885466795,36696,448.7955090473076,35.15369522563768,111.61979507303248,2473,57,1346,156,travis,jblackburne,jnothman,false,jnothman,0,0,1,0,1,false,false,false,false,0,0,0,0,0,0,17
3554071,scikit-learn/scikit-learn,python,3105,1398355346,1406903163,1406903163,142463,142463,commits_in_master,false,true,false,56,43,4,3,21,2,26,0,9,2,2,0,14,2,0,0,4,3,9,16,9,0,0,910,207,2214,670,28.338620320946163,0.6691629202240278,0,,sklearn/slda.py|sklearn/tests/test_slda.py|sklearn/slda.py|sklearn/tests/test_slda.py|sklearn/slda.py|sklearn/tests/test_slda.py|sklearn/slda.py|sklearn/tests/test_slda.py,0,0.0,1,5,false,Implemented shrinkage LDA classifier @kazemakase and I implemented shrinkage LDA (see also my comment in #1649) Note that unlike ldaLDA our implementation does only classification and  not transformation (dimensionality reduction) Note that we are not using the existing implementation because shrinkage is not possible with SVDWe would be happy if someone could review the code,,1779,0.774592467678471,0.11035818005808325,36696,448.7955090473076,35.15369522563768,111.61979507303248,2472,57,1346,205,travis,cle1109,cle1109,true,cle1109,0,0,1,0,351,true,false,false,false,1,3,0,0,3,0,172
3553362,scikit-learn/scikit-learn,python,3104,1398351061,1398895141,1398895141,9068,9068,commits_in_master,false,false,false,12,4,1,27,15,0,42,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,96,31,270,81,9.03276120718144,0.213291571665407,6,olivier.grisel@ensta.org,sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py,5,0.0048543689320388345,2,13,false,ENH: Swap rows in sparsefuncs Numpy version of https://githubcom/scikit-learn/scikit-learn/pull/3087cc @jaidevd @jnothman,,1778,0.7744656917885264,0.11067961165048544,36696,448.7955090473076,35.15369522563768,111.61979507303248,2471,56,1346,155,travis,MechCoder,GaelVaroquaux,false,GaelVaroquaux,3,0.3333333333333333,71,40,674,true,false,false,false,2,17,4,4,34,0,27
3567469,scikit-learn/scikit-learn,python,3102,1398292782,1402883258,1402883258,76507,76507,commits_in_master,false,true,false,11,41,6,23,132,21,176,0,7,0,0,2,4,2,0,0,0,0,4,4,4,0,0,780,0,5775,0,20.512478263799544,0.48436335916926204,5,manojkumarsivaraj334@gmail.com,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx,5,0.004835589941972921,0,92,false,[WIP] Releasing the GIL in the inner loop of coordinate descent ,,1777,0.7743387732132808,0.109284332688588,36696,448.7955090473076,35.15369522563768,111.61979507303248,2469,56,1345,180,travis,MechCoder,ogrisel,false,ogrisel,2,0.0,71,40,673,true,false,true,false,2,14,2,4,34,0,4
3575386,scikit-learn/scikit-learn,python,3101,1398255473,1398255604,1398255604,2,2,github,false,false,false,39,1,1,0,2,0,2,0,4,1,0,2,3,0,0,2,1,0,2,3,0,0,2,0,0,0,0,4.856744194740549,0.1149005022618266,2,gael.varoquaux@normalesup.org,doc/about.rst|doc/images/scikit-learn-logo-notext.png|doc/logos/scikit-learn-logo.png,2,0.000966183574879227,0,0,false,Add artwork with reference to logos in aboutrst Ive seen a lot people using the cutted logo (including wikipedia) from the frontpage while we have a nicer uncut logo (in high quality and vector format) in the doc/logos directory,,1776,0.7742117117117117,0.11014492753623188,36696,448.7955090473076,35.15369522563768,111.61979507303248,2463,56,1345,148,travis,fabianp,,false,,32,0.6875,209,25,1439,false,true,false,false,1,2,0,1,0,0,0
3564382,scikit-learn/scikit-learn,python,3098,1398155604,1405533031,1405533031,122957,122957,commit_sha_in_comments,false,true,false,24,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,29,0,29,0,4.199433706090241,0.09934989834703227,4,larsmans@gmail.com,sklearn/metrics/scorer.py,4,0.0038535645472061657,0,0,false,scorer: add sample_weight support This is part of the larger #1574 and adds support for sample weights in the scorer interfaceWill add tests,,1775,0.7740845070422535,0.10982658959537572,36696,448.7955090473076,35.15369522563768,111.61979507303248,2459,56,1344,193,travis,ndawe,vene,false,vene,13,0.7692307692307693,35,61,1530,true,true,false,false,0,28,4,33,43,1,9
3544590,scikit-learn/scikit-learn,python,3096,1398086766,1398086802,1398086802,0,0,github,false,false,false,5,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.318248681927243,0.10216080923551539,3,larsmans@gmail.com,sklearn/linear_model/randomized_l1.py,3,0.0028929604628736743,0,0,false,Typo: thepath - the path ,,1774,0.7739571589627959,0.10800385728061716,36696,448.7955090473076,35.15369522563768,111.61979507303248,2456,55,1343,148,travis,ash211,ash211,true,ash211,0,0,30,16,1353,false,false,false,false,0,0,0,0,0,0,-1
3544047,scikit-learn/scikit-learn,python,3095,1398075649,1398363572,1398363572,4798,4798,commit_sha_in_comments,false,false,false,41,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.331247421785034,0.10246764578409467,1,dsullivan7@hotmail.com,sklearn/ensemble/partial_dependence.py,1,0.0009624639076034649,0,1,false,fix partial dependence for python 3 I was getting errors with partial dependence in python 3 These issues were flagged by the @if_matplotlib tests in test_partial_dependencepy These tests run fine now under python 3 now well 335 at least I guess,,1773,0.7738296672306825,0.1068334937439846,36611,449.3185108300784,35.23531179153806,111.85162929174291,2455,55,1343,151,travis,jwkvam,larsmans,false,larsmans,5,0.6,3,3,1798,false,true,false,false,1,36,1,3,1,0,10
3529357,scikit-learn/scikit-learn,python,3094,1398067377,1421456637,1421456637,389821,389821,commit_sha_in_comments,false,true,false,115,2,2,9,3,0,12,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,141,48,141,48,17.974770051598895,0.42524293611901665,13,larsmans@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,12,0.011560693641618497,0,1,false,modified check_arrays to handle multiple sparse formats on pull request #3076 [it was proposed](https://githubcom/scikit-learn/scikit-learn/pull/3076#issuecomment-40739707) we extend the functionality of sklearnutilsvalidationcheck_arrays() to allow for multiple sparse formats for example:check_arrays(*arrays sparse_format[csc csr])i did my best to make sure my implementations compatible with both string arguements (sparse_formatcsc) and collections (sparse_format[csc csr]) wasnt quite sure what to do in the case of something likecheck_arrays(*arrays sparse_format[csc dense])where both sparse and dense formats are listed i assumed the most intuitive way to read that was to say csc and dense arrays are allowed and attempted to get the code to behave accordinglyi could definitely use a watchful eye on this one as check_arrays is used everywhere ,,1772,0.7737020316027088,0.10597302504816955,36611,449.3185108300784,35.23531179153806,111.85162929174291,2455,55,1343,263,travis,msalahi,amueller,false,amueller,3,1.0,7,5,1128,true,false,false,false,0,4,8,1,4,0,111
3527886,scikit-learn/scikit-learn,python,3091,1397946557,1398626617,1398626617,11334,11334,commits_in_master,false,false,false,7,2,2,1,0,0,1,0,3,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.745687843202565,0.11227253209414285,10,peter.prettenhofer@gmail.com,doc/testimonials/images/lovely.png|doc/testimonials/testimonials.rst,10,0.009708737864077669,0,2,false,Adding Lovely testimonial * paragraph* logo,,1769,0.7744488411531939,0.10097087378640776,36602,449.42899295120486,35.24397573903065,111.87913228785312,2452,55,1341,153,travis,simonfrid,ogrisel,false,ogrisel,0,0,7,15,872,false,true,true,true,0,0,0,0,0,0,188
3536873,scikit-learn/scikit-learn,python,3090,1397935463,1397936208,1397936208,12,12,github,false,false,false,29,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,18,4,18,9.084242472615104,0.21491506512353353,18,sdenton4@gmail.com,sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py,15,0.014563106796116505,0,0,false,[MRG] FIX: remove deprecation warnings in learning curves under Python 3 Quick fix to kill a warning under Python 3 I will merge as soon as travis is green,,1768,0.7743212669683258,0.09902912621359224,36602,449.40167203977927,35.24397573903065,111.87913228785312,2449,55,1341,144,travis,ogrisel,ogrisel,true,ogrisel,63,0.8253968253968254,919,123,1788,true,true,false,false,27,349,31,157,72,7,12
3513753,scikit-learn/scikit-learn,python,3087,1397828257,,1398344187,8598,,unknown,false,false,false,20,2,1,13,20,0,33,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,120,34,221,34,13.072181540276723,0.30926175248011334,6,olivier.grisel@ensta.org,sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsefuncs_fast.pyx|sklearn/utils/tests/test_sparsefuncs.py,5,0.0029469548133595285,2,15,true,[MRG+1] Swap in sparsefuncs_fast Some utils for swapping two rows This will help me in completing https://githubcom/scikit-learn/scikit-learn/pull/3079Ping @ogrisel @larsmans ,,1767,0.7747594793435201,0.09921414538310412,36602,449.40167203977927,35.24397573903065,111.87913228785312,2448,55,1340,150,travis,MechCoder,MechCoder,true,,1,0.0,71,40,668,true,false,false,false,2,7,1,1,33,0,2
3527784,scikit-learn/scikit-learn,python,3086,1397784062,1397809294,1397809294,420,420,github,false,false,false,12,1,1,0,0,0,0,0,0,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,9.503067344690294,0.2248226692464039,6,olivier.grisel@ensta.org,AUTHORS.rst|sklearn/decomposition/truncated_svd.py,6,0.005923000987166831,0,0,false,[MRG] Update authors based on #3067 I forgot the most important part,,1766,0.7746319365798414,0.0947680157946693,36601,448.23911914975,35.21761700499986,111.8002240375946,2445,55,1339,145,travis,mdbecker,mdbecker,true,mdbecker,2,1.0,53,53,939,true,true,false,false,1,8,8,4,8,0,-1
3527205,scikit-learn/scikit-learn,python,3085,1397780480,1397781985,1397781985,25,25,github,false,false,false,8,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.574285532785747,0.10821839901969789,12,olivier.grisel@ensta.org,sklearn/linear_model/ridge.py,12,0.011892963330029732,0,0,false,Update authors in sklearn/linear_model/ridgepy Title says it all,,1765,0.7745042492917847,0.09217046580773043,36589,448.38612697805354,35.147175380578865,111.6455765393971,2445,54,1339,147,travis,eickenberg,eickenberg,true,eickenberg,6,0.8333333333333334,11,9,833,true,true,false,false,0,22,2,16,37,0,11
3512315,scikit-learn/scikit-learn,python,3084,1397777672,1403885049,1403885049,101789,101789,commit_sha_in_comments,false,true,false,12,2,1,8,6,0,14,0,5,1,0,2,4,0,0,0,1,0,3,4,0,0,0,0,0,0,0,13.705924637716329,0.3242518033925141,4,gael.varoquaux@normalesup.org,doc/model_persistence.rst|doc/tutorial/basic/tutorial.rst|doc/user_guide.rst,2,0.0019880715705765406,0,4,false,[MRG+1] added a new section on model persistence Fix for issue #1332,,1764,0.7743764172335601,0.09145129224652088,36589,448.3587963595616,35.147175380578865,111.6182459209052,2444,54,1339,186,travis,raulgarreta,jnothman,false,jnothman,0,0,19,2,1128,false,false,false,false,0,0,0,0,1,0,4
3512226,scikit-learn/scikit-learn,python,3083,1397775952,1397797619,1397797619,361,361,github,false,false,false,62,3,2,1,2,0,3,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,117,18,148,18,13.552757384640152,0.32062820561685146,16,joel.nothman@gmail.com,examples/plot_learning_curve.py|sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py,12,0.010945273631840797,0,1,false,[MRG+2] Improved learning curves * Changed default number of points plotted to 5 as this is enough to see shape of curve and more can be quite expensive with complicated models* Refactored duplicated code in the example into a function which will be easy for users to copy paste and adapt* Changed cross-validation in example to provide a smoother curve,,1763,0.7742484401588202,0.09154228855721393,36589,448.3587963595616,35.147175380578865,111.6182459209052,2444,53,1339,148,travis,sdenton4,sdenton4,true,sdenton4,1,1.0,4,0,383,true,false,false,false,1,2,3,0,2,0,11
3512102,scikit-learn/scikit-learn,python,3082,1397774324,1397782020,1397782020,128,128,github,false,false,false,14,2,1,2,2,0,4,0,2,0,0,1,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,4.488396299495241,0.1061855097646379,5,larsmans@gmail.com,doc/tutorial/text_analytics/working_with_text_data.rst,5,0.0049800796812749,0,0,false,DOC: Remove dead link Link to installation instructions was pointed to a non-existant location,,1762,0.7741203178206584,0.09163346613545817,36589,448.3587963595616,35.147175380578865,111.6182459209052,2444,53,1339,147,travis,ElDeveloper,ElDeveloper,true,ElDeveloper,0,0,27,4,1331,false,false,false,false,0,0,0,0,0,0,8
3524118,scikit-learn/scikit-learn,python,3080,1397758385,1397758668,1397758668,4,4,github,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,10,0,10,4.296090509297584,0.10163604487356262,10,larsmans@users.noreply.github.com,sklearn/utils/tests/test_extmath.py,10,0.0099601593625498,0,0,false,FIX: More robust test_extmathpy compatible with older numpy Fix for #2983,,1761,0.7739920499716071,0.09163346613545817,36589,448.3587963595616,35.147175380578865,111.6182459209052,2442,53,1339,145,travis,sdenton4,sdenton4,true,sdenton4,0,0,4,0,383,true,false,false,false,1,1,0,0,1,0,-1
4124425,scikit-learn/scikit-learn,python,3079,1397718247,,1405007138,121481,,unknown,false,false,false,22,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,30,0,30,0,9.157879684205291,0.21665520322318227,14,olivier.grisel@ensta.org,sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py,12,0.011940298507462687,0,0,false,[WIP] Sparse matrix support for Randomised Lasso This is strictly a WIP Will try and finish it when I get the time,,1760,0.7744318181818182,0.09054726368159204,36579,448.4813690915553,35.15678394707346,111.6487602176112,2441,53,1339,199,travis,MechCoder,MechCoder,true,,0,0,71,40,667,true,false,false,false,2,7,0,1,31,0,-1
3521361,scikit-learn/scikit-learn,python,3078,1397704931,1397728197,1397728197,387,387,github,false,false,false,46,2,1,2,1,0,3,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,56,0,67,0,14.15386567635441,0.3348491954740391,2,g.louppe@gmail.com,examples/ensemble/plot_adaboost_multiclass.py|examples/ensemble/plot_adaboost_regression.py|examples/ensemble/plot_adaboost_twoclass.py,2,0.001984126984126984,0,0,false,[MRG] plot_adaboost_multiclasspy: handle case where boosting terminated early * Boosting can terminate early in which case n_trees is lower than the lengths of the weights and errors arrays The example is updated to handle this * PEP8 fixes and add missing author on other boosting examples,,1759,0.7743035815804434,0.09027777777777778,36579,448.4813690915553,35.15678394707346,111.6487602176112,2441,53,1338,146,travis,ndawe,ndawe,true,ndawe,12,0.75,35,61,1524,true,true,false,false,0,25,1,30,27,0,357
3510713,scikit-learn/scikit-learn,python,3076,1397694315,1397808717,1397808717,1906,1906,github,false,false,false,71,2,2,6,15,1,22,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,189,0,189,8.195826409463002,0.19389608672879435,8,olivier.grisel@ensta.org,sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py,8,0.007936507936507936,0,4,false,[MRG+1] Sparse bagging tests Added a test each for BaggingClassifier and BaggingRegressor to train on dense/sparse versions of the same data with various input parameters and make sure the outputs are equal Constrained tests to just SVC/SVR models It should be up to various models tests to make sure they handle sparse data correctly  These tests should just make sure BaggingClassifier doesnt throw a wrench in the works along the way,,1758,0.7741751990898749,0.09027777777777778,36601,448.23911914975,35.21761700499986,111.8002240375946,2440,53,1338,147,travis,msalahi,msalahi,true,msalahi,2,1.0,7,5,1123,true,false,false,false,0,1,3,0,2,0,16
3519190,scikit-learn/scikit-learn,python,3074,1397687199,1397731903,1397731903,745,745,github,false,false,false,30,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,29,0,29,0,4.496678088924401,0.1063815250041509,7,michael.hanke@gmail.com,sklearn/datasets/base.py,7,0.006958250497017893,0,0,false,open iris files with with statement to avoid ResourceWarning I was occasionally getting ResourceWarning: unclosed file warnings when running load_iris() Moved file reading into with blocks to handle IO stuff,,1757,0.774046670461013,0.0874751491053678,36579,448.4813690915553,35.15678394707346,111.6487602176112,2440,52,1338,146,travis,msalahi,msalahi,true,msalahi,1,1.0,7,5,1123,false,false,false,false,0,1,1,0,0,0,19
3518975,scikit-learn/scikit-learn,python,3073,1397685604,1397692328,1397692328,112,112,commits_in_master,false,false,false,10,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,53,0,53,4.070647670920883,0.09630258124408936,8,olivier.grisel@ensta.org,sklearn/ensemble/tests/test_bagging.py,8,0.007952286282306162,0,0,false,added tests for sparse matrix inputs to BaggingClassifier and BaggingRegressor ,,1756,0.7739179954441914,0.0874751491053678,36579,448.4813690915553,35.15678394707346,111.6487602176112,2440,52,1338,144,travis,msalahi,msalahi,true,msalahi,0,0,7,5,1123,false,false,false,false,0,1,0,0,0,0,-1
3508742,scikit-learn/scikit-learn,python,3070,1397581882,1397584289,1397584289,40,40,github,false,false,false,24,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.413250207967798,0.10440719362019873,17,olivier.grisel@ensta.org,sklearn/linear_model/coordinate_descent.py,17,0.0166015625,0,0,false,DOC Fix copy_X  default in documentation The code uses True as the default but the documentation incorrectlystated that False was the default,,1755,0.7737891737891738,0.0830078125,36525,446.7624914442163,35.09924709103354,111.62217659137578,2437,52,1337,146,travis,luispedro,luispedro,true,luispedro,1,1.0,293,43,1812,false,false,false,false,0,0,0,0,0,0,14
3508615,scikit-learn/scikit-learn,python,3069,1397581190,1397778642,1397778642,3290,3290,github,false,false,false,11,3,1,1,5,1,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,33,3.9370195132276113,0.09314068752819173,4,larsmans@gmail.com,sklearn/kernel_approximation.py,4,0.00390625,0,2,false,[MRG+1] FIX: AdditiveChi2Sample can be initialized with sample_interval #3068 Fixes #3068,,1754,0.7736602052451539,0.0830078125,36525,446.7624914442163,35.09924709103354,111.62217659137578,2437,52,1337,150,travis,ssaeger,ssaeger,true,ssaeger,1,0.0,1,0,884,true,false,false,false,1,1,1,0,1,0,1015
3506472,scikit-learn/scikit-learn,python,3067,1397530548,1397782094,1397782095,4192,4192,github,false,false,false,85,1,1,11,15,0,26,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,60,90,60,90,13.98078794454723,0.33075446011084253,4,joel.nothman@gmail.com,examples/document_clustering.py|sklearn/decomposition/tests/test_truncated_svd.py|sklearn/decomposition/truncated_svd.py,4,0.001951219512195122,0,12,false,[MRG+2] TruncatedSVD: Calculate explained variance Fixes #3047 We tested this similar to in #2663 and determined that it makes sense to calculate explained variance as part of the fit method but then we merged the _fit method with the fit_transform method to avoid doing some duplicate work This change will cause a minor performance regression in the case where fit is called by itself separate from transform (ie when calling on different inputs) which I believe is not the normal use case for this estimator,,1753,0.773531089560753,0.08195121951219513,36589,448.3587963595616,35.147175380578865,111.6182459209052,2436,51,1336,150,travis,mdbecker,mdbecker,true,mdbecker,1,1.0,53,53,936,true,true,false,false,1,2,3,0,7,0,195
3541997,scikit-learn/scikit-learn,python,3066,1397527243,1398036270,1398036270,8483,8483,commits_in_master,false,false,false,12,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,21,0,21,0,8.93727206457907,0.2114349857613396,12,larsmans@gmail.com,sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/hierarchical.py,11,0.010784313725490196,0,1,false,[MRG+1] Move pooling_func to constructor This is a partial fix for #1975,,1752,0.7734018264840182,0.07745098039215687,36525,446.7624914442163,35.09924709103354,111.62217659137578,2436,51,1336,148,travis,denisgarci,GaelVaroquaux,false,GaelVaroquaux,0,0,17,22,256,true,false,false,false,0,0,0,0,1,0,1
3514303,scikit-learn/scikit-learn,python,3065,1397523419,1397672848,1397672848,2490,2490,github,false,false,false,21,3,1,12,4,0,16,0,2,0,0,5,5,5,0,0,0,0,5,5,5,0,0,191,0,262,0,22.69750274698064,0.5369699093245456,4,larsmans@gmail.com,doc/Makefile|doc/sphinxext/gen_rst.py|doc/sphinxext/numpy_ext/docscrape.py|doc/sphinxext/numpy_ext/docscrape_sphinx.py|doc/sphinxext/numpy_ext/numpydoc.py,4,0.0,0,1,false,[MRG] Doc generation works in Python 2 and 3 Documentation can be generated using Python 2 or Python 3 and sphinx,,1751,0.7732724157624215,0.07745098039215687,36525,446.7624914442163,35.09924709103354,111.62217659137578,2436,50,1336,145,travis,abatula,ogrisel,false,ogrisel,0,0,4,1,606,true,false,false,false,0,0,0,0,1,0,2
3517065,scikit-learn/scikit-learn,python,3063,1397513175,1397673039,1397673041,2664,2664,github,false,false,false,17,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.686971561720091,0.11088279419711074,5,larsmans@gmail.com,examples/plot_digits_classification.py,5,0.004921259842519685,0,0,false,[MRG+1] Improved plot digits classification example Made two for loop statements more readable by introducing local variables,,1749,0.7735849056603774,0.07578740157480315,36524,446.77472346949946,35.10020808235681,111.62523272368853,2436,48,1336,145,travis,chalmerlowe,ogrisel,false,ogrisel,0,0,7,10,0,false,false,false,false,0,0,0,0,0,0,0
3504345,scikit-learn/scikit-learn,python,3059,1397500399,1397502795,1397502795,39,39,github,false,false,false,27,1,1,1,0,0,1,0,1,0,0,3,3,1,0,1,0,0,3,3,1,0,1,1,0,1,0,13.43977392604378,0.3179481615737074,12,olivier.grisel@ensta.org,.travis.yml|README.rst|setup.py,10,0.002952755905511811,0,1,false,TST: Switch from python 33 to 34 in travis Python 34 is supported by anaconda now so we can update travis-ci to use 34 instead of 33,,1747,0.77389811104751,0.07578740157480315,36523,446.7869561646086,35.10116912630397,111.62828902335514,2435,48,1336,139,travis,mdbecker,mdbecker,true,mdbecker,0,0,53,53,936,true,true,false,false,0,1,0,0,2,0,11
3487732,scikit-learn/scikit-learn,python,3058,1397409824,1397411659,1397411659,30,30,commit_sha_in_comments,false,false,false,6,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.613464428999758,0.10914410420859583,7,jnothman@student.usyd.edu.au,doc/tutorial/text_analytics/working_with_text_data.rst,7,0.0068694798822374874,0,0,false,typo: 1e-2 is 001 not 100 ,,1746,0.7737686139747996,0.07360157016683022,36501,446.23434974384264,35.09492890605736,111.33941535848332,2435,48,1335,143,travis,ajschumacher,larsmans,false,larsmans,4,1.0,153,317,934,false,false,false,false,0,0,4,0,7,0,30
3487578,scikit-learn/scikit-learn,python,3057,1397407201,1397407730,1397407730,8,8,commits_in_master,false,false,false,4,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.352173246068014,0.10296254892716483,5,jaquesgrobler@gmail.com,doc/tutorial/text_analytics/working_with_text_data.rst,5,0.004921259842519685,0,0,false,typo: requiered - required ,,1745,0.7736389684813754,0.0718503937007874,36501,446.23434974384264,35.09492890605736,111.33941535848332,2435,48,1335,143,travis,ajschumacher,jnothman,false,jnothman,3,1.0,153,317,934,false,false,false,false,0,0,3,0,5,0,8
3482112,scikit-learn/scikit-learn,python,3056,1397303058,1397430189,1397430189,2118,2118,commits_in_master,false,false,false,92,8,2,3,3,0,6,0,3,0,0,5,6,5,0,0,0,0,6,6,6,0,0,55,2,522,2,21.541583859565577,0.5096250210427592,90,mathieu@mblondel.org,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,68,0.03451676528599606,2,1,false,[MRG] Trees: Fix inconsistency in impurity_improvement There were still some inconsistencies regarding the computation of impurity_improvement The current implementation was correct but was not normalized by the (weighted) number of samples in the current node For trees that are built in depth-first this does not change anything However for trees that are built in best-first this may have wrongly prioritized splits Also this does not change anything for GBRT since it relies on  FriedmanMSE for which impurity improvement was already scaled by the number of samples in the nodeCC: @pprett @arjoly,,1744,0.7735091743119266,0.07001972386587771,36501,446.23434974384264,35.09492890605736,111.33941535848332,2435,48,1334,142,travis,glouppe,glouppe,true,glouppe,50,0.96,130,26,1248,true,true,false,false,13,119,13,45,24,4,108
3480887,scikit-learn/scikit-learn,python,3055,1397273944,1397287563,1397287563,226,226,commits_in_master,false,false,false,4,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.484859062522015,0.10610155818309765,0,,examples/applications/face_recognition.py,0,0.0,0,0,true,typo: fot - for ,,1743,0.7733792312105565,0.06824925816023739,36501,446.23434974384264,35.09492890605736,111.33941535848332,2434,48,1333,143,travis,ajschumacher,mblondel,false,mblondel,2,1.0,153,317,932,false,false,false,false,0,0,2,0,4,0,9
3480450,scikit-learn/scikit-learn,python,3054,1397267477,1397411848,1397411848,2406,2406,commit_sha_in_comments,false,false,false,20,1,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.39418265772613,0.10395636081899709,0,,doc/tutorial/statistical_inference/index.rst,0,0.0,0,0,true,doc: remove import note I think this is mostly a distraction here probably better for readers to take it out,,1742,0.7732491389207807,0.06831683168316832,36501,446.23434974384264,35.09492890605736,111.33941535848332,2434,48,1333,143,travis,ajschumacher,larsmans,false,larsmans,1,1.0,153,317,932,false,false,false,false,0,0,1,0,3,0,12
3480360,scikit-learn/scikit-learn,python,3053,1397266609,1397295330,1397295330,478,478,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.30798321912685,0.10191707828583567,0,,doc/tutorial/basic/tutorial.rst,0,0.0,0,0,true,target is response not explanatory ,,1741,0.7731188971855255,0.06746031746031746,36501,446.23434974384264,35.09492890605736,111.33941535848332,2434,48,1333,142,travis,ajschumacher,glouppe,false,glouppe,0,0,153,317,932,false,false,false,false,0,0,0,0,1,0,-1
3476636,scikit-learn/scikit-learn,python,3052,1397237950,1397412069,1397412069,2901,2901,commit_sha_in_comments,false,false,false,32,3,1,2,5,0,7,0,4,0,0,5,6,5,0,0,0,0,6,6,5,0,0,18,49,24,51,22.834656946880507,0.5402160132269762,49,mathieu@mblondel.org,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/utils/validation.py,28,0.014866204162537165,0,3,true,FIX: Add allow_nans option to check_arrays In some cases (in particular during grid search and cross validation)check_arrays should not panic when seeing NaNs in the input arraysFixes #2774 and #3044,,1740,0.7729885057471264,0.06739345887016848,36501,446.23434974384264,35.09492890605736,111.33941535848332,2434,48,1333,141,travis,YS-L,larsmans,false,larsmans,0,0,3,0,601,false,false,false,false,1,2,0,0,2,0,1173
3469851,scikit-learn/scikit-learn,python,3049,1397165537,1397208917,1397208917,723,723,commits_in_master,false,false,false,52,2,2,2,2,0,4,0,3,0,0,6,6,6,0,0,0,0,6,6,6,0,0,30,10,30,10,21.12028659978434,0.49965691805146323,78,mathieu@mblondel.org,sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/tests/test_tree.py,68,0.013875123885034688,2,1,false,[MRG] Trees: set correct impurity values in BestFirstBuilder In master impurity values were not set correctly when building trees in best-first The bug is minor and only affects the resulting impurity vector as well as variable importances Trees were still built in the correct way even with this bugCC: @pprett @arjoly,,1739,0.772857964347326,0.06541129831516353,36501,446.23434974384264,35.09492890605736,111.33941535848332,2434,48,1332,140,travis,glouppe,arjoly,false,arjoly,49,0.9591836734693877,130,26,1246,true,true,true,true,14,126,11,49,21,4,35
3442633,scikit-learn/scikit-learn,python,3045,1396916549,1397002112,1397002112,1426,1426,commit_sha_in_comments,false,false,false,37,1,1,0,4,0,4,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,32,11,32,11,18.607591439250033,0.4402135621717899,15,olivier.grisel@ensta.org,sklearn/decomposition/factor_analysis.py|sklearn/decomposition/nmf.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,12,0.009424083769633508,2,0,false,[MRG] sqnorm helper for fast squared norm @ogrisel and @GaelVaroquaux already approved the idea but this time I think I got the implementation right too This might be useful in more places than where its currently used,,1738,0.7727272727272727,0.06596858638743455,36487,446.37816208512623,35.10839477074026,111.38213610326966,2434,48,1329,141,travis,larsmans,larsmans,true,larsmans,105,0.7333333333333333,134,38,1359,true,true,false,false,46,295,53,71,112,14,1330
3434827,scikit-learn/scikit-learn,python,3043,1396838133,1398110687,1398110687,21209,21209,merged_in_comments,false,false,false,20,19,5,53,26,0,79,0,6,0,0,5,7,5,0,0,0,0,7,7,6,0,0,553,316,698,489,36.28428305788371,0.8584043569805234,24,olivier.grisel@ensta.org,sklearn/base.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,11,0.007743362831858407,0,16,false,[MRG] Weighted metrics This PR is part of the larger #1574 and adds support for sample weights in the metrics,,1737,0.7725964306275187,0.06747787610619468,36487,446.37816208512623,35.10839477074026,111.38213610326966,2434,48,1328,150,travis,ndawe,arjoly,false,arjoly,11,0.7272727272727273,35,61,1514,true,true,false,false,0,25,0,20,26,0,7
3423219,scikit-learn/scikit-learn,python,3041,1396647479,1396795373,1396795373,2464,2464,commit_sha_in_comments,false,false,false,79,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,17,0,17,4.6484014402889615,0.1099701846011531,3,dsullivan7@hotmail.com,sklearn/utils/tests/test_validation.py,3,0.003389830508474576,0,0,true,[BUG] safe_asarray fails for dok_matrix and lil_matrix safe_asarray doesnt like dok_matrix or lil_matrix because dok_matrix does not expose data and lil_matrix exposes data weirdly (array of dtype object list of not necessarily equally sized lists)Does this need to be worked around in sklearn or should part of it eg setting dok_matrixdata  dok_matrixvalues be asked for in scipyIf we work around in sklearn what would the desired output matrix be Same type or cast to say coo_matrix,,1736,0.7724654377880185,0.0711864406779661,36506,446.0636607680929,35.0901221716978,111.32416589053855,2434,49,1326,140,travis,eickenberg,larsmans,false,larsmans,5,0.8,11,9,820,true,true,false,false,0,10,1,11,33,0,2464
3412154,scikit-learn/scikit-learn,python,3036,1396545928,,1396550007,67,,unknown,false,false,false,28,1,1,0,1,0,1,0,1,0,0,8,8,8,0,0,0,0,8,8,8,0,0,76,0,76,0,26.08443941030442,0.6185509538901106,23,peter.prettenhofer@gmail.com,sklearn/svm/base.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c,17,0.005108556832694764,1,0,false,[MRG+1] remove _label attribute from SVC New take on Andys #2722 Already has +1 from @mblondel but I want it to pass in Travis before I push it,,1735,0.7729106628242075,0.08045977011494253,36499,446.1766075782898,35.09685196854708,111.31811830461108,2434,50,1325,143,travis,larsmans,larsmans,true,,104,0.7403846153846154,134,38,1355,true,true,false,false,36,279,50,82,89,10,40
3411994,scikit-learn/scikit-learn,python,3035,1396544808,1396549057,1396549057,70,70,github,false,false,false,15,2,1,0,1,0,1,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,4,28,4,31,9.09469469550261,0.2156662040964066,14,peter.prettenhofer@gmail.com,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_sgd.py,10,0.008974358974358974,2,0,false,[MRG+1] More informative numerical error message in SGD Also add a testPing @pprett @mblondel,,1734,0.7727797001153403,0.08076923076923077,36499,446.1766075782898,35.09685196854708,111.31811830461108,2434,50,1325,146,travis,ogrisel,ogrisel,true,ogrisel,62,0.8225806451612904,904,123,1772,true,true,false,false,32,358,32,133,63,8,9
3390723,scikit-learn/scikit-learn,python,3034,1396521574,1397582105,1397582105,17675,17675,commits_in_master,false,true,false,276,88,11,46,56,14,116,0,7,0,0,12,39,11,0,0,2,0,39,41,35,0,1,254,432,974,1173,78.25012146569763,1.855586408620114,75,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/metrics/metrics.py|sklearn/svm/base.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_ridge.py|doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,43,0.007761966364812419,0,22,false,[ENH] Ridge can use sample weights in feature space (XTdot(X) gram matrix) Hello everybodyhere is an enhancement to the ridge_regression function: Up until now if sample_weight is given the decision tree imposes a treatment in sample space which can lead to Memory errors if the number of samples is large even though the number of features is small See http://stackoverflowcom/questions/22766978/sklearn-ridge-and-sample-weight-gives-memory-error for an external issue on thisAll I added was the possibility to treat sample weights in _solve_dense_cholesky and not only in _solve_dense_cholesky_kernelI provide a batch of tests checking that the results of these two functions are the same given sample weightsFurther I provide a test to ensure that the correct branch is chose in ridge_regression (ie use _solve_dense_cholesky if n_samples  n_features even if there are sample weights) The way I implemented this is that it will cause a MemoryError if the wrong branch is taken By TDD I made sure it fails first - now it passes Please comment on this practice: Evidently in this current corrected case the MemoryError will not be raised but if ever the branching is wrong and the computer tested has 2TB of memory available this test will no longer raise a MemoryError If there are better ways of checking the right branching please let me knowPS: There are some inconsistencies in function signature default values: _solve_dense_cholesky_kernel has sample_weightNone as default which will if called directly be interpreted as though sample_weights are actually present This has never caused an error because the public function ridge_regression always calls with sample_weight1 if no sample weights are given Should I correct this in the present PR or elsewhere,,1733,0.7726485862665897,0.0815006468305304,36499,446.1492095673854,35.09685196854708,111.26332228280226,2434,50,1325,153,travis,eickenberg,ogrisel,false,ogrisel,4,0.75,11,9,819,true,true,true,false,0,0,0,0,3,0,10
3406490,scikit-learn/scikit-learn,python,3030,1396450158,1396450753,1396450753,9,9,github,false,false,false,10,4,4,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,11,32,11,32,26.25541746992957,0.6226075384491544,32,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,21,0.013368983957219251,0,0,false,Fix LARS drop for good test failure Fix for #3024,,1732,0.7725173210161663,0.08021390374331551,36494,446.1555324162876,35.10166054693922,111.25116457499863,2434,49,1324,147,travis,ogrisel,ogrisel,true,ogrisel,61,0.819672131147541,901,123,1771,true,true,false,false,30,342,27,125,52,8,5
3405203,scikit-learn/scikit-learn,python,3028,1396435246,1396450447,1396450447,253,253,commit_sha_in_comments,false,true,false,64,7,6,0,2,0,2,0,2,2,0,6,8,7,0,1,2,0,6,8,7,0,1,45,65,45,67,41.98710031277576,0.9956606175580708,33,olivier.grisel@ensta.org,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|.travis.yml|travis/before_install.sh|travis/test_script.sh|sklearn/cluster/tests/test_spectral.py|sklearn/linear_model/tests/test_bayes.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/linear_model/tests/test_least_angle.py,10,0.009446693657219974,1,0,false,[WIP] Set travis to test old versions of numpy and scipy too Modified version of @ogrisel #3022 to have more robust lars testThis is a tentative PR (to check travis build) to improve the travis config by:  *  use cleaner multiline scripts in separate files  *  test old versions of numpy and scipy with python 26  *  use the travis managed apt cache,,1731,0.7723859041016753,0.08097165991902834,36494,446.1555324162876,35.10166054693922,111.25116457499863,2434,48,1324,147,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,45,0.7555555555555555,471,3,1500,true,true,false,false,23,313,54,112,51,12,15
3400932,scikit-learn/scikit-learn,python,3027,1396383728,,1396450199,1107,,unknown,false,true,false,12,1,1,0,20,0,20,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,15,0,15,4.682587981516027,0.11104049593126156,6,olivier.grisel@ensta.org,sklearn/linear_model/tests/test_least_angle.py,6,0.008141112618724558,0,1,false,TST: more robust test of dropping in Lars Should hopefully fix #3024,,1730,0.7728323699421965,0.07869742198100407,36494,446.1555324162876,35.10166054693922,111.25116457499863,2434,48,1323,147,travis,GaelVaroquaux,GaelVaroquaux,true,,44,0.7727272727272727,471,3,1499,true,true,false,false,23,305,53,111,49,12,1
3399321,scikit-learn/scikit-learn,python,3025,1396372162,1396469425,1396469425,1621,1621,github,false,false,false,38,1,1,0,8,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,22,0,22,4.535488958045001,0.10755226493983044,7,olivier.grisel@ensta.org,sklearn/cluster/tests/test_spectral.py,7,0.009589041095890411,0,3,false,[MRG] Remove redundant yet unstable test_spectral_lobpcg_mode The lobpcg mode is already tested in the test_spectral_clustering test at [that line](https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/cluster/tests/test_spectralpy#L35)LinAlgError is raised when executing test_spectral_lobpcg_mode on somesystems depending on the version of the scipy and LAPACK libraries,,1728,0.7731481481481481,0.07945205479452055,36494,446.1555324162876,35.10166054693922,111.25116457499863,2434,47,1323,144,travis,ogrisel,ogrisel,true,ogrisel,60,0.8166666666666667,901,123,1770,true,true,false,false,30,322,26,125,49,8,0
3382276,scikit-learn/scikit-learn,python,3022,1396263335,1396549190,1396549190,4764,4764,commits_in_master,false,false,false,49,5,3,0,18,0,18,0,4,1,0,1,4,1,0,1,3,0,3,6,3,0,1,56,0,123,0,25.072731677587974,0.5945619612716841,8,gael.varoquaux@normalesup.org,.travis.yml|.travis.yml|travis/before_install.sh|.travis.yml|travis/before_install.sh,8,0.011157601115760111,0,0,false,[WIP] Set travis to test old versions of numpy and scipy too This is a tentative PR (to check travis build) to improve the travis config by: - use multiline script- test old versions of numpy and scipy with python 26- use the travis managed apt cache,,1727,0.7730167921250723,0.08089260808926081,36494,446.1555324162876,35.10166054693922,111.25116457499863,2434,46,1322,146,travis,ogrisel,agramfort,false,agramfort,59,0.8135593220338984,900,123,1769,true,true,true,true,30,321,25,125,42,8,8
3376210,scikit-learn/scikit-learn,python,3019,1396151406,1396188837,1396188837,623,623,commit_sha_in_comments,false,false,false,33,8,6,2,10,0,12,0,4,0,0,5,5,5,0,0,0,0,5,5,5,0,0,185,302,189,302,61.8649496945362,1.466940287070975,23,peter.prettenhofer@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/tests/test_base.py|sklearn/utils/validation.py|sklearn/linear_model/base.py|sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/tests/test_base.py|sklearn/utils/validation.py|sklearn/linear_model/base.py|sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py,13,0.005780346820809248,0,5,false,[MRG] drop redundant sparse_std test center_data This cleans up the implementation of and tests [sparse_]center_data with a note about the current scaling not being to unit variance It should be a quick merge,,1726,0.7728852838933952,0.0838150289017341,36508,442.86183850115043,34.978634819765524,109.94850443738359,2434,46,1320,146,travis,jnothman,jnothman,true,jnothman,74,0.6351351351351351,22,1,1796,true,true,false,false,18,383,39,267,83,3,163
3375649,scikit-learn/scikit-learn,python,3018,1396142278,1396184732,1396184732,707,707,commit_sha_in_comments,false,false,false,70,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,4.3957273650153565,0.1042313877981035,4,olivier.grisel@ensta.org,sklearn/utils/tests/test_sparsefuncs.py,4,0.005772005772005772,0,1,false,TST: Fix ValueError: Buffer dtype mismatch expected npy_intp but got long on win-amd64 This fixes the following test error on win-amd64 See also issue #3008ERROR: sklearnutilsteststest_sparsefuncstest_densify_rows----------------------------------------------------------------------Traceback (most recent call last):  File X:\Python27-x64\lib\site-packages\nose\casepy line 197 in runTest    selftest(*selfarg)  File X:\Python27-x64\lib\site-packages\sklearn\utils\tests\test_sparsefuncspy line 43 in test_densify_rows    assign_rows_csr(X rows nparange(outshape[0] dtypenpintp)[::-1] out)  File sparsefuncs_fastpyx line 313 in sklearnutilssparsefuncs_fastassign_rows_csr (sklearn\utils\sparsefuncs_fastc:4235)ValueError: Buffer dtype mismatch expected npy_intp but got long,,1725,0.7727536231884058,0.08658008658008658,36508,442.86183850115043,34.978634819765524,109.94850443738359,2434,46,1320,146,travis,cgohlke,larsmans,false,larsmans,2,1.0,28,0,1229,false,false,false,false,0,0,2,0,0,0,581
3375605,scikit-learn/scikit-learn,python,3017,1396141576,1396177054,1396177054,591,591,commits_in_master,false,false,false,82,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.326346920177673,0.10258624026028382,8,gael.varoquaux@normalesup.org,sklearn/cluster/hierarchical.py,8,0.011544011544011544,0,0,false,Fix ValueError: Buffer dtype mismatch expected INTP but got long on win-amd64 This fixes the following test failure See also issue #3008ERROR: Check that we obtain the correct number of clusters with----------------------------------------------------------------------Traceback (most recent call last):  File X:\Python27-x64\lib\site-packages\nose\casepy line 197 in runTest    selftest(*selfarg)  File X:\Python27-x64\lib\site-packages\sklearn\cluster\tests\test_hierarchicalpy line 149 in test_agglomerative_clustering    clusteringfit(X)  File X:\Python27-x64\lib\site-packages\sklearn\cluster\hierarchicalpy line 656 in fit    labels  _hierarchicalhc_get_heads(parents copyFalse)  File _hierarchicalpyx line 106 in sklearncluster_hierarchicalhc_get_heads (sklearn\cluster\_hierarchicalcpp:2810)ValueError: Buffer dtype mismatch expected INTP but got long,,1724,0.7726218097447796,0.08658008658008658,36508,442.86183850115043,34.978634819765524,109.94850443738359,2434,46,1320,146,travis,cgohlke,agramfort,false,agramfort,1,1.0,28,0,1229,false,false,false,false,0,0,1,0,0,0,10
3375476,scikit-learn/scikit-learn,python,3016,1396139605,1396181993,1396181993,706,706,commit_sha_in_comments,false,false,false,18,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.068252340312256,0.09646630742353773,72,peter.prettenhofer@gmail.com,sklearn/tree/_tree.pyx,72,0.1038961038961039,0,4,false,Fix MSVC error C2036: void * : unknown size sklearn\tree\_treec(19094) : error C2036: void * : unknown size,,1723,0.7724898432965758,0.08658008658008658,36508,442.86183850115043,34.978634819765524,109.94850443738359,2434,46,1320,145,travis,cgohlke,larsmans,false,larsmans,0,0,28,0,1229,false,false,false,false,0,0,0,0,0,0,8
3371768,scikit-learn/scikit-learn,python,3015,1396068080,1396169476,1396169476,1689,1689,commit_sha_in_comments,false,false,false,80,2,1,4,9,0,13,0,6,0,0,3,3,2,0,1,0,0,3,3,2,0,1,144,0,206,0,14.095823090467972,0.3342406985868273,3,olivier.grisel@ensta.org,.gitignore|setup.py|sklearn/__init__.py,2,0.001440922190201729,0,0,true,[MRG] Do not import the sklearn package from setuppy The execution of setuppy never imports the sklearn package to avoid cycliddependenciesThe sklearn/versionpy file is now generated each time setuppy is executedIf the version is not a release version a dev_aaaaaaa suffix is appendedwhere aaaaaaa is the 7 first digits of the hexdigest of the last gitcommit in the local repoI find this solution less hackish and more informative than the previousstateful builtins-based hack,,1722,0.7723577235772358,0.08501440922190202,36503,442.8403144946991,34.983426019779195,109.908774621264,2434,46,1319,259,travis,ogrisel,jnothman,false,jnothman,58,0.8103448275862069,900,123,1766,true,true,false,false,28,331,26,129,41,7,10
3361328,scikit-learn/scikit-learn,python,3011,1395970448,,1423749674,462987,,unknown,false,false,false,23,28,2,11,30,0,41,0,5,2,0,1,77,3,0,0,4,2,73,79,66,0,1,208,104,2344,747,26.45869024038858,0.6273846527575881,4,joel.nothman@gmail.com,sklearn/feature_selection/__init__.py|sklearn/feature_selection/select_from_model.py|sklearn/feature_selection/tests/test_select_from_model.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/select_from_model.py|sklearn/feature_selection/tests/test_select_from_model.py,4,0.0,0,2,false,[MRG] Implemented SelectFromModel meta-transformer Fix for #2160Implemented a meta-transformer to with _LearntSelectorMixin Test cases are included Documentation(with examples) needs to be completed,,1720,0.7732558139534884,0.07681159420289856,36492,442.94639921078596,34.99397128137674,109.94190507508495,2433,45,1318,287,travis,maheshakya,MechCoder,false,,10,0.6,2,0,798,true,false,false,false,3,61,16,7,8,0,77
3360291,scikit-learn/scikit-learn,python,3010,1395962761,1395973951,1395973951,186,186,commits_in_master,false,false,false,36,1,1,0,8,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,12,2,12,8.625556397299048,0.20452795115685138,11,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,10,0.014577259475218658,0,0,false,Fixed SelectKBest corner case: k0 Without this fix k  0 will have the same behavior as k  all based on howthe array indexing was writtenIve included a test that demonstrates proper behaviour,,1719,0.7731239092495636,0.07580174927113703,36492,442.94639921078596,34.99397128137674,109.94190507508495,2433,45,1318,142,travis,griffinmyers,jnothman,false,jnothman,0,0,9,10,620,false,false,false,false,0,0,0,0,0,0,2
3359984,scikit-learn/scikit-learn,python,3009,1395960745,1396118599,1396118599,2630,2630,merged_in_comments,false,false,false,60,2,1,3,4,0,7,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,7,6,14,12,9.057295627167875,0.2147652896022598,13,gael.varoquaux@normalesup.org,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,9,0.013119533527696793,1,1,false,BUG: FeatureAgglomeration: error for n_samples  0 Trivial fix for trivial bug Can I haz review plzCurrently the FeatureAgglomeration code does not raises an error when it is given no samples (a matrix of dimensions (0 n_features) but creates a meaningless clustering that only reflects the connectivity matrix@schwarty has a nasty bug in his codebase because of that,,1718,0.7729918509895227,0.07580174927113703,36492,442.94639921078596,34.99397128137674,109.94190507508495,2433,45,1318,143,travis,GaelVaroquaux,agramfort,false,agramfort,43,0.7674418604651163,471,3,1494,true,true,false,true,21,288,52,103,45,12,115
3348047,scikit-learn/scikit-learn,python,3007,1395862412,1395934367,1395934367,1199,1199,commits_in_master,false,false,false,20,5,2,10,22,4,36,0,5,0,0,3,11,3,0,0,1,0,11,12,15,0,0,144,0,325,42,17.694514364976524,0.420014146398968,6,manojkumarsivaraj334@gmail.com,sklearn/linear_model/base.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/linear_model/base.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx,6,0.00145985401459854,0,14,false,ENH: Refactoring and optimisation of sparsfuncspyx Some refactoring and optimisation in sparsefuncspyx should not matter much but is definitely faster,,1717,0.7728596389050669,0.06569343065693431,36476,442.6198048031583,34.981905910735826,109.7982234894177,2433,45,1317,141,travis,Manoj-Kumar-S,ogrisel,false,ogrisel,14,0.5714285714285714,22,15,645,true,false,false,false,1,166,14,68,11,4,3
3345503,scikit-learn/scikit-learn,python,3006,1395843116,1395851964,1395851964,147,147,commits_in_master,false,false,false,70,1,1,1,6,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,41,0,41,0,3.969167003454754,0.0942154215265381,5,peter.prettenhofer@gmail.com,sklearn/base.py,5,0.007331378299120235,0,4,false,[MRG] FIX: more robust skip of implicit constructor Parameter introspection requires introspecting the keyword arguments of theconstructor of Python classes deriving from BaseEstimatorThis can be problematic for classes that do not override the defaultconstructor Introspecting the implicit constructor used to raise a TypeErrorbut this is no longer the case in Python 34+ Hence with use a explicit checkfor the implicit constructor prior to using inspection,,1716,0.7727272727272727,0.06304985337243402,36458,442.4543310110264,34.971748313127435,109.79757529211695,2433,45,1317,142,travis,ogrisel,ogrisel,true,ogrisel,57,0.8070175438596491,900,123,1764,true,true,false,false,26,311,22,128,32,3,13
3343977,scikit-learn/scikit-learn,python,3005,1395822583,,1407684180,197693,,unknown,false,true,false,53,6,1,3,23,0,26,0,5,0,0,36,36,36,0,0,0,0,36,36,36,0,0,3422,133,3687,546,38.50025497322854,0.9138738047617329,96,peter.prettenhofer@gmail.com,sklearn/__check_build/_check_build.c|sklearn/_hmmc.c|sklearn/_isotonic.c|sklearn/cluster/_k_means.c|sklearn/datasets/_svmlight_format.c|sklearn/ensemble/_gradient_boosting.c|sklearn/feature_extraction/_hashing.c|sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/tests/test_base.py|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/pairwise_fast.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/svm/liblinear.c|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/utils/_logistic_sigmoid.c|sklearn/utils/arrayfuncs.c|sklearn/utils/graph_shortest_path.c|sklearn/utils/lgamma.c|sklearn/utils/murmurhash.c|sklearn/utils/random.c|sklearn/utils/seq_dataset.c|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_min_spanning_tree.c|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/validation.py|sklearn/utils/weight_vector.c,60,0.0,0,8,false,[MRG] FIX normalize to unit variance in [sparse_]center_data Had been normalizing to unit/n_samples variance since 295dc3cAt master:python X1  nprandomnormal(size(1000 1)) X2  nprandomnormal(size(10000 1)) center_data(X1 npzeros(X1shape[0]) True True)[0]std()0031622776601683784 center_data(X2 npzeros(X1shape[0]) True True)[0]std()00099999999999999794This also deletes sparse_std cleans up and tests the centering code,,1715,0.7731778425655976,0.06342182890855458,36458,442.4543310110264,34.971748313127435,109.79757529211695,2433,45,1317,201,travis,jnothman,jnothman,true,,73,0.6438356164383562,22,1,1793,true,true,false,false,18,364,37,262,77,2,2
3336469,scikit-learn/scikit-learn,python,3004,1395761376,,1395862527,1685,,unknown,false,true,false,8,1,1,4,17,0,21,0,6,0,0,6,6,6,0,0,0,0,6,6,6,0,0,225,13,225,13,18.209994070660883,0.43224745422744065,9,manojkumarsivaraj334@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_sparsefuncs.py,5,0.0058823529411764705,0,12,false,FIX: Sparse_std now supports CSR matrices Fixes https://githubcom/scikit-learn/scikit-learn/issues/3000 ,,1714,0.7736289381563594,0.06323529411764706,36458,442.4543310110264,34.971748313127435,109.79757529211695,2433,45,1316,142,travis,Manoj-Kumar-S,Manoj-Kumar-S,true,,13,0.6153846153846154,22,15,644,true,false,false,false,1,154,13,65,10,4,26
3328940,scikit-learn/scikit-learn,python,2999,1395691297,1396545508,1396545508,14236,14236,commit_sha_in_comments,false,false,false,85,1,1,0,5,0,5,0,5,0,0,7,7,4,0,0,0,0,7,7,4,0,0,10,0,10,0,29.967040080454378,0.7113224054464495,35,virgile.fritsch@gmail.com,doc/modules/classes.rst|doc/modules/linear_model.rst|examples/linear_model/README.txt|sklearn/feature_selection/rfe.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/least_angle.py,14,0.007374631268436578,0,1,false,DOC: disambiguate General[ized] Linear Models The documentation refers to Generalized Linear Models for the collection including OLS ridge regression etc This is not Generalized Linear Models as usually understood by statisticians - see https://enwikipediaorg/wiki/Generalized_linear_models - so the use of this term is potentially confusing for people hoping to use GLM in PythonA better term is General Linear Models - see https://enwikipediaorg/wiki/General_linear_model - so this PR updates the documentation to say that I hope this is not a controversial suggestion I merely hope to disambiguate,,1712,0.7739485981308412,0.06342182890855458,36458,442.4543310110264,34.971748313127435,109.79757529211695,2433,44,1315,147,travis,danstowell,larsmans,false,larsmans,0,0,53,14,1500,false,false,false,false,0,2,0,0,0,0,2
3327322,scikit-learn/scikit-learn,python,2997,1395680263,1395680637,1395680637,6,6,commits_in_master,false,false,false,4,1,1,0,2,0,2,0,4,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.831908287632402,0.11490911471019716,10,peter.prettenhofer@gmail.com,doc/testimonials/images/okcupid.png|doc/testimonials/testimonials.rst,10,0.014792899408284023,0,0,false,DOC: add OkCupid testimonial ,,1711,0.7738164815897136,0.060650887573964495,36458,440.8086016786439,34.45060069120632,110.12672115859345,2433,44,1315,139,travis,GaelVaroquaux,ogrisel,false,ogrisel,42,0.7619047619047619,468,3,1491,true,true,true,true,21,276,51,102,51,12,6
3317044,scikit-learn/scikit-learn,python,2991,1395521180,1395541165,1395541165,333,333,commits_in_master,false,false,false,12,3,3,1,6,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,60,33,60,33,26.47109163651241,0.6262138127819922,3,larsmans@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/base.py|sklearn/linear_model/tests/test_base.py,3,0.004470938897168405,0,2,false,Preserve CSR storage format when input is CSR in sparse_center_data Fixes https://githubcom/scikit-learn/scikit-learn/issues/2990,,1709,0.7741369221767115,0.05067064083457526,36431,441.1078477121133,34.50358211413357,110.20833905190634,2433,44,1313,141,travis,Manoj-Kumar-S,jnothman,false,jnothman,12,0.5833333333333334,22,15,641,true,false,false,false,1,163,12,65,10,4,0
3305963,scikit-learn/scikit-learn,python,2985,1395385661,1397411577,1397411577,33765,33765,commit_sha_in_comments,false,false,false,69,8,2,18,16,0,34,0,7,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,27,96,44,9.051711601848709,0.21413057413818742,2,joel.nothman@gmail.com,sklearn/linear_model/ransac.py|sklearn/linear_model/tests/test_ransac.py,2,0.0030075187969924814,0,5,true,Improve RANSACRegressor Adding a new parameter to the RANSAC regressor I am not sure why I did not include this in the original PR this should definitely be added It is not totally backwards compatible but I am not sure how you typically handle these cases Also I would theoretically group all the stop_* parameters next to each other Let me know how you want me to change this,,1708,0.7740046838407494,0.042105263157894736,36432,441.0957400087835,34.502635046113305,110.20531400966183,2433,44,1312,155,travis,ahojnnes,larsmans,false,larsmans,5,0.6,13,3,1631,false,true,false,false,0,0,0,0,2,0,22
3304668,scikit-learn/scikit-learn,python,2984,1395370723,1400684919,1400684919,88569,88569,commits_in_master,false,true,false,67,94,2,28,80,5,113,0,9,1,0,7,188,6,0,0,26,21,166,213,163,1,13,1415,368,21988,5736,40.955487183121356,0.9688578658250464,87,peter.prettenhofer@gmail.com,doc/modules/tree.rst|examples/tree/train_time_sparse_vs_dense_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|AUTHORS.rst|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py,61,0.024132730015082957,0,51,false,Added sparsity feature to decision tree classifier Added the following option to decision trees: The method fit(X y) in DecsionTreeClassifier can now take sparse matrix X (css_matrix csr_matrix or coo_matrix) This feature saves both memory and time In case of sparsity the training time with sparse matrix is much lower than the training with the same data fed as dense matrix to fit (around 20 times faster),,1707,0.7738722905682484,0.04072398190045249,36432,441.0957400087835,34.502635046113305,110.20531400966183,2433,44,1311,174,travis,fareshedayati,arjoly,false,arjoly,0,0,1,1,27,false,false,false,false,0,0,0,0,0,0,44
3304473,scikit-learn/scikit-learn,python,2982,1395369142,1395682487,1395682487,5222,5222,commits_in_master,false,false,false,90,5,4,7,4,0,11,0,3,0,0,4,6,4,0,0,0,0,6,6,6,0,0,0,70,0,172,22.52718748512968,0.5329113213156517,13,peter.prettenhofer@gmail.com,sklearn/decomposition/tests/test_sparse_pca.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/testing.py|sklearn/cluster/tests/test_k_means.py|sklearn/ensemble/tests/test_bagging.py,6,0.00904977375565611,0,4,false,[MRG] MacOS X testing improvements Hanging multiprocessing tests are skipped on Mac OS X 109 in this PRtest_k_means_plus_plus_init_2_jobs doesnt fail for me on 109 but Ive added skipping code anyways because it was intended to be skipped before and failures could be non-deterministicAlso Im not sure customization hooks in if_not_mac_os (versions and message arguments) worth keeping But at least it is better than old _is_mac_os_version function which had incorrect docstringNot Mac specific: BaggingRegressor was untested with n_jobs-1 (most likely because of a type) this is also fixed,,1706,0.7737397420867527,0.04072398190045249,36432,441.0957400087835,34.502635046113305,110.20531400966183,2433,44,1311,143,travis,kmike,ogrisel,false,ogrisel,13,0.9230769230769231,390,0,1701,true,true,false,false,2,0,0,0,2,0,746
3300539,scikit-learn/scikit-learn,python,2981,1395343404,1395350648,1395350648,120,120,commits_in_master,false,false,false,14,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.383127586631082,0.10368883614537364,0,,sklearn/decomposition/kernel_pca.py,0,0.0,0,0,false,fix issue #2901 there is no degree parameter for other kernels except poly kernel,,1705,0.7736070381231671,0.04072398190045249,36432,441.0957400087835,34.502635046113305,110.20531400966183,2433,43,1311,140,travis,matrixorz,ogrisel,false,ogrisel,0,0,61,204,657,false,false,false,false,1,2,0,0,0,0,12
3299807,scikit-learn/scikit-learn,python,2979,1395339255,1431287898,1431287898,599144,599144,commit_sha_in_comments,false,true,false,20,5,2,0,10,0,10,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,20,0,42,8,8.702815887945693,0.2058813759551544,2,gael.varoquaux@normalesup.org,sklearn/tree/export.py|sklearn/tree/export.py,2,0.0030349013657056147,0,0,false,Add optional attribute target_names Add the optional attribute to replace target values with string values of choice just like feature_names,,1703,0.7739283617146212,0.0409711684370258,36430,441.1199560801537,34.50452923414768,110.21136426022508,2433,43,1311,321,travis,l00pen,amueller,false,amueller,0,0,7,12,1043,true,false,false,false,0,0,0,0,1,0,11
3299434,scikit-learn/scikit-learn,python,2978,1395336480,1395406647,1395406647,1169,1169,commit_sha_in_comments,false,false,false,26,4,1,0,7,0,7,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,22,0,37,0,12.07030625633389,0.28554565467668747,6,peter.prettenhofer@gmail.com,sklearn/datasets/mlcomp.py|sklearn/externals/joblib/logger.py|sklearn/externals/joblib/memory.py,6,0.0,0,2,false,replace file() with open() This simple modification provides much better user experience while using scikit under python3since file() got kind of deprecated and removed :3 ,,1702,0.7737955346650999,0.041033434650455926,36430,441.1199560801537,34.50452923414768,110.21136426022508,2433,42,1311,143,travis,rurkowce,larsmans,false,larsmans,0,0,0,0,138,false,false,false,false,0,0,0,0,2,0,66
3299319,scikit-learn/scikit-learn,python,2977,1395335663,1395434294,1395434294,1643,1643,commits_in_master,false,false,false,90,32,28,8,8,2,18,0,4,0,0,8,8,8,0,0,0,0,8,8,8,0,0,1294,0,1385,0,186.7577511712932,4.418103666273507,76,peter.prettenhofer@gmail.com,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.pxd|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_utils.c,61,0.028875379939209727,0,0,false,Some cleanup in _treepyx This pr clean up the _tree module: - Avoid storing tree parameter in Tree instead pass those parameters directly to TreeBuilder - Avoid storing the random_state in Tree - Split CONSTANT_FEATURE_THRESHOLD into two constants MIN_IMPURITY_SPLIT  and CONSTANT_FEATURE_THRESHOLD - Stricter separation between the TreeBuilder splitter and the criterion - Add stopping criterion  MIN_IMPURITY_SPLIT  to BestFirstTreeBuilder as to be symmetric with DepthFirstTreeBuilder - Fix pep8 errorThis pr is ready for reviewShould we kept the CONSTANT_FEATURE_THRESHOLD feature It could lead to unexpected behaviour to users,,1701,0.7736625514403292,0.041033434650455926,36430,441.1199560801537,34.50452923414768,110.21136426022508,2433,42,1311,142,travis,arjoly,arjoly,true,arjoly,51,0.7843137254901961,22,24,821,true,true,false,false,9,148,26,104,71,2,11
3284319,scikit-learn/scikit-learn,python,2975,1395189913,,1395201778,197,,unknown,false,false,false,345,1,1,0,6,0,6,0,7,7,0,6,13,12,0,1,7,0,6,13,12,0,1,857,44,857,44,50.381637764396956,1.194086898466909,80,peter@datarobot016.(none),.gitattributes|benchmarks/bench_compiled_tree.py|sklearn/_isotonic.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/__init__.py|sklearn/tree/_compiled.c|sklearn/tree/_compiled.pxd|sklearn/tree/_compiled.pyx|sklearn/tree/_tree.c|sklearn/tree/code_gen.py|sklearn/tree/compiled.py|sklearn/tree/setup.py|sklearn/tree/tests/test_compiled.py,57,0.0,0,12,false,[RFC] Compiled evaluation of regression trees/ensembles (~15x-8x speedup) # SummaryIn some use cases predicting given a model is in the hot-path sospeeding up decision tree evaluation is very usefulAn effective way of speeding up evaluation of decision trees can be togenerate code representing the evaluation of the tree compile that tooptimized object code and dynamically load that file via dlopen/dlsymor equivalentSeehttps://coursescswashingtonedu/courses/cse501/10au/compile-machlearnpdffor a detailed discussion andhttp://tulloch/articles/decision-tree-evaluation/for some benchmarks with C++Its fairly trivial to implement this for regression trees due to thesimpler predict() method but in theory is possible do to it forarbitrary multi-class multi-output treesThis diff deals only with the simple case of single-output regressiontrees There are a few changes that need to be made to allow it to workon Linux (change the CXX compiler invocation flags) and Windows (usethe appropriate equivalents of dlopen etc)## TradeoffsSince its purely opt-in from a users perspective and fairly short interms of lines of code (~40 lines of Cython 300 lines of codegen andevaluation Python code) I wasnt sure if the additional complexity wasworth it## TestingThis is fairly easy to test on a given platform since we just need toverify that the predictions output by the compiled model exactly matchthose of the interpreted model## BenchmarksFor random forests we see 5x to 8x speedup in evaluationFor gradient boosted ensembles its between a 15x and 3x speedup inevaluation This is due to the fact that gradient boosted trees alreadyhave an optimised prediction implementationThere is a benchmark script attached that allows us to examine theperformance of evaluation across a range of ensemble configurations anddatasetsIn the graphs attached GB is Gradient Boosted RF is Random ForestD1 D3 is max-depth1 max-depth3 and B10 ismax_leaf_nodes10## Graphsbashfor dataset in friedman1 friedman2 friedman3 uniform hastie   do python /benchmarks/bench_compiled_treepy \    --iterations10 \    --num_examples1000 \    --num_features50 \    --dataset$dataset \    --max_estimators300 \    --num_estimator_values6 done[timings3907426606273805268](https://fcloudgithubcom/assets/1121581/2453407/c70a64bc-aedd-11e3-94c7-519411ae6276png)[timings-1162001441413946416](https://fcloudgithubcom/assets/1121581/2453409/c70ad4ec-aedd-11e3-972d-07a49a6bc610png)[timings5617004024503483042](https://fcloudgithubcom/assets/1121581/2453410/c70b48dc-aedd-11e3-9c68-ec3f9d4672b8png)[timings2681645894201472305](https://fcloudgithubcom/assets/1121581/2453411/c70b4de6-aedd-11e3-86bd-d534b0ad0618png)[timings2070620222460516071](https://fcloudgithubcom/assets/1121581/2453408/c70aa594-aedd-11e3-8b14-1a26eb1f3ebapng),,1700,0.7741176470588236,0.038748137108792845,36449,440.3412988010645,34.4865428406815,109.98930011797306,2433,42,1309,140,travis,ajtulloch,ajtulloch,true,,5,0.6,308,31,888,false,false,false,false,1,9,7,0,3,0,16
3278170,scikit-learn/scikit-learn,python,2973,1395144219,1395192214,1395192214,799,799,commits_in_master,false,false,false,5,2,2,0,2,0,2,0,3,2,0,1,3,0,0,2,2,0,1,3,0,0,2,0,0,0,0,9.494193681970637,0.22502002583624425,8,peter.prettenhofer@gmail.com,doc/testimonials/images/datarobot.jpg|doc/testimonials/testimonials.rst|doc/testimonials/images/datarobot.png|doc/testimonials/testimonials.rst,8,0.0,1,0,false,add datarobot testimonial cc @GaelVaroquaux,,1699,0.773984696880518,0.039097744360902256,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,42,1309,140,travis,pprett,ogrisel,false,ogrisel,44,0.8636363636363636,128,29,1687,true,true,true,true,4,77,5,23,6,0,151
3274754,scikit-learn/scikit-learn,python,2971,1395103018,1395140140,1395140140,618,618,commits_in_master,false,false,false,1,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.1395558451261785,0.09811080073817754,8,peter.prettenhofer@gmail.com,sklearn/datasets/base.py,8,0.012139605462822459,0,0,false,typo ,,1698,0.773851590106007,0.03945371775417299,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,42,1308,139,travis,jyu-rmn,arjoly,false,arjoly,1,1.0,1,0,153,true,false,false,false,0,0,3,0,1,0,618
3246635,scikit-learn/scikit-learn,python,2966,1394749162,1395227942,1395227942,7979,7979,commits_in_master,false,false,false,9,2,1,3,3,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,4,4,9,9.228327161803815,0.21871879573730846,30,peter@datarobot016.(none),sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,25,0.03787878787878788,0,1,false,[WIP] Added _LearntSelectorMixin in BaseGradientBoosting Fix for issue #2160,,1697,0.7737183264584561,0.03939393939393939,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,42,1304,138,travis,maheshakya,glouppe,false,glouppe,9,0.5555555555555556,2,0,784,true,false,false,false,2,56,15,6,3,0,7201
3245454,scikit-learn/scikit-learn,python,2964,1394742210,1394743070,1394743070,14,14,commits_in_master,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.306440201719865,0.10206610302673329,23,peter@datarobot016.(none),sklearn/ensemble/gradient_boosting.py,23,0.03490136570561457,0,0,false,staged_predict predicts classes not probabilites ,,1696,0.7735849056603774,0.03945371775417299,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,42,1304,138,travis,abhishekkrthakur,glouppe,false,glouppe,0,0,73,47,855,false,true,false,false,0,0,0,0,0,0,14
3238862,scikit-learn/scikit-learn,python,2962,1394675270,1395342396,1395342396,11118,11118,commits_in_master,false,true,false,223,5,4,4,33,0,37,0,7,0,0,7,7,6,0,0,0,0,7,7,6,0,0,1082,0,1110,0,66.33559812908906,1.5722071306782794,56,vlad@vene.ro,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|doc/whats_new.rst|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|doc/whats_new.rst|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|doc/whats_new.rst|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx,45,0.009104704097116844,0,10,false,[MRG] use threads instead of multiprocessing in SGD New take on the idea of running SGD workers in threads instead of processes Despite the locking needed to do shuffling in between iterations this actually speeds up SGD on 20 newsgroups (all categories fixed random states) Current master:Perceptron________________________________________________________________________________Training:Perceptron(alpha00001 class_weightNone eta010 fit_interceptTrue      n_iter50 n_jobs-1 penaltyNone random_state42 shuffleFalse            verbose0 warm_startFalse)train time: 10516stest time:  0138sf1-score:   0796dimensionality: 129792density: 0098179Passive-Aggressive________________________________________________________________________________Training:PassiveAggressiveClassifier(C10 fit_interceptTrue losshinge              n_iter50 n_jobs-1 random_state42 shuffleFalse                            verbose0 warm_startFalse)train time: 11422stest time:  0143sf1-score:   0854dimensionality: 129792density: 0456828Elastic-Net penalty________________________________________________________________________________Training:SGDClassifier(alpha00001 class_weightNone epsilon01 eta000       fit_interceptTrue l1_ratio015 learning_rateoptimal              losshinge n_iter50 n_jobs-1 penaltyelasticnet                     power_t05 random_state42 shuffleFalse verbose0                            warm_startFalse)train time: 29376stest time:  0140sf1-score:   0849dimensionality: 129792density: 0032481This PR:Perceptron________________________________________________________________________________Training:Perceptron(alpha00001 class_weightNone eta010 fit_interceptTrue      n_iter50 n_jobs-1 penaltyNone random_state42 shuffleFalse            verbose0 warm_startFalse)train time: 5702stest time:  0144sf1-score:   0796dimensionality: 129792density: 0098179Passive-Aggressive________________________________________________________________________________Training:PassiveAggressiveClassifier(C10 fit_interceptTrue losshinge              n_iter50 n_jobs-1 random_state42 shuffleFalse                            verbose0 warm_startFalse)train time: 7008stest time:  0141sf1-score:   0854dimensionality: 129792density: 0456828Elastic-Net penalty________________________________________________________________________________Training:SGDClassifier(alpha00001 class_weightNone epsilon01 eta000       fit_interceptTrue l1_ratio015 learning_rateoptimal              losshinge n_iter50 n_jobs-1 penaltyelasticnet                     power_t05 random_state42 shuffleFalse verbose0                            warm_startFalse)train time: 24980stest time:  0140sf1-score:   0849dimensionality: 129792density: 0032481,,1695,0.7734513274336283,0.03945371775417299,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,42,1303,141,travis,larsmans,larsmans,true,larsmans,103,0.7378640776699029,132,38,1333,true,true,false,false,32,189,47,68,88,5,0
3234220,scikit-learn/scikit-learn,python,2960,1394644674,1394645503,1394645503,13,13,commits_in_master,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.293243593405467,0.10175331566987446,4,peter.prettenhofer@gmail.com,doc/modules/linear_model.rst,4,0.0060882800608828,0,0,false,Fix typo in linear_model documentation ,,1694,0.7733175914994097,0.0395738203957382,36434,440.52258879069,34.50074106603722,110.03458308173684,2433,41,1303,138,travis,ethanwhite,VirgileFritsch,false,VirgileFritsch,0,0,74,27,1056,false,false,false,false,0,0,0,0,0,0,-1
3225343,scikit-learn/scikit-learn,python,2959,1394562492,1395246316,1395246316,11397,11397,commits_in_master,false,false,false,137,2,1,3,4,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,17,15,31,9.224447845307239,0.21863065456447334,0,,sklearn/metrics/cluster/tests/test_unsupervised.py|sklearn/metrics/cluster/unsupervised.py,0,0.0,0,1,false,Silhouette score crashed if wrong number of labels Silhouette score is only defined if 2  n_labels  n_samples -1 but before this fix silhouette_score crashed if n_labels  2 with ValueError: zero-size array to reduction operation minimum which has no identityand returned 0 if n_labels  n_samplesI thought raising a ValueError was the most appropriate solution what do you thinkI added test to it if necessary and changed such that all tests depends on sklearntestingThis fix makes it behave like silhouette {cluster} in Rhttps://statethzch/R-manual/R-devel/library/cluster/html/silhouettehtmlExample:dataset  load_iris()X  datasetdatay  nparange(Xshape[0]) # n_labels  n_samplesclustersilhouette_score(Xy)Out[39]: 00dataset  load_iris()X  datasetdatay  npzeros(Xshape[0]) # n_labels  1clustersilhouette_score(Xy)ValueError: zero-size array to reduction operation minimum which has no identity,,1693,0.7731836975782634,0.041033434650455926,36472,439.37815310375083,34.43737661767932,109.89252028953717,2432,40,1302,142,travis,Oscarlsson,ogrisel,false,ogrisel,0,0,16,19,728,false,true,true,false,0,1,0,0,0,0,2
3225254,scikit-learn/scikit-learn,python,2958,1394561174,1394640781,1394640781,1326,1326,commits_in_master,false,false,false,65,3,1,3,7,3,13,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,1,14,1,8.336757140397408,0.19759130314530218,8,peter.prettenhofer@gmail.com,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,8,0.0121580547112462,0,1,false,Implemented a check for ndim exceeding two in the utilscheck_arrays  PR for issue #1597The check has been added in utilscheck_arrays and the unit tests in utils/test/test_validationpy were expanded to include a raise assert when check_arrays is called with an array of dimension threePep8 returns no outputs for utils/validationpy and utils/test/test_validationpyThe make returns the following----------------------------------------------------------------------Ran 35 tests in 35188sOK (SKIP2),,1692,0.7730496453900709,0.041033434650455926,36472,439.37815310375083,34.43737661767932,109.89252028953717,2432,40,1302,141,travis,hamsal,agramfort,false,agramfort,0,0,3,7,437,false,false,false,false,1,2,0,0,0,0,21
3220100,scikit-learn/scikit-learn,python,2956,1394500341,,1394601906,1692,,unknown,false,false,false,355,2,1,0,5,0,5,0,6,3,0,1,5,4,0,0,3,0,2,5,4,0,0,525,68,542,68,19.244636794803625,0.456128346138608,3,joel.nothman@gmail.com,examples/compare_SGVB_RBM_MNIST.py|sklearn/neural_network/__init__.py|sklearn/neural_network/sgvb.py|sklearn/neural_network/tests/test_sgvb.py,3,0.0,0,0,false,Auto-encoder based on stochastic gradient Variational bayes ### Auto-encoder based on stochastic gradient Variational bayesThis is an auto-encoder based on Stochastic gradient Variational Bayes a technique recently published by [Kingma and Welling](http://arxivorg/abs/13126114) A month later a paper describing the same technique was published by [Rezende et al](http://arxivorg/abs/14014082) at Deepmind (the deep learning start-up recently acquired by Google) Its a great improvement on the Wake Sleep algorithm but for more info I refer to the papers which describe it more eloquently than I couldI developed this code together with Otto Fabius as part of a project for our MSc AI at the University of Amsterdam This project was under the supervision of PhD candidate Diederik Kingma who had the original idea and wrote the paper It was originally written with the Theano dependency for this pull request I rewrote the code to include the manually derived gradients and get rid of the Theano dependencyIt supports both continuous and discrete data (for more info refer to the paper by Kingma and Welling) and I think most requirements listed on the website are met I tried to follow the implementation of the RBM and some code will probably be familiar :smiley: Most important to do is to include tests for continuous data but I was unsure on what data I should base this Would Olivetti faces be a good idea Also the code is not tested on Python 3 this should be easy (change the prints)Otto wrote a demo which compares logistic regression results on the raw pixel data the data transformed by an RBM and the data transformed by this auto-encoder It relies on MNIST data provided on the deeplearning website which should probably be changed to data already incorporated in sci-kit learn It beats the RBM although the auto-encoder has to train much longerThis is my first pull request so if I did anything odd thats because my learning hasnt converged yet :wink:   I am happy to make corrections and incorporate suggestions where necessary :smiley:  I will now also send an email to the mailing-list for easier communication,,1691,0.7735068007096393,0.043740573152337855,36472,439.37815310375083,34.43737661767932,109.89252028953717,2432,39,1301,143,travis,y0ast,mblondel,false,,0,0,65,17,1209,false,false,false,false,0,0,0,0,0,0,725
3217846,scikit-learn/scikit-learn,python,2955,1394485008,1395666315,1395666315,19688,19688,commits_in_master,false,false,false,38,2,2,0,7,0,7,0,4,0,7,2,9,9,0,0,0,7,2,9,9,0,0,1526,0,1526,0,18.154215960466452,0.43028359147876855,0,,sklearn/svm/setup.py|sklearn/svm/src/blas/Makefile|sklearn/svm/src/blas/blas.h|sklearn/svm/src/blas/blasp.h|sklearn/svm/src/blas/daxpy.c|sklearn/svm/src/blas/ddot.c|sklearn/svm/src/blas/dnrm2.c|sklearn/svm/src/blas/dscal.c|sklearn/svm/src/liblinear/tron.cpp|sklearn/svm/setup.py|sklearn/svm/src/blas/Makefile|sklearn/svm/src/blas/blas.h|sklearn/svm/src/blas/blasp.h|sklearn/svm/src/blas/daxpy.c|sklearn/svm/src/blas/ddot.c|sklearn/svm/src/blas/dnrm2.c|sklearn/svm/src/blas/dscal.c|sklearn/svm/src/liblinear/tron.cpp,0,0.0,0,0,false,[MRG] use CBLAS instead of Fortran BLAS in Liblinear This change makes us diverge even further from upstream Liblinear but it also means shipping one less BLAS implementation Thats another 700 lines of code less to care about,,1690,0.7733727810650888,0.046686746987951805,36479,439.4034924202966,34.430768387291316,109.92625894350174,2432,39,1301,145,travis,larsmans,GaelVaroquaux,false,GaelVaroquaux,102,0.7352941176470589,132,38,1331,true,true,true,false,22,167,45,58,85,5,34
3216288,scikit-learn/scikit-learn,python,2954,1394474174,1395668304,1395668304,19902,19902,merged_in_comments,false,false,false,111,3,3,13,18,2,33,0,5,2,0,5,7,7,0,0,2,0,5,7,7,0,0,877,56,877,56,83.27989726181875,1.9284595754929408,11,olivier.grisel@ensta.org,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx|sklearn/metrics/setup.py|sklearn/metrics/tests/test_pairwise.py|sklearn/src/cblas/ATL_drefasum.c|sklearn/src/cblas/cblas_dasum.c|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx|sklearn/metrics/setup.py|sklearn/metrics/tests/test_pairwise.py|sklearn/src/cblas/ATL_drefasum.c|sklearn/src/cblas/cblas_dasum.c|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx|sklearn/metrics/setup.py|sklearn/metrics/tests/test_pairwise.py|sklearn/src/cblas/ATL_drefasum.c|sklearn/src/cblas/cblas_dasum.c,6,0.006069802731411229,0,1,false,[MRG] pairwise L1 distances for sparse matrices Simple O(n_features) temp space algorithm: densify row by row then subtract and compute L1 normAdded BLAS support code (cblas_dasum) to speed this up by a factor of two on x86-64 w/ GCC and ATLAS for pair of 93% sparse matrices of shape 1000*3000 Thats an order of magnitude faster than the dense versionAlso cleaned up chi2 kernel code while I was at it and added a nogil declThis replaces #2849 which does the same thing but in a more complicated way and builds CSR matrices as output I dont think thats necessary for the typical use case (k-medians algorithm pairwise argmin),,1689,0.7732386027235051,0.05007587253414264,36451,437.4365586678006,34.237743820471316,109.79122657814601,2432,37,1301,145,travis,larsmans,larsmans,true,larsmans,101,0.7326732673267327,132,38,1331,true,true,false,false,22,166,44,58,84,5,6
3212688,scikit-learn/scikit-learn,python,2952,1394420920,1396920391,1396920391,41657,41657,commits_in_master,false,false,false,109,1,0,0,6,0,6,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,7,0,0,0.0,0,,,0,0.0,0,0,false,[MRG] MAINT warn of future behaviour change proposed in #2610 Assuming my proposal in #2610 to make PRFs labels parameter more functional and deprecate pos_label goes through at some future point it will create a compatibility issue in the case where all of the following apply:* y_true and y_pred contain at most two labels* labels specifies exactly two labels* pos_label is not None (default)* average is not None (generally default)I expect this case is rare atm but since behaviour would change with #2610 or similar (unless labels changed its name) we can warn in this release (015) and change behaviour in 016 or 017,,1688,0.7731042654028436,0.0499219968798752,36451,437.4365586678006,34.237743820471316,109.79122657814601,2432,36,1300,153,travis,jnothman,jnothman,true,jnothman,71,0.647887323943662,22,1,1776,true,true,false,false,14,351,39,258,103,2,15
3195457,scikit-learn/scikit-learn,python,2951,1394354245,1394666304,1394666304,5200,5200,commits_in_master,false,false,false,23,5,3,19,13,0,32,0,8,0,0,2,2,2,0,0,0,0,2,2,2,0,0,113,0,175,0,13.112805695322688,0.30395808794281665,0,,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx,0,0.0,0,9,false,[WIP] Attempt to speed up cd_fast A naive attempt to study and try and speed up sparse coordinate descent Strictly Work in Progress,,1687,0.7729697688203913,0.05287713841368585,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1300,144,travis,Manoj-Kumar-S,agramfort,false,agramfort,11,0.5454545454545454,22,15,628,true,false,false,false,1,164,11,59,9,4,1
3209050,scikit-learn/scikit-learn,python,2950,1394352537,,1394359530,116,,unknown,false,false,true,1,75,75,0,3,0,3,0,2,3,1,39,43,24,1,8,3,1,39,43,24,1,8,1011,158,1011,158,491.5563732202633,11.394398635330344,115,vlad@vene.ro,doc/whats_new.rst|sklearn/__init__.py|doc/whats_new.rst|doc/whats_new.rst|.gitignore|Makefile|sklearn/svm/classes.py|doc/whats_new.rst|doc/themes/scikit-learn/layout.html|doc/modules/clustering.rst|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|doc/whats_new.rst|sklearn/cluster/dbscan_.py|doc/modules/clustering.rst|doc/whats_new.rst|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/Makefile|doc/README|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html|sklearn/linear_model/stochastic_gradient.py|sklearn/__init__.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|sklearn/__init__.py|sklearn/preprocessing/label.py|doc/datasets/covtype.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|setup.py|doc/themes/scikit-learn/static/nature.css_t|doc/testimonials/testimonials.rst|doc/themes/scikit-learn/static/nature.css_t|doc/testimonials/images/evernote.png|doc/testimonials/images/telecomparistech.jpg|doc/testimonials/testimonials.rst|doc/themes/scikit-learn/static/nature.css_t|doc/testimonials/images/evernote.png|doc/testimonials/testimonials.rst|doc/testimonials/images/aweber.png|doc/testimonials/testimonials.rst|doc/index.rst|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/testimonials/testimonials.rst|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t|doc/index.rst|doc/testimonials/testimonials.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_grid_search.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/_hierarchical.c|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/hierarchical.py|examples/randomized_search.py|sklearn/__init__.py|sklearn/tests/test_random_projection.py|sklearn/pls.py|doc/conf.py|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t|.mailmap|doc/whats_new.rst|sklearn/__init__.py|doc/themes/scikit-learn/layout.html|setup.py|setup.py|doc/whats_new.rst|sklearn/__init__.py|README.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/banner_example.png|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/whats_new.rst|doc/documentation.rst|examples/imputation.py,45,0.0031201248049922,0,3,false,014x ,,1686,0.7734282325029656,0.0530421216848674,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1300,140,travis,dhawaljoh,dhawaljoh,true,,0,0,5,13,276,false,false,false,false,0,0,0,0,0,0,29
3195244,scikit-learn/scikit-learn,python,2949,1394324137,1416508441,1416508441,369738,369738,commits_in_master,false,true,false,72,99,9,180,74,5,259,0,12,3,0,5,19,4,0,0,4,0,16,20,13,0,0,620,312,1991,951,88.29887252246795,2.046789762850626,10,vanderplas@astro.washington.edu,sklearn/linear_model/__init__.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/theilsen.py|doc/modules/linear_model.rst|examples/linear_model/plot_theilsen.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/theilsen.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/theilsen.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/theilsen.py|doc/modules/linear_model.rst|examples/linear_model/plot_theilsen.py|sklearn/linear_model/tests/test_theilsen.py|sklearn/linear_model/theilsen.py|doc/modules/linear_model.rst|sklearn/linear_model/theilsen.py|sklearn/linear_model/theilsen.py,9,0.0,0,63,false,Theilsen A multiple linear Theil-Sen regression for the Scikit-Learn toolbox The implementation is based on the algorithm from the paper Theil-Sen Estimators in a Multiple Linear Regression Model of Xin Dang Hanxiang Peng Xueqin Wang and Heping Zhang It is parallelized with the help of joblibOn a personal note I think that the popular Theil-Sen regression would be a nice addition to Scikit-LearnI am looking forward to your feedback Florian,,1685,0.7732937685459941,0.05255023183925812,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1299,248,travis,FlorianWilhelm,ogrisel,false,ogrisel,0,0,14,15,428,false,false,false,false,0,0,0,0,0,0,200
3194480,scikit-learn/scikit-learn,python,2948,1394233978,,1421967701,462228,,unknown,false,false,false,19,1,1,8,0,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,18,15,18,15,8.563525161140255,0.19850463578728472,14,peter.prettenhofer@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,10,0.014947683109118086,0,0,true,Ward clustering-Too few clusters bug I have raised a ValueError when too few clusters are formed in Ward Clustering,,1684,0.7737529691211401,0.05680119581464873,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,275,travis,kaushik94,amueller,false,,10,0.2,26,140,36,false,false,false,false,0,50,16,5,1,0,769
3200166,scikit-learn/scikit-learn,python,2947,1394216573,,1394225296,145,,unknown,false,false,false,17,4,4,0,0,1,1,1,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,16,24,16,16.762824259231643,0.3885664211561647,14,peter.prettenhofer@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,10,0.01488095238095238,0,0,true,Raises ValueError and Tests Added  Raises a ValueError when unable to create clusters and also added tests,,1683,0.774212715389186,0.06101190476190476,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,142,travis,kaushik94,kaushik94,true,,9,0.2222222222222222,26,140,36,false,false,false,false,0,48,12,5,1,0,-1
3192514,scikit-learn/scikit-learn,python,2946,1394209635,,1394232849,386,,unknown,false,false,false,20,5,3,2,0,1,3,1,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,24,0,27,0,12.757705014929442,0.29572666850094254,7,gael.varoquaux@normalesup.org,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py,7,0.010416666666666666,0,0,true,with pep8 compliance Raises ValueError when heap exhausts in Ward sorry for another PR having hard time with git :),,1682,0.7746730083234245,0.0625,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,142,travis,kaushik94,kaushik94,true,,8,0.25,26,140,36,false,false,false,false,0,48,11,5,1,0,4
3192241,scikit-learn/scikit-learn,python,2945,1394186947,1394279853,1394279853,1548,1548,github,false,false,false,17,3,1,2,2,0,4,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.692843131562067,0.10878147132797045,0,,doc/datasets/olivetti_faces.rst,0,0.0,0,0,true,[MRG] Added reference to the function fetch_olivetti_faces in the narrative documentation of olivetti face dataset Fixed #2902,,1681,0.7745389649018442,0.0625,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,143,travis,maheshakya,maheshakya,true,maheshakya,8,0.5,2,0,778,true,false,false,false,1,43,12,5,2,0,249
3192247,scikit-learn/scikit-learn,python,2944,1394175531,1394415556,1394415556,4000,4000,commit_sha_in_comments,false,false,false,184,10,1,2,15,1,18,0,6,1,0,2,5,3,0,0,1,0,4,5,3,0,0,103,0,553,0,8.769654523172486,0.20328314740645342,2,larsmans@gmail.com,benchmarks/bench_isotonic.py|sklearn/_isotonic.c|sklearn/_isotonic.pyx,2,0.002976190476190476,0,8,true,[ENH] Improve performance of isotonic regression This supercedes #2940PAVA runs in linear time but the existing algorithm was scalingapproximately quadratically due to the array pop in the inner loopbeing O(N)I implemented the O(N) version of PAVA using the decreasing subsequencestrick and the performance is significantly faster in benchmarkingThe benchmarks in this diff (adapted from one of the existing unittests) show significant performance improvements| Problem Size | Relative Speedup of PAVA || ------------ | ------------------------ || 10 | 2x || 100 | 3x || 1000 | 14x || 10000 | 57x || 100000 | 561x || 1000000 | 4645x |[benchmarks47115](https://fcloudgithubcom/assets/1121581/2366340/1df4e5ec-a6fd-11e3-844f-ec5550922b7bpng)On correctness - unit tests cover the isotonic regression code fairlywell and all pass before and after the change Its a fairly well knownalgorithm with a bunch of implementations so I think this is correctIn coding up this algorithm I made some mistakes and the unit testscaught the failures which makes me more confident in the correctnessnow Still the performance improvements seem suspiciously large,,1680,0.7744047619047619,0.0625,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,146,travis,ajtulloch,ajtulloch,true,ajtulloch,4,0.5,307,30,877,false,false,false,false,1,3,4,0,3,0,8
3197011,scikit-learn/scikit-learn,python,2943,1394174875,1394211145,1394211145,604,604,github,false,false,false,4,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,20,0,20,4.483867454097983,0.10393735422688688,2,peter.prettenhofer@gmail.com,sklearn/cluster/spectral.py,2,0.002976190476190476,0,1,true,DOC: fixed docstring formatting ,,1679,0.7742703990470519,0.0625,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,36,1298,146,travis,jyu-rmn,jyu-rmn,true,jyu-rmn,0,0,1,0,143,true,false,false,false,0,0,0,0,1,0,8
3192429,scikit-learn/scikit-learn,python,2942,1394163813,,1394232894,1151,,unknown,false,false,false,48,1,1,3,0,1,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.188256132798874,0.09708499765527083,7,gael.varoquaux@normalesup.org,sklearn/cluster/hierarchical.py,7,0.010432190760059613,0,0,false,raises an error when unable to create clusters in Ward clustering We can use this as a meaningful error for now but in future we have to make an enhancement to the Ward algorithm such that it decides the optimal and threshold value of K to be chosen,,1678,0.7747318235995232,0.06259314456035768,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1297,143,travis,kaushik94,kaushik94,true,,7,0.2857142857142857,26,140,35,false,false,false,false,0,48,10,4,1,0,728
3192460,scikit-learn/scikit-learn,python,2941,1394162210,1394517841,1394517841,5927,5927,commits_in_master,false,false,false,43,5,1,1,8,0,9,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,14,14,4.227904270663355,0.09800405304480755,0,,sklearn/neighbors/classification.py,0,0.0,0,0,false,BUG: avoid NaNs throwing off class probabilities In the case where neigh_dist includes at least one zero and selfweights is distance some values of weights may be infiniteThis is ok except for when normalizing proba_k when it creates some pernicious NaN values,,1677,0.774597495527728,0.06259314456035768,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1297,147,travis,perimosocordiae,GaelVaroquaux,false,GaelVaroquaux,1,1.0,44,48,1762,true,true,false,false,0,2,3,0,0,0,766
3195303,scikit-learn/scikit-learn,python,2940,1394157488,,1394175689,303,,unknown,false,true,false,145,1,1,0,0,0,0,0,1,1,0,2,3,3,0,0,1,0,2,3,3,0,0,39,0,39,0,9.047902448836384,0.20973301540738945,2,larsmans@gmail.com,benchmarks/bench_isotonic.py|sklearn/_isotonic.c|sklearn/_isotonic.pyx,2,0.0029806259314456036,0,0,false,[ENH] Improve performance of isotonic regression by tracking region lower/upper bounds To keep track of which indices were being merged during PAVA we werestoring a list of indices and appending during pooling However due tothe way the pooling is performed we only ever append contiguous rangesand so we were effectively building [1k] [k+1  k+p] arraysSimply tracking the upper and lower bound allows us to avoid theseoperations and perform the update step in O(1) (the array pop is O(N)however which is the dominant term for larger problems)The somewhat crude benchmark in this diff (adapted from one of theexisting unit tests) shows around 20% improvement in speed for problemsof size 10000 and ~5% improvement for problems of size 100000Unit tests cover the isotonic regression code fairly well and all passbefore and after the change,,1676,0.7750596658711217,0.06259314456035768,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1297,143,travis,ajtulloch,ajtulloch,true,,3,0.6666666666666666,307,30,876,false,false,false,false,1,3,3,0,3,0,-1
3193752,scikit-learn/scikit-learn,python,2939,1394147310,,1394185176,631,,unknown,false,false,false,33,20,20,0,1,0,1,0,1,0,0,4,4,2,0,0,0,0,4,4,2,0,0,240,274,240,274,115.63935339307235,2.6805539100401035,7,peter.prettenhofer@gmail.com,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/datasets/olivetti_faces.rst|doc/datasets/olivetti_faces.rst,5,0.004477611940298508,0,0,false,[MRG] Added reference to the function fetch_olivetti_faces in the narrative documentation of  olivetti face dataset Fixed #2902This PR contains previous commits from the PR #2886 as it has not been merged yet,,1675,0.7755223880597015,0.0626865671641791,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1297,142,travis,maheshakya,maheshakya,true,,7,0.5714285714285714,2,0,777,true,false,false,false,1,43,10,5,2,0,179
3187850,scikit-learn/scikit-learn,python,2938,1394095966,1394605086,1394605086,8485,8485,commits_in_master,false,false,false,51,4,2,13,6,2,21,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,93,50,134,50,13.160801081916379,0.30507120425929674,13,peter.prettenhofer@gmail.com,sklearn/preprocessing/imputation.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/imputation.py,10,0.015037593984962405,0,1,false,[MRG] some clean-up in Imputer particularly in calculation of sparse median This:* tests some edge cases in calculating a sparse median in Imputer* reimplements that functionality for simpler computation* leaves behaviour with NaNs in the array undefined since npmamedian is buggy (see #2918)* silences some numpy warnings,,1674,0.7753882915173238,0.06466165413533835,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1297,145,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,70,0.6428571428571429,22,1,1773,true,true,false,false,18,361,40,262,108,2,1040
3186096,scikit-learn/scikit-learn,python,2937,1394075096,,1445259086,853066,,unknown,false,false,false,187,16,5,11,30,0,41,0,7,1,0,7,156,7,0,0,11,9,146,166,138,0,4,193,24,4467,1888,30.74850075592488,0.7127592079229255,68,peter.prettenhofer@gmail.com,sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/forest.py|examples/tree/compute_feature_contributions.py|examples/tree/compute_feature_contributions.py,44,0.02416918429003021,0,12,false,Option to return full decision paths when predicting with decision trees or random forest A very useful feature for decision trees is the option to access the full decision path for a prediction ie the path  from root to leaf for each decisionThis enables to interpret the model in the context of data in a very useful way In particular it allows to see exactly why the tree or forest arrived at a particular result breaking down the prediction into exact components (feature contributions) This is much needed in certain application areas for example in credit card fraud detection it is important to get an understanding why the model labels a transaction as fraudulentThe pull request implements a predict option for random forest and decision tree: when *return_paths* keyword argument is set to True paths are returned instead of predictions I have not added docstrings yet I assume the API might be expected to be different (another method instead of keyword argument to predict)In addition there is a change to store values at each node not just the leaves (useful when interpreting the tree),,1673,0.775851763299462,0.0649546827794562,36393,438.133707031572,34.29230896051438,109.9662022916495,2432,35,1296,382,travis,andosa,glouppe,false,,0,0,5,0,419,false,false,false,false,0,0,0,0,0,0,6
3179385,scikit-learn/scikit-learn,python,2936,1394024179,1398465906,1398465906,74028,74028,commits_in_master,false,false,false,12,1,1,0,1,1,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.069149026437344,0.09432397723873928,0,,examples/svm/plot_svm_regression.py,0,0.0,0,0,false,Update plot_svm_regressionpy library imports have been moved to top of the file,,1672,0.7757177033492823,0.07250755287009064,36393,438.10622921990495,34.29230896051438,109.9662022916495,2432,36,1296,162,travis,ugurthemaster,arjoly,false,arjoly,3,1.0,1,0,281,false,false,false,false,0,0,3,0,0,0,74028
3176601,scikit-learn/scikit-learn,python,2935,1393988826,,1393990699,31,,unknown,false,false,false,30,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.734021141216456,0.11586943932154226,0,,examples/imputation.py,0,0.0,0,0,false,fix numpy bug in imputation example fixes issue Imputation Training Example Err #2934 I filed Using numpy version 180 on Python 276 |Anaconda 180 (x86_64)| (default Jan 10 2014 11:23:15),,1671,0.7761819269898265,0.07951070336391437,33617,420.82874735996666,33.97090757652378,105.89880120177291,2432,37,1295,138,travis,kevinpCroat,jnothman,false,,0,0,1,1,1072,false,false,false,false,1,1,0,0,0,0,30
3171039,scikit-learn/scikit-learn,python,2932,1393950254,,1424830551,514671,,unknown,false,true,false,87,1,1,8,4,0,12,0,7,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,60,3,60,8.961538819084554,0.20935314860605422,12,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,11,0.016296296296296295,0,1,false,[Feature Selection] Fix SelectFDR thresholding bug (#2771) From https://githubcom/scikit-learn/scikit-learn/issues/2771 we werenot correctly scaling the alphaparameter (http://enwikipediaorg/wiki/False_discovery_rate#BenjaminiE28093Hochberg_procedure)with the number of features ( hypothesis)  Thus the alphaparameter was not invariant with respect the number of featuresThe correction is as suggested in the original issue and a test hasbeen added that verifies that for various numbers of features anappropriate false discovery rate is generated when using the selector  This test passes with the new FDR logic and fails with the old FDR logic,,1670,0.7766467065868263,0.08,36015,439.067055393586,34.541163404137166,110.37067888379842,2432,37,1295,297,travis,ajtulloch,amueller,false,,2,1.0,307,29,874,false,false,false,false,1,1,2,0,2,0,2569
3165145,scikit-learn/scikit-learn,python,2931,1393889210,1394021237,1394021237,2200,2200,commits_in_master,false,false,false,23,4,1,0,5,0,5,0,3,0,0,3,5,3,0,0,0,0,5,5,4,0,0,48,0,89,0,12.830018415847308,0.29972657473128744,6,peter.prettenhofer@gmail.com,sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/utils/arrayfuncs.pyx,4,0.0029717682020802376,1,3,false,MAINT: remove our solve_triangular Remove solve_triangularThis finishes the great work started by @larsman to remove our compatibility layers to scipy  09,,1669,0.7765128819652487,0.08172362555720654,36013,439.09143920251023,34.54308166495432,110.37680837475355,2432,37,1294,137,travis,GaelVaroquaux,agramfort,false,agramfort,41,0.7560975609756098,457,3,1470,true,true,false,true,16,267,60,96,96,10,20
3162707,scikit-learn/scikit-learn,python,2930,1393872545,1405442610,1405442610,192834,192834,commit_sha_in_comments,false,true,false,321,10,10,0,2,0,2,0,2,2,0,5,7,7,0,0,2,0,5,7,7,0,0,663,0,663,0,59.63994882003001,1.3932698299869397,4,walks@ethz.ch,examples/gaussian_process/plot_matern_kernel.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/gp_diabetes_dataset.py|examples/gaussian_process/gp_learning_curve.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_regression.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/gp_learning_curve.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|examples/gaussian_process/plot_gp_learning_curve.py,4,0.0,0,0,false,[MRG] Enhanced correlation models and noise estimation for Gaussian Process Support in Gaussian-Process regression for enhanced correlation models and learning the noise magnitude (the nugget) from training dataCorrelation models have been extended as follows: * Matern correlation models for nu15 and nu25 have been added (see https://enwikipediaorg/wiki/Mat%C3%A9rn_covariance_function) An example script showing the potential benefit of the Matern correlation model compared to squared-exponential and absolute-exponential was added under examples/gaussian_process/plot_matern_kernelpy (see attached image) * squared_exponential absolute_exponential and Matern correlation models support factor analysis distance This can be seen as an extension of learning dimension-specific length scales in which also correlations of feature dimensions can be taken into account See Rasmussen and Williams 2006 p107 for details An example script showing the potential benefit of this extension was added under examples/gaussian_process/plot_gp_learning_curvepy (see attached image) This feature required that correlation modes get passed the componentwise differences rather than the componentwise distances (their absolute value) Learning the noise (the nugget effect) by GaussianProcess is now supported by setting the parameter learn_nugget  to True This allows to learn a homoscedastic noise model ie it assumes that the noise has globally the same magnitude The script examples/gaussian_process/plot_gp_regressionpy was modified accordingly ie it learns the noise magnitude and does not rely on specifying it externalyFurthermore a typo in gp_diabetes_datasetpy was fixed and a not yet merged bugfix (#2867 and #2798) is includedTo be discussed: * The factor analysis distance has hyperparameters that can can take on arbitrary real values (not just positive ones) Since sklearn enforces the hyperparameters theta to be positive this is internally handled by taking the log of the corresponding components of theta Are there better ideas * Learning the noise (the nugget) is internally handled by appending it to the vector theta in _arg_max_reduced_likelihood_function() This was the way which required the least changes in the current implementation but is not necessarily the best way Are their any opinions on that[plot_gp_learning_curve](https://fcloudgithubcom/assets/1116263/2310367/a7499dde-a2e2-11e3-80f5-6e98a23f0ee8png)[plot_matern_kernel](https://fcloudgithubcom/assets/1116263/2310372/adf004fc-a2e2-11e3-8d25-85e69e09fd11png),,1668,0.776378896882494,0.08172362555720654,36013,439.09143920251023,34.54308166495432,110.37680837475355,2432,37,1294,198,travis,jmetzen,jmetzen,true,jmetzen,5,0.6,10,2,875,false,true,false,false,0,4,4,0,1,0,11
3161958,scikit-learn/scikit-learn,python,2929,1393866279,1394015758,1394015758,2491,2491,commits_in_master,false,false,false,16,1,1,0,3,0,3,0,3,0,0,6,6,6,0,0,0,0,6,6,6,0,0,0,7,0,7,25.653093666394078,0.5992909477348867,34,peter.prettenhofer@gmail.com,sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_multiclass.py|sklearn/tests/test_naive_bayes.py|sklearn/tests/test_pipeline.py,20,0.008902077151335312,0,0,false,[tests] Use Python 3 zip/Python 2 zip consistently in tests Continuing issue https://githubcom/scikit-learn/scikit-learn/issues/2927 and pull https://githubcom/scikit-learn/scikit-learn/pull/2925,,1667,0.7762447510497901,0.0830860534124629,36013,439.09143920251023,34.54308166495432,110.37680837475355,2432,37,1294,137,travis,ajtulloch,GaelVaroquaux,false,GaelVaroquaux,1,1.0,307,29,873,false,false,false,false,0,0,1,0,0,0,2079
3161905,scikit-learn/scikit-learn,python,2928,1393865764,1394207519,1394207519,5695,5695,commits_in_master,false,false,false,35,2,1,0,9,0,9,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,9,4.403027966305847,0.10286068562121885,3,peter.prettenhofer@gmail.com,sklearn/cluster/tests/test_spectral.py,3,0.004451038575667656,1,1,false,[MRG] make lobpcg test pass with reference and ATLAS impl of LAPACK This is a fix for #2924I would appreciate a review by @GaelVarroquaux or anybody else who had prior experience with SciPy eigensolvers,,1666,0.776110444177671,0.0830860534124629,36013,439.09143920251023,34.54308166495432,110.37680837475355,2432,37,1294,143,travis,ogrisel,ogrisel,true,ogrisel,55,0.8181818181818182,881,123,1741,true,true,false,false,25,284,25,96,36,2,1
3161744,scikit-learn/scikit-learn,python,2926,1393864181,1394305836,1394305836,7360,7360,commit_sha_in_comments,false,false,false,25,1,1,1,4,0,5,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.655893963225964,0.10876798756483243,3,sergio.pasra@gmail.com,doc/install.rst,3,0.004457652303120356,0,2,false,ENH: update installation instructions for Ubuntu / Debian Update the install instructions to better reflect the current behavior of Debian & Ubuntu packages  / config,,1665,0.775975975975976,0.08320950965824665,36012,439.10363212262575,34.5440408752638,110.3798733755415,2432,37,1294,141,travis,ogrisel,larsmans,false,larsmans,54,0.8148148148148148,881,123,1741,true,true,true,true,24,282,24,96,36,2,7
3161686,scikit-learn/scikit-learn,python,2925,1393863587,1393865377,1393865377,29,29,commits_in_master,false,false,false,28,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.5371019338253875,0.10599284490078204,24,mathieu@mblondel.org,sklearn/cross_validation.py,24,0.03571428571428571,0,0,false,[Cross Validation] Use itertoolsizip consistently across Python 2/3 when computing folds in StratifiedKFold This isnt an especially hot code path but it seemed to be preferablefor consistency,,1664,0.7758413461538461,0.08333333333333333,36012,439.10363212262575,34.5440408752638,110.3798733755415,2432,37,1294,133,travis,ajtulloch,jnothman,false,jnothman,0,0,307,29,873,false,false,false,false,0,0,0,0,0,0,8
3160768,scikit-learn/scikit-learn,python,2923,1393853880,1393856719,1393856719,47,47,commit_sha_in_comments,false,false,false,153,57,4,0,0,0,0,0,8,1,0,3,8,3,0,0,1,0,7,8,5,0,0,828,2084,2129,5050,75.15755340328124,1.7557822189548395,31,peter.prettenhofer@gmail.com,benchmarks/bench_vectorizer_parallel.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|benchmarks/bench_vectorizer_parallel.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|benchmarks/bench_vectorizer_parallel.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|benchmarks/bench_vectorizer_parallel.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,18,0.017937219730941704,0,4,false,ENH Parallelize CountVectorizer Add parameter n_jobs to CountVectorizer (also TfidfVectrizer which uses CountVectorizer) similar to one in KMeansInclude timing benchmark on Brown corpus (using nltk)Include unit tests for the multiprocessing version of CountVectorizerFix a few typosBenchmark result on Brown corpus (500 documents amounting to 1161192 words) shows that the one-core version finishes in 38s while the multi-core version finishes in 12s on a four-core (266 GHz Intel Core i5) iMac running OSX 1085 a three-fold speedup On another personal test on different machine the one-core version finishes in 155s while the multi-core version finishes in 38s a four-fold speedup So the parallelization clearly improves the speed even for Brown corpus which is not very largeThe parallelization is very useful for very large data (like the one Im building this one for around 115k documents) which can mean the difference between a few days and a couple of hours,,1663,0.7757065544197234,0.08370702541106129,36012,439.10363212262575,34.5440408752638,110.3798733755415,2432,37,1294,132,travis,justhalf,justhalf,true,justhalf,0,0,1,1,1161,true,false,false,false,0,0,0,0,3,0,-1
3159152,scikit-learn/scikit-learn,python,2920,1393829292,1393848625,1393848625,322,322,commits_in_master,false,false,false,10,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.473135679450867,0.10449850856234665,2,ameasure@gmail.com,doc/modules/naive_bayes.rst,2,0.0030075187969924814,0,0,false,Fix a minor typo: They requires should be They require ,,1662,0.7755716004813478,0.08421052631578947,36012,439.10363212262575,34.5440408752638,110.3798733755415,2432,36,1294,132,travis,yoni,agramfort,false,agramfort,0,0,45,76,1643,false,false,false,false,0,0,0,0,0,0,8
3158771,scikit-learn/scikit-learn,python,2919,1393823387,1394569067,1394569067,12428,12428,commit_sha_in_comments,false,false,false,19,3,3,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,12.798360918939078,0.29898704709890256,3,larsmans@gmail.com,sklearn/hmm.py|sklearn/hmm.py|sklearn/hmm.py,3,0.004511278195488722,0,0,false,Fix Error: ValueError: Rows of transmat must sum to 10 inf in stat[trans] is the cause of this error,,1661,0.7754364840457556,0.08421052631578947,36012,439.10363212262575,34.5440408752638,110.3798733755415,2432,36,1294,145,travis,csytracy,larsmans,false,larsmans,1,0.0,2,3,424,false,false,false,false,0,0,1,0,1,0,10620
3156847,scikit-learn/scikit-learn,python,2917,1393798639,1393799141,1393799141,8,8,commits_in_master,false,false,false,6,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.910766993751588,0.1147168659974963,0,,doc/modules/cross_decomposition.rst,0,0.0,0,0,false,DOC: Update dead link in cross_decompositionrst ,,1660,0.7753012048192771,0.08472012102874432,36146,437.5587893542854,34.443645216621476,109.99834006529076,2432,36,1293,132,travis,ariddell,GaelVaroquaux,false,GaelVaroquaux,0,0,54,164,2033,false,true,false,false,0,0,0,0,0,0,8
3155821,scikit-learn/scikit-learn,python,2916,1393783251,1393802105,1393802105,314,314,commit_sha_in_comments,false,false,false,82,7,2,0,13,0,13,0,4,0,0,38,50,33,0,0,0,0,50,50,45,0,0,1290,124,1692,231,351.2145258109044,8.204467804130926,130,sergio.pasra@gmail.com,README.rst|benchmarks/bench_sparsify.py|doc/developers/index.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/computational_performance.rst|examples/applications/plot_model_complexity_influence.py|examples/applications/plot_prediction_latency.py|examples/ensemble/plot_gradient_boosting_regularization.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|sklearn/cross_validation.py|sklearn/datasets/twenty_newsgroups.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/text.py|sklearn/lda.py|sklearn/linear_model/tests/test_omp.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/metrics.py|sklearn/neighbors/base.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_cross_validation.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/utils/class_weight.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_class_weight.py|sklearn/utils/tests/test_fixes.py|README.rst|benchmarks/bench_sparsify.py|doc/developers/index.rst|doc/developers/utilities.rst|doc/install.rst|doc/modules/computational_performance.rst|examples/applications/plot_model_complexity_influence.py|examples/applications/plot_prediction_latency.py|examples/ensemble/plot_gradient_boosting_regularization.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|sklearn/cross_validation.py|sklearn/datasets/twenty_newsgroups.py|sklearn/dummy.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/text.py|sklearn/lda.py|sklearn/linear_model/tests/test_omp.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/metrics.py|sklearn/neighbors/base.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_cross_validation.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/utils/class_weight.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_class_weight.py|sklearn/utils/tests/test_fixes.py,23,0.0045045045045045045,3,1,false,[WIP] drop old NumPy and SciPy support As discussed on the ML this PR drops support for NumPy  161 SciPy  09All tests except one pass in a virtualenv with NumPy 161 SciPy 0131 The single failure is a ValueError thrown by SciPy unrelated to our codeIve not yet managed to get SciPy 09 to install on my machine so Id appreciate if others can try thatTheres also a bit more cleanup to doPing @GaelVaroquaux @ogrisel @amueller,,1659,0.7751657625075347,0.09009009009009009,36146,437.5587893542854,34.443645216621476,109.99834006529076,2432,36,1293,131,travis,larsmans,larsmans,true,larsmans,100,0.73,132,38,1323,true,true,false,false,28,170,45,62,83,5,23
3139502,scikit-learn/scikit-learn,python,2911,1393564208,,1394306242,12367,,unknown,false,false,false,32,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.050077102164427,0.09461201235051182,2,peter.prettenhofer@gmail.com,sklearn/mixture/gmm.py,2,0.0029985007496251873,0,0,true,Fixed GMM score to return single value  In the score function I have applied npsum() to the log probabilities for each data point in X so that it returns a single value  ,,1658,0.7756332931242461,0.10644677661169415,36137,435.9797437529402,34.34153360821319,109.49995849129701,2432,37,1291,142,travis,kaushik94,larsmans,false,,6,0.3333333333333333,26,139,29,false,false,false,false,0,42,9,4,1,0,3
3137153,scikit-learn/scikit-learn,python,2910,1393545990,1393608966,1393608966,1049,1049,commits_in_master,false,false,false,126,2,2,0,7,2,9,0,6,0,0,7,7,7,0,0,0,0,7,7,7,0,0,65,72,65,72,31.16639405721497,0.7280641125718322,36,vlad@vene.ro,sklearn/ensemble/bagging.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/svm/base.py|sklearn/tests/test_common.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_multiclass.py,19,0.0076103500761035,5,1,false,[MRG] make hasattr(clf predict_proba) work with probabilities disabled Heres a trick with property that @mblondel suggested earlier to make hasattr(clf predict_proba) work with SVC(probabilityFalse) and SGDClassifier(losshinge): models that define the method but where it doesnt actually workThis is a bit of a hack so I dont suggest using it in new code (see comment in SVC) but it simplifies other code such as bagging and the smoke tests ie it localizes the problem that these exceptional estimators have instead of spreading it across the codebase There might be more places where the code can be simplifiedThe documentation comes out almost right despite the property:[predict_proba](https://fcloudgithubcom/assets/335383/2287207/46037cde-9fea-11e3-9128-54e5df18e5ffpng)(The returns X is there in master too Ill change it in a minute)Ping @jnothman @GaelVaroquaux @agramfort @ogrisel,,1657,0.7754978877489439,0.1080669710806697,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1290,135,travis,larsmans,larsmans,true,larsmans,99,0.7272727272727273,132,38,1320,true,true,false,false,24,161,42,69,75,5,3
3134608,scikit-learn/scikit-learn,python,2909,1393529457,1393536085,1393536085,110,110,commits_in_master,false,false,false,2,1,1,0,2,0,2,0,2,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.665204655685528,0.10918243374435789,10,peter.prettenhofer@gmail.com,doc/testimonials/images/peerindex.png|doc/testimonials/testimonials.rst,10,0.015267175572519083,0,0,false,PeerIndex testimonial ,,1656,0.7753623188405797,0.1083969465648855,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1290,134,travis,ogrisel,NelleV,false,NelleV,53,0.8113207547169812,881,123,1737,true,true,false,false,23,300,25,109,36,2,7
3131458,scikit-learn/scikit-learn,python,2907,1393501253,,1393501279,0,,unknown,false,false,false,17,14,14,0,0,0,0,0,0,0,0,4,4,2,0,0,0,0,4,4,2,0,0,220,167,220,167,88.68872090175346,2.0756181859686884,16,peter.prettenhofer@gmail.com,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|doc/datasets/olivetti_faces.rst,15,0.0015360983102918587,0,0,false,[MRG] Added reference to the function fetch_olivetti_faces in the narrative documentation of  olivetti face dataset Fixed #2902,,1655,0.7758308157099698,0.11213517665130568,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1290,134,travis,maheshakya,maheshakya,true,,6,0.6666666666666666,2,0,770,true,false,false,false,0,25,8,4,2,0,-1
3131450,scikit-learn/scikit-learn,python,2906,1393501138,1393501196,1393501196,0,0,commit_sha_in_comments,false,false,false,17,14,14,0,0,0,0,0,1,0,0,4,4,2,0,0,0,0,4,4,2,0,0,220,167,220,167,88.68872090175346,2.0756181859686884,16,peter.prettenhofer@gmail.com,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|doc/modules/model_evaluation.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|sklearn/tests/test_dummy.py|doc/datasets/olivetti_faces.rst,15,0.0015455950540958269,0,0,false,[MRG] Added reference to the function fetch_olivetti_faces in the narrative documentation of  olivetti face dataset Fixed #2902,,1654,0.7756952841596131,0.11282843894899536,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1290,134,travis,maheshakya,maheshakya,true,maheshakya,5,0.6,2,0,770,true,false,false,false,0,25,6,4,2,0,-1
3128218,scikit-learn/scikit-learn,python,2905,1393465237,1393705378,1393705378,4002,4002,commit_sha_in_comments,false,false,false,41,13,2,12,21,4,37,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,51,49,94,9.078934730031925,0.21247799994491093,8,peter.prettenhofer@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,8,0.012364760432766615,0,5,false,Sparse paramater to OneHotEncoder i have added a few lines of code in datapy and test_datapy to add a sparse parameteri havent made much commit messages since all the tests are same as before but have been tested with sparseFalse,,1653,0.7755595886267392,0.11282843894899536,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1289,134,travis,kaushik94,larsmans,false,larsmans,5,0.2,26,139,27,false,false,false,false,0,30,8,2,1,0,24
3123592,scikit-learn/scikit-learn,python,2900,1393430903,,1405708801,204631,,unknown,false,false,false,21,1,1,5,0,1,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,4.1948070759449045,0.09817277512781494,0,,sklearn/datasets/olivetti_faces.py,0,0.0,0,0,false,Update olivetti_facespy i have written a little documentation for the return object in olivetti_facespyand changed the ordering of the parameters,,1652,0.7760290556900726,0.11574074074074074,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1289,200,travis,kaushik94,arjoly,false,,4,0.25,26,139,27,false,false,false,false,0,27,7,1,0,0,26
3122454,scikit-learn/scikit-learn,python,2897,1393419341,1393421084,1393421084,29,29,commits_in_master,false,false,false,12,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.674383206806155,0.10939649025924414,0,,examples/ensemble/plot_forest_importances.py,0,0.0,0,0,false,Update plot_forest_importancespy import statement has moved to the top of the file,,1651,0.775893397940642,0.11591962905718702,36134,436.01594066530134,34.34438478994852,109.50904964853048,2432,37,1289,132,travis,ugurthemaster,arjoly,false,arjoly,2,1.0,1,0,274,false,false,false,false,0,0,2,0,0,0,9
3118862,scikit-learn/scikit-learn,python,2895,1393378713,1393415882,1393415882,619,619,commits_in_master,false,false,false,21,2,1,2,3,1,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,9,0,4.379512383566443,0.10249549763003635,0,,sklearn/metrics/cluster/supervised.py,0,0.0,0,1,false, supervisedpy with updated documentation i have added a few lines of documentation for the adjusted_mutual_infosorry for the mess :),,1650,0.7757575757575758,0.11782945736434108,36133,436.02800763844687,34.34533528907093,109.5120803697451,2432,37,1288,135,travis,kaushik94,agramfort,false,agramfort,3,0.0,26,139,26,false,false,true,false,0,23,3,0,0,0,1
3118308,scikit-learn/scikit-learn,python,2894,1393375060,,1393379781,78,,unknown,false,false,false,34,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,56,24,56,9.062550274993658,0.21209452534445733,8,peter.prettenhofer@gmail.com,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,8,0.012403100775193798,0,0,false,sparse parameter for one hot encoder(a cleaner PR) i have previously submitted a PR for one hot encoder but it was all messed upi have written a fresh PR(with tests added to test_datapy),,1649,0.7762280169799879,0.11782945736434108,36133,436.02800763844687,34.34533528907093,109.5120803697451,2432,37,1288,135,travis,kaushik94,kaushik94,true,,2,0.0,26,139,26,false,false,false,false,0,23,2,0,0,0,-1
3116583,scikit-learn/scikit-learn,python,2893,1393362668,,1393379139,274,,unknown,false,false,false,28,2,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,12,0,4.4273129846620565,0.10361419440758106,0,,sklearn/metrics/cluster/supervised.py,0,0.0,0,2,false, supervisedpy documentation i added a little documentation regarding AMII tried to look for particular type of cases when specifically negative values occur But couldnt find one,,1648,0.7766990291262136,0.11937984496124031,36133,436.02800763844687,34.34533528907093,109.5120803697451,2432,36,1288,135,travis,kaushik94,kaushik94,true,,1,0.0,26,139,26,false,false,false,false,0,21,1,0,0,0,16
3103156,scikit-learn/scikit-learn,python,2886,1393233118,1394480266,1394480266,20785,20785,commits_in_master,false,false,false,11,34,2,32,55,1,88,0,7,0,0,2,3,2,0,0,0,0,3,3,2,0,0,141,160,405,530,18.28669778475229,0.4279709760539181,1,joel.nothman@gmail.com,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py,1,0.0016025641025641025,0,6,false,[WIP] ENH Implemented median and constant strategies in DummyRegressor Issue #2718,,1647,0.7765634486945963,0.125,36133,436.02800763844687,34.34533528907093,109.5120803697451,2432,36,1287,147,travis,maheshakya,arjoly,false,arjoly,4,0.5,2,0,767,true,false,false,false,0,7,4,0,2,0,8
3100889,scikit-learn/scikit-learn,python,2885,1393200933,,1393379801,2981,,unknown,false,false,false,27,13,1,2,13,2,17,0,3,0,0,1,3,1,0,0,0,0,3,3,3,0,0,11,0,73,53,4.088501341198231,0.09568619660281927,8,peter.prettenhofer@gmail.com,sklearn/preprocessing/data.py,8,0.012841091492776886,0,0,false,Added sparse parameter to OneHotEncoder  Added a few lines of code in _transform method to enable OneHotEncoder to return dense matricessomeone please review the pull request ,,1646,0.7770352369380316,0.12520064205457465,36141,436.4295398577793,34.3654021748153,109.65385573171744,2432,36,1286,136,travis,kaushik94,kaushik94,true,,0,0,26,139,24,false,false,false,false,0,10,0,0,0,0,235
3100760,scikit-learn/scikit-learn,python,2884,1393199383,1393245424,1393245424,767,767,commits_in_master,false,false,false,40,1,1,0,6,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,8,0,8,4.424614301187365,0.10355249480998828,2,joel.nothman@gmail.com,sklearn/cluster/bicluster/spectral.py,2,0.0032102728731942215,0,0,false,Sphinx formatting issue for Coclustering and Biclustering Im not entirely sure why this solved the problem but sphinx was failing if the second line was longer than the first line for the Attributes comment section This should resolve Issue #2483,,1645,0.7768996960486322,0.12520064205457465,36141,436.4295398577793,34.3654021748153,109.65385573171744,2432,36,1286,135,travis,dsullivan7,agramfort,false,agramfort,4,0.75,7,19,198,false,true,true,false,0,3,0,0,0,0,8
3094669,scikit-learn/scikit-learn,python,2883,1393100891,1393189448,1393189448,1475,1475,commit_sha_in_comments,false,false,false,46,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.098804431066025,0.0959262640020844,1,jaquesgrobler@gmail.com,sklearn/feature_extraction/dict_vectorizer.py,1,0.0016286644951140066,0,0,false,FIX ValueError was not being raised len(indptr) is always greater that 0 because its initialized with non-empty listBecause of that this branch is never taken and the ValueError never raisedAditionaly (as VirgileFritsch) suggested the next if was subsumed in the else of the first,,1644,0.7767639902676399,0.13029315960912052,36141,436.3465316399657,34.3654021748153,109.65385573171744,2432,36,1285,134,travis,rafacarrascosa,larsmans,false,larsmans,1,0.0,29,3,631,false,false,false,false,0,0,1,0,0,0,265
3094009,scikit-learn/scikit-learn,python,2882,1393090639,1393206079,1393206079,1924,1924,commits_in_master,false,false,false,102,3,2,1,11,0,12,0,7,0,0,5,5,5,0,0,0,0,5,5,5,0,0,142,90,153,99,46.42834006471237,1.0865844616713818,21,peter.prettenhofer@gmail.com,sklearn/neural_network/rbm.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_fixes.py|sklearn/neural_network/rbm.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_fixes.py,13,0.011400651465798045,0,10,false,[MRG] speed up RBM training with scipyspecialexpit SciPy 010 already has an implementation of the logistic function called scipyspecialexpit Moved our logistic_sigmoid to fixesexpit keeping its log-of-logistic functionality as log_logistic (should that be log_expit I dont like the name expit at all)Using this makes RBM training 12% faster as measured by observing the time per iteration as reported by the plot_rbm_logistic_classificationpy example disregarding the first iteration as an outlier (its consistently faster than the rest not sure why) The speedup is in the expit function not the inplace operations those are there to make benchmarking easier but they certainly wont hurt,,1643,0.7766281192939745,0.13029315960912052,36141,436.3465316399657,34.3654021748153,109.65385573171744,2432,36,1285,135,travis,larsmans,larsmans,true,larsmans,98,0.7244897959183674,132,38,1315,true,true,false,false,27,160,39,64,75,5,1353
3089051,scikit-learn/scikit-learn,python,2881,1393018766,,1393100970,1370,,unknown,false,true,false,31,1,1,0,1,1,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.8817140163551294,0.09084559406915392,1,jaquesgrobler@gmail.com,sklearn/feature_extraction/dict_vectorizer.py,1,0.0016286644951140066,0,2,true,FIX ValueError was not being raised len(indptr) is always greater that 0 because its initialized with non-empty listBecause of that this branch is never taken and the ValueError never raised,,1642,0.7771010962241169,0.13192182410423453,36141,436.3465316399657,34.3654021748153,109.65385573171744,2432,36,1284,134,travis,rafacarrascosa,rafacarrascosa,true,,0,0,29,3,630,false,false,false,false,0,0,0,0,0,0,1370
3072025,scikit-learn/scikit-learn,python,2877,1392851192,1392894945,1392894945,729,729,commits_in_master,false,false,false,33,1,1,0,1,7,8,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.761183052365083,0.11142814511242712,17,peter.prettenhofer@gmail.com,sklearn/feature_extraction/text.py,17,0.027960526315789474,0,0,false,Fix citation in TfidfTransformer Corrected names order for tf-idf citation Should beCD Manning P Raghavan and H Schuetze instead ofCD Manning H Schuetze and P RaghavanCorrected pages numbers See: http://nlpstanfordedu/IR-book/pdf/irbookprintpdf,,1641,0.7769652650822669,0.13486842105263158,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,36,1282,132,travis,pmandera,GaelVaroquaux,false,GaelVaroquaux,0,0,0,2,445,false,false,false,false,0,0,0,0,0,0,7
3071311,scikit-learn/scikit-learn,python,2876,1392845475,1393704490,1393704490,14316,14316,commits_in_master,false,false,false,62,8,1,36,36,0,72,0,6,0,0,2,7,2,0,0,0,0,7,7,6,0,0,7,12,116,79,9.137347828367751,0.21384553136563825,20,peter.prettenhofer@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,11,0.018092105263157895,0,40,false,FIX: sample_weightauto for RidgeClassifier Nothing great I came across this bug while playing with the class_weights option  I dont know if there is an issue already for thisIn master     X  nparray([[-10 -10] [-10 0] [-8 -10] [10 10]])     y  nparray([1 1 -1 -1])     clfa  RidgeClassifier(class_weightauto)     clfafit(X y)     ValueError: object too deep for desired arrayThis fixes this,,1640,0.776829268292683,0.13486842105263158,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,36,1282,135,travis,Manoj-Kumar-S,larsmans,false,larsmans,10,0.5,22,15,610,true,false,false,false,1,141,10,45,6,4,11
3070711,scikit-learn/scikit-learn,python,2875,1392841624,1394540233,1394540233,28310,28310,commits_in_master,false,false,false,84,52,18,18,31,0,49,0,4,0,0,7,7,7,0,0,0,0,7,7,7,0,0,1554,0,2956,0,197.21530640394047,4.6155200374944565,61,peter.prettenhofer@gmail.com,sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_tree.pyx,54,0.029654036243822075,0,8,false,[WIP] Faster depth-based tree builder by avoiding to splitting on known constant features in the sample subset This is a resurrection of pr #2813 The idea is the same If a constant feature is found while trying to split the information is passed to the left and right child which will avoid trying to search a valid split on this featureHowever here the invariant that the order of features known to be constant is enforced by copying data in an auxiliary array constant_features,,1639,0.776693105552166,0.13509060955518945,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,36,1282,144,travis,arjoly,arjoly,true,arjoly,50,0.78,22,24,792,true,true,false,false,9,148,34,78,40,0,9
3068610,scikit-learn/scikit-learn,python,2874,1392825827,1392842909,1392842909,284,284,merged_in_comments,false,false,false,16,1,1,0,8,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.221999642612123,0.09880938910926912,21,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py,21,0.03494176372712146,0,3,false,DOC: Fixed roc_curve docstring It does not make sense for y_score of roc_curve to be binary,,1638,0.7765567765567766,0.13643926788685523,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,36,1282,154,travis,chrisfilo,larsmans,false,larsmans,0,0,54,13,1414,false,false,false,false,0,0,0,0,2,0,8
3066022,scikit-learn/scikit-learn,python,2871,1392790925,,1392791012,1,,unknown,false,false,false,10,1,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Merge pull request #1 from scikit-learn/master BUG: _alpha_grid undefined symbol,,1637,0.7770311545510079,0.13689482470784642,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,35,1282,130,travis,RWalecki,RWalecki,true,,1,0.0,3,6,756,false,false,false,false,0,0,2,0,0,0,-1
3066017,scikit-learn/scikit-learn,python,2870,1392790862,,1392790880,0,,unknown,false,false,false,10,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Merge pull request #1 from scikit-learn/master BUG: _alpha_grid undefined symbol,,1636,0.7775061124694377,0.13689482470784642,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,35,1282,130,travis,RWalecki,RWalecki,true,,0,0,3,6,756,false,false,false,false,0,0,0,0,0,0,-1
3065552,scikit-learn/scikit-learn,python,2869,1392785055,1392791767,1392791767,111,111,github,false,false,false,8,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.805693291005836,0.11246983877642007,11,peter.prettenhofer@gmail.com,sklearn/linear_model/sgd_fast.pyx,11,0.018363939899833055,0,0,false,Fix typo in fast sgd classifier implementation comments ,,1635,0.7773700305810397,0.13689482470784642,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,35,1281,129,travis,jperla,jnothman,false,jnothman,0,0,74,1,2156,false,false,false,false,0,0,0,0,0,0,9
3059448,scikit-learn/scikit-learn,python,2867,1392731138,1403700288,1403700288,182819,182819,commit_sha_in_comments,false,false,false,46,2,2,1,7,0,8,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,10,4,10,8.525061385486115,0.19951611385187273,4,walks@ethz.ch,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,4,0.006700167504187605,0,3,false,FIX Bounds in anisotropic GP hyperparameter optimization Currently if multi-dimensional thetaU and thetaL are given as bounds for anisotropic GP hyperparameter optimization only the bounds in the last dimensions are obeyed (see added unittest) This pull request fixes this bug The bug was related to https://stackoverflowcom/questions/10452770/python-lambdas-binding-to-local-values,,1634,0.7772337821297429,0.1373534338358459,36141,436.3465316399657,34.3654021748153,109.65385573171744,2431,35,1281,188,travis,jmetzen,larsmans,false,larsmans,4,0.5,10,2,862,false,true,false,false,0,3,3,0,1,0,11
3028661,scikit-learn/scikit-learn,python,2866,1392664920,1392725566,1392725566,1010,1010,commits_in_master,false,false,false,6,2,1,8,6,0,14,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,28,0,56,4.196612364552019,0.09821457659873464,8,peter.prettenhofer@gmail.com,sklearn/linear_model/tests/test_ridge.py,8,0.013468013468013467,0,5,false,Fixes Issue 2751 Fixes https://githubcom/scikit-learn/scikit-learn/issues/2751 ,,1633,0.777097366809553,0.13468013468013468,36141,435.9314905508979,34.3654021748153,109.59851691984174,2431,35,1280,130,travis,Manoj-Kumar-S,agramfort,false,agramfort,9,0.4444444444444444,22,15,608,true,false,false,false,1,136,10,42,6,4,0
3047388,scikit-learn/scikit-learn,python,2865,1392569138,1392668137,1392668137,1649,1649,commits_in_master,false,false,false,50,1,1,0,5,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,2,3,2,8.29863304384938,0.19421539564494425,5,peter.prettenhofer@gmail.com,sklearn/datasets/base.py|sklearn/datasets/tests/test_base.py,5,0.008431703204047217,0,1,false,BF: load_boston() returns 13 features but 14 feature names Last column has the target value hence only report first 13 namesIt could be argued that it would be useful to report the name of the target as well but a better and different name than feature_names would avoid confusion,,1632,0.7769607843137255,0.1399662731871838,36141,435.9314905508979,34.3654021748153,109.59851691984174,2431,34,1279,131,travis,hanke,GaelVaroquaux,false,GaelVaroquaux,0,0,44,1,1593,false,false,false,false,0,0,0,0,0,0,169
3045239,scikit-learn/scikit-learn,python,2864,1392517539,1392579495,1392579495,1032,1032,github,false,false,false,13,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.464936226196591,0.10449655809081249,1,larsmans@gmail.com,examples/plot_digits_classification.py,1,0.0017543859649122807,0,0,false,DOC: minor improvement in comments of an example HiA tiny patch :),,1631,0.776824034334764,0.1456140350877193,36141,435.9314905508979,34.3654021748153,109.59851691984174,2431,34,1278,130,travis,sciunto,GaelVaroquaux,false,GaelVaroquaux,0,0,5,4,1308,false,false,false,false,0,0,0,0,0,0,1032
3045088,scikit-learn/scikit-learn,python,2863,1392515723,,1392693078,2955,,unknown,false,false,false,53,1,1,0,12,0,12,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,0,10,0,3.826064623326017,0.08954465013262479,60,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,57,0.10017574692442882,1,5,false,[MRG] Cache features value for extra trees Caching feature values speed up extra trees by a fair amount  for only 10 lines of code changeHere some benchmark results on the covertype datasetClassifier   train-time test-time error-rate--------------------------------------------master    524233s   04902s     00381 this pr    444372s   04892s     00381  Cheers cc @glouppe ,,1630,0.7773006134969325,0.14586994727592267,36141,435.9314905508979,34.3654021748153,109.59851691984174,2431,34,1278,129,travis,arjoly,larsmans,false,,49,0.7959183673469388,22,24,788,true,true,true,true,9,143,33,78,34,0,612
3015390,scikit-learn/scikit-learn,python,2862,1392508157,1392538763,1392538763,510,510,commits_in_master,false,false,false,94,539,266,161,145,19,325,0,11,1,0,4,12,4,0,0,1,0,11,12,7,0,0,11494,2254,26480,7386,1750.5665585632353,40.96994241607842,3,larsmans@gmail.com,sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py|sklearn/linear_model/logistic.py|sklearn/utils/optimize.py,2,0.0017605633802816902,2,90,false,Log cv Continuation of the work by @fabianp and @GaelVaroquaux TODOsI think the some of the private functions in logisticpy can be simplified In particular it is my intuition that we could squash some of the functions _logistic* and _logistic_*_intercept without sacrificing a significant amount of performancesparse matrix support (any help from the people who actually use sparse matrices would be great here)implement multinomial logistic or raise a meaningful error for multiclass pointing to OvA OvO meta-estimatorsMore tests for LogisticCV object (and some are failing)Document tolerance and stopping criterions,,1629,0.7771639042357275,0.14612676056338028,36141,435.9314905508979,34.3654021748153,109.59851691984174,2431,34,1278,200,travis,Manoj-Kumar-S,Manoj-Kumar-S,true,Manoj-Kumar-S,8,0.375,22,15,606,true,false,false,false,1,133,5,44,6,4,608
3042808,scikit-learn/scikit-learn,python,2861,1392474277,,1405784696,221840,,unknown,false,false,false,47,5,2,11,26,0,37,0,9,0,0,2,2,2,0,0,1,0,2,3,2,0,1,68,54,112,172,17.709789000028785,0.41447464924511973,2,larsmans@gmail.com,sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py,2,0.0035335689045936395,0,7,false,[MRG] n_components for random projection transformer can be given as a ratio of n_features The number of components (n_components) for random projections can now be given conveniently as a ratio of the number of components and the number of features à la max_features of tree based methods ,,1628,0.7776412776412777,0.14664310954063603,36135,435.81015635810155,34.371108343711086,109.56136709561368,2431,34,1278,203,travis,arjoly,arjoly,true,,48,0.8125,22,24,788,true,true,false,false,9,151,33,77,33,0,6
3039200,scikit-learn/scikit-learn,python,2860,1392413748,,1392493029,1321,,unknown,false,false,false,20,2,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,5,0,4.373340922647502,0.10235209354908113,3,larsmans@gmail.com,sklearn/hmm.py,3,0.005357142857142857,0,0,true,Fixing ValueError: startprob must sum to 10 The error happens when there is 0 in the _covars_ of the data,,1627,0.778119237861094,0.14821428571428572,36135,435.7824823578248,34.371108343711086,109.56136709561368,2431,34,1277,134,travis,csytracy,larsmans,false,,0,0,2,3,407,false,false,false,false,0,0,0,0,1,0,-1
3037233,scikit-learn/scikit-learn,python,2859,1392397269,1392397340,1392397340,1,1,github,false,false,false,8,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.3450347548496495,0.10168962698825053,5,peter.prettenhofer@gmail.com,sklearn/pipeline.py,5,0.008944543828264758,0,0,true,s/svn/svm parameter C of the {svn - svm},,1626,0.7779827798277983,0.15026833631484796,36135,435.7824823578248,34.371108343711086,109.56136709561368,2431,34,1277,133,travis,eltermann,GaelVaroquaux,false,GaelVaroquaux,7,0.8571428571428571,7,5,1124,true,false,false,false,0,2,10,1,0,0,-1
3034379,scikit-learn/scikit-learn,python,2858,1392355633,1392380014,1392380014,406,406,github,false,false,false,31,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.540379956634671,0.10626143966522812,1,jaquesgrobler@gmail.com,doc/modules/naive_bayes.rst,1,0.0017921146953405018,0,0,true,Update naive_bayesrst Updated the Bernoulli equation for P(x_i|y) to match http://wwwinfedacuk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2uppdf The equation currently displayed cant possibly be correct because it gives a 0 probability no matter what value x_i takes,,1625,0.7778461538461539,0.15053763440860216,36135,435.7824823578248,34.371108343711086,109.56136709561368,2431,33,1277,133,travis,ameasure,agramfort,false,agramfort,0,0,1,0,1122,false,true,false,false,0,0,0,0,0,0,-1
3031941,scikit-learn/scikit-learn,python,2857,1392332634,1392332695,1392332695,1,1,github,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.770100092805642,0.11163772983776349,1,jaquesgrobler@gmail.com,doc/conf.py,1,0.0018281535648994515,0,0,false,/s/2013/2014: Updated project copyright date 2013 - 2014:),,1624,0.7777093596059114,0.15356489945155394,36135,435.7824823578248,34.371108343711086,109.56136709561368,2431,33,1276,133,travis,eltermann,agramfort,false,agramfort,6,0.8333333333333334,7,5,1123,true,false,false,false,0,2,8,1,0,0,0
3031660,scikit-learn/scikit-learn,python,2856,1392330202,,1392505181,2916,,unknown,false,false,false,9,5,4,0,5,0,5,0,3,2,0,2,4,4,0,0,2,0,2,4,4,0,0,487,297,503,301,39.50787832127275,0.9246283643283686,1,larsmans@gmail.com,sklearn/linear_model/logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py|sklearn/utils/tests/test_optimize.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py,1,0.0,1,3,false,Logistic Regression CV Carrying on @GaelVaroquaux  s work,,1623,0.7781885397412199,0.15356489945155394,36135,435.7824823578248,34.371108343711086,109.56136709561368,2431,33,1276,133,travis,Manoj-Kumar-S,Manoj-Kumar-S,true,,7,0.42857142857142855,22,15,604,true,false,false,false,1,129,4,44,7,4,15
3013458,scikit-learn/scikit-learn,python,2854,1392213720,1419699632,1419699632,458098,458098,merged_in_comments,false,true,false,93,8,8,7,40,0,47,0,8,2,0,4,6,5,0,0,2,0,4,6,5,0,0,294,278,294,278,80.61016478157235,1.8865709681870273,25,peter.prettenhofer@gmail.com,sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/utils/metaestimators.py|sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/utils/metaestimators.py|sklearn/tests/test_metaestimators.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/tests/test_metaestimators.py,19,0.00992063492063492,0,16,false,[MRG] Ensure delegated ducktyping in MetaEstimators Supersedes #2019 with a more readable () reimplementation and an extension of the test to fix #2853This patch ensures that GridSearchCV RandomizedSearchCV Pipeline RFE and RFECV have  hasattr(metaest method)  True iff the sub-estimator does for the set of standard methods: inverse_transform transform predict predict_proba predict_log_proba decision_function score (with some exceptions where delegation doesnt apply)To fix #2853 hasattr must be True before the metaestimator is fit and if the delegating method is called before fit an exception will be raised as with other un-fit estimators,,1622,0.7780517879161529,0.17063492063492064,36133,435.80660338195,34.373010821133036,109.56743143386932,2431,33,1275,251,travis,jnothman,larsmans,false,larsmans,69,0.6376811594202898,22,1,1751,true,true,false,false,17,323,41,230,87,2,9
3012360,scikit-learn/scikit-learn,python,2852,1392205945,1392208985,1392208986,50,50,github,false,false,false,111,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,0,1,4.542396519414759,0.10630860090353712,3,joel.nothman@gmail.com,sklearn/ensemble/tests/test_weight_boosting.py,3,0.006036217303822937,0,0,false,Remove superfluous AdaBoostRegressor call I get this test failure occasionallyFAIL: Check classification on a toy dataset----------------------------------------------------------------------Traceback (most recent call last):  File /usr/lib64/python27/site-packages/nose/casepy line 197 in runTest    selftest(*selfarg)  File /scikit-learn/sklearn/ensemble/tests/test_weight_boostingpy line 59 in test_regression_toy    assert_array_equal(clfpredict(T) y_t_regr)  File /usr/lib64/python27/site-packages/numpy/testing/utilspy line 718 in assert_array_equal    verboseverbose headerArrays are not equal)  File /usr/lib64/python27/site-packages/numpy/testing/utilspy line 644 in assert_array_compare    raise AssertionError(msg)AssertionError: Arrays are not equal(mismatch 333333333333%) x: array([ 1  1  1]) y: array([-1  1  1])Using the regressor with random_state0 seems to solve the problem for me although Im not sure if this is hinting at a deeper problemThis [commit](https://githubcom/scikit-learn/scikit-learn/commit/10ee464177be4b931197e46ba735a52488edbfad#diff-e9f9c11c51c9b39b67f2a0dd9928adbdR56) introduced the additional regressor but Im not sure why,,1621,0.7779148673658236,0.17303822937625754,36133,435.8342789140121,34.373010821133036,109.56743143386932,2431,33,1275,133,travis,jwkvam,mblondel,false,mblondel,4,0.5,3,3,1730,false,true,false,false,1,29,0,5,0,0,9
3012874,scikit-learn/scikit-learn,python,2849,1392152604,1395668351,1395668351,58595,58595,commit_sha_in_comments,false,false,false,91,16,3,7,9,6,22,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,135,31,826,88,18.285747684638388,0.42795297752015987,9,peter.prettenhofer@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx,5,0.006237006237006237,0,0,false,Added support for sparse matrices in manhattan_distances I wasnt sure if in the case where the size of the broadcasted dense array would exceed the memory limit the sparse method should be used too It seems like the correct thing to do but I didnt want to change the implementation of that before askingpython        if temporary_size  size_threshold and sum_over_features:        # Broadcasting the full thing would be too big: its on the order        # of magnitude of the gigabyte        # should this also use the sparse method    ,,1619,0.7782581840642372,0.18087318087318088,36133,435.8342789140121,34.373010821133036,109.56743143386932,2431,33,1274,149,travis,mattilyra,larsmans,false,larsmans,1,1.0,6,1,879,false,true,false,false,0,0,0,0,0,0,13
3013778,scikit-learn/scikit-learn,python,2848,1392145520,,1401130087,149742,,unknown,false,true,false,58,10,8,3,1,11,15,3,4,0,0,5,5,5,0,0,0,0,5,5,5,0,0,301,37,301,74,73.14858341744932,1.7119501423216954,76,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,55,0.05219206680584551,0,0,false,[WIP] Sparse matrix support for Decision tree Implementation relies on a new method sparse_get_item() which was tested separately (https://gistgithubcom/eltermann/8936354)There are two open issues:1- sparse toy dataset test is failingArrays are not equalFailed with DecisionTreeClassifier(mismatch 333333333333%) x: array([-1  1 -1]) y: array([-1  1  1])2- as Gilles [pointed out](https://githubcom/eltermann/scikit-learn/commit/5ba9c367661446c3eba7e6ea54adc1ff5cdfd39f#commitcomment-5316571) sparse_get_item() implementation should be optimized,,1618,0.7787391841779975,0.18162839248434237,36138,435.7739775305772,34.368255022414075,109.55227184680945,2431,33,1274,182,travis,eltermann,arjoly,false,,5,1.0,7,5,1121,true,false,false,false,0,2,7,0,0,0,239
3027623,scikit-learn/scikit-learn,python,2847,1392140887,,1396545557,73411,,unknown,false,false,false,127,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.341288457435298,0.10160236938768932,9,peter.prettenhofer@gmail.com,sklearn/cluster/k_means_.py,9,0.0189873417721519,0,0,false,FIX: make K-Means accept 1D vectors Other unsupervised learning implementations such as GMM accept 1D vectors as input K-Means however expects a 2D array because 1D vectors are mangled by atleast2d_or_csr() I think that this should be fixed for consistency: either all should *not* accept 1D vectors or all should accept themIm not sure what is the best way to do this but here is my take: I copy-pasted the check from GMM but wrapped it into a try / except statement because apparently its not a good idea to run asarray() on the array_like since atleast2d_or_csr() is used as an input check So I check from the ndim  1 if there is no such attribute then this check is a nopHope this helps,,1617,0.7792207792207793,0.18354430379746836,36138,435.7739775305772,34.368255022414075,109.55227184680945,2431,33,1274,158,travis,zyv,larsmans,false,,1,1.0,6,0,1099,false,false,false,false,0,0,1,0,0,0,5900
3001521,scikit-learn/scikit-learn,python,2845,1392117321,1392493808,1392493808,6274,6274,commit_sha_in_comments,false,false,false,11,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,29,0,29,0,4.530765433834649,0.10603683854018661,10,jakevdp@gmail.com,doc/modules/learning_curve.rst|examples/plot_underfitting_overfitting.py,10,0.020833333333333332,0,0,false,[MRG] Rename example plot_polynomial_regressionpy - plot_underfitting_overfittingpyThis would fix issue #2834,,1616,0.7790841584158416,0.19375,36138,435.7739775305772,34.368255022414075,109.55227184680945,2429,33,1274,136,travis,AlexanderFabisch,larsmans,false,larsmans,7,0.7142857142857143,25,25,964,true,true,true,false,2,65,6,69,71,0,9
3001514,scikit-learn/scikit-learn,python,2843,1392070451,1393233359,1393233359,19381,19381,commit_sha_in_comments,false,false,false,13,25,25,0,3,3,6,0,3,5,2,12,19,11,3,0,5,2,12,19,11,3,0,1621,598,1621,598,201.7325390982106,4.721295107646817,33,ugurthemaster@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/forest.py|examples/cluster/plot_lena_segmentation.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/ensemble/.ipynb_checkpoints/Implement ``class_weights`` parameter for Random Forests-checkpoint.ipynb|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/dummy.py|sklearn/tests/test_dummy.py|examples/cluster/plot_lena_segmentation.py|examples/plot_johnson_lindenstrauss_bound.py|examples/tree/plot_tree_regression_multioutput.py|sklearn/dummy.py|sklearn/tests/test_dummy.py,16,0.004319654427645789,0,0,false,[WIP] ENH Implemented median and constant strategies in the DummyRegressor Fixed issue #2718,,1615,0.7789473684210526,0.20086393088552915,36138,435.7739775305772,34.368255022414075,109.55227184680945,2424,33,1273,142,travis,maheshakya,maheshakya,true,maheshakya,3,0.3333333333333333,2,0,753,true,false,false,false,0,5,3,0,1,0,14
3001515,scikit-learn/scikit-learn,python,2842,1392068015,,1392069925,31,,unknown,false,false,false,19,23,23,0,0,3,3,0,0,5,2,11,18,10,3,0,5,2,11,18,10,3,0,1608,596,1608,596,178.58340055685156,4.179518778304388,31,peter.prettenhofer@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/forest.py|examples/cluster/plot_lena_segmentation.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/ensemble/.ipynb_checkpoints/Implement ``class_weights`` parameter for Random Forests-checkpoint.ipynb|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/dummy.py|sklearn/tests/test_dummy.py,16,0.0021598272138228943,0,0,false,[WIP] ENH Median and Constant strategies in dummy regressor issue #2718 Implemented median and constant strategies in dummy regressor,,1614,0.7794299876084263,0.20086393088552915,36138,435.7739775305772,34.368255022414075,109.55227184680945,2424,33,1273,128,travis,maheshakya,maheshakya,true,,2,0.5,2,0,753,true,false,false,false,0,5,1,0,1,0,-1
2995067,scikit-learn/scikit-learn,python,2841,1392022761,1393549894,1393549894,25452,25452,commit_sha_in_comments,false,false,false,23,1,1,1,2,0,3,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.2007518889158435,0.09831328862579325,0,,sklearn/preprocessing.py,0,0.0,0,0,false,Fixed issue #2839 raised ValueError if value  n_values Code newbie only my second ever PR so sorry if I screwed up somewhere ,,1613,0.7792932424054557,0.2025862068965517,36138,435.7739775305772,34.368255022414075,109.55227184680945,2420,33,1273,142,travis,SreudianFlip,larsmans,false,larsmans,0,0,0,3,402,false,false,false,false,1,0,0,0,0,0,25429
2993322,scikit-learn/scikit-learn,python,2840,1392012595,1392012813,1392012813,3,3,github,false,false,false,36,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.378000854559009,0.10246157642299991,9,peter.prettenhofer@gmail.com,sklearn/linear_model/ridge.py,9,0.019438444924406047,0,0,false,Fixed incredibly minor spelling mistake I happened to be browsing the documentation online at http://scikit-learnorg/stable/modules/generated/sklearnlinear_modelRidgeClassifierhtml#sklearnlinear_modelRidgeClassifierWhen I happened to note that Singular was spelled incorrectly as Sinvular This is just a correction of that mis-spelled word,,1612,0.7791563275434243,0.20302375809935205,36138,435.7739775305772,34.368255022414075,109.55227184680945,2420,32,1273,127,travis,earino,jnothman,false,jnothman,0,0,17,11,2156,false,false,false,false,0,0,0,0,0,0,3
2958569,scikit-learn/scikit-learn,python,2837,1391805231,1391812174,1391812174,115,115,merged_in_comments,false,false,false,63,3,1,5,7,0,12,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,2,22,8,28,8.968016259030591,0.20988638666698173,13,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,12,0.027149321266968326,0,3,true,BF: Correct degrees of freedom in f_regression + test Degrees of freedom are equal to n - 2 only if we center the variates Otherwise it should be n - 1I use the syntax  - int(center) because it is concise but using a if center:  else  statement instead would afford checking that we actually test it through code coverage,,1611,0.7790192427063936,0.22850678733031674,36138,435.4142453926615,34.368255022414075,109.46925673805966,2420,32,1270,128,travis,VirgileFritsch,ogrisel,false,ogrisel,8,0.75,11,0,1376,false,true,false,false,0,0,0,0,1,0,2
2951221,scikit-learn/scikit-learn,python,2836,1391725023,1391774163,1391774163,819,819,commits_in_master,false,false,false,11,6,2,6,6,0,12,0,4,0,0,4,5,4,0,0,0,0,5,5,5,0,0,36,9,59,16,12.705018990257187,0.29734675802463123,64,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/tests/test_forest.py,51,0.11512415349887133,1,6,false,[MRG] Fix for #2835 This should hopefully fix #2835 CC: @arjoly,,1610,0.7788819875776397,0.23476297968397292,36138,435.4142453926615,34.368255022414075,109.46925673805966,2420,32,1269,129,travis,glouppe,arjoly,false,arjoly,48,0.9583333333333334,125,26,1183,true,true,true,true,14,145,22,39,15,2,1
2944665,scikit-learn/scikit-learn,python,2832,1391648526,1392494761,1392494761,14103,14103,commit_sha_in_comments,false,false,false,167,16,16,0,2,0,2,0,2,0,0,15,15,15,0,0,0,0,15,15,15,0,0,62,4,62,4,69.88218531152921,1.6409727636682536,2,peter.prettenhofer@gmail.com,examples/linear_model/plot_ransac.py|examples/linear_model/plot_ols_ridge_variance.py|examples/ensemble/plot_forest_iris.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/exercises/plot_cv_digits.py|examples/covariance/plot_outlier_detection.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_stock_market.py|examples/cluster/plot_cluster_iris.py|examples/plot_classification_probability.py|examples/plot_digits_classification.py|examples/neighbors/plot_digits_kde_sampling.py|examples/mixture/plot_gmm_classifier.py,2,0.0,0,0,false,Updated examples for Python 3 compatibility I updated a variety of examples to make them work with Python 3 Most fixes were pretty straightforward (adding parenthesis to print wrapping dict and map in lists as they return iterators in Python 3)Link to original issuehttps://githubcom/scikit-learn/scikit-learn/issues/2815#issuecomment-33905566I ran the tests (nosetests-33 --exe sklearn) and I got one error which seems unrelated to the examples I edited **Could you please verify this**Traceback:pythonFAIL: sklearnlinear_modelteststest_least_angletest_lasso_lars_vs_lasso_cd_ill_conditioned----------------------------------------------------------------------Traceback (most recent call last):  File /usr/local/lib/python33/site-packages/nose/casepy line 198 in runTest    selftest(*selfarg)  File /Users/hendrikheuer/Projects/scikit-learn/sklearn/linear_model/tests/test_least_anglepy line 327 in test_lasso_lars_vs_lasso_cd_ill_conditioned    linear_modellars_path X y methodlasso)  File /Users/hendrikheuer/Projects/scikit-learn/sklearn/utils/testingpy line 189 in assert_warns_message    % (func__name__ warning_class w[0]))noseproxyAssertionError: First warning for lars_path is not a class UserWarning( is {message : DeprecationWarning(converting an array with ndim  0 to an index will result in an error in the future) category : DeprecationWarning filename : /Users/hendrikheuer/Projects/scikit-learn/sklearn/linear_model/least_anglepy lineno : 368 line : None})Thank you for this awesome libraryKind regards and all the best from StockholmHendrik,,1609,0.7787445618396519,0.24324324324324326,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,32,1268,136,travis,h10r,larsmans,false,larsmans,0,0,20,64,765,false,true,false,false,0,2,0,0,0,0,64
2942453,scikit-learn/scikit-learn,python,2831,1391629762,1391629874,1391629874,1,1,github,false,false,false,5,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.442931072392781,0.10432886811523756,5,peter.prettenhofer@gmail.com,examples/applications/plot_out_of_core_classification.py,5,0.01126126126126126,0,0,false,Fixed small typo Small typo,,1608,0.7786069651741293,0.24774774774774774,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,31,1268,127,travis,eltermann,eltermann,true,eltermann,4,1.0,7,5,1115,false,false,false,false,0,1,4,0,0,0,-1
2942086,scikit-learn/scikit-learn,python,2830,1391626937,1392148413,1392148413,8691,8691,commits_in_master,false,false,false,15,2,1,0,5,0,5,0,4,0,0,12,12,8,0,0,0,0,12,12,8,0,0,22,6,22,6,48.34178989608269,1.135161482441532,40,robertlayton@gmail.com,doc/modules/clustering.rst|doc/modules/dp-derivation.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|examples/applications/plot_species_distribution_modeling.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/common.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/tests/test_cross_validation.py,11,0.01126126126126126,0,0,false,Replaced abbreviated wrt to with regards to While making documentation smaller the abbreviation reduce readability,,1607,0.7784691972619788,0.24774774774774774,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,31,1268,132,travis,eltermann,GaelVaroquaux,false,GaelVaroquaux,3,1.0,7,5,1115,false,false,false,false,0,1,3,0,0,0,27
2941495,scikit-learn/scikit-learn,python,2829,1391621443,1392473896,1392473896,14207,14207,commits_in_master,false,true,false,153,10,4,0,34,0,34,0,5,0,0,2,4,2,0,0,0,0,4,4,3,0,0,1414,0,1720,58,15.857956287362484,0.37237638917284116,55,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,52,0.11711711711711711,0,15,false,[MRG] Uniformize max_features semantics of extra trees and random forest The goal of this pr is to uniformize the semantics of max_features:- random forest: currently draws features until it found a non constant features then it begins to count the number of features drawn- extra trees: currently draws max_features non constant featuresThis pr uniformize all behaviours as discussed in #2813: we draw max_feature and at least one must be  valid (ie non constant)Benchmarks on covertype:this pr   train-time test-time error-rate--------------------------------------------RandomForest  706019s   03386s     00344  (max_featuressqrt)RandomForest 1090293s   02501s     00263  (max_features15)ExtraTrees    533487s   04981s     00381  (max_featuressqrt)ExtraTrees    931047s   03927s     00277  (max_features15)CART          165673s   00155s     00425  master   train-time test-time error-rate--------------------------------------------ExtraTrees   1031957s   03149s     00212  (max_featuressqrt)RandomForest  739088s   03621s     00309  (max_featuressqrt)CART          176760s   00194s     00426  There is a decrease of performance due to the change in semantics Increasing max_features allows to get back to previous error-rate,,1606,0.7783312577833126,0.24774774774774774,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,31,1268,136,travis,arjoly,arjoly,true,arjoly,47,0.8085106382978723,22,24,778,true,true,false,false,11,139,29,68,30,0,12
2938759,scikit-learn/scikit-learn,python,2828,1391578934,1404821925,1404821925,220716,220716,merged_in_comments,false,true,false,61,3,1,16,22,0,38,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,54,12,118,12,11.798650141499321,0.27705588226779215,69,peter.prettenhofer@gmail.com,sklearn/datasets/samples_generator.py|sklearn/metrics/tests/test_metrics.py|sklearn/tests/test_multiclass.py,54,0.0365296803652968,0,4,false,ENH make_multilabel_classification for large n_features: faster and sparse output support This is a more focussed #2773make_multilabel_classification is currently very slow for large n_features and only outputs dense matrices although it effectively generates a sparse structureAlthough this function can certainly be more optimised the per-sample Python iteration will slow it down I dont think theres much value in further optimisation,,1605,0.7781931464174455,0.2534246575342466,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,31,1268,194,travis,jnothman,arjoly,false,arjoly,68,0.6323529411764706,22,1,1744,true,true,false,true,14,298,38,224,77,2,8
2937521,scikit-learn/scikit-learn,python,2827,1391566256,1391579203,1391579203,215,215,github,false,false,false,33,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.874749409583981,0.1144688572259897,2,peter.prettenhofer@gmail.com,doc/modules/sgd.rst,2,0.0045662100456621,0,0,false,MRG DOC make loss function in SGD consistent with subgradient  Comment by Martin Jaggi via mail :)(alternatively we could add a factor in the subgradient but I think this one is nicer),,1604,0.7780548628428927,0.2534246575342466,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,31,1267,127,travis,amueller,amueller,true,amueller,195,0.8564102564102564,810,38,1201,true,true,false,false,17,176,14,33,69,3,5
2937383,scikit-learn/scikit-learn,python,2826,1391565198,1391566117,1391566117,15,15,github,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.569861724251394,0.10730947480386478,3,peter.prettenhofer@gmail.com,doc/modules/label_propagation.rst,3,0.006864988558352402,0,0,false,DOC: grammar and spelling fixes ,,1603,0.7779164067373674,0.2540045766590389,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,30,1267,126,travis,perimosocordiae,perimosocordiae,true,perimosocordiae,0,0,44,48,1732,false,true,false,false,0,1,0,0,0,0,15
2931877,scikit-learn/scikit-learn,python,2825,1391519494,1391520743,1391520743,20,20,github,false,false,false,4,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.807635198899646,0.11289281046938544,8,peter.prettenhofer@gmail.com,doc/modules/ensemble.rst,8,0.018223234624145785,0,0,false,Fixed FS1995 citation http://wwwface-recorg/algorithms/Boosting-Ensemble/decision-theoretic_generalizationpdf,,1602,0.7777777777777778,0.25968109339407747,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,30,1267,127,travis,bryan-lunt,arjoly,false,arjoly,0,0,9,19,1750,false,false,false,false,0,0,0,0,0,0,8
2930720,scikit-learn/scikit-learn,python,2823,1391502175,,1391516697,242,,unknown,false,false,false,76,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.129182315934092,0.09696139105832684,17,peter.prettenhofer@gmail.com,sklearn/metrics/scorer.py,17,0.0387243735763098,0,0,false,Fixing MAE and MSE errors should not be negative when used in cross_val Setting greater_is_betterTrue causes the sign in _PredictScorer to be negative  This leads to negative values when using LinearRegression in cross_val_scoreAlternatively we could adjust the return statement in _PredictScorer__call__return self_sign * self_score_func(y_true y_pred **self_kwargs)Also I found this mailthread on the issue: https://wwwmail-archivecom/scikit-learn-general@listssourceforgenet/msg08945htmlThat mailthread implies that this is not the correct fix but I wanted to discuss how to correct this,,1601,0.778263585259213,0.25968109339407747,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,30,1267,127,travis,arahuja,mblondel,false,,0,0,43,7,1196,false,false,false,false,0,0,0,0,0,0,86
2929374,scikit-learn/scikit-learn,python,2822,1391484958,1391519200,1391519200,570,570,commits_in_master,false,false,false,141,206,7,56,102,0,158,0,9,7,0,1,23,8,0,0,9,2,14,25,15,0,0,3569,0,15306,2439,142.9540065752466,3.356843625288439,0,,sklearn/manifold/binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/tsne.py|sklearn/manifold/binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/tsne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py|examples/manifold/plot_tsne_iris.py|sklearn/manifold/__init__.py|sklearn/manifold/_binary_search.c|sklearn/manifold/_binary_search.pyx|sklearn/manifold/setup.py|sklearn/manifold/t_sne.py,0,0.0,0,27,false,[WIP] t-SNE This is an implementation of (non-parametric) t-SNE for visualizationSee [Laurens van der Maatens paper](http://jmlrorg/papers/volume9/vandermaaten08a/vandermaaten08apdf) or [his website about t-SNE](http://homepagetudelftnl/19j49/t-SNEhtml) for details In comparison to other implementations and the original paper this version has* parameter optimization with L-BFGS* it is designed and optimized for Python* the degrees of freedom of the Students t-distribution are determined with a heuristicTODO----- [ ] Implement real transform (and maybe even inverse_transform)- [ ] reference for the trustworthiness score- [ ] integrate in sklearn (relative imports build with cython etc)- [ ] tests- [ ] example (documentation)- [ ] remove Python function calls from binary search (try to accelerate binary search with parakeet)- [ ] early compressionExample-------See for details and here for an example with the digits dataset:[digits](http://wwwinformatikuni-bremende/~afabisch/files/digits_p40png),,1600,0.778125,0.25968109339407747,36011,439.4212879397962,34.71161589514315,110.38293854655522,2420,30,1266,183,travis,AlexanderFabisch,jnothman,false,jnothman,6,0.6666666666666666,25,25,956,true,true,true,false,2,49,5,53,29,0,97
2926717,scikit-learn/scikit-learn,python,2820,1391463329,1391464953,1391464953,27,27,github,false,false,false,7,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.732056562992816,0.11112159271826653,0,,doc/modules/tree.rst,0,0.0,0,0,false,Minor DecisionTree doc example fix Fixes https://githubcom/scikit-learn/scikit-learn/issues/2819,,1599,0.7779862414008756,0.26558891454965355,36016,439.6101732563305,34.70679697912039,110.47867614393603,2420,29,1266,127,travis,ankit-maverick,GaelVaroquaux,false,GaelVaroquaux,2,1.0,16,7,436,false,false,false,false,0,7,1,0,0,0,27
2924318,scikit-learn/scikit-learn,python,2818,1391443300,1391478372,1391478372,584,584,github,false,false,false,11,1,1,0,2,0,2,0,2,0,0,27,27,27,0,0,0,0,27,27,27,0,0,37,21,37,21,118.34866211699061,2.779149330835236,79,vanderplas@astro.washington.edu,examples/applications/plot_prediction_latency.py|examples/decomposition/plot_pca_vs_fa_model_selection.py|examples/plot_rfe_with_cross_validation.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_spectral.py|sklearn/cross_decomposition/pls_.py|sklearn/datasets/lfw.py|sklearn/datasets/tests/test_svmlight_format.py|sklearn/decomposition/truncated_svd.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_base.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/tests/test_variance_threshold.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/stochastic_gradient.py|sklearn/neural_network/__init__.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/svm/base.py|sklearn/utils/__init__.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_extmath.py,21,0.009237875288683603,0,0,false,COSMIT remove unused imports and variables A number of pyflakes errors,,1598,0.7778473091364205,0.26558891454965355,36016,439.6101732563305,34.70679697912039,110.47867614393603,2420,29,1266,126,travis,jnothman,jnothman,true,jnothman,67,0.6268656716417911,22,1,1742,true,true,false,false,14,289,33,220,75,2,296
2921908,scikit-learn/scikit-learn,python,2817,1391399154,,1391530189,2183,,unknown,false,false,false,31,1,1,0,5,0,5,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.7632830986739485,0.11185485737228047,0,,examples/plot_rfe_with_cross_validation.py,0,0.0,0,0,false,Update plot_rfe_with_cross_validationpy Line 34 says nb of misclassifications but accuracy was used as the scoring parameter Noticed zero_one_loss was imported but not used so assumed this is what was intended (),,1597,0.778334376956794,0.2662037037037037,36053,439.15901589326825,34.67117854270103,110.3652955371259,2420,29,1265,127,travis,ltiao,GaelVaroquaux,false,,0,0,8,8,848,false,false,false,false,0,5,0,0,0,0,8
2915256,scikit-learn/scikit-learn,python,2814,1391275337,,1391276882,25,,unknown,false,true,false,89,4,4,3,22,0,25,0,8,0,0,6,6,5,0,0,0,0,6,6,5,0,0,457,145,457,145,72.76105878540444,1.7085404213145035,12,vanderplas@astro.washington.edu,examples/document_classification_20newsgroups.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/linear_model/logistic.py|doc/modules/linear_model.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|doc/modules/linear_model.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py,10,0.002364066193853428,0,6,false,[WIP] Multinomial logistic regression This adds the true multinomial (multiclass) logistic regression to linear_models It uses L-BFGS which means that only L2 penalty is supported and I didnt bother to implement the dual objectiveThe docs need a bit more love the code could be optimized just a little more and Im not really sure how to handle the regularization Currently the batch training objective is    -1/m * C * log_loss(w X y) + ||w||²and Im not sure if the 1/m is appropriate how does Liblinear do this,,1596,0.7788220551378446,0.2718676122931442,36053,439.1312789504341,34.67117854270103,110.3652955371259,2420,29,1264,203,travis,larsmans,jnothman,false,,97,0.7319587628865979,132,38,1294,true,true,false,false,25,132,27,63,63,3,7
2915078,scikit-learn/scikit-learn,python,2813,1391271622,,1391612940,5688,,unknown,false,false,false,105,24,4,7,35,0,42,0,7,0,0,6,7,6,0,0,0,0,7,7,7,0,0,160,0,2555,51,41.223447197545404,0.9679916535865004,60,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.pxd|sklearn/tree/_utils.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c,54,0.05952380952380952,0,11,false,[MRG] Faster decision tree / random forest / extra trees by avoiding trying to split on constant features + FIX bug with constant features If we find a constant features we avoid trying to split later on it This optimization is lightweight for tree based on  DepthFirstTreeBuilder TODO list:- [x] RandomSplitter- [x] BestSplitter- [x] Presort-bestsplitter- [x] ~~Benchmarking~~- [x] code styleFor extra trees I already got $ (sklearn) ± python benchmarks/bench_covertypepy --classifierExtraTrees  --random-seed0Classifier   train-time test-time error-rate--------------------------------------------master        902975s   03086s     00214  this branch   694262s   03143s     00209  NB: Error rates are different since we samples fewer random number,,1595,0.7793103448275862,0.2761904761904762,36053,439.1312789504341,34.67117854270103,110.3652955371259,2420,28,1264,132,travis,arjoly,,false,,46,0.8260869565217391,22,24,774,true,true,false,false,10,125,27,67,29,0,7
2915028,scikit-learn/scikit-learn,python,2812,1391270656,1391277363,1391277363,111,111,merged_in_comments,false,false,false,21,1,1,2,3,0,5,0,6,0,0,5,5,5,0,0,0,0,5,5,5,0,0,76,0,76,0,22.759966781274493,0.5344399699181768,6,peter.prettenhofer@gmail.com,sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py,4,0.004761904761904762,0,0,false,CLN: Fix PEP8 warnings in sklearn/covariance Ran autopep8 on all py filesIf this is not helpful please let me know,,1594,0.7791718946047679,0.2761904761904762,36053,439.1312789504341,34.67117854270103,110.3652955371259,2420,28,1264,130,travis,bwignall,larsmans,false,larsmans,4,0.75,9,17,124,false,true,false,false,0,1,5,1,4,0,8
2914959,scikit-learn/scikit-learn,python,2811,1391269361,1391276540,1391276540,119,119,merged_in_comments,false,false,false,36,2,1,1,2,0,3,0,3,0,0,3,4,3,0,0,0,0,4,4,4,0,0,92,0,94,0,13.839865777249184,0.3249819088378507,21,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py,17,0.01909307875894988,0,0,false,CLN: Fix PEP8 warnings in linear_model Ran autopep8 on each file in the folder then fixed remaining warnings manually Tests still pass locallyFixes for bayespy were not added because of technical issues on my side,,1593,0.7790332705586943,0.27684964200477324,36053,439.1312789504341,34.67117854270103,110.3652955371259,2420,28,1264,130,travis,bwignall,larsmans,false,larsmans,3,0.6666666666666666,9,17,124,false,true,false,false,0,1,4,1,3,0,101
2914911,scikit-learn/scikit-learn,python,2810,1391268147,,1391269090,15,,unknown,false,false,false,35,8,8,0,0,0,0,0,0,0,0,5,5,4,0,0,0,0,5,5,4,0,0,108,31,108,31,32.19627871731846,0.7560194790494054,67,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|doc/whats_new.rst|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py,50,0.03341288782816229,0,0,false,CLN: Fix PEP8 warnings in linear_model Ran autopep8 on each py on each file in the folder then fixed remaining warnings manuallyFixes for bayespy were not added because of technical issues on my side,,1592,0.7795226130653267,0.27684964200477324,36053,439.1312789504341,34.67117854270103,110.3652955371259,2420,28,1264,127,travis,bwignall,bwignall,true,,2,1.0,9,17,124,false,true,false,false,0,1,2,1,3,0,-1
2911190,scikit-learn/scikit-learn,python,2808,1391209217,1391277932,1391277932,1145,1145,commit_sha_in_comments,false,false,false,14,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.234565407623134,0.09943428436497621,21,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py,21,0.050119331742243436,0,1,true,DOC fix roc_curve docstring tpr description had a false leftover from fpr description copy-paste,,1591,0.7793840351979887,0.27684964200477324,36053,439.1312789504341,34.67117854270103,110.3652955371259,2419,28,1263,128,travis,cmd-ntrf,larsmans,false,larsmans,2,1.0,13,3,947,false,true,false,false,0,0,0,0,1,0,18
2907478,scikit-learn/scikit-learn,python,2804,1391175077,1405775148,1405775148,243334,243334,commits_in_master,false,false,false,76,37,20,28,27,0,55,0,5,0,0,5,7,3,0,0,2,0,7,9,5,0,0,418,906,667,989,213.03357773763761,5.002364896742137,76,peter.prettenhofer@gmail.com,sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,56,0.050239234449760764,0,4,true,[MRG] Label ranking average precision The goal of this pull request is to add a first metric for multilabel ranking problem label ranking average precision The definition can be found in [Mining multilabel data](http://lkmfriuni-ljsi/xaigor/slo/pedagosko/dr-ui/tsoumakas09-dmkdhpdf) at page 14 or in the documentationFor the moment I decided to add a new function instead of a new possible average value to the current average_precision_score  But this could change I am also open to suggestions for a shorter name,,1589,0.7797356828193832,0.27751196172248804,36053,439.1312789504341,34.67117854270103,110.3652955371259,2419,28,1263,198,travis,arjoly,arjoly,true,arjoly,45,0.8222222222222222,22,24,773,true,true,false,false,10,121,26,67,29,0,95
2891436,scikit-learn/scikit-learn,python,2800,1391024105,1391024822,1391024822,11,11,github,false,false,false,8,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.269043048587495,0.10024387734848508,25,peter.prettenhofer@gmail.com,sklearn/tree/_tree.pxd,25,0.06082725060827251,0,0,false,Fixed documentation on sklearn/tree Typo and semantic fixes,,1588,0.7795969773299748,0.29683698296836986,36053,439.1312789504341,34.67117854270103,110.3652955371259,2419,28,1261,124,travis,eltermann,glouppe,false,glouppe,2,1.0,7,5,1108,false,false,false,false,0,0,2,0,0,0,11
2890959,scikit-learn/scikit-learn,python,2799,1391021644,1391022436,1391022436,13,13,github,false,false,false,26,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,30,0,30,0,4.559257998692497,0.10705861423905418,1,jaquesgrobler@gmail.com,sklearn/semi_supervised/label_propagation.py,1,0.0024390243902439024,0,0,false,Added docstrings for model attributes in LabelPropagation and LabelSprea I have made an attempt to add docstrings for LabelPropagation and LabelSpreading classes per Issue 2711 https://githubcom/scikit-learn/scikit-learn/issues/2711,,1587,0.7794580970384373,0.2951219512195122,36033,439.37501734521135,34.69042266810979,110.42655343712708,2419,28,1261,124,travis,charlescearl,GaelVaroquaux,false,GaelVaroquaux,0,0,1,3,1522,false,false,false,false,1,2,0,0,0,0,10
3001510,scikit-learn/scikit-learn,python,2798,1390986920,1403700267,1403700267,211889,211889,commit_sha_in_comments,false,false,false,129,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,19,4,19,8.728559264013267,0.20496042544131204,3,walks@ethz.ch,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,3,0.0072992700729927005,0,0,false,Fixed gaussian process parameter optimization not respecting optimization limits Theres a bug in gaussian_processpy that causes the parameter optimization via random starts to not respect optimization limits when there is more than one parameter to optimize (ie len(theta)  1)  The problem is that the optimization constraints are enforced by lambda functions that are constructed inside a loop and that depend on the loop counter when the lambda is later evaluated only the last value of the loop counter is remembered not the value at the time the lambda was created  The solution to this is to save the value of the loop counter as a named parameterI also added a simple unit test that checks that the theta bounds are being respected  It fails without this fix,,1586,0.7793190416141236,0.29683698296836986,36033,439.37501734521135,34.69042266810979,110.42655343712708,2419,28,1261,192,travis,lgarrison,larsmans,false,larsmans,0,0,1,0,113,false,false,false,false,0,0,0,0,0,0,8
2890542,scikit-learn/scikit-learn,python,2797,1390955352,1391296935,1391296935,5693,5693,github,false,false,false,95,5,2,9,10,1,20,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,10,18,39,18,12.862213052857115,0.30202517731640893,21,peter.prettenhofer@gmail.com,sklearn/datasets/samples_generator.py|sklearn/ensemble/tests/test_forest.py|sklearn/linear_model/least_angle.py,14,0.0267639902676399,0,4,false,Fix deprecation warning in Python 3 This PR fixed the second failed test case in #2737 The following line caused the failure    drop_idx  activepop(idx)type of idx is numpyndarray and pop operation raised     DeprecationWarning: converting an array with ndim  0 to an index will result in an error in the futureTo fix it just using activepop(idx[0])This PR also changed floating point division to floor division Floating point division caused numpy warning     DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future in Python 3,,1585,0.7791798107255521,0.29683698296836986,36033,439.37501734521135,34.69042266810979,110.42655343712708,2419,28,1260,127,travis,cli248,GaelVaroquaux,false,GaelVaroquaux,1,1.0,1,2,736,false,true,false,false,0,3,1,0,0,0,11
2884641,scikit-learn/scikit-learn,python,2796,1390773920,,1390779139,86,,unknown,false,false,false,20,22,22,0,0,3,3,0,1,5,2,9,16,8,3,0,5,2,9,16,8,3,0,1535,523,1535,523,169.65678121809472,3.9838126071111173,32,peter.prettenhofer@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/forest.py|examples/cluster/plot_lena_segmentation.py|examples/plot_johnson_lindenstrauss_bound.py|sklearn/ensemble/.ipynb_checkpoints/Implement ``class_weights`` parameter for Random Forests-checkpoint.ipynb|sklearn/ensemble/BaggingBaseIPython.ipynb|sklearn/ensemble/BaggingTestsIPython.ipynb|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_bagging.py|sklearn/utils/__init__.py|sklearn/utils/validation.py,21,0.0024330900243309003,0,0,false,Shift around docstrings in certain examples Fixes#2038  This PR is regarding the issue Shift around docstrings in certain examples,,1584,0.7796717171717171,0.29927007299270075,36024,439.484787919165,34.69908949589163,110.45414168332222,2419,28,1258,124,travis,maheshakya,agramfort,false,,1,1.0,2,0,738,true,false,false,false,0,1,0,0,1,0,-1
2880889,scikit-learn/scikit-learn,python,2795,1390698580,1412219045,1412219045,358674,358674,commit_sha_in_comments,false,false,false,57,38,6,59,54,1,114,0,6,0,0,4,5,3,0,0,0,0,5,5,3,0,0,606,302,1557,820,89.4549213312562,2.10054464554219,74,peter.prettenhofer@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,49,0.061124694376528114,0,17,false,[WIP] Enable grid search with classifiers that may fail on individual fits Supersedes #2587 The old pull request was so outdated (and the functionality affected was moved to different files) that I decided to re-do the work starting with a fresh checkout from master Since this is technically a new branch Im opening a new pull request,,1583,0.7795325331648768,0.30073349633251834,36024,439.484787919165,34.69908949589163,110.45414168332222,2419,28,1257,222,travis,romaniukm,jnothman,false,jnothman,1,0.0,3,9,242,false,false,false,false,0,9,3,3,0,0,49
2879792,scikit-learn/scikit-learn,python,2794,1390682240,1390683579,1390683579,22,22,github,false,false,false,69,1,1,0,2,0,2,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,4.613303133983347,0.10832888454752063,2,gael.varoquaux@normalesup.org,sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx,2,0.0049382716049382715,0,1,false,fix numpy deprecationWarning(using a non-integer number ) in svm Using the same strategy in #2791 This PR fixed three failed test cases related toDeprecationWarning(using a non-integer number instead of an integer will result in an error in the future) in #2737 and the second one DeprecationWarning(converting an array with ndim  0 to an index will result in an error in the futureneed more work to fix,,1582,0.7793931731984829,0.3012345679012346,36024,439.484787919165,34.69908949589163,110.45414168332222,2419,28,1257,124,travis,cli248,GaelVaroquaux,false,GaelVaroquaux,0,0,1,2,733,false,true,false,false,0,2,0,0,0,0,7
2873184,scikit-learn/scikit-learn,python,2791,1390588996,1390591209,1390591209,36,36,github,false,false,false,51,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,137,0,137,0,9.835536112607171,0.2309568443257182,0,,sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx,0,0.0,0,0,true,svm fit numpy array indexing deprecation warning fix To fix this:/sklearn/svm/basepy:234: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future max_iterselfmax_iter random_seedrandom_seed)One change in libsvmpyx:207-    intercept  npempty(n_class*(n_class-1)/2 dtypenpfloat64)207  +    intercept  npempty(int((n_class*(n_class-1))/2) dtypenpfloat64)Then cython generated libsvmc,,1581,0.7792536369386465,0.29484029484029484,36022,439.5091888290489,34.701016045749824,110.46027427683082,2419,28,1256,125,travis,adrinjalali,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,644,false,false,false,false,0,0,0,0,0,0,10
2868738,scikit-learn/scikit-learn,python,2790,1390532502,1390670146,1390670146,2294,2294,github,false,false,false,75,3,2,8,25,0,33,0,8,0,0,4,4,4,0,0,0,0,4,4,4,0,0,123,0,205,0,22.687793694564867,0.5327519717159105,61,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,56,0.13658536585365855,0,12,false,[MRG] Avoid reference cycles in Tree Fixes #2787~~This is a WIP because~~ we need to check if wrapping value with a ndarray at predict time is too expensive in time If so we can implement our own take using memcpy (which Id rather over reverting to the version where predict duplicates apply) but for the moment Im having trouble getting that to workThis also needs to be tested for any further memory leaks,,1580,0.779113924050633,0.29024390243902437,36022,439.5091888290489,34.701016045749824,110.46027427683082,2419,28,1255,125,travis,jnothman,glouppe,false,glouppe,66,0.6212121212121212,21,1,1731,true,true,false,false,14,267,30,202,67,2,13
2865100,scikit-learn/scikit-learn,python,2789,1390502593,1390551762,1390551762,819,819,github,false,false,false,13,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.988464390239606,0.11713850520297267,0,,examples/tree/plot_tree_regression_multioutput.py,0,0.0,0,0,false,Update plot_tree_regression_multioutputpy Import statements have been moved to the top of the file,,1579,0.77897403419886,0.2916666666666667,36022,439.5091888290489,34.701016045749824,110.46027427683082,2419,28,1255,125,travis,ugurthemaster,mblondel,false,mblondel,1,1.0,1,0,240,false,false,false,false,0,0,1,0,0,0,11
2864548,scikit-learn/scikit-learn,python,2788,1390497435,,1390779039,4693,,unknown,false,false,false,33,6,2,4,28,0,32,0,8,0,0,2,3,2,0,0,0,0,3,3,3,0,0,49,48,145,134,17.90186006428156,0.42036926882463144,19,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py,15,0.03676470588235294,0,55,false,Fixing alpha_grid for sparse matrices Fixes https://githubcom/scikit-learn/scikit-learn/issues/2526 partially Centering of the sparse matrices is done indirectly The correct output and marginally faster than in master though still much slower then than dense matrices,,1578,0.779467680608365,0.2916666666666667,36022,439.5091888290489,34.701016045749824,110.46027427683082,2419,28,1255,125,travis,Manoj-Kumar-S,agramfort,false,,6,0.5,22,15,583,true,false,false,false,1,134,6,57,6,0,23
2849708,scikit-learn/scikit-learn,python,2784,1390338110,,1390351068,215,,unknown,false,false,false,77,55,6,0,1,0,1,0,9,0,0,8,11,8,0,0,0,0,11,11,9,0,0,614,62,3512,1096,64.85864165680302,1.5258531263742712,12,peter.prettenhofer@gmail.com,sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h|sklearn/svm/src/liblinear/linear.cpp|sklearn/svm/src/liblinear/linear.h|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/tests/test_svm.py,12,0.0,0,9,false,Liblinear Sample Weights As discussed in #409 I have patched the included liblinear sources with the update to scale C for each sample individuallyI mostly just copied the libsvm python wrapper to expose the new parameter The existing tests didnt fully cover the LinearSVC+sample_weights case so I modified themIm admittedly not familiar with the liblinear internals but the patching was straightforward and preserved the existing sklearn modifications The previous SVM tests still pass as well,,1577,0.7799619530754597,0.29873417721518986,36016,438.61061750333187,34.67903154153709,110.17325633051978,2419,27,1253,126,travis,HapeMask,HapeMask,true,,0,0,0,0,656,false,false,false,false,0,0,0,0,0,0,215
2848311,scikit-learn/scikit-learn,python,2782,1390328178,1390329700,1390329700,25,25,github,false,false,false,2,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.251467585087513,0.10001928998526215,16,peter.prettenhofer@gmail.com,sklearn/feature_extraction/text.py,16,0.04071246819338423,0,0,false,FIX: Typo ,,1576,0.7798223350253807,0.30025445292620867,36016,438.61061750333187,34.67903154153709,110.17325633051978,2419,27,1253,127,travis,blagarde,GaelVaroquaux,false,GaelVaroquaux,3,1.0,7,3,650,false,false,false,false,0,0,3,0,0,0,13
2848039,scikit-learn/scikit-learn,python,2781,1390325805,1390329627,1390329627,63,63,github,false,false,false,2,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.554730911100201,0.10715381046302644,9,peter.prettenhofer@gmail.com,doc/modules/feature_extraction.rst,9,0.022900763358778626,0,0,false,FIX: Typo ,,1575,0.7796825396825396,0.30025445292620867,36016,438.61061750333187,34.67903154153709,110.17325633051978,2418,27,1253,127,travis,blagarde,GaelVaroquaux,false,GaelVaroquaux,2,1.0,7,3,650,false,false,false,false,0,0,2,0,0,0,12
2846159,scikit-learn/scikit-learn,python,2780,1390303165,1390303700,1390303700,8,8,github,false,false,false,13,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.655585834936008,0.10952900073735358,0,,examples/plot_digits_pipe.py,0,0.0,0,0,false,Update plot_digits_pipepy Import statements have been moved to the top of the file,,1574,0.7795425667090216,0.30357142857142855,36016,438.61061750333187,34.67903154153709,110.17325633051978,2418,27,1253,126,travis,ugurthemaster,arjoly,false,arjoly,0,0,1,0,238,false,false,false,false,0,0,0,0,0,0,8
2845085,scikit-learn/scikit-learn,python,2779,1390285349,1390302657,1390302657,288,288,github,false,false,false,70,1,1,0,2,0,2,0,2,0,0,3,3,0,0,0,0,0,3,3,0,0,0,0,0,0,0,12.525343447577843,0.2946884161660603,0,,doc/templates/class.rst|doc/templates/class_with_call.rst|doc/templates/function.rst,0,0.0,1,0,false,[MRG] DOC add link to module reference from class/function pages Another fix for a documentation peeve This links the module in the title of autogenerated pages to the relevant section of reference The Up link on the left takes you back to the top of the reference page rather than the modules section and I dont know of a nice way to fix that So this helps insteadPing @jaquesgrobler,,1573,0.7794024157660522,0.30612244897959184,36016,438.61061750333187,34.67903154153709,110.17325633051978,2418,27,1253,126,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,65,0.6153846153846154,21,1,1729,true,true,false,false,13,247,29,199,64,2,9
2844656,scikit-learn/scikit-learn,python,2778,1390278867,1390280533,1390280533,27,27,commit_sha_in_comments,false,false,false,12,4,4,0,1,0,1,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,16,31,16,31,18.356412940069276,0.43187819535203925,58,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|doc/whats_new.rst,48,0.03571428571428571,0,0,false,FIX: typos in whats_newrst Found a few typos in whats_newrst via ispell,,1572,0.7792620865139949,0.30612244897959184,36016,438.61061750333187,34.67903154153709,110.17325633051978,2418,27,1252,126,travis,bwignall,jnothman,false,jnothman,1,1.0,8,16,112,false,true,false,false,0,1,1,1,3,0,10
2844549,scikit-learn/scikit-learn,python,2777,1390277352,1405963129,1405963129,261429,261429,commits_in_master,false,true,false,98,44,6,12,57,3,72,0,10,1,0,2,4,3,0,0,1,0,3,4,3,0,0,433,0,1642,0,46.225122057221824,1.0875557419185713,9,peter.prettenhofer@gmail.com,doc/conf.py|doc/conf.py|doc/conf.py|doc/conf.py|Makefile|doc/conf.py|doc/sphinxext/github_link.py|Makefile|doc/conf.py|doc/sphinxext/github_link.py,8,0.0076726342710997444,1,10,false,[MRG] DOC add links to github sourcecode in API reference I enjoy using the API reference but think a link to the source code on github would often help~~Because sphinxextlinkcode does not provide the full path or line number of the class/function Im getting this stuff using grep which relies on a unix shell and certain path structures which is a bit not-nice Since github needs a branch/commit reference Im using git rev-parse --abbrev-ref HEAD which can return a branch/tag name but a more static commit reference (ie rev-parse --short) might be more sensible~~WDYT ping @jaquesgrobler ,,1571,0.7791215786123489,0.3069053708439898,36016,438.61061750333187,34.67903154153709,110.17325633051978,2418,27,1252,199,travis,jnothman,ogrisel,false,ogrisel,64,0.609375,21,1,1728,true,true,false,false,13,246,27,199,63,2,967
2840996,scikit-learn/scikit-learn,python,2776,1390244838,1390342548,1390342548,1628,1628,github,false,false,false,9,1,1,0,2,0,2,0,2,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.8035107024205095,0.11301513264384197,15,vijay@change.org,doc/testimonials/images/howaboutwe.png|doc/testimonials/testimonials.rst,15,0.038560411311053984,0,0,false,added howaboutwe testimonial testimonial and logo image for howaboutwe,,1570,0.7789808917197453,0.3110539845758355,35926,439.45888771363354,34.68240271669543,110.31008183488282,2418,27,1252,128,travis,DanielWeitzenfeld,GaelVaroquaux,false,GaelVaroquaux,0,0,4,2,958,false,false,false,false,0,1,0,0,0,0,1494
2840257,scikit-learn/scikit-learn,python,2775,1390238259,1390342621,1390342621,1739,1739,github,false,false,false,29,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.441839614967287,0.10450587588266273,1,jaquesgrobler@gmail.com,sklearn/gaussian_process/gaussian_process.py,1,0.0025575447570332483,0,0,false,python 3 compatibility fix This fixes a warning in python 3: gaussian_processpy:53: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future,,1569,0.7788400254939452,0.309462915601023,35926,439.45888771363354,34.68240271669543,110.31008183488282,2417,27,1252,127,travis,stefan-w,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,708,false,false,false,false,0,0,0,0,0,0,8
2839531,scikit-learn/scikit-learn,python,2773,1390230256,,1391577023,22446,,unknown,false,false,false,28,9,2,28,23,0,51,0,3,0,0,1,4,1,0,0,0,0,4,4,4,0,0,110,0,219,43,8.917013040102393,0.20979601669383757,14,peter.prettenhofer@gmail.com,sklearn/datasets/samples_generator.py|sklearn/datasets/samples_generator.py,14,0.03571428571428571,0,7,false,[MRG] ENH fast make_multilabel classification with sparse output support I wanted sparse output from make_multilabel_classification but in setting n_features high I discovered quite how inefficient generatormultinomial(1 )argmax() is,,1568,0.7793367346938775,0.3086734693877551,35926,439.45888771363354,34.68240271669543,110.31008183488282,2417,27,1252,133,travis,jnothman,,false,,63,0.6190476190476191,21,1,1728,true,true,false,false,13,240,23,199,57,2,126
2839191,scikit-learn/scikit-learn,python,2772,1390226521,1390268252,1390268252,695,695,github,false,false,false,24,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,49,0,49,0,9.072045747315796,0.2134435659667116,14,peter.prettenhofer@gmail.com,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,14,0.03571428571428571,0,0,false,[MRG] ENH remove unnecessary CSR-CSC transform in text feature extractors A small change to avoid a copy/transformation of sparse matrices in text feature extraction,,1567,0.7791959157626037,0.3086734693877551,35926,439.45888771363354,34.68240271669543,110.31008183488282,2417,27,1252,126,travis,jnothman,jnothman,true,jnothman,62,0.6129032258064516,21,1,1728,true,true,false,false,13,240,22,199,56,2,695
2835576,scikit-learn/scikit-learn,python,2770,1390165102,1390343288,1390343288,2969,2969,merged_in_comments,false,false,false,5,3,2,6,2,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,65,17,66,16.601050109247645,0.39058306607211646,22,mathieu@mblondel.org,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,22,0.056555269922879174,0,2,false,[MRG] StratifiedShuffleSplit bug Fixes #2764,,1566,0.7790549169859514,0.3110539845758355,35926,439.45888771363354,34.68240271669543,110.31008183488282,2417,26,1251,125,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,40,0.75,446,3,1427,true,true,false,false,9,176,40,82,77,6,16
2833735,scikit-learn/scikit-learn,python,2768,1390120251,1391429437,1391429437,21819,21819,github,false,false,false,23,4,3,3,3,0,6,0,4,0,0,2,2,1,0,0,0,0,2,2,1,0,0,19,0,19,0,13.854186326374462,0.32595592072066604,9,peter.prettenhofer@gmail.com,sklearn/cluster/mean_shift_.py|sklearn/cluster/mean_shift_.py|doc/modules/clustering.rst,9,0.023195876288659795,0,0,false,MRG: Update to Mean Shift clustering documentation Nothing substantial just clarified how the algorithm works and what is expected (ie the bandwidth parameter),,1565,0.7789137380191693,0.3118556701030928,35926,439.45888771363354,34.68240271669543,110.31008183488282,2417,26,1251,130,travis,robertlayton,GaelVaroquaux,false,GaelVaroquaux,24,0.75,12,10,975,false,true,true,false,0,2,0,0,0,0,6
2833251,scikit-learn/scikit-learn,python,2767,1390107717,1390108978,1390108978,21,21,github,false,false,false,37,3,1,0,2,0,2,0,4,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,5.195612276147324,0.12224542616649071,10,peter.prettenhofer@gmail.com,.travis.yml,10,0.025839793281653745,0,1,false,TST: get travis working on Py26 and Py27 I had to rely on conda for Py26 Waiting for wheels for laterThis pull request is only to test on travis I will merge if everything is green,,1564,0.7787723785166241,0.31266149870801035,35926,439.45888771363354,34.68240271669543,110.31008183488282,2416,26,1251,121,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,39,0.7435897435897436,446,3,1427,true,true,false,false,9,172,37,82,75,5,8
2832801,scikit-learn/scikit-learn,python,2766,1390099828,1390103763,1390103763,65,65,github,false,false,false,5,3,1,0,11,2,13,0,3,0,0,1,2,0,0,1,0,0,2,2,1,0,1,0,0,0,26,4.651853583366568,0.10945819130213151,7,peter.prettenhofer@gmail.com,.travis.yml,7,0.01818181818181818,0,3,false,TST: add Py26 on travis ,,1563,0.7786308381317978,0.3142857142857143,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,26,1250,121,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,38,0.7368421052631579,446,3,1426,true,true,false,false,9,166,34,82,73,4,0
2828721,scikit-learn/scikit-learn,python,2765,1390017243,1391653870,1391653870,27277,27277,github,false,false,false,65,33,6,28,42,1,71,0,8,0,0,2,8,2,0,0,3,0,8,11,6,0,0,153,236,643,372,39.79161993467334,0.9362978225811031,14,mathieu@mblondel.org,sklearn/learning_curve.py|sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py|sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py|sklearn/learning_curve.py|sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py|sklearn/tests/test_learning_curve.py,14,0.034739454094292806,0,11,true,[MRG] Validation curves  the second part of #2584Here is an example:pythonimport numpy as npfrom sklearndatasets import load_digitsfrom sklearnsvm import SVCfrom sklearnlearning_curve import validation_curveimport matplotlibpyplot as pltdigits  load_digits()X y  digitsdata digitstargetparam_range  nplogspace(-5 -1 5)train_scores test_scores  validation_curve(    SVC() X y param_namegamma param_rangeparam_range    cv10 scoringaccuracy n_jobs4)pltsemilogx(param_range train_scores)pltsemilogx(param_range test_scores)pltshow(),,1562,0.7784891165172856,0.3002481389578164,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,26,1249,133,travis,AlexanderFabisch,jakevdp,false,jakevdp,5,0.6,25,25,939,true,true,false,false,2,32,4,51,24,0,7
2825855,scikit-learn/scikit-learn,python,2763,1389990504,1391275640,1391275640,21418,21418,commit_sha_in_comments,false,false,false,21,24,1,6,11,0,17,0,3,0,0,1,35,1,0,0,0,0,35,35,35,0,0,37,0,128,108,3.2670107414123892,0.0768728453014816,9,peter.prettenhofer@gmail.com,sklearn/decomposition/dict_learning.py,9,0.022332506203473945,0,0,true,MRG: lots of pep8 lots of pep8 fixes all aroundready for mergeThis should lessen thishttps://jenkinsshiningpanda-cicom/scikit-learn/job/python-27-numpy-162-scipy-0101/violations/a bit hopefully,,1561,0.7783472133247918,0.3002481389578164,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,26,1249,135,travis,jaquesgrobler,jnothman,false,jnothman,74,0.9324324324324325,15,13,723,true,true,false,false,7,79,9,1,7,1,10
2821115,scikit-learn/scikit-learn,python,2762,1389929373,1389936567,1389936567,119,119,github,false,false,false,36,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.772534841749995,0.1122979346983166,0,,doc/install.rst,0,0.0,0,0,false,Update the install documentation to add a paragraph about installing a binary release in Fedora Fedora 20 ships binary packages of scikit-learn for python 27 and python 33 This added paragraph explains how to install them,,1560,0.7782051282051282,0.30303030303030304,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,25,1248,122,travis,sergiopasra,mblondel,false,mblondel,2,1.0,1,0,929,false,false,false,false,0,0,0,0,0,0,13
2821051,scikit-learn/scikit-learn,python,2761,1389928800,1389936654,1389936654,130,130,github,false,false,false,6,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.9333711925405845,0.11608242043118018,5,ralf.gommers@googlemail.com,doc/about.rst,5,0.012626262626262626,0,0,false,[MRG][DOC] Add thanks for infrastructure supporters ,,1559,0.7780628608082104,0.30303030303030304,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,25,1248,121,travis,ogrisel,mblondel,false,mblondel,52,0.8076923076923077,861,123,1695,true,true,true,true,24,301,39,147,58,5,16
2820696,scikit-learn/scikit-learn,python,2760,1389925767,1390084609,1390084609,2647,2647,github,false,false,false,48,2,1,2,4,0,6,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,29,0,53,9.058674749710036,0.21315097725367552,31,peter.prettenhofer@gmail.com,sklearn/metrics/tests/test_score_objects.py|sklearn/utils/testing.py,21,0.05303030303030303,1,4,false,[MRG] Check error message with regex Use nosetoolsassert_raises_regexp as suggested by @ogrisel [here](https://githubcom/scikit-learn/scikit-learn/pull/2736/files#r8889008)On the one hand we should not check for the exact error message on the other hand checking for small substrings might not be accurate enough Checking for patterns seems to be a good compromise,,1558,0.7779204107830552,0.30303030303030304,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,25,1248,122,travis,AlexanderFabisch,GaelVaroquaux,false,GaelVaroquaux,4,0.5,25,25,938,true,true,true,false,2,32,3,50,24,0,13
2818135,scikit-learn/scikit-learn,python,2759,1389904600,,1389922988,306,,unknown,false,true,false,134,57,36,1,3,13,17,11,8,0,0,9,11,9,0,0,1,0,11,12,11,0,0,1755,380,2527,563,319.2293880070048,7.511479079154537,43,peter.prettenhofer@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/feature_selection/rfe.py|sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/metrics/scorer.py|sklearn/tests/test_grid_search.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/cross_validation.py|sklearn/learning_curve.py|sklearn/tests/test_learning_curve.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/feature_selection/rfe.py|sklearn/cross_validation.py|sklearn/learning_curve.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/metrics/tests/test_score_objects.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/feature_selection/rfe.py|sklearn/learning_curve.py|sklearn/metrics/scorer.py|sklearn/grid_search.py|sklearn/grid_search.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/metrics/tests/test_score_objects.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py,22,0.03291139240506329,0,24,false,[WIP] Multiple-metric grid search This PR brings multiple-metric grid search This is important for finding the best-tuned estimator on a per metric basis *without* redoing the grid / randomized search from scratch for each metric Highlights:* No public API has been broken* Some parts are actually *simplified** Updates cross_val_score so as to support lists as scoring parameter In this case a 2d array of shape (n_scoring n_folds) is returned instead of a 1d array of shape (n_folds)* Adds multiple metric support to GridSearchCV and RandomizedSearchCV* Add cross_val_report for obtaining training scores/times in addition to test scores* Introduces _evaluate_scorers for computing several scorers without recomputing the predictions every time* Fixes #2588 (regressors can now be used with metrics that require need_thresholdTrueTagging this PR with the v015 milestone,,1557,0.7784200385356455,0.3037974683544304,35926,438.9021878305406,34.68240271669543,110.36575182319213,2416,25,1248,120,travis,mblondel,jnothman,false,,27,0.7777777777777778,318,32,1388,true,true,true,false,7,122,9,16,44,1,10
2809451,scikit-learn/scikit-learn,python,2756,1389811774,1389814249,1389814249,41,41,github,false,false,false,62,2,1,0,2,0,2,0,1,0,0,6,7,6,0,0,0,0,7,7,6,0,0,84,23,84,23,26.507973379030943,0.623743232757774,9,peter.prettenhofer@gmail.com,sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_parallel.py,8,0.010582010582010581,0,0,false,[MRG] FIX #1565: fix race condition in parallel pre-dispatch by upgrading joblib I fixed a race condition upstream in joblib  This should fix potential crash of cross_val_score and GridSearchCV with the [Parallel] pool seems closed and ValueError: generator already executing error messages as reported in #1565If no one objects I will merge this fix as soon as travis is green,,1556,0.7782776349614395,0.31746031746031744,35923,439.6626116972413,34.65746179328007,110.59766723269216,2415,24,1247,122,travis,ogrisel,ogrisel,true,ogrisel,51,0.803921568627451,859,123,1694,true,true,false,false,23,313,41,150,58,5,16
2801183,scikit-learn/scikit-learn,python,2754,1389719029,1389749301,1389749301,504,504,github,false,false,false,6,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.5396363263909985,0.10794066889239946,8,peter.prettenhofer@gmail.com,sklearn/feature_extraction/text.py,8,0.021333333333333333,0,0,false,Fixed documentation typo Small documentation fix,,1555,0.7781350482315113,0.3253333333333333,35749,441.8025679039973,34.82614898318834,111.13597583149179,2415,24,1246,126,travis,eltermann,larsmans,false,larsmans,1,1.0,7,5,1093,false,false,false,false,0,0,1,0,0,0,9
2800447,scikit-learn/scikit-learn,python,2753,1389710566,1389711392,1389711392,13,13,github,false,false,false,31,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.082858632629462,0.09707969383216127,32,peter@datarobot016.(none),sklearn/ensemble/gradient_boosting.py,32,0.0855614973262032,0,0,false,fix: remaining time not currectly computed After a last minute change in #2570 I forgot to change the way the remaining time is computed in the VerboseReporter Adresses issue #2740 ,,1554,0.777992277992278,0.32620320855614976,35749,441.8025679039973,34.82614898318834,111.13597583149179,2415,24,1246,126,travis,pprett,pprett,true,pprett,43,0.8604651162790697,127,29,1624,true,true,false,false,4,98,7,30,10,1,9
2798671,scikit-learn/scikit-learn,python,2750,1389680801,1392149306,1392149306,41141,41141,commits_in_master,false,false,false,178,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,3.947491476348199,0.09386070011584792,5,peter.prettenhofer@gmail.com,sklearn/base.py,5,0.013477088948787063,0,0,false,FIX remove confusing BaseEstimator__str__ I propose that str(some_estimator) and repr(some_estimator) should produce the same stringExamples of their difference:python from sklearncluster import SpectralClustering from sklearnpipeline import make_pipeline make_pipeline(SpectralClustering())Pipeline(steps[(spectralclustering SpectralClustering(affinityrbf assign_labelskmeans coef01 degree3          eigen_solverNone eigen_tol00 gamma10 kernel_paramsNone          n_clusters8 n_init10 n_neighbors10 random_stateNone))]) print(make_pipeline(SpectralClustering()))Pipeline(spectralclusteringSpectralClustering(affinityrbf assign_labelskmeans coef01 degree3          eigen_solverNone eigen_tol00 gamma10 kernel_paramsNone          n_clusters8 n_init10 n_neighbors10 random_stateNone)     spectralclustering__affinityrbf     spectralclustering__assign_labelskmeans spectralclustering__coef01     spectralclustering__degree3 spectralclustering__eigen_solverNone     spectralclustering__eigen_tol00 spectralclustering__gamma10     spectralclustering__kernel_paramsNone     spectralclustering__n_clusters8 spectralclustering__n_init10     spectralclustering__n_neighbors10     spectralclustering__random_stateNone)Note that str (ie print):* includes deep parameter settings which is redundant where the name of the sub-estimator is clear and it also inherits from BaseEstimator however these are not valid constructor parameters* removes quotes from around string-valued parameters (eg affinityrbf) making it almost-but-not-quite valid construction code* If a parameter is a numpy array there are more substantial differences due to the differences in numpys (and scipysparses) __str__ and __repr__ implementationsI think that at a minimum we should use repr for string parameters but Im not altogether convinced that there should be a difference between str and repr at all for estimators ,,1553,0.7778493238892467,0.3288409703504043,35749,441.49486698928644,34.79817617276008,111.10800302106352,2415,24,1246,140,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,61,0.6065573770491803,21,1,1722,true,true,false,false,13,216,18,175,54,2,9
2797275,scikit-learn/scikit-learn,python,2747,1389665033,1390788306,1390788306,18721,18721,github,false,true,false,139,20,15,15,71,0,86,0,9,0,0,5,5,5,0,0,0,0,5,5,5,0,0,1769,0,1906,0,70.49038841531808,1.6760689527750332,72,peter.prettenhofer@gmail.com,benchmarks/bench_covertype.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,66,0.07547169811320754,3,57,false,[MRG] faster sorting in trees random forests almost 2× as fast Changed the heapsort in the tree learners into a quicksort and gave it cache-friendlier data access Speeds up RF longer almost two-fold In fact profiling with @fabianps yep tool show the time taken by sort to go down from 65% to 10% of total running time in the covertype benchmarkThis is taking longer than I thought but I figured I should at least show @glouppe and @pprett what Ive got so farTODO:* more benchmarks esp on a denser dataset than covertype (sparse data is easy :)* make tests pass* clean up code* filter out the cruft* decide on the final algorithm: quicksort takes O(n²) time in the worst case which can be avoided by [introsort](https://enwikipediaorg/wiki/Introsort) at the expense of more code,,1551,0.778207607994842,0.3288409703504043,35751,441.47016866661073,34.79622947609857,111.10178736259125,2415,24,1245,131,travis,larsmans,larsmans,true,larsmans,96,0.7291666666666666,130,38,1275,true,true,false,false,27,148,25,83,66,4,17
2795617,scikit-learn/scikit-learn,python,2745,1389650033,1389728624,1389728624,1309,1309,github,false,false,false,121,11,6,4,26,0,30,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,78,0,456,0,25.148155041402617,0.597954456373918,53,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,50,0.13440860215053763,0,13,false,[MRG] Optimize mean square criterion The mse criterion could be simplified and optimized The main idea is to avoid an online algorithm to compute the meanThe speedup is particularly good with binary input features (because we have to made many call to the update method or iterate in the for loop) as in the following script pythonfrom __future__ import division print_functionfrom functools import partialfrom timeit import timeitimport numpy as npfrom sklearntree import DecisionTreeRegressornprandomseed(0)X  nprandomrandint(0 2 size(10000 500))y  nprandomuniform(size(10000 50))estimator  DecisionTreeRegressor(random_state0)chrono  timeit(partial(estimatorfit XX yy) number1)print(chrono  {}format(chrono))The results are on my boxthis branch  973273801804 smaster  170945119858 s,,1550,0.7780645161290323,0.3279569892473118,35751,441.47016866661073,34.79622947609857,111.10178736259125,2415,24,1245,126,travis,arjoly,glouppe,false,glouppe,44,0.8181818181818182,22,24,755,true,true,true,true,6,117,25,83,13,0,7
2794877,scikit-learn/scikit-learn,python,2744,1389643941,1389646270,1389646270,38,38,github,false,false,false,9,2,2,0,3,0,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,42,0,42,8.648709521339786,0.20564181096632472,13,peter.prettenhofer@gmail.com,sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py,13,0.03513513513513514,0,1,false,TST Gini is equivalent to mse in binary classification ,,1549,0.7779212395093609,0.32972972972972975,35751,440.04363514307295,34.76825823053901,111.04584487147213,2415,24,1245,123,travis,arjoly,arjoly,true,arjoly,43,0.813953488372093,22,24,755,true,true,false,false,6,116,22,83,12,0,17
2793063,scikit-learn/scikit-learn,python,2743,1389626292,1389645322,1389645322,317,317,github,false,false,false,22,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.217055712503534,0.10026955467751228,36,peter.prettenhofer@gmail.com,doc/whats_new.rst,36,0.09574468085106383,0,0,false,Small documentation fix: from CMS to VCS GIT is better defined as a version control system rather than a content management system,,1548,0.7777777777777778,0.324468085106383,35749,440.0682536574449,34.77020336233182,111.052057400207,2415,24,1245,124,travis,eltermann,mblondel,false,mblondel,0,0,7,5,1092,false,false,false,false,0,0,0,0,0,0,7
2788666,scikit-learn/scikit-learn,python,2742,1389546240,1389575235,1389575235,483,483,github,false,false,false,35,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.4509422718288425,0.10583074017001451,7,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py,7,0.01856763925729443,0,0,false,Changed assert_array_equal() in Line 45 and 46 to assert_array_almost_equal Changed assert_array_equal() in Line 45 and 46 to assert_array_almost_equal(decimal5) This has fixed the AssertionError which occurs during the installation testThis is related to issue #2741,,1547,0.7776341305753071,0.32360742705570295,35749,440.0682536574449,34.77020336233182,111.052057400207,2415,24,1244,124,travis,ai8rahim,agramfort,false,agramfort,0,0,2,5,1031,false,false,false,false,0,2,0,0,0,0,7
2783710,scikit-learn/scikit-learn,python,2739,1389428054,1389487913,1389487913,997,997,merged_in_comments,false,false,false,8,2,1,3,4,0,7,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,10,0,16,31,4.558364620010572,0.10838452052138105,6,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py,6,0.01634877384196185,0,0,false,Add option to restrict LassoCV to positive-only coefficients ,,1546,0.777490297542044,0.33242506811989103,35740,440.1790710688304,34.77895914941242,111.0800223838836,2415,24,1243,128,travis,bwignall,agramfort,false,agramfort,0,0,8,16,103,false,true,false,false,0,0,0,0,1,0,7
2755797,scikit-learn/scikit-learn,python,2736,1389321887,,1389858861,8949,,unknown,false,false,false,45,20,2,79,30,0,109,0,6,0,0,3,9,3,0,0,0,0,9,9,9,0,0,246,0,998,81,21.968259982688195,0.5223406908829468,8,shoyer@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/learning_curve.py,4,0.008287292817679558,0,11,false,[MRG] Refactor CV and grid search While implementing #2701 I have seen some duplicate code in grid_search and cross_validation Before I will implement the rest of issue #2584 I would like to clean that up a little bitTodo:- [x] merge cross_validationcross_val_score and grid_searchfit_grid_point,,1544,0.7784974093264249,0.3370165745856354,35740,440.1790710688304,34.77895914941242,111.0800223838836,2415,24,1241,130,travis,AlexanderFabisch,mblondel,false,,3,0.6666666666666666,25,25,931,true,true,true,false,2,24,2,21,6,0,24
2771186,scikit-learn/scikit-learn,python,2734,1389276564,1389466868,1389466868,3171,3171,github,false,false,false,20,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.024449897100453,0.09568959677879878,4,shoyer@gmail.com,sklearn/cross_validation.py,4,0.0110803324099723,0,1,false,Wrong format specifier used when formatting exception message A trivial fix for a broken format specifier in the cross-validation code,,1543,0.778353856124433,0.3379501385041551,35740,440.1790710688304,34.77895914941242,111.0800223838836,2415,24,1241,128,travis,eloj,larsmans,false,larsmans,0,0,8,18,900,false,false,false,false,0,1,0,0,0,0,3
2743240,scikit-learn/scikit-learn,python,2732,1389255775,1389888906,1389888906,10552,10552,github,false,false,false,66,12,1,19,84,0,103,0,7,0,0,5,6,5,0,0,0,0,6,6,6,0,0,495,0,2160,36,13.004480575994215,0.3092083475883676,64,peter.prettenhofer@gmail.com,sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,59,0.06128133704735376,0,21,false,[MRG] ENH/FIX Change Tree underlying data structure * avoid segfault (#2726) by setting base on numpy arrays* A struct array for tree nodes and values array (size indeterminate at compile time) are the only underlying structure so each node is locally grouped memory and joblib dumps will save only two numpy files per tree* predict uses the value arrays take method reducing code repetition,,1541,0.7787151200519143,0.3398328690807799,35740,440.1790710688304,34.77895914941242,111.0800223838836,2415,24,1241,131,travis,jnothman,arjoly,false,arjoly,60,0.6,21,1,1717,true,true,false,true,12,179,14,137,53,1,110
2742761,scikit-learn/scikit-learn,python,2731,1389210095,1389459361,1389459361,4154,4154,merged_in_comments,false,false,false,46,2,1,1,7,0,8,0,5,0,0,5,5,3,0,0,0,0,5,5,3,0,0,43,0,45,0,22.772100643345517,0.5426072219746103,39,rmcgibbo@gmail.com,doc/modules/hmm.rst|doc/whats_new.rst|examples/applications/plot_hmm_stock_analysis.py|examples/plot_hmm_sampling.py|sklearn/hmm.py,33,0.0,0,0,false,MRG: deprecate HMMs For issue #2646regarding [this thread](http://sourceforgenet/mailarchive/messagephpmsg_id31726218) among manyDecided to just do it since its standing stillIf Ive missed anything let me know This does the job in terms of examples docs API docs and of course instantiating any kind of HMM ,,1540,0.7785714285714286,0.34173669467787116,35522,439.2489161646304,34.76718653228985,110.97348122290411,2415,24,1240,130,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,73,0.9315068493150684,15,13,714,true,true,false,false,7,103,11,18,9,0,16
2766144,scikit-learn/scikit-learn,python,2729,1389208538,,1389794693,9769,,unknown,false,false,false,163,1,1,0,14,0,14,0,5,2,0,1,3,3,0,0,2,0,1,3,3,0,0,68,45,68,45,13.74999481658316,0.32763140421570286,5,peter.prettenhofer@gmail.com,sklearn/base.py|sklearn/utils/safe_warnings.py|sklearn/utils/tests/test_warnings.py,5,0.0,0,3,false,[RFC] make BaseEstimatorget_params and clone thread-safe by default Modifying the configuration warnings filters of the Python standard library and using warningscatch_warnings are unfortunately not thread-safe operationsHowever our get_params implementation needs to record deprecation warning when introspecting estimator parameters to avoid including them in the model description rendering the sklearnbaseclone function thread-unsafeThis problem has caused the recent random Jenkins failures reported in #2721 (since random forests now use threads internally when n_jobs  1) The specific issue reported in #2721 has been fixed by making RF models call clone only in the sequential section of the code However I think that get_params should be thread safe by default to avoid very confusing behaviors and other hard to debug crash to our usersHere is a proposal to make this thread-safe by default by using a threadingRLock I am not sure this is the best solution but I thought it would be more constructive to discuss this issue with a working possible solution,,1538,0.7795838751625488,0.3446327683615819,35522,440.6001914306627,34.76718653228985,111.02978435898879,2415,24,1240,130,travis,ogrisel,ogrisel,true,,50,0.82,855,123,1687,true,true,false,false,22,335,48,148,63,6,11
2757897,scikit-learn/scikit-learn,python,2724,1389102283,1389103438,1389103438,19,19,github,false,false,false,27,1,1,0,3,0,3,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,29,4,29,4,8.878065929278327,0.2124927118440258,9,olivier.grisel@ensta.org,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,9,0.02601156069364162,1,1,false,[FIX] Pre-initialize all trees before dispatching This should help fixing #2721 by pre-initializing all trees before dispatching and hopefully avoid the race condition in get_params CC: @ogrisel,,1537,0.7794404684450228,0.3468208092485549,35414,438.6683232619868,34.534364940419046,110.71892471903767,2414,24,1239,123,travis,glouppe,glouppe,true,glouppe,47,0.9574468085106383,124,26,1153,true,true,false,false,8,73,19,48,10,0,8
2753458,scikit-learn/scikit-learn,python,2722,1389036420,,1396545934,125158,,unknown,false,false,false,8,1,1,0,2,0,2,0,2,0,0,8,8,8,0,0,0,0,8,8,8,0,0,75,0,75,0,26.169390456437032,0.6263625414895888,2,larsmans@users.noreply.github.com,sklearn/svm/base.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c,2,0.0,0,0,false,MRG remove _label attribute from SVC Fixes #1573,,1536,0.7799479166666666,0.34782608695652173,35414,438.6683232619868,34.534364940419046,110.71892471903767,2414,24,1238,164,travis,amueller,larsmans,false,,194,0.8608247422680413,796,38,1172,true,true,true,true,13,190,15,58,71,2,9742
2752702,scikit-learn/scikit-learn,python,2720,1389026079,1389062966,1389062966,614,614,github,false,false,false,63,1,1,0,4,0,4,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,0,13,0,4.560553711760804,0.10915641556324773,0,,sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx,0,0.0,0,0,false,MRG define fused types in chi2 for less code Fixes #1522This causes the C code to only contain two versions of the function one for float and one for double not 8 as before Actually the decrease in file size is only from 22700 to 20708 so Im not entirely convinced it is worth itThe functionality of the code doesnt change,,1535,0.7798045602605863,0.3519061583577713,35414,439.4872084486361,34.534364940419046,110.71892471903767,2414,23,1238,124,travis,amueller,ogrisel,false,ogrisel,193,0.8601036269430051,796,38,1172,true,true,true,false,13,186,14,58,69,2,8
2752636,scikit-learn/scikit-learn,python,2719,1389025050,1389028915,1389028915,64,64,github,false,false,false,20,1,1,0,6,0,6,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,39,0,39,4.298260093620028,0.10287844297682072,22,vanderplas@astro.washington.edu,sklearn/tests/test_common.py,22,0.06451612903225806,0,1,false,FIX remove description from test generators As they are not thread-safe Also because we cant have nice thingsSee #2716,,1534,0.7796610169491526,0.3519061583577713,35414,439.4872084486361,34.534364940419046,110.71892471903767,2414,23,1238,123,travis,amueller,arjoly,false,arjoly,192,0.859375,796,38,1172,true,true,false,true,13,185,13,58,69,2,9
2736669,scikit-learn/scikit-learn,python,2717,1389006695,1389224477,1389224477,3629,3629,github,false,false,false,9,4,1,31,27,0,58,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,37,13,54,48,9.566230199483593,0.22794144794139787,57,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,53,0.15451895043731778,0,15,false,Testing log_loss and hinge_loss under THRESHOLDED_METRICS Fixes Issue #2637,,1533,0.7795172863666014,0.3498542274052478,35522,439.2489161646304,34.76718653228985,110.97348122290411,2414,23,1238,125,travis,Manoj-Kumar-S,arjoly,false,arjoly,5,0.4,22,15,566,true,false,false,false,0,124,6,52,3,0,1
2736050,scikit-learn/scikit-learn,python,2715,1388956891,1389466788,1389466788,8498,8498,github,false,true,false,67,13,10,11,34,0,45,0,6,0,0,6,8,6,0,0,0,0,8,8,7,0,1,3637,4,3637,10,98.25988234386666,2.346335295363674,64,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_utils.c|sklearn/tree/_utils.pyx,53,0.05865102639296188,1,11,false,[MRG] fix malloc errors in trees There were still a few unchecked mallocs and constructs were the checks would actually cause double free I also fixed some other minor stuff See commit log for detailsI was actually trying to optimize the sorting by implementing three-way quicksort but when I tried to cache some stuff in a new mallocd array the tree code started crashingPing @glouppe,,1532,0.779373368146214,0.3489736070381232,35399,439.67343710274304,34.54899855928134,110.76584084296167,2413,22,1237,130,travis,larsmans,larsmans,true,larsmans,95,0.7263157894736842,130,38,1267,true,true,false,false,29,125,23,69,69,2,7
2747531,scikit-learn/scikit-learn,python,2714,1388905556,1388923846,1388923846,304,304,github,false,false,false,20,2,1,0,2,0,2,0,2,0,0,1,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,4.806925665423629,0.11478397363551397,0,,COPYING,0,0.0,0,0,false,Updating copyright year to 2014 and changing striving to thriving Updating copyright year to 2014 and changing striving to thriving,,1531,0.7792292619203135,0.3480825958702065,35399,439.67343710274304,34.54899855928134,110.76584084296167,2413,22,1237,121,travis,amormachine,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,321,true,true,false,false,0,0,0,0,1,0,8
2738147,scikit-learn/scikit-learn,python,2712,1388753465,1388754644,1388754644,19,19,github,false,false,false,18,2,1,0,2,0,2,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,16,0,20,0,13.252576638752739,0.3164559551606064,11,mathieu@mblondel.org,doc/modules/preprocessing.rst|sklearn/preprocessing/imputation.py|sklearn/utils/validation.py,9,0.0058997050147492625,0,0,true,FIX: dont force finite in imputation This should fix Jenkins sorry for that Waiting for Travis green light ,,1530,0.7790849673202614,0.3421828908554572,35396,439.71070177421177,34.5519267713866,110.77522883941688,2413,20,1235,121,travis,glouppe,glouppe,true,glouppe,46,0.9565217391304348,122,26,1149,true,true,false,false,8,65,16,44,11,0,6
2734844,scikit-learn/scikit-learn,python,2710,1388705771,1388705987,1388705987,3,3,github,false,false,false,8,1,1,0,0,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.250080511029528,0.10148894732436849,0,,sklearn/datasets/base.py,0,0.0,0,0,false,DOC Remove target_names from boston dataset object description ,,1529,0.7789404839764552,0.3483483483483483,35399,438.82595553546713,34.54899855928134,110.53984575835476,2413,20,1234,124,travis,eshilts,glouppe,false,glouppe,1,1.0,3,5,1008,false,true,false,false,0,1,1,0,0,0,-1
2732773,scikit-learn/scikit-learn,python,2708,1388685582,1388752211,1388752212,1110,1110,github,false,false,false,35,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.963190354737568,0.11851751117786655,0,,doc/modules/dp-derivation.rst,0,0.0,0,0,false,Update dp-derivationrst The subscript has been mislabeledThe code use this correct formulae and can be seen in the thesis --Variational Inference for Dirichlet Process Mixtures David Blei Michael Jordan Bayesian Analysis 2006 as well,,1528,0.7787958115183246,0.34626865671641793,35399,438.82595553546713,34.54899855928134,110.53984575835476,2413,20,1234,124,travis,likang7,agramfort,false,agramfort,2,1.0,5,4,484,false,true,false,false,0,0,3,0,0,0,26
2732754,scikit-learn/scikit-learn,python,2707,1388685341,,1389004479,5318,,unknown,false,false,false,134,5,5,0,4,0,4,0,5,2,0,3,5,3,0,0,2,0,3,5,3,0,0,342,0,342,0,27.306793785758437,0.6520671194176765,0,,sklearn/decomposition/sparse_filtering.py|examples/decomposition/plot_sparse_filtering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/sparse_filtering.py|examples/decomposition/plot_sparse_filtering.py|sklearn/decomposition/sparse_filtering.py,0,0.0,0,2,false,[WIP] Sparse filtering This implements the sparse-filtering algorithm by Ngiam et al (http://csstanfordedu/~jngiam/papers/NgiamKohChenBhaskarNg2011pdf) Sparse filtering is an unsupervised feature learning In contrast to most other feature learning methods sparse filtering does not explicitly construct a model of the data distribution but aims at generating features which are sparsely activated on the training data It is relatively efficient and has the number of features to be learned as the only hyperparameterThe implementation is based on the authors Matlab code in their supplementary material (http://csstanfordedu/~jngiam/papers/NgiamKohChenBhaskarNg2011_Supplementarypdf) Hence the slightly un-pythonic naming but I have preferred consistency with the authors code I am not sure if decomposition is the right module for this algorithm maybe neural_network would be more appropriateI welcome any feedback on the code and if this algorithm is interesting for sklearn in general ,,1527,0.779305828421742,0.34626865671641793,35399,438.82595553546713,34.54899855928134,110.53984575835476,2413,20,1234,123,travis,jmetzen,jmetzen,true,,3,0.6666666666666666,9,2,815,false,true,false,false,0,0,1,0,0,0,468
2732719,scikit-learn/scikit-learn,python,2706,1388684868,1388752265,1388752265,1123,1123,github,false,false,false,7,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.2471681059619995,0.10141940113019203,0,,sklearn/mixture/dpgmm.py,0,0.0,0,0,false,Update dpgmmpy The document has been moved,,1526,0.7791612057667103,0.34626865671641793,35399,438.82595553546713,34.54899855928134,110.53984575835476,2413,20,1234,122,travis,likang7,agramfort,false,agramfort,1,1.0,5,4,484,false,true,false,false,0,0,2,0,0,0,8
2732683,scikit-learn/scikit-learn,python,2705,1388684155,1388684497,1388684497,5,5,commit_sha_in_comments,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.181835957544267,0.09985931516190319,0,,sklearn/mixture/dpgmm.py,0,0.0,0,0,false,Update dpgmmpy The document has been moved,,1525,0.779016393442623,0.34626865671641793,35399,438.82595553546713,34.54899855928134,110.53984575835476,2413,20,1234,122,travis,likang7,likang7,true,likang7,0,0,5,4,484,false,true,false,false,0,0,0,0,0,0,-1
2729278,scikit-learn/scikit-learn,python,2702,1388609798,1388743810,1388743810,2233,2233,github,false,false,false,42,4,2,7,7,0,14,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,109,39,109,18.103811290112468,0.4323063399915359,6,mathieu@mblondel.org,sklearn/preprocessing/imputation.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/tests/test_imputation.py,5,0.014925373134328358,1,4,false,[MRG] Fix #2697 - Copy parameter in Imputer This PR fixes #2697 It ensures that the same buffers are used when copyFalse and that data is indeed not dupplicated The corresponding test has been rewritten to check for all cases Ping @jnothman,,1524,0.7788713910761155,0.34626865671641793,35399,438.82595553546713,34.54899855928134,110.53984575835476,2412,20,1233,124,travis,glouppe,GaelVaroquaux,false,GaelVaroquaux,45,0.9555555555555556,122,26,1147,true,true,false,false,7,64,13,64,10,0,7
2728444,scikit-learn/scikit-learn,python,2701,1388583358,,1389238236,10914,,unknown,false,true,false,43,43,6,67,55,0,122,0,8,2,0,2,7,3,0,0,3,0,5,8,5,0,0,406,0,2041,338,36.23398365572575,0.8652037076965821,1,larsmans@gmail.com,sklearn/grid_search.py|sklearn/learning_curve.py|examples/plot_learning_curve.py|sklearn/learning_curve.py|sklearn/learning_curve.py|sklearn/grid_search.py|examples/plot_learning_curve.py|sklearn/learning_curve.py,1,0.0,0,11,false,[MRG] Learning curves This is the pull request for the function learning_curve that has been proposed in issue #2584 I would like to get feedback on the interface and codeHere is an example with naive base on the digits dataset:[learning curve](http://wwwinformatikuni-bremende/~afabisch/files/nb_digits_learning_curvepng),,1523,0.7793827971109653,0.3473053892215569,35405,440.86993362519416,34.627877418443724,110.4928682389493,2412,20,1233,125,travis,AlexanderFabisch,amueller,false,,2,1.0,25,25,923,false,true,true,false,1,3,1,0,2,0,33
2720849,scikit-learn/scikit-learn,python,2700,1388423424,1388431071,1388431071,127,127,github,false,false,false,31,2,2,0,3,0,3,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1380,0,1380,8.693233702554334,0.20759013978683344,18,vanderplas@astro.washington.edu,sklearn/tests/test_common.py|sklearn/tests/test_common.py,18,0.05405405405405406,0,2,false,MRG Use yield to generate common tests This is a slight improvement of the common tests Actually more should be refactored but I think this at least gives more meaningful errors,,1522,0.7792378449408672,0.3483483483483483,35398,439.7423583253291,34.63472512571332,110.65596926380022,2411,20,1231,121,travis,amueller,ogrisel,false,ogrisel,191,0.8586387434554974,793,38,1165,true,true,true,false,11,161,11,57,68,1,8
2720843,scikit-learn/scikit-learn,python,2699,1388423224,1389617130,1389617130,19898,19898,commit_sha_in_comments,false,false,false,10,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,19,0,19,4.464061858579269,0.10659959883135904,18,vanderplas@astro.washington.edu,sklearn/tests/test_common.py,18,0.05405405405405406,0,0,false,FIX simplify class_weightauto test assumtions A possible fix for #2698  ,,1521,0.7790927021696252,0.3483483483483483,35398,439.7423583253291,34.63472512571332,110.65596926380022,2411,20,1231,127,travis,jnothman,jnothman,true,jnothman,59,0.5932203389830508,21,1,1707,true,true,false,false,8,141,13,90,48,0,7
2711499,scikit-learn/scikit-learn,python,2694,1388184197,1405539898,1405539898,289261,289261,commits_in_master,false,false,false,129,9,1,11,37,0,48,0,6,0,0,3,7,3,0,0,0,0,7,7,7,0,0,57,0,166,285,13.397715015462953,0.3206461046101316,12,shoyer@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/utils/__init__.py,9,0.011494252873563218,0,14,true,MRG allow y to be a list in GridSearchCV cross_val_score and train_test_split Fixes #2508This makes input validation consistent across the three cross-validating functions and less intrusiveI am not entirely sure this is the right thing to do as it has some slight backward-incompatibilities When X or y was given as a list to train_test_split it was converted to an array before but is no longer In the case of X this could be considered a bug-fix (for lists of strings) There is an off-chance that this breaks someones code though GridSearchCV now passes y though as a list without converting I think that is fine and very unlikely to break anyones code (as I think an estimator should handle exactly the same kind of input GridSearchCV gets),,1520,0.7789473684210526,0.3275862068965517,35023,438.9401250606744,34.77714644662079,110.09907774890786,2410,20,1228,201,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,190,0.8578947368421053,793,38,1162,true,true,true,false,10,148,9,57,68,1,68
2711040,scikit-learn/scikit-learn,python,2693,1388177749,1388205523,1388205523,462,462,merged_in_comments,false,false,false,18,2,2,1,4,0,5,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,54,0,54,0,26.497471836641864,0.6341611924250219,6,joel.nothman@gmail.com,sklearn/grid_search.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/ridge.py|sklearn/grid_search.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/ridge.py,6,0.0,0,0,true,Documentation for cross validation Added documentation for attributes of GridSearchCV RidgeCV and LassoCV Fixed typo in dual_gap documentation,,1519,0.7788018433179723,0.3285302593659942,35022,438.95265832905034,34.778139455199586,110.10222146079607,2410,20,1228,123,travis,eshilts,larsmans,false,larsmans,0,0,3,5,1002,false,true,false,false,0,0,0,0,0,0,7
2703492,scikit-learn/scikit-learn,python,2692,1388118336,1388206746,1388206746,1473,1473,commit_sha_in_comments,false,false,false,61,2,1,0,5,0,5,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,26,20,34,8.71556740036668,0.20858678538035552,11,mathieu@mblondel.org,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,8,0.0226628895184136,0,1,false,Do one run with MiniBatchKMeans and explicit centers * this will fix issue #2686* if n_init is not 1 it will be set to one and a warning will be printed* this commit will add a test case which checks for the warning* this commit will modify other test cases so that they do not cause this warning,,1518,0.7786561264822134,0.32011331444759206,35022,438.89555136771173,34.778139455199586,110.07366798012677,2410,20,1227,125,travis,AlexanderFabisch,larsmans,false,larsmans,1,1.0,25,25,917,false,true,true,false,1,1,0,0,1,0,489
2697635,scikit-learn/scikit-learn,python,2687,1387967442,,1445286843,955323,,unknown,false,false,false,10,4,3,4,9,0,13,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,18,84,22,98,22.396731873921127,0.5360153507000721,2,alexandre.gramfort@m4x.org,sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,2,0.005555555555555556,0,3,false,[MRG] GaussianProcess improve tests coverage and fix prediction in batches ,,1517,0.7791694133157547,0.3277777777777778,35023,437.0556491448477,34.69148845044685,109.72789309882077,2410,20,1226,381,travis,arpangarg,glouppe,false,,0,0,0,0,113,false,false,false,false,0,0,0,0,0,0,6
2695193,scikit-learn/scikit-learn,python,2685,1387903891,1387983594,1387983594,1328,1328,github,false,false,false,61,1,1,0,6,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,13,5,13,8.706841439338916,0.20837864621808727,3,olivier.grisel@ensta.org,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,3,0.008287292817679558,0,2,false,Fix OOB score calculation for non-contiguous targets OOB scores of RandomForestClassifier are incorrectly calculatedbecause of confusion between predicted labels and indices of a targetFor example when you label the digits dataset from 1 or greaternumber but 0 OOB score becomes nearly zeroThis patch will fix this error and a test is added in order to avoidregression,,1516,0.7790237467018469,0.3314917127071823,35023,437.0556491448477,34.69148845044685,109.72789309882077,2410,20,1225,124,travis,bicycle1885,glouppe,false,glouppe,0,0,23,3,898,false,true,false,false,0,0,0,0,0,0,7
2689512,scikit-learn/scikit-learn,python,2684,1387810722,1387812011,1387812011,21,21,github,false,false,false,2,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.68270951641766,0.11207006955760454,3,baptiste.lagarde@centraliens.net,doc/modules/feature_extraction.rst,3,0.008450704225352112,0,0,false,FIX: Typos ,,1515,0.7788778877887789,0.3380281690140845,35023,436.99854381406504,34.69148845044685,109.72789309882077,2410,19,1224,123,travis,blagarde,glouppe,false,glouppe,1,1.0,7,3,621,false,false,false,false,0,0,1,0,0,0,6
2685055,scikit-learn/scikit-learn,python,2683,1387691057,1387711866,1387711866,346,346,github,false,false,false,2,2,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.521571177657047,0.10821358690759986,0,,doc/modules/feature_extraction.rst,0,0.0,0,0,false,FIX: Typo ,,1514,0.7787318361955086,0.34277620396600567,35023,436.99854381406504,34.69148845044685,109.72789309882077,2410,19,1223,123,travis,blagarde,agramfort,false,agramfort,0,0,7,3,620,false,false,false,false,0,0,0,0,0,0,6
2679727,scikit-learn/scikit-learn,python,2681,1387578368,,1388374770,13273,,unknown,false,false,false,71,18,6,3,25,0,28,0,9,3,0,13,23,15,0,0,3,1,19,23,20,0,0,1632,1216,2568,1344,142.90658367864322,3.4201460963377532,40,peter.prettenhofer@gmail.com,doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_multiprocessing.py|sklearn/externals/joblib/format_stack.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/common.py|sklearn/externals/joblib/test/test_func_inspect.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_parallel.py|sklearn/externals/joblib/test/test_pool.py|sklearn/externals/joblib/__init__.py|sklearn/externals/joblib/_multiprocessing.py|sklearn/externals/joblib/format_stack.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/pool.py|sklearn/externals/joblib/test/common.py|sklearn/externals/joblib/test/test_func_inspect.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_parallel.py|sklearn/externals/joblib/test/test_pool.py|sklearn/ensemble/forest.py|doc/whats_new.rst,32,0.0,0,9,true,[MRG] Upgrade to Joblib 080a and use the threading backend for parallel fitting of forests This brings very nice memory improvements to fitting random forestsThe new memory mapping support should also make it possible to improve the memory usage of parallel grid search and cross validation although not as drastically because of the memory copy caused internally by the fancy indexing of the data with the train / test indices,,1513,0.7792465300727033,0.35,35023,436.99854381406504,34.69148845044685,109.72789309882077,2409,19,1221,124,travis,ogrisel,larsmans,false,,49,0.8367346938775511,851,123,1668,true,true,true,true,18,344,49,143,93,6,8
2674281,scikit-learn/scikit-learn,python,2679,1387504648,1418108424,1418108424,510062,510062,commits_in_master,false,false,false,151,50,23,40,56,4,100,0,8,0,0,26,26,21,0,0,0,0,26,26,21,0,0,840,1304,1344,1646,875.2331424628944,20.94672714527461,179,zehzinho@gmail.com,doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_common.py|sklearn/tests/test_cross_validation.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/model_selection/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/model_selection/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/classification.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/model_selection/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/whats_new.rst|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/whats_new.rst|examples/text/document_classification_20newsgroups.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/classification.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/utils/estimator_checks.py|benchmarks/bench_multilabel_metrics.py|doc/datasets/twenty_newsgroups.rst|doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|examples/model_selection/grid_search_digits.py|examples/text/document_classification_20newsgroups.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_sgd.py|sklearn/metrics/__init__.py|sklearn/metrics/classification.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_classification.py|sklearn/metrics/tests/test_common.py|sklearn/metrics/tests/test_score_objects.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_cross_validation.py|sklearn/utils/estimator_checks.py,62,0.0215633423180593,0,28,false,[MRG] Require explicit average arg for multiclass/label P/R/F metrics and scorers In order to avoid problems like #2094 and to avoid people unwittingly reporting weighted average this goes towards making average a required parameter for multiclass/multilabel precision recall f-score Closely related to #2676After a deprecation cycle we can turn the warning into an error or make macro/micro defaultThis PR also shards the builtin scorers to make the averaging explicit This avoids users getting binary behaviour when they shouldnt (cf #2094 where scoring isnt used) I think this is extra important because weighted F1 isnt especially common in the literature and having people report it without realising thats what it is is unhelpful to the applied ML community This helps IMO towards a more explicit and robust API for binary classification metrics (cf #2610)It also entails a deprecation procedure for scorers and more API there: public get_scorer and list_scorers,,1511,0.7796161482461945,0.3881401617250674,35023,436.99854381406504,34.69148845044685,109.72789309882077,2408,19,1220,250,travis,jnothman,jnothman,true,jnothman,58,0.5862068965517241,21,1,1696,true,true,false,false,8,131,12,94,44,0,16
2669789,scikit-learn/scikit-learn,python,2677,1387457471,,1388159480,11700,,unknown,false,false,false,86,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.518906149654189,0.10814980548559691,6,larsmans@gmail.com,sklearn/metrics/pairwise.py,6,0.01643835616438356,0,0,false,removed the type checking for the customer defined distance metric I asked question in stackoverflow about How to use a customer distance metric for KNeighboursRegressorlink: http://stackoverflowcom/q/20655287/807695Scikits builtin metrics dont support mixture types of datasetso I have to write a customer metric function But even Im using my defined metric function I still cannt use the KNeighboursRegressor because these two lines codes it force the input dataset to be pure float type  I think this is kind of bug so I removed these two lines,,1510,0.7801324503311259,0.40273972602739727,35023,436.99854381406504,34.69148845044685,109.72789309882077,2408,19,1220,124,travis,simomo,amueller,false,,0,0,0,1,741,false,true,false,false,0,0,0,0,0,0,11700
2665868,scikit-learn/scikit-learn,python,2676,1387446145,1390098240,1390098240,44201,44201,merged_in_comments,false,true,false,97,3,1,1,9,0,10,0,3,0,0,11,12,8,0,0,0,0,12,12,8,0,0,71,91,77,91,49.38443581893939,1.1819076815439806,47,zehzinho@gmail.com,doc/modules/classes.rst|doc/modules/cross_validation.rst|doc/modules/model_evaluation.rst|examples/grid_search_digits.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py,16,0.013736263736263736,0,0,false,[MRG] replace f1 scorer by explicit variants This makes the averaging options for P/R/F scorers clearer for users and avoids users getting binary behaviour when they shouldnt (cf #2094 where scoring isnt used) I think this is extra important because weighted F1 isnt especially common in the literature and having people report it without realising thats what it is is unhelpful to the applied ML community This helps IMO towards a more explicit and robust API for binary classification metrics (cf #2610)It also entails a deprecation procedure for scorers and more API there: get_scorer and list_scorers,,1509,0.7799867461895295,0.40384615384615385,35022,436.782593798184,34.66392553252241,109.70247273142596,2408,19,1220,132,travis,jnothman,jnothman,true,jnothman,57,0.5789473684210527,21,1,1696,true,true,false,false,8,128,11,93,35,0,1
2667804,scikit-learn/scikit-learn,python,2675,1387420715,,1388338221,15291,,unknown,false,false,false,26,4,2,6,9,0,15,0,6,0,0,8,8,5,0,0,0,0,8,8,5,0,0,139,45,196,56,46.01348553324424,1.1012323446691648,42,vanderplas@astro.washington.edu,sklearn/pipeline.py|sklearn/tests/test_pipeline.py|doc/modules/classes.rst|doc/modules/pipeline.rst|doc/whats_new.rst|examples/document_clustering.py|examples/feature_selection_pipeline.py|examples/linear_model/plot_polynomial_interpolation.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py,35,0.008403361344537815,0,1,false,[MRG] make_pipeline utility function Shorthand for the Pipeline constructor with automatic naming of stepsExamples and tutorial may need to be changed to show this syntax,,1508,0.7805039787798409,0.40896358543417366,35022,436.782593798184,34.66392553252241,109.70247273142596,2408,19,1219,124,travis,larsmans,amueller,false,,94,0.7340425531914894,129,38,1249,true,true,true,true,28,139,29,77,66,2,12
2665501,scikit-learn/scikit-learn,python,2674,1387418115,1387450543,1387450543,540,540,github,false,false,false,43,2,1,7,5,0,12,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,10,11,15,11,13.938343745136248,0.3335838349427242,33,peter.prettenhofer@gmail.com,doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,33,0.0,0,0,false,BUG: OneVsOneClassifier was broken with string labels Pretty nasty bug It comes from the fact that assigning 0 or 1 to a string array will create entries of 0 or 1 (due to a  silent cast) and then lead to an incorrect predict,,1507,0.7803583278035833,0.4101123595505618,35022,436.782593798184,34.66392553252241,109.70247273142596,2408,19,1219,119,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,37,0.7297297297297297,440,3,1395,true,true,false,false,5,130,37,54,63,3,14
2655014,scikit-learn/scikit-learn,python,2673,1387306704,1388167903,1388167903,14353,14353,github,false,false,false,35,13,13,2,5,0,7,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,148,0,148,0,64.49922522352087,1.5436481761618557,0,,sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.cpp,0,0.0,0,0,false,[MRG] update LibSVM to version 310 I cherry-picked the relevant patches from the [LibSVM GitHub repo](https://githubcom/cjlin1/libsvm) I stopped at 310 because 311 introduces a max_iter that seems to work a bit differently from our own,,1506,0.7802124833997344,0.34265734265734266,35022,436.782593798184,34.66392553252241,109.70247273142596,2408,20,1218,123,travis,larsmans,larsmans,true,larsmans,93,0.7311827956989247,129,38,1248,true,true,false,false,28,142,28,79,65,2,14
2650877,scikit-learn/scikit-learn,python,2672,1387250786,1388169639,1388169639,15314,15314,merged_in_comments,false,false,false,46,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.00551476120232,0.0958632531521412,0,,sklearn/neighbors/binary_tree.pxi,0,0.0,0,0,false,DOC: typo in binary_tree docs Small typo in the Binary Tree cython sourceIm doing a PR rather than just pushing the fix because Im not sure if I should re-generate the C code for this minor change or wait until a more major change happens,,1505,0.7800664451827243,0.2735674676524954,35022,436.782593798184,34.66392553252241,109.70247273142596,2407,20,1217,122,travis,jakevdp,amueller,false,amueller,41,0.8780487804878049,1203,0,950,true,true,false,true,0,55,11,25,5,1,13
2649929,scikit-learn/scikit-learn,python,2671,1387242171,1387250457,1387250457,138,138,github,false,false,false,6,2,1,0,7,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,9,6,9,8.933970784385586,0.21381496983286824,6,larsmans@users.noreply.github.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,3,0.0055658627087198514,0,2,false,fix f_oneway with ints address #2670,,1504,0.7799202127659575,0.274582560296846,35022,436.782593798184,34.66392553252241,109.70247273142596,2407,20,1217,115,travis,agramfort,GaelVaroquaux,false,GaelVaroquaux,37,0.8918918918918919,143,184,1475,true,true,true,false,8,54,20,30,18,0,11
2646844,scikit-learn/scikit-learn,python,2669,1387216550,1387240414,1387240414,397,397,github,false,false,false,4,1,1,0,3,0,3,0,5,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.820857184817032,0.11559320190887429,9,vijay@change.org,doc/testimonials/images/phimeca.png|doc/testimonials/testimonials.rst,9,0.016697588126159554,0,0,false,DOC: add phimeca testimonial ,,1503,0.7797737857618097,0.2764378478664193,35022,436.782593798184,34.66392553252241,109.70247273142596,2406,20,1217,115,travis,GaelVaroquaux,agramfort,false,agramfort,36,0.7222222222222222,439,3,1393,true,true,false,true,6,125,34,54,62,3,3
2641554,scikit-learn/scikit-learn,python,2667,1387122472,1387126825,1387126825,72,72,github,false,false,false,21,2,1,0,8,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,5,0,4.426756843994719,0.10614292478427666,6,nelle.varoquaux@gmail.com,sklearn/manifold/mds.py,6,0.011111111111111112,0,2,false,fixed bug in mdspy this error is raised in OSX 109 with float64: use npallclose make code cleaner and less buggy,,1502,0.7796271637816246,0.2833333333333333,34998,438.4250528601634,34.773415623749926,110.14915138007886,2406,20,1216,117,travis,AndreaCatalucci,GaelVaroquaux,false,GaelVaroquaux,0,0,7,7,1287,false,true,false,false,0,0,0,0,0,0,41
2635220,scikit-learn/scikit-learn,python,2664,1386979667,1387206625,1387206625,3782,3782,github,false,false,false,7,7,1,10,14,0,24,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,9,16,26,47,8.913736658495724,0.21372985077621082,10,mathieu@mblondel.org,sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py,10,0.01841620626151013,0,3,true,FIX : explained_variance_ratio_ in RandomizedPCA see #2663,,1501,0.7794803464357095,0.285451197053407,34998,438.4250528601634,34.773415623749926,110.14915138007886,2406,21,1214,116,travis,agramfort,ogrisel,false,ogrisel,36,0.8888888888888888,143,184,1472,true,true,true,true,8,46,17,18,17,1,12
2620264,scikit-learn/scikit-learn,python,2660,1386830422,1387207402,1387207402,6283,6283,github,false,false,false,41,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,55,0,55,0,4.371955440272087,0.10482891963820792,6,mathieu@mblondel.org,sklearn/cluster/k_means_.py,6,0.010948905109489052,0,0,false,MRG make minibatch k-means use mini-batches in prediction Fixes #2627Do you think I should benchmark thisI would imagine it is a bit slower but I ran into the issue before (I think I closed the issue at some point),,1500,0.7793333333333333,0.2956204379562044,34998,438.4250528601634,34.773415623749926,110.14915138007886,2403,21,1213,116,travis,amueller,jnothman,false,jnothman,189,0.8571428571428571,785,38,1147,true,true,false,false,8,93,6,55,59,1,7
2618939,scikit-learn/scikit-learn,python,2659,1386813179,1386825733,1386825733,209,209,github,false,false,false,19,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.335720848239266,0.10396005437534975,3,larsmans@gmail.com,sklearn/naive_bayes.py,3,0.005555555555555556,0,0,false,SPD: reuse variable in GaussianNB for speedup Small change which speeds up GaussianNB by about ~10% for large problems,,1499,0.7791861240827218,0.3,34998,438.4250528601634,34.773415623749926,110.14915138007886,2403,21,1212,116,travis,jakevdp,mblondel,false,mblondel,40,0.875,1198,0,945,true,true,false,false,0,47,10,24,4,1,30
2616906,scikit-learn/scikit-learn,python,2658,1386793643,1386841893,1386841893,804,804,commit_sha_in_comments,false,false,false,9,1,1,0,2,0,2,0,3,0,5,3,8,5,0,0,0,5,3,8,5,0,0,1429,708,1429,708,12.88544348174937,0.3089616356544633,12,rmcgibbo@gmail.com,doc/modules/classes.rst|doc/modules/hmm.rst|doc/unsupervised_learning.rst|examples/applications/plot_hmm_stock_analysis.py|examples/plot_hmm_sampling.py|sklearn/__init__.py|sklearn/hmm.py|sklearn/tests/test_hmm.py,8,0.001838235294117647,0,1,false,#2646 Removed hmm module its docs tests and examples ,,1498,0.7790387182910548,0.30514705882352944,34998,438.4250528601634,34.773415623749926,110.14915138007886,2402,21,1212,116,travis,Melevir,Melevir,true,Melevir,0,0,3,2,650,false,true,false,false,1,0,0,0,0,0,11
2614455,scikit-learn/scikit-learn,python,2657,1386767367,1401979502,1401979502,253535,253535,merged_in_comments,false,true,false,76,46,0,42,47,0,89,0,8,0,0,0,20,0,0,0,0,0,20,20,14,0,0,0,0,1401,1134,0,0.0,0,,,0,0.0,0,13,false,[WIP] deprecate sequences of sequences multilabel support Towards #2270* add warnings: most uses of the deprecated format are through type_of_target Its helper is_sequence_of_sequences triggers the warning A further warning applies to make_multilabel_classification* provide alternative binarizer sklearnpreprocessingMultiLabelBinarizer* fix documentationTODO: deprecate use of LabelBinarizer for multilabel data including label indicator matricesThis will require a bit of updating once the alternative sparse matrix support in is incorporated from #2458 and https://githubcom/jnothman/scikit-learn/tree/sparse_multi_metrics  or vice-versa,,1497,0.7788911155644622,0.3100917431192661,34998,438.4250528601634,34.773415623749926,110.14915138007886,2402,21,1212,187,travis,jnothman,ogrisel,false,ogrisel,56,0.5714285714285714,20,1,1688,true,true,false,false,8,102,10,86,16,0,47
2614373,scikit-learn/scikit-learn,python,2656,1386766264,1386767088,1386767088,13,13,github,false,false,false,37,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,23,0,23,0,4.430083571656154,0.10622264607051987,15,olivier.grisel@ensta.org,sklearn/metrics/metrics.py,15,0.027573529411764705,0,0,false,[MRG] FIX silence numpy 18 warning for using non integer In numpy 18 using a boolean for the axis argument of nptake is deprecated and throw tons of warningsI am fixing that in the metric module,,1496,0.7787433155080213,0.3088235294117647,34998,438.4250528601634,34.773415623749926,110.14915138007886,2402,21,1212,114,travis,arjoly,arjoly,true,arjoly,42,0.8095238095238095,20,24,722,true,true,false,false,5,154,23,171,17,0,10
2608955,scikit-learn/scikit-learn,python,2654,1386703172,1386714050,1386714050,181,181,github,false,false,false,6,2,1,0,3,0,3,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,67,18,68,59,12.655128802907615,0.3040045584717521,52,olivier.grisel@ensta.org,sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,50,0.022598870056497175,0,0,false,[MRG] Remove deprecated zero_one and zero_one_score ,,1495,0.7785953177257525,0.3088512241054614,35010,438.41759497286495,34.818623250499854,110.31133961725222,2402,21,1211,116,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,41,0.8048780487804879,20,24,721,true,true,true,false,5,156,22,172,17,0,9
2608496,scikit-learn/scikit-learn,python,2653,1386699445,1386702050,1386702050,43,43,github,false,false,false,19,1,1,0,2,0,2,0,2,0,0,4,4,3,0,0,0,0,4,4,3,0,0,11,6,11,6,18.288075858336665,0.4393191828723296,45,vanderplas@astro.washington.edu,doc/whats_new.rst|sklearn/covariance/__init__.py|sklearn/covariance/outlier_detection.py|sklearn/tests/test_common.py,33,0.03018867924528302,0,0,false,remove EllipticEnvelop deprecation Remove the support for EllipticEnvelop since its been deprecated for quite a whileLeave only EllipticEnvelope,,1494,0.7784471218206158,0.30754716981132074,35015,438.3549907182636,34.8136512923033,110.29558760531201,2402,21,1211,116,travis,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,72,0.9305555555555556,14,13,685,true,true,true,false,7,125,14,20,9,0,9
3001508,scikit-learn/scikit-learn,python,2651,1386645522,1386727863,1386727863,1372,1372,github,false,false,false,6,1,0,0,6,0,6,0,5,0,0,0,2,0,0,0,1,0,2,3,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,MRG Website: add spotify testimonials :),,1493,0.7782987273945077,0.31086142322097376,34998,438.4250528601634,34.773415623749926,110.14915138007886,2402,21,1210,115,travis,amueller,,false,,188,0.8563829787234043,783,38,1144,true,true,false,false,9,85,5,51,49,1,7
2604184,scikit-learn/scikit-learn,python,2650,1386642604,1388515533,1388515533,31215,31215,commit_sha_in_comments,false,true,false,74,12,2,3,19,0,22,0,4,0,0,1,5,1,0,0,0,0,5,5,5,0,0,32,0,182,106,9.256704728019947,0.2223660919117613,5,olivier.grisel@ensta.org,sklearn/datasets/samples_generator.py|sklearn/datasets/samples_generator.py,5,0.009363295880149813,0,4,false,[MRG] Reduce complexity of cluster sampling in make_classification  Fixes #2534 reducing complexity from space and time exponential in n_informative to space proportional to n_informative * n_clusters_per_class * n_classes (with small time complexity)Makes make_classification output backwards incompatible (maybe we could keep the old behaviour for small n_informative but this would still be faster)Also fixes a likely bug where hypercubeFalse: rows of C were being transformed that unlikely had an impact on the output,,1492,0.7781501340482574,0.31086142322097376,35015,438.3549907182636,34.8136512923033,110.29558760531201,2402,21,1210,121,travis,jnothman,amueller,false,amueller,55,0.5636363636363636,20,1,1686,true,true,false,false,8,105,9,90,14,0,1363
2559840,scikit-learn/scikit-learn,python,2649,1386620800,1386703281,1386703281,1374,1374,github,false,false,false,22,3,1,5,7,0,12,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.897187437073546,0.11764086943555768,17,olivier.grisel@ensta.org,doc/modules/linear_model.rst,17,0.03171641791044776,1,3,false,DOC: Polynomial regression narrative doc A small narrative documentation section under linear_model to address the new PolynomialFeatures preprocessor  Spurred by @ogrisels [comment](https://githubcom/scikit-learn/scikit-learn/pull/2585#issuecomment-30137000),,1491,0.778001341381623,0.31156716417910446,35008,438.4426416819013,34.82061243144424,110.31764168190128,2402,21,1210,117,travis,jakevdp,ogrisel,false,ogrisel,39,0.8717948717948718,1194,0,943,true,true,false,false,0,37,9,25,3,0,9
2598587,scikit-learn/scikit-learn,python,2647,1386577170,1386591423,1386591423,237,237,github,false,false,false,23,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,3.8211692477313015,0.09189056677328336,2,larsmans@gmail.com,sklearn/base.py,2,0.0037313432835820895,0,0,false,Fix exception message when cloning an estimator - Fix typo on TypeError when the estimator to be cloned do not implement get_params method,,1490,0.7778523489932886,0.31156716417910446,34881,440.0389897078639,34.947392563286606,110.71930277228292,2402,21,1210,115,travis,trein,agramfort,false,agramfort,0,0,16,30,663,false,false,false,false,0,0,0,0,0,0,7
2558432,scikit-learn/scikit-learn,python,2643,1386431739,1386605277,1386605277,2892,2892,github,false,false,false,81,2,1,17,9,0,26,0,4,1,0,0,2,1,0,0,1,0,1,2,1,0,0,182,0,209,0,4.481432552666012,0.10776694693755827,0,,benchmarks/bench_multilabel_metrics.py,0,0.0,0,2,false,[MRG] Add benchmarking script for multilabel metrics These are not very important metrics in the context of scikit-learn Yet whenever metric implementations gets changed people seem to be interested in how it affects execution time This makes such reports easy to calculateThis benchmarks metrics for different multilabel target formats also giving us an idea of their relative performance Benchmarks are otherwise parametrised by (number of samples classes average density of positive labels) one of which may be plotted against time,,1489,0.7777031564808596,0.3142292490118577,34877,440.37617914384833,35.00874501820684,110.70332884135676,2402,21,1208,117,travis,jnothman,arjoly,false,arjoly,54,0.5555555555555556,20,1,1684,true,true,false,true,4,88,8,76,12,0,6
2557194,scikit-learn/scikit-learn,python,2642,1386386491,1386471534,1386471534,1417,1417,github,false,false,false,109,3,2,5,3,0,8,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,68,28,73,28,17.546318989377884,0.42194556268763134,12,thomas.unterthiner@gmx.net,sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py,10,0.019801980198019802,2,0,true,[MRG] faster convergence reporting in RBMs Our RBMs used to take about 45% of their time just computing convergence information This PR brings that down to a few percent Also sparse matrices are no longer densified in score_samples given a memory reduction of several orders of magnitude (for my typical data that can be up to eight orders) Finally the reported figures should be more realistic because they reflect the actual change that occurred during an iterationThe downside is that the fit procedure requires a bit more memory now because it computes pseudo-likelihood for the entire input matrixPing @vene and maybe @IssamLaradji will find this interesting too,,1488,0.7775537634408602,0.31485148514851485,34881,440.81305008457326,35.00473036896878,110.74797167512399,2401,21,1207,117,travis,larsmans,larsmans,true,larsmans,92,0.7282608695652174,128,38,1237,true,true,false,false,31,174,39,90,78,2,13
2559981,scikit-learn/scikit-learn,python,2638,1386308141,,1405374550,317773,,unknown,false,true,false,55,51,18,15,56,0,71,0,6,0,0,2,4,2,0,0,1,0,4,5,7,0,0,42,183,1150,243,78.92701898364164,1.9067785298563213,12,olivier.grisel@ensta.org,sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py,8,0.015779092702169626,0,12,true,MRG Minibatch reassignment fixes This should address the remaining issues of #2185 including #2611 (which I only saw after writing this)I still have to merge #2185 for the tests to pass I thought about using sampling from nprandom instead of coding up a new function to do multinomial sampling with replacement (which should exist),,1487,0.7780766644250168,0.3116370808678501,34704,442.5714615029967,35.18326417704011,111.25518672199169,2400,21,1207,201,travis,amueller,GaelVaroquaux,false,,187,0.8609625668449198,782,38,1141,true,true,true,false,8,86,4,50,23,1,3003
2553874,scikit-learn/scikit-learn,python,2636,1386248527,1386641773,1386641773,6554,6554,merged_in_comments,false,false,false,122,1,1,12,13,0,25,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,4,23,4,8.607713029445435,0.20677559469540302,12,larsmans@users.noreply.github.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,9,0.017964071856287425,0,2,false,ENH: Raise explicitly on non-unique vocab A little better error message in this case Its a bit ugly but AFAICT theres not a better way since the user may give an object without a __len__Incidentally what are the thoughts on just handling non-unique vocabularies Something like vocabulary  list(set(vocabulary))This wont work because things like this that are actually mappings would get clobbered by the current check for a mappingvocab  {vocab : 0 beer : 1}from collections import Mappingisinstance(iter(vocab) Mapping)But it seems like it would be nice to just do this if possible Maybe raise a warning that the feature names of the vectorizer are now different than the given vocabulary Thoughts,,1486,0.7779273216689099,0.30538922155688625,35008,438.4426416819013,34.82061243144424,110.31764168190128,2399,20,1206,118,travis,jseabold,larsmans,false,larsmans,0,0,131,21,1280,true,false,false,false,1,2,0,0,4,0,7
2577930,scikit-learn/scikit-learn,python,2635,1386246676,,1415913981,494455,,unknown,false,true,false,14,2,2,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,8.65287672129995,0.20904348552868204,8,mathieu@mblondel.org,sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,8,0.01609657947686117,0,1,false,Issue #2611: MiniBatchKmeans crashes Fixes [crash](https://githubcom/scikit-learn/scikit-learn/issues/2611) by making sure arrays are the correct dimensions,,1485,0.7784511784511785,0.29979879275653926,34690,441.5105217641972,35.11098299221678,111.09829922167772,2399,19,1206,238,travis,czxcjx,amueller,false,,0,0,0,0,592,false,false,false,false,1,0,0,0,0,0,6
2572213,scikit-learn/scikit-learn,python,2633,1386176505,1386180630,1386180630,68,68,github,false,false,false,7,1,1,0,1,0,1,0,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,8.853036219208104,0.21387869094795453,12,olivier.grisel@ensta.org,doc/modules/cross_validation.rst|doc/tutorial/statistical_inference/settings.rst,12,0.025,0,0,false,Just fixing small typos in the docs  ,,1484,0.7783018867924528,0.30625,34690,441.27990775439605,35.11098299221678,111.09829922167772,2399,19,1205,115,travis,josericardo,arjoly,false,arjoly,0,0,7,20,599,false,false,false,false,0,0,0,0,0,0,10
2568494,scikit-learn/scikit-learn,python,2632,1386121705,1407766721,1407766721,360750,360750,commits_in_master,false,false,false,96,2,2,2,10,0,12,0,8,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,0,5,0,8.51490844443047,0.205710698848521,5,sergiopr@fis.ucm.es,examples/gaussian_process/gp_diabetes_dataset.py|sklearn/gaussian_process/gaussian_process.py,5,0.01026694045174538,0,2,false,Bugfixes in gaussian_process subpackage Hi listIts been a while -)Ive been told the random_start feature of the GaussianProcess estimator was making it worse rather than making it betterIt was indeed due to a bad handling of sign in the randomly restarted maximization of the reduced likelihood function (implemented as the minimization of the opposite reduced likelihood function with fmin_cobyla)While running GP tests and examples I spotted another mistake in the gp_diabetes_dataset example which appeared when the optimized theta value stored in the estimator was made private by renaming it to theta_Cheers,,1483,0.7781523937963587,0.3141683778234086,34690,441.25108100317095,35.08215624099164,111.09829922167772,2399,19,1204,195,travis,dubourg,larsmans,false,larsmans,2,1.0,6,8,1174,false,true,false,false,0,0,0,0,2,0,10
2563935,scikit-learn/scikit-learn,python,2631,1386072706,1386167385,1386167385,1577,1577,github,false,false,false,45,4,4,0,13,0,13,0,5,0,0,5,5,4,0,0,0,0,5,5,4,0,0,18,20,18,20,22.175153986864437,0.5357270079275129,39,peter.prettenhofer@gmail.com,examples/plot_train_error_vs_test_error.py|examples/plot_train_error_vs_test_error.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_sgd.py|doc/whats_new.rst,34,0.004056795131845842,2,10,false,[MRG] Fix sgd l1 ratio This fixes a regression that happened when we changed from rho to l1_ratio l1_ratio is actually 10 - l1_ratio  I added a test that now checks if L1 and L2 penalty matches elastic net with extreme l1_ratio@amueller @mblondel,,1482,0.7780026990553306,0.3265720081135903,34690,441.25108100317095,35.08215624099164,111.09829922167772,2398,19,1204,115,travis,pprett,ogrisel,false,ogrisel,42,0.8571428571428571,122,29,1582,true,true,true,true,3,72,4,12,5,2,7
2558455,scikit-learn/scikit-learn,python,2629,1386014967,1386016013,1386016013,17,17,github,false,false,false,8,2,2,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,230,0,230,8.971835272798877,0.21674908720923894,43,olivier.grisel@ensta.org,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,43,0.08739837398373984,0,1,false,Fix average precision score test on numpy 13 ,,1480,0.7783783783783784,0.3252032520325203,34690,441.25108100317095,35.08215624099164,111.09829922167772,2398,19,1203,113,travis,arjoly,arjoly,true,arjoly,40,0.8,20,24,713,true,true,false,false,4,134,15,183,13,0,0
2545660,scikit-learn/scikit-learn,python,2628,1385957271,1386000802,1386000802,725,725,github,false,false,false,64,9,2,13,14,0,27,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,55,0,112,0,9.511324872256989,0.22978630652906953,0,,examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_blind_source_separation.py,0,0.0,0,1,false,ENH: color scheme + add PCA For educational reasons Ive added PCA to this example with sawtooth waveform to more clearly expose the idea behind ICA and better relate this example to the PCA vs ICA example  Also Ive changed default colors to make the output more barrier free (red-green ) Let me know what you think[Uploading Screenshot 2013-12-02 010722png   ](),,1479,0.7782285327924273,0.32172131147540983,34592,441.0268270120259,35.15263644773358,111.18177613321,2398,19,1202,115,travis,dengemann,agramfort,false,agramfort,14,0.9285714285714286,36,30,519,true,true,true,true,3,33,4,30,20,6,1
2547418,scikit-learn/scikit-learn,python,2626,1385827130,1386451356,1386451356,10403,10403,github,false,false,false,70,1,1,0,8,0,8,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,3,85,3,85,16.551856930050118,0.39987510243388896,57,vanderplas@astro.washington.edu,sklearn/metrics/tests/test_metrics.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py,49,0.018072289156626505,1,1,false,ENH support multiclass targets of dtypeobject strings Heres a first hack at fixing #2374 #2618 I think we should get this into the next release since it seems it was a regression of 014 relative to 013 (or so says @samuela)There are probably more parts of the code to test but I thought the main invariance tests were low-hanging fruit Feel free to offer further commits to the patch,,1478,0.7780784844384303,0.3192771084337349,34703,442.5842146212143,35.184278016309825,111.25839264616891,2396,19,1201,121,travis,jnothman,GaelVaroquaux,false,GaelVaroquaux,53,0.5471698113207547,20,1,1677,true,true,false,false,4,68,7,33,10,0,6
2544937,scikit-learn/scikit-learn,python,2624,1385766148,1385807249,1385807249,685,685,github,false,false,false,18,1,1,0,2,0,2,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.289324396047396,0.10362679427581306,15,olivier.grisel@ensta.org,sklearn/utils/testing.py,15,0.029644268774703556,3,0,true,FIX: py3k syntax in recently added doctest  I hope this makes the buildbot happycc @GaelVaroquaux @larsmans @ogrisel,,1477,0.7779282329045363,0.31225296442687744,34592,441.0268270120259,35.15263644773358,111.18177613321,2396,19,1200,114,travis,dengemann,agramfort,false,agramfort,13,0.9230769230769231,36,30,517,true,true,true,true,3,44,4,39,20,6,0
2544416,scikit-learn/scikit-learn,python,2623,1385759295,,1386437526,11303,,unknown,false,false,false,16,1,1,0,12,0,12,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,11,11,11,9.376512100396642,0.2265248604116151,21,mathieu@mblondel.org,sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,13,0.02564102564102564,0,0,true,[WIP] fast squared Euclidean/Frobenius norm helper function For NMF primarily Lets see if the tests pass,,1476,0.7784552845528455,0.3136094674556213,34704,442.5714615029967,35.18326417704011,111.25518672199169,2396,19,1200,121,travis,larsmans,larsmans,true,,91,0.7362637362637363,128,38,1230,true,true,false,false,28,188,44,101,78,2,9
2542047,scikit-learn/scikit-learn,python,2622,1385746666,1385746746,1385746746,1,1,github,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.348834257817075,0.10644158137548612,0,,doc/documentation.rst,0,0.0,0,0,true,fix broken doc link for stable version source #2621,,1475,0.7783050847457628,0.3115079365079365,33617,420.82874735996666,33.97090757652378,105.89880120177291,2396,19,1200,114,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,71,0.9295774647887324,14,13,674,true,true,false,false,4,100,11,20,8,0,1
2539286,scikit-learn/scikit-learn,python,2620,1385693447,1385732835,1385732835,656,656,github,false,false,false,9,2,1,1,5,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,8,0,4.6302976018276425,0.1118649492195471,13,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py,13,0.02584493041749503,0,0,false,ENH: speedup for docbuild using joblib for issue #2604,,1474,0.7781546811397557,0.3081510934393638,34590,440.93668690372937,35.06793871061,110.78346342873662,2396,19,1199,114,travis,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,70,0.9285714285714286,14,13,673,true,true,true,false,4,88,10,20,7,0,6
3001504,scikit-learn/scikit-learn,python,2619,1385616456,1385624159,1385624159,128,128,merged_in_comments,false,false,false,10,0,0,1,3,0,4,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,FIX stop forcing deprecation warnings for external packages For #2531,,1473,0.7780040733197556,0.3013972055888224,34590,440.93668690372937,35.06793871061,110.78346342873662,2395,19,1199,114,travis,jnothman,,false,,52,0.5384615384615384,20,1,1675,true,true,false,false,4,62,6,33,9,0,5
2529134,scikit-learn/scikit-learn,python,2617,1385564829,1385567727,1385567727,48,48,github,false,false,false,21,1,1,0,3,0,3,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,30,4,30,7.809516859101573,0.18867312668030156,6,peter.prettenhofer@gmail.com,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,6,0.01195219123505976,0,2,false,[MRG] pass warm_start to BaseSGDRegressor also make SGDRegressor test case extend CommonTest (move two clf specifc tests to clf test case),,1472,0.7778532608695652,0.2968127490039841,34582,438.75426522468337,34.84471690474813,110.51992365970736,2395,19,1198,114,travis,pprett,pprett,true,pprett,41,0.8536585365853658,121,29,1576,true,true,false,false,3,60,1,13,4,2,1
2526158,scikit-learn/scikit-learn,python,2615,1385518711,1385554516,1385554516,596,596,github,false,false,false,9,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.7812584817700845,0.11551221312967712,0,,doc/about.rst,0,0.0,0,0,false,MAINT: fix broken links to numfocusorg on donations page ,,1470,0.7782312925170068,0.2962226640159046,34582,438.75426522468337,34.84471690474813,110.51992365970736,2394,19,1197,114,travis,rgommers,GaelVaroquaux,false,GaelVaroquaux,0,0,50,13,1617,false,false,false,false,0,0,0,0,0,0,9
2523026,scikit-learn/scikit-learn,python,2614,1385490856,1385491931,1385491931,17,17,github,false,false,false,5,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.376605617227797,0.10573605430214027,0,,sklearn/mixture/gmm.py,0,0.0,0,0,false,BUG: typo fixes in sklearnmixturegmm ,,1469,0.7780803267528931,0.2962226640159046,34582,438.75426522468337,34.84471690474813,110.51992365970736,2394,19,1197,113,travis,zyv,arjoly,false,arjoly,0,0,6,0,1022,false,false,false,false,0,0,0,0,0,0,8
2508774,scikit-learn/scikit-learn,python,2610,1385269516,,1422811140,625693,,unknown,false,true,false,191,9,2,0,15,0,15,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,48,0,354,245,8.684609450207978,0.2098142521769527,31,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,31,0.06019417475728155,0,2,false,[WIP] enhance labels and deprecate pos_label in PRF metrics This intends to make the parameter labels clearly defined for the precision_recall_fscore_support family of metrics This amounts to a (partial) fix for #1983 #1989 #2029 and #2094 As implied by the comment I have committed labels will:* determine the sort order when returning a result for each class* determine which labels will be included in an average (allowing one or more negative/ignored classes in multiclass classification)* by default all labels are used with no special handling of binary -the current special handling of binary classification is preserved but pos_label is determined implicitly-There are some potential issues in the deprecation process and in making binary classifier metrics available as Scorers- [x] Define and document intended behaviour- [x] Tests for new labels functionality- [x] Implement new labels functionality- [x] Ensure labels is set correctly for legacy functionality- [x] Deprecation warnings for pos_label- [ ] Some solution for scorers at least handling the binary case- [ ] Copy documentation to derived functions- [ ] Update narrative documentation- [ ] Update whats new,,1468,0.7786103542234333,0.287378640776699,34577,438.7887902362842,34.84975561789629,110.50698441160309,2393,19,1195,270,travis,jnothman,jnothman,true,,51,0.5490196078431373,20,1,1671,true,true,false,false,3,52,5,23,6,0,1140
2504147,scikit-learn/scikit-learn,python,2608,1385169900,1385169916,1385169916,0,0,github,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.563168964708976,0.11024341449978445,0,,doc/modules/density.rst,0,0.0,0,0,true,DOC: typo in kernel formulas ,,1467,0.7784594410361282,0.2840466926070039,34460,438.827626233314,34.793964016250726,110.67904817179338,2392,19,1193,113,travis,jakevdp,jakevdp,true,jakevdp,38,0.868421052631579,1175,0,926,true,true,false,false,0,35,4,35,1,0,0
2503061,scikit-learn/scikit-learn,python,2607,1385160070,,1392493185,122218,,unknown,false,false,false,89,9,3,8,15,2,25,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,18,16,72,52,13.725135440744772,0.33159100773210326,10,vanderplas@astro.washington.edu,sklearn/naive_bayes.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,6,0.011650485436893204,1,5,true,Improve Naive Bayes test coverage This PR adds tests for the binarizeNone option in BernoulliNB If binarizeNone and non-binary data is used an error should be raised (which is now the case)Test coverage has been improved in f6bf82f by removing a conditional that was always true (the check_arrays call above serves the same purpose)Coverage is now 99% for naive_bayespy it should be 100% but the @abstractmethod decorator on line 39 is incorrectly counted as a missing branch due to a bug in coveragepy (see https://bitbucketorg/ned/coveragepy/issue/129/misleading-branch-coverage-of-empty for details),,1466,0.7789904502046384,0.283495145631068,34460,438.827626233314,34.793964016250726,110.67904817179338,2392,19,1193,147,travis,craigdsthompson,larsmans,false,,0,0,0,0,1,false,false,false,false,0,0,0,0,1,0,9
2486150,scikit-learn/scikit-learn,python,2602,1384970873,1384977586,1384977586,111,111,github,false,false,false,60,2,2,0,2,3,5,1,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,71,23,71,23,12.801665561064507,0.3092774287048921,6,stjeansam@gmail.com,sklearn/decomposition/dict_learning.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py,4,0.007707129094412331,1,0,false,Fix MiniBatchDictLearning partial_fit This PR overides and completes #2600 : it adds to the syntax fix of @yarikoptic a logical fix to the partial_fit of MiniBatchDictLearning so that now the online code actually is equivalent to the batch code and the tests passThis should make MiniBatchDictLearning when used in online setting much more performant in the sens of estimation,,1465,0.778839590443686,0.279383429672447,34441,439.01164309979384,34.81315873522836,110.74010626869138,2391,19,1191,115,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,35,0.7142857142857143,423,3,1367,true,true,false,false,12,106,23,66,17,0,8
2481618,scikit-learn/scikit-learn,python,2600,1384910990,1384971031,1384971031,1000,1000,commits_in_master,false,true,false,11,1,1,0,5,1,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.178944942134508,0.100959780603361,4,stjeansam@gmail.com,sklearn/decomposition/dict_learning.py,4,0.007662835249042145,0,1,false,BF: use hasattr with providing attr name (Thanks to Timo Schulz) ,,1464,0.7786885245901639,0.2777777777777778,34441,439.01164309979384,34.81315873522836,110.74010626869138,2391,20,1190,115,travis,yarikoptic,GaelVaroquaux,false,GaelVaroquaux,9,0.7777777777777778,77,7,1804,false,true,false,false,0,0,0,0,0,0,24
2481381,scikit-learn/scikit-learn,python,2599,1384908886,,1385072942,2734,,unknown,false,false,false,97,7,6,2,0,0,2,0,3,1,0,4,5,2,0,1,1,0,4,5,2,0,1,1399,0,1399,0,55.236491234240276,1.334466979950193,11,rohit.sprasad@gmail.com,.gitignore|doc/modules/feature_extraction.rst|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|examples/text_topics_extractor.py|.gitignore|examples/text_topics_extractor.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,5,0.007662835249042145,0,0,false,Feature extraction text ldalsi vectorizers I would like to send a pull request on features I have added to the sklearnfeature_extractiontext moduleI have added two new classes LdaVectorizer and LsiVectorizer in the above module to support extracting latent topics from text documents As a result a collection of text documents can be transformed into a set of topic weights in a much lower dimensionTo support the above added features I also added one example text_topics_extractorpy in the examples directory In addition I created one section in doc/modules/feature_extractionrst to describe those two vectorizers in more detail,,1463,0.7792207792207793,0.2777777777777778,34441,439.01164309979384,34.81315873522836,110.74010626869138,2391,20,1190,113,travis,nanli0416,larsmans,false,,0,0,0,1,173,false,false,true,false,0,0,0,0,0,0,781
2476998,scikit-learn/scikit-learn,python,2598,1384870281,,1390268524,89970,,unknown,false,true,false,39,126,7,117,144,21,282,0,9,0,0,2,7,2,0,0,0,0,7,7,5,0,0,2681,0,9738,1397,61.29391061430205,1.4808091165664308,28,peter.prettenhofer@gmail.com,sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py,24,0.045714285714285714,0,92,false,[MRG] ENH: MultiTaskElasticNet (and Lasso) CV New MultiTaskElasticNet and Lasso CVDo look at the discussions in https://githubcom/scikit-learn/scikit-learn/pull/2590https://githubcom/scikit-learn/scikit-learn/issues/2402it was finally decided that having a separate MultiTaskElasticNetCV would be more useful than multiple usage of ElasticNetCV across n_tasks,,1462,0.7797537619699042,0.2761904761904762,34441,439.01164309979384,34.81315873522836,110.74010626869138,2391,20,1190,138,travis,Manoj-Kumar-S,jnothman,false,,4,0.5,22,15,518,false,false,false,false,0,88,5,44,2,0,10
2463221,scikit-learn/scikit-learn,python,2595,1384647409,1384868793,1384868793,3689,3689,commit_sha_in_comments,false,false,false,22,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,35,4,35,8.312359296468815,0.20082153262679542,5,larsmans@gmail.com,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,4,0.007619047619047619,0,0,false,Test and fix regression in CountVectorizer handling of float min_df/max_df This updated test used to pass on 0131 broken in newer versions,,1461,0.7796030116358659,0.2761904761904762,34438,439.0208490620826,34.816191416458565,110.72071548870434,2391,20,1187,113,travis,bdkearns,larsmans,false,larsmans,1,0.0,2,0,786,false,false,false,false,0,8,3,2,1,0,7
2455519,scikit-learn/scikit-learn,python,2594,1384523158,1384553621,1384553621,507,507,commit_sha_in_comments,false,false,false,14,1,1,0,5,0,5,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.0699120714510135,0.09832659425493046,4,manojkumarsivaraj334@gmail.com,doc/modules/model_evaluation.rst,4,0.007633587786259542,0,4,true,Fix scoring function string Change it to the correct key from the sklearnmetricsSCORERS dict,,1460,0.7794520547945205,0.27099236641221375,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1186,113,travis,djv,arjoly,false,arjoly,2,0.5,62,577,1292,false,true,false,false,0,0,0,0,0,0,8
2453363,scikit-learn/scikit-learn,python,2592,1384487018,1386250936,1386250936,29398,29398,github,false,false,false,9,3,1,4,16,0,20,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,35,51,35,71,8.202647189390268,0.19817095501033113,29,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,20,0.03816793893129771,0,12,false,FIX limit warnings for recall_score precision_score f1_score fixes #2586,,1459,0.7793008910212474,0.27099236641221375,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1185,122,travis,jnothman,ogrisel,false,ogrisel,50,0.54,20,1,1661,true,true,false,false,4,45,3,22,4,0,9
2449536,scikit-learn/scikit-learn,python,2590,1384453955,1384730179,1384730179,4603,4603,merged_in_comments,false,false,false,8,3,1,13,11,0,24,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,78,0,164,8,4.108928109901206,0.099269197956135,11,mathieu@mblondel.org,sklearn/linear_model/coordinate_descent.py,11,0.02099236641221374,0,6,false,Fixes Issue 2402 An attempt to fix https://githubcom/scikit-learn/scikit-learn/issues/2402,,1457,0.7796842827728209,0.27099236641221375,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1185,113,travis,Manoj-Kumar-S,agramfort,false,agramfort,3,0.3333333333333333,22,15,513,false,false,false,false,0,82,4,38,1,0,1
2448025,scikit-learn/scikit-learn,python,2589,1384436201,,1386013092,26281,,unknown,false,false,false,100,1,1,0,26,0,26,0,9,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,20,12,20,8.819148668869971,0.2130652548787687,3,larsmans@gmail.com,sklearn/base.py|sklearn/tests/test_pipeline.py,2,0.003766478342749529,1,6,false,[Proof of concept] Syntactic sugar for pipelines Inspired by https://githubcom/aht/streampy by @ahtThis PR adds a simple syntactic sugar for easily creating pipelines iris  load_iris() X y  irisdata iristarget pipeline  StandardScaler() | PCA(n_components2) | SVC(random_state0) pipeline__class__class sklearnpipelinePipeline y_pred  pipelinefit(X y)predict(X) npmean(y  y_pred)092000000000000004I used the bitwise or operator because of the similarity between pipelines and UNIX pipes but the right shift operator could be nice too (easier to read and convey the idea of direction): pipeline  StandardScaler()  PCA(n_components2)  SVC(random_state0),,1456,0.7802197802197802,0.2655367231638418,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1185,117,travis,mblondel,mblondel,true,,26,0.8076923076923077,304,32,1325,true,true,false,false,9,125,14,49,36,1,7
2444789,scikit-learn/scikit-learn,python,2587,1384390203,,1390700084,105164,,unknown,false,false,false,67,1,1,8,18,0,26,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,33,76,33,76,8.467638734112821,0.20457298916770114,4,mathieu@mblondel.org,sklearn/grid_search.py|sklearn/tests/test_grid_search.py,3,0.005649717514124294,0,1,false,Enable grid_search with classifiers that fail on individual training folds Enable grid_search with classifiers that fail on individual training foldsThe improved functionality is repeated in two places with the same code for the case where y is not None and when y is None I would welcome a suggestion on how to avoid this duplication There are also two nearly identical tests for those two cases,,1455,0.7807560137457045,0.2655367231638418,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1184,136,travis,romaniukm,romaniukm,true,,0,0,3,9,169,false,false,false,false,0,0,0,0,0,0,24430
2431204,scikit-learn/scikit-learn,python,2585,1384233947,1385230596,1385230596,16610,16610,github,false,true,false,118,9,1,17,26,0,43,0,10,0,0,4,5,4,0,0,0,0,5,5,5,0,0,143,28,357,53,18.36170991095018,0.44360791591767273,0,,examples/linear_model/plot_polynomial_interpolation.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py,0,0.0,0,2,false,[MRG] ENH: add PolynomialFeatures preprocessor This is a simple pre-processor that creates mixed polynomial features of any given degree This is something I use often in tutorials and this will make the process much more streamlinedWhat this PR contains:- A new sklearnpreprocessingPolynomialFeatures transformer class- A sklearnpreprocessingpolynomial_features function- Tests of the new functionality- A modified polynomial interpolation example which uses this in a pipeline rather than using npvander to construct the features by handNote that full polynomial features are more general than npvander because theyll work for more than one-dimensional input (see the docstring for an example)This doesnt support sparse yet: well have to think about how to most efficiently implement that,,1454,0.78060522696011,0.2640449438202247,34438,439.0208490620826,34.816191416458565,110.72071548870434,2389,20,1183,114,travis,jakevdp,jakevdp,true,jakevdp,37,0.8648648648648649,1165,0,916,true,true,false,false,0,22,4,23,1,0,340
2424094,scikit-learn/scikit-learn,python,2583,1384141321,1384171889,1384171889,509,509,github,false,false,false,22,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.591769336105528,0.11093568372596667,0,,examples/svm/plot_rbf_parameters.py,0,0.0,0,0,false,MRG DOC explanatory sentence for svm grid search example Gave rule of thumb for rbf kernel parameter search prompted by [reddit question](http://wwwredditcom/r/MachineLearning/comments/1qbnp2/gridsearch_how_to_determine_the_range_of_c_and/utm_sourcetwitterfeed&utm_mediumtwitter),,1453,0.7804542326221611,0.25426944971537,34438,439.0208490620826,34.816191416458565,110.72071548870434,2388,20,1181,108,travis,amueller,agramfort,false,agramfort,186,0.8602150537634409,762,38,1115,true,true,true,false,7,104,7,61,4,0,8
2407633,scikit-learn/scikit-learn,python,2579,1383866410,,1384278889,6814,,unknown,false,false,false,11,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,21,0,21,0,4.299377883067703,0.10386998764516779,2,larsmans@gmail.com,sklearn/decomposition/truncated_svd.py,2,0.003676470588235294,0,1,false,Attempt at implementing eigh algorithm for truncatedSVD Attempt at fixing https://githubcom/scikit-learn/scikit-learn/issues/2572,,1451,0.7815299793246038,0.2555147058823529,34411,438.0866583359972,34.756327918398185,110.63322774694139,2388,20,1178,109,travis,Manoj-Kumar-S,Manoj-Kumar-S,true,,2,0.5,21,15,506,false,false,false,false,0,72,2,36,0,0,0
2390649,scikit-learn/scikit-learn,python,2576,1383675056,1383740697,1383740697,1094,1094,github,false,false,false,11,3,1,1,10,0,11,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,18,4.169195516890937,0.10072519866832488,1,larsmans@gmail.com,sklearn/preprocessing/imputation.py,1,0.0017035775127768314,0,5,false,Fixing issue 2560 : Imputer bug with median and dense input ,,1450,0.7813793103448275,0.26916524701873934,34408,436.9332713322483,34.75935828877005,110.52662171588003,2385,21,1176,107,travis,ankit-maverick,agramfort,false,agramfort,1,1.0,12,6,346,false,false,false,false,0,2,1,0,0,0,0
2387872,scikit-learn/scikit-learn,python,2574,1383647498,1383648020,1383648020,8,8,github,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.636679727297148,0.11201768279818228,4,olivier.grisel@ensta.org,.travis.yml,4,0.0065252854812398045,0,0,false,[MRG] Only run make test when coverage is disabled ,,1449,0.7812284334023465,0.2642740619902121,34408,436.9332713322483,34.75935828877005,110.52662171588003,2385,21,1176,107,travis,ogrisel,ogrisel,true,ogrisel,48,0.8333333333333334,829,123,1623,true,true,false,false,18,460,53,226,108,6,7
2379779,scikit-learn/scikit-learn,python,2571,1383538529,1383595830,1383595830,955,955,github,false,false,false,45,2,1,6,6,0,12,0,5,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,9.521531298986439,0.22947562622930567,0,,CONTRIBUTING.md|README.rst,0,0.0,0,1,false,Added contributing subsection to README As discussed on the mailing list I believe its better to link both pages in the README as developers who come from within GitHub (eg by searching for python projects) may miss the website and the file on the repository,,1448,0.7810773480662984,0.2633279483037157,34460,435.1712130005804,34.67788740568775,110.12768427161927,2385,22,1175,108,travis,edran,ogrisel,false,ogrisel,0,0,16,36,284,false,true,false,false,0,0,0,0,0,0,11
2376620,scikit-learn/scikit-learn,python,2570,1383475774,1389205178,1389205178,95430,95430,github,false,true,false,200,74,13,79,139,8,226,0,11,0,0,9,17,8,0,0,3,0,17,20,15,0,0,1668,206,6572,688,134.0583685712179,3.2309015318588563,53,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/ensemble.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,34,0.014539579967689823,0,70,false,[MRG] Gradient Boosting enhancements This PR contains a number of enhancements for GBRT:1 warm_start argument now allows to add additional trees to an already trained model2 The fit parameter monitor allows to inject a callback into the learning procedure that can implement early stopping evaluate training progress on held-out creating snap-shots 3 Trees can now be grown using max_leaf_nodes as a stopping criterion    Setting max_leaf_nodes  0 will grow the tree in a best-first fashing Nodes are pushed on a PriorityQueue (impl as a binary heap) and the node with the highest impurity improvement is expanded next    If max_leaf_nodes  0 then trees are grown in depth-first fashion by using a stack instead of the PriorityQueue    Trees grown with max_leaf_nodes  0 have at most depth max_leaf_nodes - 1 and thus can model interactions of (at most) max_leaf_nodes - 1 features Individual trees might however be shallower then max_leaf_nodes - 14 New estimator ZeroEstimator if you want to start your GBRT model from scratch5 Tree code now supports both fortran and c-style inputs X    Tree ensemble estimators dont enforce a specific layout of X Benchmarks indicate that fortran is indeed faster -- even for ExtraTrees,,1447,0.7809260539046303,0.2633279483037157,34460,435.1712130005804,34.67788740568775,110.12768427161927,2385,22,1174,129,travis,pprett,pprett,true,pprett,40,0.85,116,29,1552,true,true,false,false,2,39,1,4,1,1,95
2364712,scikit-learn/scikit-learn,python,2566,1383260679,1383900229,1383900229,10659,10659,github,false,false,false,9,8,2,40,21,0,61,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,96,60,215,85,17.16572731387781,0.41442201025068337,0,,sklearn/dummy.py|sklearn/tests/test_dummy.py|sklearn/dummy.py|sklearn/tests/test_dummy.py,0,0.0,0,20,false,Constant output dummy classifier An attempt to fix https://githubcom/scikit-learn/scikit-learn/issues/2003,,1444,0.7818559556786704,0.265625,34460,435.1712130005804,34.67788740568775,110.12768427161927,2385,23,1171,109,travis,Manoj-Kumar-S,glouppe,false,glouppe,1,0.0,21,15,499,false,false,false,false,0,63,1,24,0,0,0
2364628,scikit-learn/scikit-learn,python,2565,1383260058,1383319682,1383319682,993,993,github,false,false,false,34,1,1,0,2,0,2,0,2,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.710544707182669,0.1137238971137634,16,olivier.grisel@ensta.org,doc/testimonials/images/change-logo.png|doc/testimonials/testimonials.rst,16,0.025,0,0,false,Adding changeorg to testimonials Per a discussion I had with somebody on Reddit a few months back finally got around to writing up a testimonial on how Changeorg uses scikit-learn in production and R&D ,,1443,0.7817047817047817,0.265625,34460,435.1712130005804,34.67788740568775,110.12768427161927,2385,23,1171,106,travis,vijaykramesh,GaelVaroquaux,false,GaelVaroquaux,0,0,8,4,1025,false,false,false,false,0,0,0,0,0,0,13
2351867,scikit-learn/scikit-learn,python,2558,1383117456,1383119767,1383119767,38,38,github,false,false,false,84,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.110029407661749,0.09922714990859996,1,L.J.Buitinck@uva.nl,sklearn/manifold/mds.py,1,0.0015290519877675841,0,0,false,BUG: Compare strings for equality not identity In a script I found that calling MDS with dissimilarityprecomputed was failing because strings are compared by identity A quick way to reproduce the bug is this:    from sklearnmanifold import MDS    import numpy as np    mds  MDS(dissimilarityjoin(list(precomputed)))    X  npones(1010)    pos  mdsfit_transform(X)raises a    ValueError: Proximity must be precomputed or euclidean Got precomputed insteadThe fix is really simple but I can add a test case to it or an issue if you prefer that,,1441,0.7820957668285913,0.26452599388379205,34456,435.2217320640817,34.681913164615736,110.14046900394708,2385,23,1170,104,travis,Jorge-C,mblondel,false,mblondel,0,0,4,1,874,false,false,false,false,0,0,0,0,0,0,12
2347898,scikit-learn/scikit-learn,python,2557,1383072635,,1442950525,997904,,unknown,false,true,false,37,1,1,11,8,0,19,0,8,0,0,1,1,1,0,0,0,0,1,1,1,0,0,15,0,15,0,4.293549110501041,0.1036578084899886,10,mathieu@mblondel.org,sklearn/decomposition/nmf.py,10,0.01524390243902439,0,0,false,[WIP] FIX Projected Gradient NMF stopping condition I caught a bug in the PG NMF stopping condition All in all it seems that this solver is not at all bad but I need to understand it properly,,1440,0.7826388888888889,0.26676829268292684,34456,435.2217320640817,34.681913164615736,110.14046900394708,2385,23,1169,352,travis,vene,agramfort,false,,45,0.8222222222222222,60,32,1297,true,true,true,true,5,40,0,43,1,2,5
2340908,scikit-learn/scikit-learn,python,2556,1382987555,1383174644,1383174644,3118,3118,github,false,false,false,22,1,1,1,4,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,32,0,32,0,5.052074262101861,0.12197070855414238,0,,examples/mixture/plot_gmm_pdf.py,0,0.0,0,1,false,[MRG] Improve the GMM PDF example Make the example easier to follow for people new to numpy and switch to matplotlibpyplot API,,1439,0.7824878387769284,0.28005865102639294,34457,435.20910119859536,34.680906637258026,110.13727254258932,2385,23,1168,105,travis,lpierron,ogrisel,false,ogrisel,0,0,0,0,0,false,true,false,false,0,0,0,0,0,0,1
2335128,scikit-learn/scikit-learn,python,2552,1382897053,,1383025975,2148,,unknown,false,false,false,64,2,2,1,13,1,15,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,12,0,8.638504853542585,0.20855775741412858,3,mathieu@mblondel.org,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py,3,0.004021447721179625,0,2,false,Fix for ridge regression with sparse matrix input Fix for ridge regression with sparse matrix input- Re-enable selecting SVD for sparse input (even though it is converted todense one day sparse solving may be supported and there are currentvalid use cases where sparse input is typical and doesnt blow up theSVD but does blow up the eigen solver)(Fixes #2354),,1438,0.7830319888734353,0.3351206434316354,34455,435.118270207517,34.682919750399066,110.23073574227253,2383,23,1167,104,travis,bdkearns,bdkearns,true,,0,0,2,0,766,false,false,false,false,0,2,0,0,0,0,11
2326481,scikit-learn/scikit-learn,python,2551,1382712488,1382723418,1382723418,182,182,github,false,false,false,48,2,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,39,0,41,0,4.816955981171993,0.11629532978707012,6,gael.varoquaux@normalesup.org,examples/decomposition/plot_pca_vs_fa_model_selection.py,6,0.006779661016949152,0,1,true,ENH: improve FA vs PCA example This improves the plot FactorAnalysis VS PCA example in demonstrating that the selected models covariance is more appropriate for our low rank scenario than the ones obtained from other covariance estimatorscf[screenshot 2013-10-25 12 48 36](https://fcloudgithubcom/assets/1908618/1407680/abdfa76e-3d73-11e3-86cd-17c4b9789c6dpng)[screenshot 2013-10-25 12 48 46](https://fcloudgithubcom/assets/1908618/1407681/abe211d4-3d73-11e3-9b73-4bc6ebac0b36png),,1437,0.7828810020876826,0.4214689265536723,34441,435.2951424174675,34.697018088905665,110.2755436834006,2383,23,1165,103,travis,dengemann,agramfort,false,agramfort,12,0.9166666666666666,35,30,482,true,true,true,true,5,113,9,75,29,6,0
2325892,scikit-learn/scikit-learn,python,2550,1382705309,1382705679,1382705679,6,6,github,false,false,false,12,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.263166432741899,0.10292524261679961,0,,examples/grid_search_text_feature_extraction.py,0,0.0,0,0,true,Two small doc typo fixes Change there - their name - names,,1436,0.7827298050139275,0.42665173572228443,34441,435.2951424174675,34.697018088905665,110.2755436834006,2383,23,1165,103,travis,StevenMaude,glouppe,false,glouppe,2,1.0,5,1,230,false,false,false,false,0,2,2,0,0,0,6
2323831,scikit-learn/scikit-learn,python,2549,1382668241,1445287810,1445287810,1043599,1043599,commit_sha_in_comments,false,true,false,150,6,6,11,2,0,13,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,133,27,133,27,37.77088358863099,0.9119063871396103,5,sergiopr@fis.ucm.es,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py,5,0.002197802197802198,0,0,false,Gaussian Process extension: multiple hyperparameters HelloI made some simple changes to the gaussian process module Previously the hyperparameters theta (parameters to optimize from the kernel function) were limited to one though that one hyperparam could be multifeatured This is no good if you want to combine kernels (like in Rasmussen & Williams book) because each part of the new kernel will have its own hyperparams so I modified the code to allow thisTheta can now be a matrix where columns are features / dimensions (just as before) but rows now correspond to different hyperparametersI also added a test for it and added a check in the built-in correlation functions to make sure they arent taking in bad parameters All the tests seemed to pass for mePS I hope Im following the right procedure Ive never contributed before so please let me know if Im missing anything,,1435,0.7825783972125435,0.44505494505494503,34440,435.1335656213705,34.66898954703833,110.2787456445993,2383,23,1164,368,travis,hadsed,glouppe,false,glouppe,0,0,36,34,460,false,false,false,false,0,0,0,0,0,0,9
2314011,scikit-learn/scikit-learn,python,2547,1382555763,,1446696016,1068944,,unknown,false,false,false,49,10,1,18,12,0,30,0,7,2,0,1,6,3,0,0,3,0,4,7,4,0,0,191,26,368,33,13.682326870392913,0.3303334544323693,1,L.J.Buitinck@uva.nl,sklearn/feature_selection/__init__.py|sklearn/feature_selection/multivariate_filtering.py|sklearn/feature_selection/tests/test_multivariate_filtering.py,1,0.0,0,0,false,Minimum redundancy maximum relevance (mRMR) feature selection HiI have created a new class computing the mRMR filtering feature selectionpep8 pyflakes and nosetests run succesfully on the submitted codeI am planning to create the documentation for this class as soon as I receive your approvalThanksAndrea,,1434,0.7831241283124128,0.4618556701030928,34439,435.1462005284706,34.66999622520979,110.28194779174773,2381,24,1163,386,travis,AndreaBravi,MechCoder,false,,0,0,5,1,3,false,false,false,false,0,0,0,0,1,0,2166
2311565,scikit-learn/scikit-learn,python,2546,1382533336,1382533924,1382533924,9,9,github,false,false,false,15,1,1,0,1,0,1,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.416580678032457,0.10662983842467773,3,larsmans@gmail.com,doc/modules/feature_selection.rst,3,0.0030241935483870967,0,0,false,Update feature_selectionrst minor doc fix Fix inconsistency in name of classifier and link to classifier,,1433,0.7829727843684577,0.4727822580645161,34439,435.1462005284706,34.66999622520979,110.28194779174773,2381,24,1163,103,travis,StevenMaude,glouppe,false,glouppe,1,1.0,5,1,228,false,false,false,false,0,1,1,0,0,0,9
2309705,scikit-learn/scikit-learn,python,2545,1382500323,1382903522,1382903522,6719,6719,commit_sha_in_comments,false,false,false,186,11,1,12,17,0,29,0,5,0,0,1,3,1,0,0,1,0,3,4,3,0,0,17,0,31,60,4.202555916976207,0.1014626231511156,5,philippe.gervais@inria.fr,sklearn/utils/__init__.py,5,0.0050150451354062184,0,1,false,Fix for out of bounds error in RBM  with odd # of rows Fixes out of bounds error in RBM with odd numbered matrix rows due togen_even_slices generating even slices that overshoot bounds of matrix/usr/local/lib/python27/dist-packages/sklearn/neural_network/rbmpycin fit(self X y)    310    311             for batch_slice in batch_slices:-- 312                 pl_batch  self_fit(X[batch_slice] rng)    313    314                 if verbose:/usr/lib/python27/dist-packages/scipy/sparse/csrpyc in__getitem__(self key)    279    280         elif isintlike(key) or isinstance(keyslice):-- 281             return self[key:]#[i] or [1:2]    282         else:    283             return self[asindices(key):]#[[12]]/usr/lib/python27/dist-packages/scipy/sparse/csrpyc in__getitem__(self key)    239                 #[1:2]    240                 if isintlike(col) or isinstance(col slice):-- 241                     return self_get_submatrix(row col)#[1:2j]    242                 else:    243                     P  extractor(colselfshape[1])T#[1:2[12]]/usr/lib/python27/dist-packages/scipy/sparse/csrpyc in_get_submatrix(self row_slice col_slice)    380         i0 i1  process_slice( row_slice M )    381         j0 j1  process_slice( col_slice N )-- 382         check_bounds( i0 i1 M )    383         check_bounds( j0 j1 N )    384/usr/lib/python27/dist-packages/scipy/sparse/csrpyc incheck_bounds(i0 i1 num)    376                 raise IndexError( \    377                       index out of bounds: 0%d%d 0%d%d%d%d %\-- 378                       (i0 num i1 num i0 i1) )    379    380         i0 i1  process_slice( row_slice M )IndexError: index out of bounds: 0113000113966 0114000113966113000114000,,1432,0.7828212290502793,0.4754262788365095,34439,435.1462005284706,34.66999622520979,110.28194779174773,2381,24,1162,106,travis,jfelectron,larsmans,false,larsmans,0,0,7,10,1334,false,false,false,false,0,2,0,0,0,0,82
2306332,scikit-learn/scikit-learn,python,2544,1382461300,1382517509,1382517509,936,936,github,false,false,false,6,1,1,0,2,0,2,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.386976702509518,0.10591510802780998,14,olivier.grisel@ensta.org,doc/modules/cross_validation.rst,14,0.013793103448275862,0,0,false,Update cross_validationrst Minor doc typo fixes,,1431,0.782669461914745,0.48177339901477834,34439,435.1462005284706,34.66999622520979,110.28194779174773,2381,24,1162,103,travis,StevenMaude,amueller,false,amueller,0,0,5,1,227,false,false,false,false,0,0,0,0,0,0,16
2301611,scikit-learn/scikit-learn,python,2543,1382413631,1382695747,1382695747,4701,4701,github,false,false,false,47,3,1,19,12,1,32,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.946859401905836,0.11943239809127008,14,olivier.grisel@ensta.org,doc/modules/cross_validation.rst,14,0.01348747591522158,0,1,false,LOO is bad doc Left out the example since it was too difficult to show a case where a simulation could run long enough to be asymptotic given the time and comparison constraints Here is the updated doc Let me know if theres anything else to do,,1430,0.7825174825174825,0.4932562620423892,34439,435.1462005284706,34.66999622520979,110.28194779174773,2380,24,1161,104,travis,johncollins,GaelVaroquaux,false,GaelVaroquaux,1,0.0,2,1,557,false,false,false,false,1,7,1,0,0,0,14
2294359,scikit-learn/scikit-learn,python,2541,1382316318,1385758742,1385758742,57313,57313,github,false,false,false,16,3,1,4,6,2,12,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,3,0,7,4.489001133566481,0.10825202411491185,27,olivier.grisel@ensta.org,sklearn/utils/tests/test_extmath.py,27,0.02608695652173913,1,3,false,[MRG]: fix tests centralize warnings and reset __warningregistry__ @larsmans this tackles the mysterious UnboundLocalError ValueError ,,1429,0.7823652904128762,0.49178743961352656,34519,433.99287348996205,34.58964628175787,109.99739274023001,2380,24,1160,118,travis,dengemann,larsmans,false,larsmans,11,0.9090909090909091,35,30,477,true,true,false,false,5,125,10,81,39,6,0
2295413,scikit-learn/scikit-learn,python,2540,1382312978,,1440695675,972984,,unknown,false,false,false,45,8,5,6,55,0,61,0,7,0,0,7,8,4,0,0,0,0,8,8,5,0,0,2426,216,2472,218,101.24837254282339,2.4415991308501592,64,vlad@vene.ro,sklearn/decomposition/nmf.py|benchmarks/bench_plot_nmf.py|doc/modules/decomposition.rst|sklearn/decomposition/nmf.py|benchmarks/bench_plot_nmf.py|doc/modules/decomposition.rst|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|benchmarks/bench_plot_nmf.py|doc/developers/performance.rst|doc/modules/classes.rst|doc/modules/decomposition.rst|sklearn/decomposition/__init__.py|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|benchmarks/bench_plot_nmf.py|doc/developers/performance.rst|doc/modules/classes.rst|doc/modules/decomposition.rst|sklearn/decomposition/__init__.py|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,30,0.008695652173913044,1,11,false,[WIP] NMF estimator based on the Lee and Seung algorithm Based on code by @vene Needs deltests/del deldocs/del Benchmark needs to be updated must check usage of NMF class in the scikitQuestion: does it make sense at all to bootstrap this thing using NNDSVD,,1428,0.7829131652661064,0.49178743961352656,34519,433.99287348996205,34.58964628175787,109.99739274023001,2380,24,1160,355,travis,larsmans,ogrisel,false,,90,0.7444444444444445,125,38,1190,true,true,true,true,25,253,72,161,146,0,79
2294734,scikit-learn/scikit-learn,python,2538,1382300044,1382347087,1382347088,784,784,github,false,false,false,16,4,1,0,17,0,17,0,5,0,0,1,12,1,0,0,0,0,12,12,12,0,0,0,4,0,41,4.268161887420067,0.10292649741352582,18,olivier.grisel@ensta.org,sklearn/tests/test_common.py,18,0.017408123791102514,0,2,false,[MRG] Investigate test_common non-running on travis This is related to issue scikit-learn/scikit-learn#2503: BUG: coveralls skips test_common,,1427,0.782761037140855,0.4922630560928433,34519,433.99287348996205,34.58964628175787,109.99739274023001,2380,24,1160,103,travis,ogrisel,ogrisel,true,ogrisel,47,0.8297872340425532,819,123,1607,true,true,false,false,23,497,59,270,135,5,15
2290520,scikit-learn/scikit-learn,python,2536,1382194168,1382351977,1382351977,2630,2630,github,false,false,false,16,1,1,0,4,0,4,0,4,0,1,0,1,1,0,0,0,1,0,1,1,0,0,145,0,145,0,0,0.0,1,amueller@ais.uni-bonn.de,examples/mlcomp_sparse_document_classification.py,1,0.0009689922480620155,0,0,false,MAINT remove mlcomp document classif example Its redundant given the other 20newsgroups example See also #1284,,1426,0.782608695652174,0.49806201550387597,34373,430.1341168940738,34.300177464870686,109.53364559392546,2379,25,1159,103,travis,larsmans,ogrisel,false,ogrisel,89,0.7415730337078652,125,38,1189,true,true,true,true,24,249,71,161,141,0,8
2285094,scikit-learn/scikit-learn,python,2533,1382105454,1382106713,1382106713,20,20,github,false,false,false,18,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.9784752220039477,0.09625637495534872,2,amueller@ais.uni-bonn.de,sklearn/decomposition/dict_learning.py,2,0.0019193857965451055,0,2,true,Update dict_learningpy Added parallelisation for online dictionnary learning algorithm Seems like the option was forgotten defaulting to n_jobs1,,1425,0.7824561403508772,0.5028790786948176,34373,430.1341168940738,34.300177464870686,109.53364559392546,2379,26,1158,101,travis,samuelstjean,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,309,false,false,false,false,0,2,0,0,0,0,1
2276508,scikit-learn/scikit-learn,python,2529,1381984928,,1404216853,370472,,unknown,false,true,false,48,2,2,1,10,0,11,0,4,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,10.06741268224632,0.24214507827171797,25,zehzinho@gmail.com,doc/modules/cross_validation.rst|doc/images/cross_validation_comparison.svg,25,0.02390057361376673,0,2,false,Added documentation and references for advice against LOO versus other forms of cross validation Issue #1427I put in a few lines an image (svg) comparing various validation techniques as a function of model complexity and cleaned up a minor math-type typo I found in the doc/modules/cross_validationrst file,,1422,0.7841068917018285,0.5200764818355641,34360,430.2968568102445,34.313154831199064,109.57508731082655,2379,26,1157,191,travis,johncollins,ogrisel,false,,0,0,2,1,553,false,false,false,false,1,3,0,0,0,0,8
2272580,scikit-learn/scikit-learn,python,2528,1381941080,1382282215,1382282215,5685,5685,github,false,false,false,59,2,1,2,30,0,32,0,8,0,0,3,3,3,0,0,0,0,3,3,3,0,0,96,0,156,0,7.4832489630166705,0.1799888985382872,115,olivier.grisel@ensta.org,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,108,0.07298578199052133,0,5,false,[MRG] Release the GIL at tree building time Good news Thanks to Gilles releasing the GIL on most splitter methods we can now release the GIL for most of the tree construction as wellI did a bench and I get a 10x speedup on a 12 cores machine with a python thread pool (individual trees build in 6s),,1421,0.7839549612948628,0.524170616113744,34334,430.3314498747597,34.33913904584377,109.59981359585251,2379,26,1156,104,travis,ogrisel,ogrisel,true,ogrisel,46,0.8260869565217391,818,123,1603,true,true,false,false,22,476,57,264,119,5,1
2270123,scikit-learn/scikit-learn,python,2527,1381905883,1381912353,1381912353,107,107,github,false,false,false,6,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.325408622327093,0.10403631882087833,21,olivier.grisel@ensta.org,sklearn/cross_validation.py,21,0.01975540921919097,0,1,false,DOC fix permutation_test_score docstring See #2062,,1420,0.7838028169014084,0.5305738476011289,34316,430.52803357034617,34.35715118312157,109.65730271593425,2378,26,1156,103,travis,shoyer,GaelVaroquaux,false,GaelVaroquaux,0,0,73,10,692,false,true,false,false,0,0,0,0,0,0,8
2267795,scikit-learn/scikit-learn,python,2525,1381873133,1381993110,1381993110,1999,1999,github,false,false,false,48,2,1,1,6,0,7,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,74,0,103,0,4.300862361392735,0.10344592312618128,2,jaquesgrobler@gmail.com,sklearn/hmm.py,2,0.0018656716417910447,0,1,false,Fixes to HMM docstrings As pointed out in #1817 many of the params and init_params docstrings were specific to the correct subclass and some options were not documented Also the fit() docstring of BaseHMM that was being inherited in some of the subclass had information specific to GaussianHMM,,1419,0.7836504580690627,0.5345149253731343,34316,430.52803357034617,34.35715118312157,109.65730271593425,2378,26,1155,104,travis,rmcgibbo,GaelVaroquaux,false,GaelVaroquaux,2,1.0,73,77,961,false,true,false,false,1,0,1,0,0,0,32
2267575,scikit-learn/scikit-learn,python,2524,1381870973,1382695604,1382695604,13743,13743,github,false,false,false,12,2,1,0,10,0,10,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,10,15,10,8.730038164718728,0.20997808834404977,2,jaquesgrobler@gmail.com,sklearn/hmm.py|sklearn/tests/test_hmm.py,2,0.0018656716417910447,0,2,false,Fix bug identified in #1817 comment 17340049 Heres the bug report:https://githubcom/scikit-learn/scikit-learn/issues/1817#issuecomment-17340048,,1418,0.7834978843441467,0.5345149253731343,34316,430.52803357034617,34.35715118312157,109.65730271593425,2378,26,1155,110,travis,rmcgibbo,ogrisel,false,ogrisel,1,1.0,73,77,961,false,true,true,false,1,0,0,0,0,0,22
2266001,scikit-learn/scikit-learn,python,2523,1381854702,1381933462,1381933462,1312,1312,github,false,false,false,28,3,1,3,9,0,12,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,19,38,29,9.203452528694214,0.22136482472097943,61,vlad@vene.ro,sklearn/metrics/tests/test_metrics.py|sklearn/preprocessing/label.py,49,0.045581395348837206,0,0,false,[MRG] FIX #2481: add warning for bug in old numpy with unicode This is a fix for  #2481 which causes the jenkins tests to fail with numpy 130,,1417,0.7833450952717008,0.5358139534883721,34316,430.52803357034617,34.35715118312157,109.65730271593425,2378,26,1155,102,travis,ogrisel,ogrisel,true,ogrisel,45,0.8222222222222222,817,123,1602,true,true,false,false,22,472,52,262,122,5,17
2264211,scikit-learn/scikit-learn,python,2521,1381832884,1381835202,1381835202,38,38,commits_in_master,false,false,false,7,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.5016314094275875,0.10839463734064253,4,nelle.varoquaux@gmail.com,doc/modules/linear_model.rst,4,0.003727865796831314,0,0,false,Update linear_modelrst minor fixes a doubled the,,1416,0.7831920903954802,0.5368126747437092,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1155,101,travis,dmedri,arjoly,false,arjoly,0,0,1,21,222,false,false,false,false,0,0,0,0,0,0,9
2261120,scikit-learn/scikit-learn,python,2519,1381783575,1381931761,1381931761,2469,2469,github,false,false,false,17,3,2,2,4,0,6,0,3,0,0,2,2,1,0,0,0,0,2,2,1,0,0,9,0,11,0,8.870292231229952,0.21358748019575188,14,samuel_ainsworth@brown.edu,sklearn/cluster/mean_shift_.py|doc/modules/naive_bayes.rst,9,0.008387698042870456,0,1,false,Minor doc fixes 1 Making the results in the NaiveBayes doc example more explicit2 Fixing https://githubcom/scikit-learn/scikit-learn/issues/2059,,1415,0.7830388692579505,0.5368126747437092,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1154,103,travis,ankit-maverick,ogrisel,false,ogrisel,0,0,11,6,324,false,false,false,false,0,0,0,0,0,0,17
2260369,scikit-learn/scikit-learn,python,2518,1381775519,1383556341,1383556341,29680,29680,commit_sha_in_comments,false,false,false,5,3,2,4,7,0,11,0,4,0,0,4,6,2,0,0,0,2,4,6,4,0,0,25,0,143,0,18.895832646700168,0.4549921441144781,7,vlad@vene.ro,doc/tutorial/statistical_inference/supervised_learning.rst|examples/svm/plot_iris.py|doc/modules/sgd.rst|examples/svm/plot_separating_hyperplane_unbalanced.py,6,0.001863932898415657,0,5,false,Doc remove duplicate svm examples ,,1414,0.7828854314002829,0.5368126747437092,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1154,113,travis,jaquesgrobler,amueller,false,amueller,69,0.927536231884058,13,13,628,true,true,true,true,10,149,29,24,22,0,1
2259557,scikit-learn/scikit-learn,python,2517,1381767809,1382010511,1382010511,4045,4045,github,false,false,false,18,2,2,0,4,0,4,0,4,1,5,1,7,0,0,5,1,5,1,7,0,0,5,0,0,0,0,8.85379369886796,0.21319027443467942,11,olivier.grisel@ensta.org,doc/images/minBox.png|doc/images/minBoxHighlight.png|doc/images/noneBox.png|doc/images/plusBox.png|doc/images/plusBoxHighlight.png|doc/tune_toc.rst|doc/user_guide.rst,9,0.001869158878504673,0,0,false,Cleaner Toctree collapse for useguide This removes the static images for the toctree and uses character triangles instead,,1413,0.7827317763623496,0.5364485981308411,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1154,103,travis,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,68,0.9264705882352942,13,13,628,true,true,true,false,10,147,26,24,19,0,3
2257898,scikit-learn/scikit-learn,python,2516,1381743315,1381838341,1381838341,1583,1583,github,false,false,false,49,2,1,0,7,0,7,0,4,1,0,1,3,2,0,0,1,0,2,3,2,0,0,62,0,66,0,9.390494932412922,0.22611348985628008,3,jaquesgrobler@gmail.com,doc/conf.py|doc/themes/scikit-learn/static/js/copybutton.js,3,0.0028011204481792717,0,1,false,DOC add copybutton to code examples This adds a non-intrusive button to codeblocks in the docsIt hides the  ect and makes it easy to copy into ipythonSee [scipy-lecture-notes](http://scipy-lecturesgithubio/intro/language/control_flowhtml#while-break-continue) for an example of this[screenshot at 2013-10-14 09 32 35](https://fcloudgithubcom/assets/1378870/1323993/120cc974-34a3-11e3-8afc-040122d35b85png)becomes[screenshot at 2013-10-14 09 32 46](https://fcloudgithubcom/assets/1378870/1323996/1aad8334-34a3-11e3-9dfe-0dd21dc9c42epng),,1412,0.7825779036827195,0.5368814192343604,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1154,101,travis,jaquesgrobler,ogrisel,false,ogrisel,67,0.9253731343283582,13,13,628,true,true,true,true,10,145,25,22,19,0,8
2257785,scikit-learn/scikit-learn,python,2515,1381740798,1381770979,1381770979,503,503,github,false,false,false,30,4,1,5,7,0,12,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,20,0,4.3886824059822604,0.10567497260046113,14,olivier.grisel@ensta.org,doc/sphinxext/gen_rst.py,14,0.013071895424836602,0,0,false,adds minutes + seconds to examples This adds the amount of minutes and seconds in addition to the usual time in seconds to examples[screenshot at 2013-10-14 08 50 51](https://fcloudgithubcom/assets/1378870/1323887/2f7de868-349d-11e3-88b6-dca3d5d5ce88png),,1411,0.7824238128986535,0.5368814192343604,34316,430.52803357034617,34.35715118312157,109.65730271593425,2377,26,1154,99,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,66,0.9242424242424242,13,13,628,true,true,false,false,10,145,24,21,19,0,8
2245747,scikit-learn/scikit-learn,python,2514,1381494855,1440539259,1440539259,984013,984013,merged_in_comments,false,true,false,361,84,6,106,111,9,226,0,8,1,0,14,21,15,0,0,3,1,19,23,20,0,0,3704,2086,7575,6108,142.55858725677666,3.4327867319646668,109,vanderplas@astro.washington.edu,doc/modules/preprocessing.rst|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/linear_model/base.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/_sparsefuncs.c|sklearn/utils/_sparsefuncs.pyx|sklearn/utils/setup.py|sklearn/utils/sparsefuncs.py|sklearn/utils/tests/test_sparsefuncs.py|doc/modules/preprocessing.rst|sklearn/pipeline.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|doc/modules/preprocessing.rst|sklearn/cluster/k_means_.py|sklearn/decomposition/pca.py|sklearn/decomposition/truncated_svd.py|sklearn/linear_model/base.py|sklearn/pipeline.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/sparsefuncs.py|sklearn/utils/sparsefuncs_fast.c|sklearn/utils/sparsefuncs_fast.pyx|sklearn/utils/tests/test_sparsefuncs.py,40,0.0071877807726864335,1,33,true,[MRG]  Refactoring and expanding sklearnpreprocessing scaling Hi thereThis PR refactors the data-scaling code from  sklearnpreprocessing to remove some duplicated code and adds some new features More specifically:* adds RobustScaler and robust_scale functions that use robust estimates of data center/scale (median & interquartile range) which should work better for outliers* Adds possibility to scale by sample instead of by feature via an axis1 parameter (the scale function could already do this now the *Scaler classes can too)* adds minmax_scale function (requrested by @ogrisel)* adds MaxAbsScaler similar in functionality to MinMaxScaler but also works on Sparse Matrices as proposed in the discussions of  #1799* Reuses the code common to StandardScaler RobustScaler MinMaxScaler and MaxAbsScaler by  putting it in an abstract base class Essentially the *Scaler classes are only responsible for estimating the necessary statistics in fit the rest of the Transformer API (transform/inverse_transform handling sparseness/different axis-parameters) is implemented in a BaseScaler as this code is common to all of the scalers This caused some parameters to be renamed and some attributes to be renamed: with_centering and with_scaling are the parameters that control if scaling/centering is performed and the center_ and scale_ attributes are used to store the centering/scaling values* *_scale functions now simply reuse the *Scaler classes internally to avoid code duplication* adds a lot of new tests for all the new functionality**Notes and Caveats*** ~~StandardScaler had parameters with_mean and with_std which are renamed to with_centering and with_scaling to fall in line with the other Scalers I wasnt sure how to handle deprecating the old parameter names  in __init__ -- whats the protocol here~~* RobustScaler cannot be fitted on sparse matrices:    - centering doesnt make sense because it risks destroying sparsity (similar to what StandardScaler does)    - Scaling doesnt work  because there is no decent code to calculate the IQR of sparse matrices available in scipy   ~~As an alternative we could advice people to scale using the MinMaxScaler instead to scale features to the same range with::~~  scaler  MinMaxScaler()  scalerwith_centeringFalse   scalerfit_transform(X)~~(MinMaxScaler doesnt support the with_centering parameter directly because I wasnt sure if this would lead to confusion)~~,,1410,0.7822695035460993,0.531895777178796,34432,428.0030204460967,34.212360594795534,109.14265799256506,2376,26,1151,357,travis,untom,amueller,false,amueller,4,0.5,7,0,235,true,false,false,false,2,7,6,4,3,0,19
2234785,scikit-learn/scikit-learn,python,2506,1381358762,1381685643,1381685643,5448,5448,commits_in_master,false,false,false,90,10,9,19,9,1,29,0,7,0,0,11,11,11,0,0,0,0,11,11,11,0,0,552,78,575,78,130.20802771967155,3.135387340157077,90,vlad@vene.ro,sklearn/metrics/pairwise.py|sklearn/utils/validation.py|sklearn/neighbors/kde.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/metrics/pairwise.py|sklearn/utils/extmath.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/metrics/pairwise.py|sklearn/utils/extmath.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_extmath.py|sklearn/neighbors/kde.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/metrics/pairwise.py|sklearn/utils/extmath.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_extmath.py|sklearn/neighbors/kde.py,31,0.004317789291882556,4,3,false,[MRG] Refactor norm computations k-means and metricspairwise both had optimized code to compute row-wise (squared) norms but k-means optimized the sparse case while metrics optimized the dense case This PR moves the best of both to the extmath module so that dense k-means no longer copies its entire X Ping @ogrisel @mblondel @GaelVaroquauxI also changed the KDE code to use the new row_norms function even though it doesnt really seem to be dealing with norms conceptually @jakevdp is this a good idea Otherwise I can remove the last commit,,1409,0.7821149751596878,0.5379965457685665,34432,428.0030204460967,34.212360594795534,109.14265799256506,2375,27,1149,100,travis,larsmans,larsmans,true,larsmans,88,0.7386363636363636,123,37,1179,true,true,false,false,25,252,71,153,146,0,70
2233580,scikit-learn/scikit-learn,python,2505,1381339365,1381949277,1381949277,10165,10165,github,false,false,false,174,13,3,12,13,2,27,0,5,0,0,4,5,3,0,0,1,1,4,6,3,0,1,28,1,109,32,18.01323712597585,0.43375571098908117,20,philippe.gervais@inria.fr,doc/modules/covariance.rst|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py,16,0.0060396893874029335,0,3,false,bug fix in GraphLasso/CV and corresponding smoke test + EmpiricalCovariance updates Bug fix in GraphLasso and GraphLassoCV---when scoring newly observed data X with the GraphLasso covariance estimator the location_ parameter was reported missing To this end a parameter assume_centered has been added to the :method:__init__ of bothIn the :method:fit the parameter assume_centered is the used to either calculate the mean and store it in location_ or store a zero vector solving the compatibility issue This issue drove me to update the doc of the covariance modulesmoke test---a smoke test has been added accordingly to test whether data scoring after fitting a GraphLasso works finecovariance module (doc)---in the documentation of the covariance the use of assume_centered is elaborated upon to warn the user about its implicationsEmpiricalCovariance::log_likelihood---the :method:log-likelihood of :class:EmpiricalCovariance returns a normalized biased and scaled version of the log-likelihood renamed it empirical expected log-likelihood (accounting for the normalization) rescaled it and added its constants so universal comparison is possible (beyond this software package),,1408,0.7819602272727273,0.5383951682484901,34432,428.0030204460967,34.212360594795534,109.14265799256506,2374,27,1149,104,travis,rphlypo,GaelVaroquaux,false,GaelVaroquaux,0,0,2,0,35,false,false,false,false,0,0,0,0,0,0,947
2232334,scikit-learn/scikit-learn,python,2504,1381326156,1381933033,1381933033,10114,10114,github,false,false,false,60,5,5,2,8,0,10,0,6,0,0,4,4,4,0,0,0,0,4,4,4,0,0,409,0,409,0,40.181508475865094,0.9676727795418042,111,olivier.grisel@ensta.org,sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,102,0.061206896551724135,1,9,false,[MRG] Small improvements to the tree code This PR brings some new improvements to the tree code In particular it makes node_reset node_split and node_value to be gil-free which brings us one step closer to a possible multi-threaded splitterIn any case this results in slight speed improvements - but it is barely noticeable to be honest though CC: @pprett,,1407,0.7818052594171997,0.5396551724137931,34427,428.06518139832104,34.21732942167485,109.15850930955355,2374,27,1149,105,travis,glouppe,arjoly,false,arjoly,44,0.9545454545454546,115,26,1063,true,true,true,true,19,235,46,177,49,0,11
2224767,scikit-learn/scikit-learn,python,2502,1381320152,1381602856,1381602856,4711,4711,github,false,false,false,78,3,1,3,3,0,6,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,25,0,50,0,4.681811915031937,0.11274992206507381,0,,examples/cluster/plot_dbscan.py,0,0.0,0,0,false,ENH: Call plot twice per class label rather than for every point Hi this is a proposed enhancement for Issue #2491 which I createdThe example code for dbscan (examples/cluster/plot_dbscanpy) calls plot for every point making it slower than it could be On an old machine I measured 8 seconds for the calls to plot with 2000 points versus 02 seconds after modifying to call plot for each labeled cluster as an array of pointsThanks-- Eric,,1406,0.7816500711237553,0.5396551724137931,34427,428.06518139832104,34.21732942167485,109.15850930955355,2374,27,1149,101,travis,ericjster,ogrisel,false,ogrisel,0,0,0,0,4,false,false,false,false,0,0,0,0,0,0,9
2230650,scikit-learn/scikit-learn,python,2500,1381296414,1381307786,1381307786,189,189,github,false,false,false,11,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.035656384486274,0.09729807390314227,18,larsmans@gmail.com,sklearn/ensemble/weight_boosting.py,18,0.015503875968992248,0,0,false,[MRG] AdaBoostRegressor: fix redundant recalculation of error_vectmax() A one-line trivial change,,1405,0.7814946619217081,0.540913006029285,34427,428.06518139832104,34.21732942167485,109.15850930955355,2374,27,1149,98,travis,ndawe,glouppe,false,glouppe,10,0.7,29,57,1335,true,true,true,false,2,20,6,1,9,0,189
2229165,scikit-learn/scikit-learn,python,2497,1381273045,1381273418,1381273418,6,6,github,false,false,false,59,2,1,0,0,0,0,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,7,0,4.588227973654249,0.11062039892124705,26,vlad@vene.ro,sklearn/utils/extmath.py,26,0.02239448751076658,0,0,false,FIX: support older scipy API for fast_dot We should have seen this earlier With this fix proposed the fast_dot should also be available for scipy versions below 09 With 100 a type check was introduced that wrapped the names argument in get_blas_funcs in a tupleThese changes should also be visible on Jenkins (which so far skipped the fast_dot),,1404,0.7813390313390314,0.5417743324720069,34427,428.06518139832104,34.21732942167485,109.15850930955355,2374,27,1148,98,travis,dengemann,agramfort,false,agramfort,10,0.9,35,30,465,true,true,true,true,3,122,9,81,39,5,-1
2208517,scikit-learn/scikit-learn,python,2495,1381228567,1381318336,1381318336,1496,1496,github,false,false,false,49,3,1,0,17,0,17,0,5,1,0,2,4,0,0,3,1,0,3,4,0,0,3,0,0,0,0,13.757991236216295,0.33169983872731457,11,olivier.grisel@ensta.org,.coveragerc|.gitignore|.travis.yml,7,0.003404255319148936,0,5,false,add coverage test for travis For issue #2494- this is a quick version -  just wanted to get it up so long Will check through travis docs now and try testing itAny feedback welcome I tried to follow more or less what was done in [nipys](https://githubcom/matthew-brett/nipy/commit/20bf11ff5952abce9e2d8256ccae5a9453e5651c) commit,,1403,0.7811831789023521,0.5412765957446809,34427,428.06518139832104,34.21732942167485,109.15850930955355,2373,27,1148,97,travis,jaquesgrobler,ogrisel,false,ogrisel,65,0.9230769230769231,12,13,622,true,true,true,true,10,130,26,13,22,0,16
2206303,scikit-learn/scikit-learn,python,2493,1381080485,,1405859324,412920,,unknown,false,false,false,69,30,2,82,148,5,235,0,9,0,0,1,3,1,0,0,0,0,3,3,2,0,0,154,0,1765,258,9.347573709903134,0.22536638080598712,81,peter.prettenhofer@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,81,0.06755629691409508,1,92,false,[MRG] Regression metrics return arrays for multi-output cases Hi I tried to solve issue https://githubcom/scikit-learn/scikit-learn/issues/2200  It now returns arrays for multi-output cases- [x] - Consensus on what the default argument to output_weights should be Micro or Macro averaging- [x] - On what the keyword should be output_weights or multi_output as @GaelVaroquaux  as suggested Once there is a consensus on these two things this can be merged,,1402,0.7817403708987162,0.5312760633861552,34427,428.06518139832104,34.21732942167485,109.15850930955355,2373,27,1146,195,travis,Manoj-Kumar-S,MechCoder,false,,0,0,19,15,474,false,false,false,false,0,2,0,0,0,0,62
2215282,scikit-learn/scikit-learn,python,2492,1381037666,1381685393,1381685393,10795,10795,commit_sha_in_comments,false,false,false,23,2,2,0,8,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,9.209432327310239,0.22203584558896997,2,olivier.grisel@ensta.org,Makefile|Makefile,2,0.0016680567139282735,0,1,false,Makefile: fix unsafe file deletion Piping things into xargs like this isnt safe Use finds -delete command or -exec rm -f {} +,,1401,0.7815845824411135,0.5312760633861552,34427,428.06518139832104,34.21732942167485,109.15850930955355,2372,27,1146,100,travis,ndawe,ogrisel,false,ogrisel,9,0.6666666666666666,29,57,1332,true,true,true,true,1,18,5,1,9,0,2460
2211342,scikit-learn/scikit-learn,python,2489,1380930942,1380988571,1380988571,960,960,github,false,false,false,44,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.919987581172542,0.1186189944870637,7,olivier.grisel@ensta.org,doc/modules/naive_bayes.rst,7,0.005804311774461028,0,0,true,Fix typo in the Gaussian PDF in Naive Bayes docs Hopefully this same issue doesnt exist in the codeI would also feel more comfortable if mu_y and sigma_y were mu_iy and sigma_iy respectively seeing as there are separate distributions per feature per class,,1400,0.7814285714285715,0.5281923714759535,34427,428.06518139832104,34.21732942167485,109.15850930955355,2372,27,1144,95,travis,samuela,jakevdp,false,jakevdp,0,0,15,28,1294,true,true,false,false,0,3,0,0,1,0,960
2206727,scikit-learn/scikit-learn,python,2488,1380876568,1386355551,1386355551,91256,91256,github,false,false,false,88,43,1,64,78,2,144,0,10,1,0,0,10,1,0,0,5,1,8,14,5,0,0,101,0,1353,0,4.654818869272354,0.11222583571605231,0,,examples/applications/plot_prediction_latency.py,0,0.0,0,15,true,[MRG] DOC Prediction Speed As discussed with Olivier and Lars on the list here is a PR to incorporate documentation and examples related to prediction latencyHere is the list of tasks:- [x] create topical / howto section on performance (not prediction power)- [x] cover atomic / bulk prediction mode- [x] cover influence of n_features- [x] cover influence of input data representation- [x] cover prediction throughput- [x] cover feature extraction- [x] cover blas/lapack libraries- [x] cover model compression (aka sparsify) ,,1399,0.7812723373838456,0.5286307053941909,34427,428.06518139832104,34.21732942167485,109.15850930955355,2372,26,1144,124,travis,oddskool,GaelVaroquaux,false,GaelVaroquaux,5,0.6,2,2,865,true,true,false,false,1,27,4,5,3,0,0
2201298,scikit-learn/scikit-learn,python,2487,1380812938,1381333890,1381333890,8682,8682,github,false,false,false,151,6,1,4,19,1,24,0,4,0,0,2,7,1,1,0,3,3,4,10,1,1,6,34,0,76,0,9.559174749979618,0.23046790975132983,71,vanderplas@astro.washington.edu,doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/layout.html,64,0.05320033250207814,0,10,false,another speedup attempt for indexhtml:  carousel images to use thumbnails instead 1 blocking js removed for page (sidebarjs)as discussed in and further addressing #2477Some notes: We were generating thumbnails already for the examples gallery however only for the first image of each set Thus some of the ones we desired for the carousel did not have their own thumbnailsSecond problem was that because of the example gallery thumbnail dimensions quite a few of the thumbnails didnt work well in the carousel directly due to extra white-space Thus during the thumbnail generation I added that the few needed ones or ones needing carousel-dimensions be generated too and written to the _images directoryLast thing is that I removed the sidebar script for the front page which acts as a blocking script according to google pagespeed Hopefully these changes will add to the speedup of the index a bit further,,1398,0.7811158798283262,0.5295095594347464,34427,428.06518139832104,34.21732942167485,109.15850930955355,2372,25,1143,98,travis,jaquesgrobler,ogrisel,false,ogrisel,64,0.921875,12,13,617,true,true,true,true,10,122,25,11,22,0,20
2175031,scikit-learn/scikit-learn,python,2485,1380553893,1380569048,1380569048,252,252,commit_sha_in_comments,false,false,false,93,1,1,1,3,0,4,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,55,17,55,17,9.196163835987141,0.22171636109669232,32,vanderplas@astro.washington.edu,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,30,0.024350649350649352,0,6,false,[MRG] sparse matrix support in pairwise_distances_argmin_min + optimizations Fixes #2417 Additionally Ive refactored the code and optimized the dense Euclidean distance computations for NumPy  16 using its great new tool [einsum](http://docsscipyorg/doc/numpy-170/reference/generated/numpyeinsumhtml): X  nprandomrandn(900 800) %timeit (X * X)sum(axis1)1000 loops best of 3: 175 ms per loop %timeit npeinsum(ijij-i X X)1000 loops best of 3: 341 us per loopFor larger arrays the difference gets more extreme einsum also takes much less memory as it doesnt have to allocate a large temporary of shape Xshape,,1397,0.780959198282033,0.5284090909090909,34422,433.4146766602754,34.512811573993375,110.59787345302423,2369,27,1140,94,travis,larsmans,larsmans,true,larsmans,87,0.735632183908046,123,37,1170,true,true,false,false,29,289,77,170,157,0,30
2171686,scikit-learn/scikit-learn,python,2482,1380394124,1380532186,1380532186,2301,2301,merged_in_comments,false,false,false,8,2,2,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,80,38,80,38,13.395694068229465,0.3229668460549539,5,vanderplas@astro.washington.edu,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/tests/test_gaussian_process.py,5,0.0040128410914927765,0,1,false,Gaussian process for arbitrary-dimensional output spaces was #1611,,1396,0.7808022922636103,0.536115569823435,34401,432.86532368245105,34.533879829074735,110.6072497892503,2368,29,1138,96,travis,agramfort,agramfort,true,agramfort,35,0.8857142857142857,129,181,1396,true,true,false,false,6,116,48,113,88,1,-1
2160449,scikit-learn/scikit-learn,python,2479,1380216274,1380233670,1380233670,289,289,github,false,false,false,30,3,2,0,1,0,1,0,1,0,0,30,31,0,1,29,0,0,31,31,1,1,29,0,0,0,0,4.827997635930037,0.1163312638823348,71,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html|doc/images/last_digit.png|doc/images/minBox.png|doc/images/minBoxHighlight.png|doc/images/ml_map.png|doc/images/no_image.png|doc/images/plot_digits_classification.png|doc/images/plot_face_recognition_1.png|doc/images/plot_face_recognition_2.png|doc/images/plusBox.png|doc/images/plusBoxHighlight.png|doc/images/rbm_graph.png|doc/logos/scikit-learn-logo-notext.png|doc/logos/scikit-learn-logo-small.png|doc/logos/scikit-learn-logo-thumb.png|doc/logos/scikit-learn-logo.png|doc/modules/glm_data/lasso_enet_coordinate_descent.png|doc/testimonials/images/aweber.png|doc/testimonials/images/evernote.png|doc/testimonials/images/rangespan.png|doc/testimonials/images/yhat.png|doc/themes/scikit-learn/static/img/FNRS-logo.png|doc/themes/scikit-learn/static/img/forkme.png|doc/themes/scikit-learn/static/img/glyphicons-halflings.png|doc/themes/scikit-learn/static/img/google.png|doc/themes/scikit-learn/static/img/plot_classifier_comparison_1.png|doc/themes/scikit-learn/static/img/plot_manifold_sphere_1.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-notext.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-small.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.png,62,0.0008084074373484236,0,0,false,[MRG] improve website speed hopefully This addresses #2477 The PNGs are in total about half an MB smaller and I removed two examples from the carousel: LDA-QDA and decomposing faces,,1395,0.7806451612903226,0.5432497978981407,34403,432.84015928843417,34.53187222044589,110.60081969595676,2365,29,1136,96,travis,larsmans,larsmans,true,larsmans,86,0.7325581395348837,123,37,1166,true,true,false,false,31,290,76,169,157,0,204
2130171,scikit-learn/scikit-learn,python,2478,1380157294,1380741092,1380741092,9729,9729,commit_sha_in_comments,false,false,false,71,1,1,7,1,0,8,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,24,3,24,9.132832812875835,0.22005676344686362,16,vanderplas@astro.washington.edu,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,14,0.011382113821138212,0,0,false,FIX #2443: _limit_features now keeps correct top features when max_features is not None A fix for issue #2443- I have added a regression test for CountVectorizer but I am not sure if there should be another one for TfidfVectorizer Perhaps someone more well-versed with tf-idf should look into this- I could not run the full tests due to (I think) the OpenBLAS issue but tests for sklearnfeature_extraction are successful,,1394,0.7804878048780488,0.5455284552845528,34403,432.84015928843417,34.53187222044589,110.60081969595676,2365,29,1135,95,travis,flukeskywalker,larsmans,false,larsmans,0,0,15,2,260,false,false,false,false,1,4,0,0,1,0,2613
2146692,scikit-learn/scikit-learn,python,2476,1380022284,,1442533139,1041787,,unknown,false,false,false,218,3,2,0,0,0,0,0,0,3,0,6,9,5,0,1,3,0,6,9,5,0,1,351,127,352,127,36.87234279628494,0.888431639544905,38,vlad@vene.ro,sklearn/cluster/bicluster/fabia.py|sklearn/cluster/bicluster/tests/test_fabia.py|doc/images/fabia.png|doc/modules/biclustering.rst|sklearn/cluster/bicluster/__init__.py|sklearn/cluster/bicluster/fabia.py|sklearn/cluster/bicluster/tests/test_fabia.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py,22,0.0008176614881439084,1,0,false,[WIP] FABIA biclustering algorithm This PR adds the FABIA biclustering algorithm to sklearn as discussed off-list with @kemaleren (and his mentors)As this is my first larger contribution please point out any rough edges I might have overlooked in the PR processThings worth mentioning:* Two of the included tests are skipped because of their long running time as they are not simple functionality tests but rather reproduce part of a simulation study from the original paper (eg they make sure results are up to par with the R reference implementation) Also these two tests rely on Issue  #2445 being fixed* I couldnt build the docs due what seems to be something related to Issue #1140 but Im unable to fix it on my computer :( So I couldnt verify if there are formatting errors in the accompanying documentation* I havent gotten around writing a good example yet (ideas welcome)* Theres no benchmarking included (as the reference implementation is in R) However this implementation runs faster than the pure R version of FABIA (part of the reference package) but slower than the current reference implementation (which is C wrapped in R) by a factor of 2-5 (depending on data size)References: * Hochreiter Bodenhofer et al 2010 *FABIA: factor analysis  for bicluster acquisition*   https://wwwncbinlmnihgov/pmc/articles/PMC2881408/,,1393,0.7810480976310122,0.5437448896156991,34388,430.6734907525881,34.401535419332326,109.74758636733745,2365,29,1134,355,travis,untom,untom,true,,3,0.6666666666666666,7,0,218,false,false,false,false,2,5,5,4,0,0,-1
2139832,scikit-learn/scikit-learn,python,2474,1379929572,1379929821,1379929821,4,4,github,false,false,false,25,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,4.30078531837894,0.10342781693656526,28,vanderplas@astro.washington.edu,sklearn/tree/tests/test_tree.py,28,0.02280130293159609,0,0,false,[MRG] Add tests for splitterpresort-best This adds trees built with a PresortBestSplitter to the tests in test_treepyWaiting for Travis green light to merge this ,,1392,0.7808908045977011,0.5415309446254072,34388,430.5280911946028,34.401535419332326,109.74758636733745,2365,29,1133,97,travis,glouppe,glouppe,true,glouppe,43,0.9534883720930233,112,26,1047,true,true,false,false,18,232,42,154,46,0,-1
2135586,scikit-learn/scikit-learn,python,2471,1379818370,,1380479577,11020,,unknown,false,false,false,41,2,1,0,2,0,2,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,3,4.242229014218752,0.10201832028406506,7,vlad@vene.ro,sklearn/pipeline.py,7,0.0056634304207119745,0,0,false,Pipeline string formatting TypeError When pipeline tries to raise a ValueError because of non-unique names Python tries to look into the tuple and throws a TypeError: Not all strings converted during formattingThis is fixed by wrapping names in a 1-tuple,,1391,0.7814521926671459,0.5364077669902912,34383,421.5746153622429,33.97027600849257,105.37184073524706,2364,29,1131,102,travis,paulgb,larsmans,false,,0,0,136,47,1712,false,true,false,false,0,0,0,0,1,0,5214
2116969,scikit-learn/scikit-learn,python,2470,1379812182,,1410206127,506505,,unknown,false,false,false,60,4,1,4,1,1,6,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,16,0,30,8,4.623731791088553,0.11119280670370761,22,vanderplas@astro.washington.edu,sklearn/feature_extraction/text.py,22,0.017813765182186234,0,2,false,change default token_pattern in Vectorizer class HeyI suggest changing default token_pattern in Vectorizer class to exclude any words start with numerical numberUsually those are meaningless numbers and should be treated as noise in text miningFor example when I run topics_extraction_with_nmf script I got a weird topic with top words10 00 space new 11 12 15Thanks,,1390,0.7820143884892087,0.5368421052631579,34383,421.5746153622429,33.97027600849257,105.37184073524706,2364,29,1131,207,travis,chyikwei,chyikwei,true,,0,0,12,1,361,false,true,false,false,0,0,0,0,0,0,1014
2116747,scikit-learn/scikit-learn,python,2469,1379788434,1379878195,1379878195,1496,1496,github,false,true,false,79,4,1,1,27,0,28,0,4,0,0,4,5,4,0,0,0,0,5,5,4,0,0,225,0,297,0,12.875035476324307,0.3096224858417374,127,vanderplas@astro.washington.edu,sklearn/ensemble/gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py,92,0.04854368932038835,1,15,false,[MRG] Make GBRT faster by presorting X This aims at fixing #2466 It implements our old splitting strategy in a PresortBestSplitter object It basically precomputes X_argsorted to save computations which may be advantageous when building many small trees with max_features set to a high value I have wired it with GBRTs Forests stay the same  All test pass @pprett Now I count on you to benchmark this On my box tests for GBRTs already goes from 55s to 32s ,,1389,0.7818574514038877,0.5364077669902912,34383,421.5746153622429,33.97027600849257,105.37184073524706,2364,29,1131,100,travis,glouppe,glouppe,true,glouppe,42,0.9523809523809523,112,26,1045,true,true,false,false,18,216,39,154,45,0,24
2133157,scikit-learn/scikit-learn,python,2465,1379752587,1379785308,1379785308,545,545,commit_sha_in_comments,false,false,false,2,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.515171836231954,0.10856259418693011,7,vlad@vene.ro,doc/modules/svm.rst,7,0.00558659217877095,0,0,false,Small typo ,,1387,0.7822638788752704,0.5283320031923384,34381,421.5991390593642,33.9722521159943,105.37797039062274,2361,29,1131,98,travis,rsivapr,larsmans,false,larsmans,2,0.5,20,7,498,false,false,false,false,0,9,2,5,1,0,-1
2116648,scikit-learn/scikit-learn,python,2464,1379717753,1381608959,1381608959,31520,31520,commit_sha_in_comments,false,false,false,26,3,1,3,4,0,7,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,53,41,70,9.11499304643755,0.21916049422005973,8,vlad@vene.ro,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,7,0.00558659217877095,0,1,true,[MRG] Fix FeatureUnion when n_jobs  1 It didnt work previously because transformers were fit in another processes and information they learned was not transferred back,,1386,0.7821067821067821,0.5275339185953711,34381,421.5991390593642,33.9722521159943,105.37797039062274,2360,29,1130,107,travis,kmike,larsmans,false,larsmans,12,0.9166666666666666,366,0,1520,false,true,false,true,0,8,1,0,1,0,131
2117132,scikit-learn/scikit-learn,python,2463,1379716669,1380124761,1380124761,6801,6801,github,false,false,false,102,1,1,3,17,0,20,0,3,0,0,7,7,4,0,0,0,0,7,7,4,0,0,36,260,36,260,32.8157244771522,0.7906827189275634,153,vlad@vene.ro,doc/modules/cross_validation.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_naive_bayes.py,93,0.0176,1,1,true,[MRG] FIX #2372: non-shuffling StratifiedKFold implementation This is a refactoring of @dnouris fix for issue #2372 in PR #2450 The goal is to make the StratifiedKFold CV scheme not underestimate overfitting too much on datasets that have a strong samples dependencies This important as StratifiedKFold is used by default by cross_val_score and GridSearchCV for classification problemsThe current implementation of  StratifiedKFold in master shuffles the data before computing the splits which hides the potential non-respect of the IID assumption by the dataset This is highlighted in [a test on the digits dataset](https://githubcom/scikit-learn/scikit-learn/pull/2463/files#L5R282) that as strong dependency between samples (co-authorship of consecutive samples),,1385,0.7819494584837545,0.5272,34388,430.70257066418515,34.401535419332326,109.74758636733745,2360,29,1130,102,travis,ogrisel,ogrisel,true,ogrisel,44,0.8181818181818182,807,122,1577,true,true,false,false,21,438,42,311,140,3,2797
2121144,scikit-learn/scikit-learn,python,2462,1379703461,1379819780,1379819780,1938,1938,github,false,false,false,20,3,3,0,1,0,1,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,33,2,33,18.20213265818561,0.4376512566144185,86,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,67,0.053557154276578735,0,5,true,[MRG] Add unicode support to sklearnmetricsclassification_report sklearnmetricsclassification_report with unicode labels was broken in Python 2x because of how {0}format works,,1384,0.7817919075144508,0.5267785771382894,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,29,1130,102,travis,kmike,GaelVaroquaux,false,GaelVaroquaux,11,0.9090909090909091,366,0,1520,false,true,false,false,0,7,0,0,0,0,1938
2114460,scikit-learn/scikit-learn,python,2460,1379618080,1386008582,1386008582,106448,106448,github,false,false,false,115,227,35,28,54,1,83,0,8,0,0,2,9,2,0,0,0,0,9,9,7,0,0,1316,3136,2604,6383,225.98501428714937,5.4450251634046465,105,vlad@vene.ro,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,71,0.0575831305758313,0,22,false,[MRG] Multilabel-indicator roc auc and average precision The goal of this pr is to add multilabel-indicator support with various types of averagingfor roc_auc_score and average_precision_scoreStill to do:- [x] Implementation of micro macro weighted and sample roc_auc_score- [x] Implementation of micro macro weighted and sample average_precision_score- [x] Add a general test for binary metric that are extended through averaging- [x] Harmonize how global tests are performed for the metrics module- [x] Write the narrative documentation- [x] Ensure that the scorer interface work with both possible representations (see #2451)- [x] pep8A priori I wont add ranking-based average_precision_score I dont want to add support for the multilabel-sequence format,,1382,0.7821997105643994,0.5198702351987023,34388,430.70257066418515,34.401535419332326,109.74758636733745,2360,29,1129,123,travis,arjoly,ogrisel,false,ogrisel,38,0.8157894736842105,19,22,639,true,true,true,false,29,243,36,240,85,0,972
2118955,scikit-learn/scikit-learn,python,2459,1379555782,1441097000,1441097000,1025626,1025626,commits_in_master,false,false,false,257,3,1,2,12,0,14,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,16,91,56,9.355218061541326,0.2249364410985228,47,vlad@vene.ro,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,41,0.03333333333333333,0,3,false,support X_norm_squared in euclidean_distances Theres a comment in euclidean_distances saying    # should not need X_norm_squared because if you could precompute that as    # well as Y then you should just pre-compute the output and not even    # call this functionThats not necessarily true though Ive run into a situation today where I have a whole bunch of sets and need to do something based on the distances between each pair of sets Its helpful to cache the squared norms for each of the sets if I did that and called it with just Y_norm_squared for each pair thatd still be recomputing the norms for X all the time (Of course I can just do it without the helper function which is what Im doing now but its nicer to use helpers)Another situation is when you happen to already have the squared norms for a set X and then you want euclidean_distances(X) I guessed that maybe euclidean_distances(X Y_norm_squaredX_norm_sq) would work but looking at the code that doesnt actually use X_norm_sq Now euclidean_distances can handle that case tooThis also adds an extremely simple test that passing X_norm_squared and/or Y_norm_squared gives the same result previously there was no test that used Y_norm_squaredAs an aside: I have no idea why XX is being computed with X * X and YY with Y ** 2 (which necessitates the annoying copy code when its sparse) it seems like it should be exactly the same situation except for the very minor difference of the shape I left it as-is though,,1381,0.782041998551774,0.5186991869918699,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,29,1128,343,travis,dougalsutherland,GaelVaroquaux,false,GaelVaroquaux,2,1.0,26,20,1758,false,true,false,false,0,0,0,0,0,0,436
2112163,scikit-learn/scikit-learn,python,2458,1379541459,,1401321028,362932,,unknown,false,false,false,123,29,9,42,50,0,92,0,6,0,0,3,7,3,0,0,0,0,7,7,6,0,0,130,0,454,113,56.70967325249755,1.3637702230800983,57,vanderplas@astro.washington.edu,sklearn/preprocessing/label.py|sklearn/preprocessing/label.py|sklearn/utils/multiclass.py|sklearn/utils/multiclass.py|sklearn/preprocessing/label.py|sklearn/utils/multiclass.py|sklearn/preprocessing/label.py|sklearn/multiclass.py|sklearn/utils/multiclass.py|sklearn/multiclass.py|sklearn/preprocessing/label.py|sklearn/utils/multiclass.py|sklearn/multiclass.py,27,0.014634146341463415,0,32,false,[WIP]: Sparse label_binarizer and OneVsRestClassifier https://githubcom/scikit-learn/scikit-learn/issues/2441- [x] modify the function sklearnutilsmulticlasstype_of_target to recognise sparse binary matrix as a multilabel-indicator format- [x] modify the function sklearnutilsmuticlassunique_labels to work with this new format- [x] modify the label_binarize function to return a sparse label indicator- [x] modify OVR to work with sparse output matrices For instance by extracting a dense column of the sparse matrix and fit the classifier on it Prediction would be made by creating the sparse matrix incrementally out of each fitted classifier- [ ] Write tests- [x] Edit _fit_binary method to take in dense and sparse matrices- [x] Single label Multi-class broken- [ ] import scipysparse as sp instead of from scipysparse import coo_matrix,,1380,0.782608695652174,0.5186991869918699,34383,421.5746153622429,33.97027600849257,105.37184073524706,2360,29,1128,187,travis,rsivapr,arjoly,false,,1,1.0,20,7,495,false,false,false,false,0,5,1,0,0,0,5
2110895,scikit-learn/scikit-learn,python,2457,1379525048,1380131187,1380131187,10102,10102,github,false,false,false,89,7,2,12,7,0,19,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,22,24,37,13.359246724185525,0.32120912565883075,31,larsmans@gmail.com,sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/neural_network/rbm.py,24,0.01948051948051948,0,1,false,[MRG] Switch BernoulliRBM to CSR format While taking a look at Issue #2455 Ive noticed that BernoulliRBM  enforces CSC format for sparse data However it accesses the data row-wise (it trains in Minibatches of batch_size rows) Thus CSR format seems like a much more natural choiceI ran some short tests on a 2000x100000 sparse matrix with 95% sparsity on an RBM with 128 components and saw a speedup of ~ 5% when switching from CSC to CSR matrices (Id expect the speedup to be larger on larger matrices),,1379,0.7824510514865844,0.5178571428571429,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,29,1128,100,travis,untom,larsmans,false,larsmans,2,0.5,7,0,212,false,false,false,false,2,0,2,0,0,0,41
2115191,scikit-learn/scikit-learn,python,2456,1379522449,1379792182,1379792182,4495,4495,commits_in_master,false,false,false,27,1,1,2,5,0,7,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,22,7,22,8.986796228201559,0.21607812315560712,31,larsmans@gmail.com,sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py,24,0.01948051948051948,0,0,false,Issue #2455: Make RBM work with sparse input when verboseTrue This fixes Issue #2455:the BernoulliRBM class was unable to handle sparse inputs when verboseTrue was set,,1378,0.7822931785195936,0.5178571428571429,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,29,1128,100,travis,untom,untom,true,untom,1,0.0,7,0,212,false,false,false,false,2,0,1,0,0,0,4388
2112275,scikit-learn/scikit-learn,python,2450,1379424066,1379716877,1379716877,4880,4880,merged_in_comments,false,true,false,16,7,1,7,6,0,13,0,3,0,0,3,6,3,0,0,0,0,6,6,3,0,0,14,44,30,82,13.5073554449491,0.32477024506033025,49,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_naive_bayes.py,22,0.017346053772766695,0,10,false,[MRG] Fix #2372: StratifiedKFold less impact on the original order of samples See #2372 for motivation,,1376,0.782703488372093,0.5524718126626192,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,30,1127,100,travis,dnouri,ogrisel,false,ogrisel,3,0.6666666666666666,227,48,1670,false,true,true,false,1,0,0,0,1,0,2703
2137461,scikit-learn/scikit-learn,python,2446,1379345498,,1379875180,8828,,unknown,false,false,false,27,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.589774690055682,0.11035633562293527,4,vlad@vene.ro,sklearn/metrics/cluster/bicluster/bicluster_metrics.py,4,0.0038022813688212928,0,0,false,Fixes issue #2445 This commit fixes issue #2445: When comparing bicluster-results that contain different number of biclusters sklearnmetricsconsensus_score can sometimesgive wrong results This is fixed here,,1375,0.7832727272727272,0.6036121673003803,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,30,1126,102,travis,untom,untom,true,,0,0,7,0,210,false,false,false,false,1,0,0,0,0,0,-1
3001503,scikit-learn/scikit-learn,python,2444,1379337168,,1392492845,219201,,unknown,false,false,false,41,2,2,0,0,0,0,0,0,0,0,3,3,3,0,0,0,0,3,3,3,0,0,46,2,46,2,17.318952776053365,0.416416119364832,20,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py,16,0.007619047619047619,0,0,false,[MRG] Simplify linear models Two changes to streamline the sklearnlinear_model module:* to fix #1404 I removed decision_function from regression estimators* CD estimators now have densify and sparsify like the SGD ones to support sparse coefficients Deprecated the previous hack,,1374,0.7838427947598253,0.6028571428571429,34379,421.62366560981997,33.97422845341633,105.38410075918439,2360,30,1126,149,travis,larsmans,larsmans,true,,85,0.7411764705882353,121,37,1156,true,true,false,false,33,282,74,157,151,0,-1
2105843,scikit-learn/scikit-learn,python,2442,1379099041,1380062317,1380062317,16054,16054,github,false,true,false,44,9,3,7,49,0,56,0,4,0,0,2,9,2,0,0,0,0,9,9,8,0,0,20,0,178,83,8.266348210906006,0.1993650932734898,99,vanderplas@astro.washington.edu,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,96,0.09142857142857143,0,22,true,[MRG] Trees: random features selection is biased I am currently studying for my research how actually random are randomized trees I stumbled upon a bug introduced with the new implementation which caused the random feature selection to be biased This patch solves the issue,,1373,0.7836853605243991,0.599047619047619,34372,421.65134411730475,33.952053997439776,105.40556266728733,2360,31,1123,109,travis,glouppe,ogrisel,false,ogrisel,41,0.9512195121951219,112,26,1037,true,true,true,false,18,183,38,111,44,0,1
2097774,scikit-learn/scikit-learn,python,2438,1378994369,1379004831,1379004831,174,174,github,false,false,false,38,2,2,0,3,0,3,0,3,0,0,5,5,5,0,0,0,0,5,5,5,0,0,64,36,64,36,30.04970282485108,0.7247325840587379,128,vanderplas@astro.washington.edu,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,90,0.02865329512893983,1,0,false,[MRG] ENH reduce memory overhead of storing tree ensemble This pr  remove the storage of the splitter and criterion in order to release unused memoryThis is useful when one wants to build large tree ensemble(ping @glouppe),,1372,0.7835276967930029,0.5950334288443171,34294,422.61036916078615,34.029276258237594,105.6453023852569,2360,31,1122,98,travis,arjoly,arjoly,true,arjoly,37,0.8108108108108109,19,22,632,true,true,false,false,27,230,32,235,83,0,41
2097630,scikit-learn/scikit-learn,python,2437,1378992814,,1378994387,26,,unknown,false,true,false,43,2,2,0,0,0,0,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,125,0,125,0,17.056391050614074,0.4113625493362638,96,vanderplas@astro.washington.edu,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,90,0.05534351145038168,1,0,false,[WIP] Reduce memory overhead due the splitter and criterion This pr add a finalize method to the Splitter class and Criterion class of _treepyxin order to release unused memoryThis is useful when one wants to build large tree ensemble(ping @glouppe),,1371,0.7840991976659373,0.5944656488549618,34294,422.61036916078615,34.029276258237594,105.6453023852569,2360,31,1122,98,travis,arjoly,arjoly,true,,36,0.8333333333333334,19,22,632,true,true,false,false,27,230,31,235,83,0,-1
2095450,scikit-learn/scikit-learn,python,2436,1378953268,,1379337619,6405,,unknown,false,false,false,15,3,3,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,109,51,109,51,17.431711065147468,0.42041443755755775,33,vlad@vene.ro,sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py,24,0.022878932316491896,0,0,false,MAINT deprecate explicit indices parameter to CV generators even if True Fills holes in #2334,,1370,0.7846715328467153,0.5929456625357483,34294,422.61036916078615,34.029276258237594,105.6453023852569,2359,31,1121,98,travis,jnothman,jnothman,true,,48,0.5625,18,1,1597,true,true,false,false,13,203,28,113,17,0,912
2093679,scikit-learn/scikit-learn,python,2435,1378932645,,1379338870,6770,,unknown,false,false,false,18,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,29,26,29,26,8.861799905234415,0.21372775222838952,15,olivier.grisel@ensta.org,sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_label.py,12,0.011483253588516746,0,0,false,[MRG] FIX issue #1993: passing a multilabel indicator is no more noop As the title suggest fix issue #1993,,1369,0.7852447041636231,0.5913875598086125,34229,423.41289549796954,34.09389698793421,105.84592012620878,2359,31,1121,97,travis,arjoly,larsmans,false,,35,0.8571428571428571,19,22,631,true,true,true,true,27,230,30,235,83,0,1259
2085962,scikit-learn/scikit-learn,python,2434,1378836831,1379069345,1379069345,3875,3875,merged_in_comments,false,false,false,41,1,1,2,3,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,306,0,306,0,4.959654547888564,0.11999827870515736,1,vanderplas@astro.washington.edu,sklearn/externals/six.py,1,0.0009596928982725527,0,0,false,Update six to version 141 scikit-learn bundles a modified copy of version 120 of six This PR updates six to version 141 This new version extends the managed modules and its backwards compatible with the modified version of six currently shipped,,1368,0.7850877192982456,0.5902111324376199,33896,422.29171583667693,34.36983714892613,106.47274014632994,2359,31,1120,100,travis,sergiopasra,larsmans,false,larsmans,1,1.0,1,0,801,false,false,false,false,0,0,1,0,1,0,1419
2083197,scikit-learn/scikit-learn,python,2432,1378806185,1378806345,1378806345,2,2,github,false,false,false,11,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.5525851710724226,0.11014801282532884,12,vlad@vene.ro,doc/modules/clustering.rst,12,0.011549566891241578,0,0,false,DOC: Update pyamg site and fix a typo pyamg website moved,,1367,0.7849305047549379,0.5909528392685275,33892,422.63661040953616,34.37389354419922,106.24926236279948,2359,31,1120,102,travis,jwkvam,glouppe,false,glouppe,2,0.5,2,3,1575,false,true,false,false,0,0,0,0,0,0,2
2076211,scikit-learn/scikit-learn,python,2430,1378716039,1378716410,1378716410,6,6,github,false,false,false,12,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.287483014157321,0.10373398351383124,25,vlad@vene.ro,sklearn/ensemble/forest.py,25,0.02403846153846154,0,0,false,FIX: free selfestimators_ This fixes #2414Waiting for Travis to merge this,,1366,0.7847730600292826,0.5865384615384616,33891,422.64908087692896,34.374907792629315,106.2523973916379,2358,31,1119,102,travis,glouppe,glouppe,true,glouppe,40,0.95,112,26,1033,true,true,false,false,15,171,33,104,35,0,-1
2073462,scikit-learn/scikit-learn,python,2428,1378655901,1395844941,1395844941,286424,286424,commits_in_master,false,false,false,8,17,1,20,38,0,58,0,8,0,0,1,3,1,0,0,0,0,3,3,2,0,0,4,0,92,175,4.1450595596300435,0.10044506671922436,38,thomas.unterthiner@gmx.net,sklearn/neural_network/rbm.py,38,0.03657362848893166,0,9,false,changing private _fit to public partial_fit Issue #2400,,1365,0.7846153846153846,0.5871029836381135,33891,422.64908087692896,34.374907792629315,106.2523973916379,2357,31,1118,173,travis,dsullivan7,ogrisel,false,ogrisel,3,0.6666666666666666,7,19,30,false,true,true,false,0,1,3,0,2,0,7
2070556,scikit-learn/scikit-learn,python,2426,1378572362,1379785147,1379785147,20213,20213,github,false,false,false,106,1,1,2,44,0,46,0,10,0,0,9,9,9,0,0,0,0,9,9,9,0,0,305,0,305,0,27.154843491186192,0.6580628357636163,10,vanderplas@astro.washington.edu,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pxd|sklearn/utils/seq_dataset.pyx|sklearn/utils/setup.py|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx,8,0.0019286403085824494,0,32,false,[MRG] SGD Cython improvements Here are some safe bits from #2419 which can be pulled without waiting for my threading experiments* nogil declarations where possible (but no with nogil blocks since the performance benefit of those is not clear)* use BLAS where appropriate* replaced DOUBLE by double INTEGER and int32 by int/npintc since thats the type used by scipysparse* explicit checks that dataset dims dont exceed INT_MAX* replaced an npall by an explicit loop (npymath library for C99 isfinite)I also cleaned up the code a little bit (eg min and max no longer need to be defined for current Cython),,1364,0.7844574780058651,0.5882352941176471,33891,422.64908087692896,34.374907792629315,106.2523973916379,2356,31,1117,117,travis,larsmans,larsmans,true,larsmans,84,0.7380952380952381,120,37,1147,true,true,false,false,25,249,64,139,138,0,99
2064336,scikit-learn/scikit-learn,python,2425,1378483552,1378663197,1378663197,2994,2994,merged_in_comments,false,false,false,15,4,2,2,1,0,3,0,2,1,0,1,2,0,0,1,1,0,1,2,0,0,1,0,0,0,0,4.797383489320549,0.11647634988806717,11,olivier.grisel@ensta.org,doc/images/bestofmedia-logo.gif|doc/testimonials/testimonials.rst,11,0.010607521697203472,1,0,true,[MRG] Bestofmedia Testimonial Following on @GaelVaroquauxs request for testimonials here is one from my employer,,1363,0.7842993396918562,0.5882352941176471,33891,422.64908087692896,34.374907792629315,106.2523973916379,2356,31,1116,103,travis,oddskool,GaelVaroquaux,false,GaelVaroquaux,4,0.5,2,2,837,true,true,false,false,1,33,3,21,3,0,2
2064321,scikit-learn/scikit-learn,python,2424,1378483301,1378640930,1378640930,2627,2627,github,false,false,false,27,3,3,0,0,0,0,0,1,1,0,5,6,6,0,0,1,0,5,6,6,0,0,8,0,8,0,27.723033397574756,0.6730914351881052,17,vlad@vene.ro,sklearn/gaussian_process/gaussian_process.py|sklearn/datasets/setup.py|sklearn/gaussian_process/__init__.py|sklearn/gaussian_process/correlation_models.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/regression_models.py|examples/plot_rbm_logistic_classification.py,14,0.0009633911368015414,0,0,true,No shebang This PR fixes #2423 by removing the shebang in files that belong to the library code Other files (under directory examples mainly) are not affected,,1362,0.7841409691629956,0.5876685934489403,33891,422.64908087692896,34.374907792629315,106.2523973916379,2356,31,1116,102,travis,sergiopasra,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,797,false,false,false,false,0,0,0,0,0,0,-1
2063334,scikit-learn/scikit-learn,python,2421,1378467150,,1378819499,5872,,unknown,false,false,false,8,2,2,0,0,1,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,0,13,0,8.677057644335694,0.21067150550512131,14,vanderplas@astro.washington.edu,sklearn/utils/fixes.py|sklearn/datasets/lfw.py,13,0.012512030798845043,0,0,true,A few fixes when following this lecture http://scipy-lecturesgithubio/advanced/scikit-learn/,,1361,0.7847171197648788,0.5871029836381135,33891,422.64908087692896,34.374907792629315,106.2523973916379,2356,31,1116,101,travis,honnix,larsmans,false,,0,0,18,14,1379,false,false,false,false,0,0,0,0,0,0,-1
2057736,scikit-learn/scikit-learn,python,2420,1378388483,1378921929,1378921929,8890,8890,commit_sha_in_comments,false,false,false,17,10,8,5,5,2,12,0,5,2,0,3,5,3,0,0,2,0,3,5,3,0,0,440,105,474,171,59.236134428379245,1.4427027844615696,2,g.louppe@gmail.com,sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py,2,0.0,0,3,false,Bagging Base class for bagging and Bagged regression class has been implementedInitial tests have been conducted,,1360,0.7845588235294118,0.5906488549618321,33775,423.0940044411547,34.46336047372317,106.439674315322,2354,31,1115,102,travis,maheshakya,larsmans,false,larsmans,0,0,2,0,595,false,false,false,false,0,1,0,1,0,0,1607
2057259,scikit-learn/scikit-learn,python,2419,1378381974,,1379069412,11457,,unknown,false,true,false,38,6,1,0,41,0,41,0,5,0,0,5,14,5,0,0,0,0,14,14,14,0,0,59,0,542,0,13.892500639465004,0.33835343155153763,3,vanderplas@astro.washington.edu,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx,3,0.0019083969465648854,0,26,false,[WIP] nogil decls in SGD and WeightVector code Early PR: nogil declarations for SGD and related code I want to see if I can use a threadpool-based joblib to avoid the overhead of forking and copying data around,,1359,0.7851361295069904,0.5906488549618321,33775,423.0940044411547,34.46336047372317,106.439674315322,2354,31,1115,102,travis,larsmans,larsmans,true,,83,0.7469879518072289,120,37,1145,true,true,false,false,25,240,63,138,138,0,0
2055404,scikit-learn/scikit-learn,python,2418,1378345862,1378374325,1378374325,474,474,github,false,false,false,2,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.510750399902036,0.1098598539492778,24,vlad@vene.ro,doc/modules/feature_extraction.rst,24,0.022922636103151862,0,0,false,Edit typo ,,1358,0.7849779086892489,0.5912129894937918,33775,423.0940044411547,34.46336047372317,106.439674315322,2354,31,1114,99,travis,rsivapr,GaelVaroquaux,false,GaelVaroquaux,0,0,20,6,481,false,false,false,false,0,0,0,0,0,0,474
2056713,scikit-learn/scikit-learn,python,2415,1378215459,1378374938,1378374938,2657,2657,github,false,false,false,34,1,1,0,2,0,2,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,4,0,4,4.573096423311554,0.11137687303185957,12,vlad@vene.ro,sklearn/cluster/bicluster/tests/test_spectral.py,12,0.011352885525070956,0,7,false,skip perfect checkerboard test This test is failing on the buildbot but I have not been able to reproduce it For now Im disabling it so every commit does not cause a build failure,,1357,0.7848194546794399,0.5704824976348155,33735,422.8249592411442,34.504224099599824,106.3287386986809,2354,31,1113,99,travis,kemaleren,mblondel,false,mblondel,6,0.3333333333333333,7,1,891,true,true,false,false,0,18,3,30,89,2,2175
2068740,scikit-learn/scikit-learn,python,2413,1378142716,1378922931,1378922931,13003,13003,commit_sha_in_comments,false,false,false,71,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.807058259949835,0.11707496801204213,12,vlad@vene.ro,doc/modules/clustering.rst,12,0.011342155009451797,0,0,false, Issue #2051: Added reference to the k-means++ paper in the clustering docs  Issue #2051I couldnt get the docs to build properly on my machine so I hope this is in orderIve added a reference to the original paper If there was supposed to be more extensive discussion on the implementation off the algorithm in the docs let me know and Ill see if I can write something up,,1356,0.7846607669616519,0.5699432892249527,33735,422.8249592411442,34.504224099599824,106.3287386986809,2354,31,1112,102,travis,schmee,larsmans,false,larsmans,0,0,0,0,217,false,false,false,false,0,0,0,0,1,0,4210
2016612,scikit-learn/scikit-learn,python,2412,1378063705,1378064747,1378064747,17,17,github,false,false,false,10,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.492506346975618,0.1094141045996994,3,vlad@vene.ro,doc/modules/outlier_detection.rst,3,0.002840909090909091,0,0,false,[DOC] Typo fixes in documentation for Novelty and Outlier Detection ,,1355,0.7845018450184502,0.5700757575757576,33735,422.8249592411442,34.504224099599824,106.3287386986809,2354,31,1111,99,travis,rolisz,agramfort,false,agramfort,4,1.0,25,12,1063,false,true,false,false,0,0,1,0,0,0,2
2010930,scikit-learn/scikit-learn,python,2411,1377940685,1377948859,1377948859,136,136,github,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.360276349305978,0.1061904286275733,8,vanderplas@astro.washington.edu,sklearn/tree/export.py,8,0.007611798287345386,0,0,false,removed unused import from example Removed unused import from doc example,,1354,0.7843426883308715,0.570884871550904,33731,422.875100056328,34.50831579259435,106.3709940410898,2353,31,1110,100,travis,Balu-Varanasi,mblondel,false,mblondel,3,1.0,40,109,871,false,true,false,false,0,0,1,0,0,0,-1
2010928,scikit-learn/scikit-learn,python,2410,1377940636,1377948796,1377948796,136,136,github,false,false,false,14,18,18,0,2,0,2,0,5,0,0,23,23,23,0,0,0,0,23,23,23,0,0,70,42,70,42,92.64160530197658,2.2561991460310105,295,vlad@vene.ro,sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/covariance/graph_lasso_.py|sklearn/grid_search.py|sklearn/cluster/dbscan_.py|sklearn/pipeline.py|sklearn/covariance/robust_covariance.py|sklearn/decomposition/pca.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/ridge.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_grid_search.py|sklearn/tree/export.py|sklearn/tree/tests/test_tree.py,65,0.014272121788772598,0,10,false,Pep8 fixes Fixed few indentation visual indentation new line endings and other pep8 warnings,,1353,0.7841832963784183,0.570884871550904,33731,422.875100056328,34.50831579259435,106.3709940410898,2353,31,1110,100,travis,Balu-Varanasi,mblondel,false,mblondel,2,1.0,40,109,871,false,true,false,false,0,0,0,0,0,0,108
2016693,scikit-learn/scikit-learn,python,2407,1377826415,1377857878,1377857878,524,524,merged_in_comments,false,false,false,27,1,1,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,8.427120145540126,0.20523260875615454,7,vanderplas@astro.washington.edu,examples/applications/wikipedia_principal_eigenvector.py|sklearn/decomposition/__init__.py,7,0.0067243035542747355,0,1,false,[MRG] export randomized_svd publicly This patch makes randomized_svd a public function in sklearndecomposition so that the Wikipedia eigenvectors example no longer has to import it from sklearnutils,,1352,0.7840236686390533,0.5763688760806917,33614,423.4247634914024,34.42018206699589,106.44374367822931,2352,31,1108,100,travis,larsmans,larsmans,true,larsmans,82,0.7439024390243902,119,37,1138,true,true,false,false,24,234,58,133,133,0,411
2016692,scikit-learn/scikit-learn,python,2406,1377813965,1378333095,1378333095,8652,8652,github,false,false,false,95,21,3,79,23,0,102,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,29,4,221,90,22.469544409835162,0.5472193509942146,11,vlad@vene.ro,sklearn/decomposition/factor_analysis.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/tests/test_factor_analysis.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/tests/test_factor_analysis.py,11,0.010576923076923078,2,15,false,[MRG]: use randomized_svd in FactorAnalysis (addresses #1272) We can change the API later for WIP Id like to keep the use_randomized_svd flagSo far theres only one issue the loglikelihood does not sum up The error is bigger for lower rank problems For n_components  n_features there is maximum match often around 2 decimals as compared to 100 to 10 depending on dataDownstream applications seem reasonably sane while the speed + memory benefit is highly significant[factor_analysis_svd_comparison](https://fcloudgithubcom/assets/1908618/1052929/170a620a-10e6-11e3-81c1-57725462fce6png)See https://gistgithubcom/dengemann/6379368Priorities:- fix test- settle API- think about warm startcc @agramfort @ogrisel,,1351,0.7838638045891931,0.5769230769230769,33614,423.4247634914024,34.42018206699589,106.44374367822931,2352,31,1108,101,travis,dengemann,agramfort,false,agramfort,9,0.8888888888888888,30,29,425,true,true,true,true,2,100,8,48,25,2,39
2003087,scikit-learn/scikit-learn,python,2405,1377776483,1378130814,1378130814,5905,5905,merged_in_comments,false,false,false,30,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.411118011541397,0.10742759583403062,13,vanderplas@astro.washington.edu,sklearn/feature_extraction/text.py,13,0.0125,0,1,false,BUG: remove remaining utf-8 characters in docstrings This is the last non-ascii character in docstring Adding this on top of other non ascii-related recent commits I can import MDP 33,,1350,0.7837037037037037,0.5759615384615384,33614,423.4247634914024,34.42018206699589,106.44374367822931,2352,30,1108,99,travis,cournape,larsmans,false,larsmans,0,0,100,2,1806,false,true,false,false,0,0,0,0,1,0,226
1998526,scikit-learn/scikit-learn,python,2404,1377708647,1378458531,1378458531,12498,12498,github,false,false,false,64,31,4,36,33,0,69,0,7,0,0,2,8,2,0,0,1,0,8,9,5,0,0,131,82,545,263,27.65161699571743,0.6734131691574958,35,vanderplas@astro.washington.edu,sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/pca.py,30,0.028873917228103944,0,19,false,Fix probabilitic pca there is a serious bug in ProbabilisticPCA The estimate of the covariance is wrongRather than fixing it with hacks I moved the score method to PCA and propose toremove ProbabilisticPCA Basically I would have need to recompute the SVDto as PCAfit drops the extra singular values Also the homoscedasticFalsemode is much better addressed by the FactorAnalysis estimator,,1349,0.7835433654558932,0.5765158806544755,33614,423.4247634914024,34.42018206699589,106.44374367822931,2351,30,1107,100,travis,agramfort,GaelVaroquaux,false,GaelVaroquaux,34,0.8823529411764706,123,181,1365,true,true,true,false,6,113,54,97,87,0,9
1987492,scikit-learn/scikit-learn,python,2397,1377544355,1379331705,1379331705,29789,29789,merged_in_comments,false,false,false,55,1,1,1,1,0,2,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.867939450706035,0.11855127840757755,24,vlad@vene.ro,doc/modules/feature_extraction.rst,24,0.022922636103151862,0,0,false,WIP improve encoding docs I added an example using chardet but on short texts it seems to be getting the encoding wrong consistently All three strings are Heinrich Heine quotes in UTF-8 Latin-1 and UTF-16LE respectively chardet gets the first two wrong thinking theyre both Latin-2 For the second string that doesnt matter Fixes #2105,,1348,0.7833827893175074,0.5797516714422158,33614,423.4247634914024,34.42018206699589,106.44374367822931,2349,30,1105,101,travis,larsmans,larsmans,true,larsmans,81,0.7407407407407407,119,37,1135,true,true,false,false,23,235,58,126,131,0,1498
1982576,scikit-learn/scikit-learn,python,2396,1377442305,1377447026,1377447026,78,78,github,false,false,false,59,2,1,0,6,0,6,0,3,0,0,3,4,3,0,0,0,0,4,4,3,0,0,34,0,34,0,13.61863492591398,0.3316589077977942,83,vlad@vene.ro,sklearn/decomposition/factor_analysis.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/pca.py,57,0.024928092042186004,0,4,false,ENH: put fast dot to PCA ICA and FA This PR inserts the recently merged fast_dot #2288 into the parts of the decomposition module for which I spotted the related issues back in July Should be good to go minor changes which only affect the multiplications between data and weights that is the nasty square X rectangular use case,,1347,0.7832219747587231,0.5800575263662512,33606,423.52556091174193,34.42837588525859,106.46908290186276,2349,30,1104,98,travis,dengemann,ogrisel,false,ogrisel,8,0.875,30,29,421,false,true,false,false,2,92,7,43,24,0,0
1982158,scikit-learn/scikit-learn,python,2395,1377429402,,1389783489,205841,,unknown,false,true,false,220,2,2,0,6,0,6,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,40,50,40,50,22.49601752250335,0.5478623048125729,161,vanderplas@astro.washington.edu,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,121,0.08357348703170028,4,1,false,Gbrt stability This PR addresses differences on 64bit and 32bit architectures and in particular #2225 The main problem seems to be the computation of thresholds in the tree code Currently we compute the split threshold as follows::    threshold  (a + b) / 20While the above formular is subject is overflow this should not be an issue on most platforms (see [1] page 12 - a and b should be promoted to 80bit precision thus no overflow should occur - the div by 2 should make it fit in double prec)The result of the above compuation differs on 32bit and 64bit The problem can be solved by either:  a) explicitly cast a and b to double precision  b) threshold  a + (b - a ) / 2  c) threshold  a / 2 + b / 2  d) threshold  a + (b / 2 - a / 2)the above compuations of threshold are equivalent but are subject to different rounding errors / cancelationsI currently use solution c -- test suite passes on both 32bit and 64bit archThe strange thing is that AFAIK (a + b) / 2 should be numerically more accurate than other alternatives such as b)  @glouppe @larsmans @GaelVaroquaux @amueller do you have any thoughts on that[1] http://halinriafr/docs/00/57/66/41/PDF/computing-midpointpdf,,1346,0.7838038632986627,0.5811719500480308,33593,423.54061858125203,34.411931057065466,106.48051677432798,2349,30,1104,146,travis,pprett,glouppe,false,,39,0.8717948717948718,110,28,1482,true,true,true,true,12,87,13,4,13,0,13
1981333,scikit-learn/scikit-learn,python,2394,1377394691,1377515990,1377515990,2021,2021,github,false,false,false,82,2,2,0,8,0,8,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,35,4,35,4,18.362352024303597,0.44719157058330317,11,vanderplas@astro.washington.edu,sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/export.py|sklearn/tree/tests/test_export.py,9,0.008653846153846154,2,1,false,[MRG] tree export_graphviz: unused close parameter  and close the file if out_file was a string@glouppe @pprett I think this makes more sense Firstly the close parameter was not needed and then I think it would be more reasonable to just close the out_file in the function if the caller passed a string and not bother return the file object If the caller needs the file object it can pass one as out_file (in which case export_graphviz does not close the file),,1345,0.7836431226765799,0.5817307692307693,33594,423.52801095433705,34.41090670953147,106.47734714532358,2348,30,1103,98,travis,ndawe,glouppe,false,glouppe,8,0.625,28,55,1289,true,true,true,false,1,14,4,2,9,0,502
1979980,scikit-learn/scikit-learn,python,2392,1377358296,,1379332440,32902,,unknown,false,false,false,89,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.275354253670951,0.10412075625867619,6,vlad@vene.ro,examples/applications/plot_stock_market.py,6,0.005774783445620789,0,0,false,removing NWS from list of stock prices Issue #2389The reason this is a problem is because News Corp (NWS) was spun-off into two companies earlier this June (21st Century Fox and News Corp) The Yahoo Historical Stock Prices API which matplotlibfinance calls wont return historical data for companies that have had this type of readjustment The same result can be seen when calling historical data for GM:http://ichartyahoocom/tablecsva2&b2&c2003&d3&e3&f2007&sNWS&y0&gd&ignorecsvhttp://ichartyahoocom/tablecsva2&b2&c2003&d3&e3&f2007&sGM&y0&gd&ignorecsvBut a company like Google works fine:http://ichartyahoocom/tablecsva2&b2&c2003&d3&e3&f2007&sGOOG&y0&gd&ignorecsvThis should be an issue raised with the Yahoo API folks,,1344,0.7842261904761905,0.5822906641000962,33593,423.42154615544905,34.35239484416396,106.39121245497574,2348,30,1103,103,travis,dsullivan7,larsmans,false,,2,1.0,7,19,15,false,true,false,false,0,0,2,0,2,0,-1
1978864,scikit-learn/scikit-learn,python,2390,1377315457,1377536643,1377536643,3686,3686,merged_in_comments,false,false,false,58,1,1,0,0,0,0,0,1,0,0,25,25,25,0,0,0,0,25,25,25,0,0,54,4,54,4,102.17318141248943,2.4882961454899157,271,vlad@vene.ro,benchmarks/bench_random_projections.py|doc/sphinxext/gen_rst.py|examples/document_clustering.py|sklearn/cluster/k_means_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/lfw.py|sklearn/datasets/samples_generator.py|sklearn/ensemble/gradient_boosting.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/manifold/mds.py|sklearn/metrics/metrics.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_metrics.py|sklearn/mixture/gmm.py|sklearn/neighbors/base.py|sklearn/pipeline.py|sklearn/preprocessing/label.py|sklearn/random_projection.py|sklearn/svm/base.py|sklearn/utils/arpack.py|sklearn/utils/testing.py,65,0.00966183574879227,0,0,true,Issue #2380 adding whitespace to strings Issue #2380 There are several files mentioned in the summary that I didnt add whitespace for:examples/applications/plot_out_of_core_classificationpy URLsklearn/cluster/bicluster/spectralpy calling format methodsklearn/datasets/covtypepy URLsklearn/datasets/twenty_newsgroupspy URLsklearn/externals/joblib/parallelpy string should be concatenatedsklearn/feature_extraction/tests/test_textpy concatenated unicode characterssklearn/metrics/metricspy:114 calling format methodsklearn/tree/tests/test_treepy calling format methodLet me know if youd prefer a line break ,,1342,0.7846497764530551,0.5835748792270531,33592,423.43415098833054,34.35341748035246,106.39437961419387,2348,30,1102,98,travis,dsullivan7,larsmans,false,larsmans,1,1.0,7,19,14,false,true,false,false,0,0,1,0,1,0,-1
1975070,scikit-learn/scikit-learn,python,2388,1377262309,1389629090,1389629090,206053,206053,merged_in_comments,false,false,false,46,1,1,0,15,0,15,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,3.7725566451554373,0.09233667491860391,0,,sklearn/_build_utils.py,0,0.0,0,5,true,Allow build to succeed with OpenBLAS On Ubuntu 1304 with OpenBLAS installed according to instructions inhttp://osdfgithubio/blog/numpyscipy-with-openblas-for-ubuntu-1204-second-tryhtmlnumpydistutilssystem_infoget_info returns{define_macros: [(ATLAS_INFO \\None\\)] include_dirs: [/usr/local/include] language: c libraries: [openblas] library_dirs: [/usr/local/lib]}The build then fails when unable to import lcblasThis change allows the build to succeed,,1341,0.7844891871737509,0.5828516377649325,33617,420.82874735996666,33.97090757652378,105.89880120177291,2348,30,1102,144,travis,maciejkula,ogrisel,false,ogrisel,0,0,33,0,336,false,false,false,false,0,0,0,0,0,0,4576
1973065,scikit-learn/scikit-learn,python,2385,1377221141,1377246478,1377246479,422,422,github,false,false,false,11,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.3111587607704545,0.10499260356504896,28,vanderplas@astro.washington.edu,sklearn/tree/tree.py,28,0.026871401151631478,1,0,false,[MRG] DOC: missing doc of splitter parameter in treepy ping @glouppe,,1339,0.7849141150112023,0.581573896353167,33592,423.43415098833054,34.35341748035246,106.39437961419387,2348,30,1101,97,travis,ndawe,glouppe,false,glouppe,7,0.5714285714285714,28,55,1287,true,true,true,false,0,7,2,1,6,0,422
1973004,scikit-learn/scikit-learn,python,2384,1377219972,,1378819548,26659,,unknown,false,false,false,18,2,2,0,5,0,5,0,2,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,9.04926004337422,0.22038283093087307,14,vanderplas@astro.washington.edu,.gitignore|.gitignore,14,0.013435700575815739,0,0,false,[MRG] add _configtestc to gitignore This file hangs out in the repo after an install or make test,,1338,0.7855007473841554,0.581573896353167,33592,423.43415098833054,34.35341748035246,106.39437961419387,2348,30,1101,106,travis,ndawe,larsmans,false,,6,0.6666666666666666,28,55,1287,true,true,false,false,0,7,1,1,6,0,2111
1971349,scikit-learn/scikit-learn,python,2383,1377200090,,1377275785,1261,,unknown,false,false,false,24,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.78149641626726,0.11644705879287397,5,vlad@vene.ro,doc/developers/index.rst,5,0.004803073967339097,0,0,false,DOC: removed duplicated Python 3 section For some reason the Python 3x section was doubled (maybe in the course of fixing a spelling mistake),,1337,0.7860882572924458,0.5821325648414986,33592,423.43415098833054,34.35341748035246,106.39437961419387,2348,30,1101,97,travis,jamestwebber,larsmans,false,,2,1.0,1,0,747,false,false,false,false,0,0,0,0,1,0,-1
1968585,scikit-learn/scikit-learn,python,2382,1377169947,,1377255714,1429,,unknown,false,false,false,5,2,1,0,5,0,5,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,61,0,89,0,4.299358283251559,0.10470521844963784,40,vlad@vene.ro,sklearn/grid_search.py,40,0.03827751196172249,0,3,false,[WIP] grid_search: add sample_weight support ,,1336,0.7866766467065869,0.5818181818181818,33592,423.43415098833054,34.35341748035246,106.39437961419387,2348,30,1101,95,travis,ndawe,ndawe,true,,5,0.8,28,55,1287,true,true,false,false,0,2,0,1,5,0,201
1959793,scikit-learn/scikit-learn,python,2376,1377040576,1377092207,1377092207,860,860,github,false,false,false,18,2,2,0,3,0,3,0,3,1,0,2,3,0,0,1,1,0,2,3,0,0,1,0,0,0,0,16.05922205403418,0.3918050789947939,43,nelle.varoquaux@gmail.com,doc/index.rst|doc/testimonials/images/yhat.png|doc/testimonials/testimonials.rst|doc/index.rst|doc/testimonials/images/yhat.png|doc/testimonials/testimonials.rst,37,0.007677543186180422,0,0,false,[MRG] WEBSITE add yhat testimonial This adds a yhat testimonial to the website Should be good to go,,1334,0.7871064467766117,0.5834932821497121,33596,423.38373615906653,34.34932730086915,106.38171210858435,2348,30,1099,93,travis,amueller,amueller,true,amueller,185,0.8594594594594595,704,36,1033,true,true,false,false,27,394,37,160,107,0,676
1958279,scikit-learn/scikit-learn,python,2375,1377023973,,1378907537,31392,,unknown,false,true,false,31,286,99,125,58,1,184,0,8,2,4,5,19,5,0,0,4,5,12,21,11,0,0,8473,2344,13236,3661,660.7851192093412,16.12151354276611,2,g.louppe@gmail.com,sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py,2,0.0,0,23,false,[MRG] Bagging meta-estimator Git history in #2198 was messed up so I make a new pull request Sorry for the noise TODO:* [x] narrative documentation* [x] add an example,,1333,0.7876969242310577,0.5829338446788112,33596,423.38373615906653,34.34932730086915,106.38171210858435,2347,30,1099,105,travis,glouppe,larsmans,false,,39,0.9743589743589743,110,26,1013,true,true,false,false,15,133,33,65,30,0,13
1951852,scikit-learn/scikit-learn,python,2371,1376933199,1377008924,1377008924,1262,1262,merged_in_comments,false,false,false,58,2,2,4,8,0,12,0,4,2,0,2,4,3,0,0,2,0,2,4,3,0,0,153,29,153,29,32.33316118762077,0.7912665314333776,14,vlad@vene.ro,doc/modules/feature_selection.rst|sklearn/feature_selection/__init__.py|sklearn/feature_selection/variance_threshold.py|doc/modules/feature_selection.rst|sklearn/feature_selection/__init__.py|sklearn/feature_selection/tests/test_variance_threshold.py|sklearn/feature_selection/variance_threshold.py,14,0.0,0,0,false,[MRG] add VarianceThreshold feature selection method This is a very simple feature selection method: compute per-feature variance and remove all features that do not meet some threshold by default zero I frequently use something like this to get rid of spurious features from a FeatureHasher and thought Id generalize my approachNote that class does unsupervised feature selection,,1332,0.7875375375375375,0.5834932821497121,33573,423.3163554046406,34.31328746314002,106.27587644833646,2347,30,1098,93,travis,larsmans,larsmans,true,larsmans,80,0.7375,119,37,1128,true,true,false,false,22,226,52,115,123,0,2
1951576,scikit-learn/scikit-learn,python,2370,1376930390,1378809438,1378809438,31317,31317,github,false,false,false,28,5,1,17,9,0,26,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.910797917835583,0.12017852496533829,11,vlad@vene.ro,doc/modules/cross_validation.rst,11,0.010566762728146013,0,0,false,MRG: DOC: Simpler cross-validation iterator doc Remove the reference to data arrays in the inline example and be more explicit to explain the difference between LOLO and StratifiedKFold,,1331,0.7873779113448535,0.5840537944284342,33573,422.9887111667113,34.28350162332827,106.21630476871294,2347,30,1098,109,travis,ogrisel,ogrisel,true,ogrisel,43,0.813953488372093,791,121,1545,true,true,false,false,17,298,27,197,138,0,1664
1950086,scikit-learn/scikit-learn,python,2369,1376911740,1376931430,1376931430,328,328,github,false,false,false,47,2,2,0,12,0,12,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,22,11,22,11,13.314637415554968,0.32583981499995285,6,vanderplas@astro.washington.edu,sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,6,0.005763688760806916,0,1,false,[MRG] remove warnings in univariate feature selection These warnings are practically always triggered when doing text classification or any task with lots of boolean features I suggest to just remove them since in those cases the warning is so confusing that it does more harm than good,,1330,0.7872180451127819,0.5840537944284342,33573,422.9887111667113,34.28350162332827,106.21630476871294,2347,30,1098,94,travis,larsmans,ogrisel,false,ogrisel,79,0.7341772151898734,119,37,1128,true,true,true,true,22,222,51,114,122,0,24
1948029,scikit-learn/scikit-learn,python,2368,1376863356,1376931356,1376931356,1133,1133,github,false,false,false,188,3,2,3,3,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,57,8,65,8,13.722077825250615,0.3358108836271784,10,vlad@vene.ro,sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,10,0.009615384615384616,0,2,false,Cosine distance metric for sparse matrices I wanted to use Cosine as a distance metric in the NearestCentroid classifier but the existing implementation calls scipyspatialdistancecosine which cannot handle sparse matrices The same issue started #732 Although a lot of good things resulted from this it seems the issue that started it -- Cosine distance for sparse matrices -- was never addressed As there already is a Cosine similarity metric that works with sparse input (sklearnmetricspairwisecosine_similarity) adding a corresponding distance metric is trivial (simply 10 minus Cosine sim)I incidentally noticed that the doc string for the function pairwise_distances suggests that sparse matrices are supported for all metrics in pairwisePAIRWISE_DISTANCE_FUNCTIONS However all metrics calling on  manhattan_distances (ie cityblock l1 and manhattan) raise an exception with sparse input So I took the liberty of correcting that doc string too To sum up the changes:  - Added cosine_distances function to sklearnmetricspairwise- Added cosine as metric in pairwise_distances function- Corrected doc string of same function because all metrics based onthe manhattan_distances function (ie cityblock l1 andmanhattan) do currently NOT support sparse matrices- Added corresponding corresponding unit test,,1329,0.7870579382994732,0.5846153846153846,33574,422.9761124679812,34.28248049085602,106.21314112110562,2347,30,1097,94,travis,emsrc,mblondel,false,mblondel,0,0,1,0,900,false,false,false,false,0,0,0,0,0,0,69
1945788,scikit-learn/scikit-learn,python,2367,1376765392,1376782392,1376782392,283,283,merged_in_comments,false,false,false,8,2,2,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.363040250898342,0.2291357443094113,17,vanderplas@astro.washington.edu,doc/modules/neighbors.rst|doc/modules/neighbors.rst,17,0.01633045148895293,0,0,false,Doc This is to resolve Issue #2365:https://githubcom/scikit-learn/scikit-learn/issues/2365,,1328,0.7868975903614458,0.5840537944284342,33574,422.9761124679812,34.28248049085602,106.21314112110562,2346,30,1096,92,travis,dsullivan7,amueller,false,amueller,0,0,7,19,8,false,true,true,false,0,0,0,0,0,0,279
1929637,scikit-learn/scikit-learn,python,2364,1376491670,1376581680,1376581680,1500,1500,merged_in_comments,false,false,false,69,10,7,3,4,0,7,0,3,0,0,6,7,5,0,0,0,0,7,7,6,0,0,53,49,58,67,44.80849184338687,1.0965909988752007,91,vlad@vene.ro,sklearn/cluster/affinity_propagation_.py|doc/whats_new.rst|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_mean_shift.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_mean_shift.py,80,0.008637236084452975,0,3,false,Add predict method to AffinityPropogation and MeanShift This is a simple PR for adding predict() to AffinityPropagation and MeanShiftFor SpectralClustering Im not sure if its possible to directly generalize to new data (its always possible to fit a classifier with the labels obtained from fit_predict though)For DBSCAN it seems like it should be possible but Im not familiar enough with the algorithm so I didnt do it,,1327,0.7867370007535796,0.5834932821497121,33555,422.8281925197437,34.182685143793776,106.15407539859932,2344,30,1093,97,travis,mblondel,mblondel,true,mblondel,25,0.8,267,30,1233,true,true,false,false,6,152,6,87,27,0,1219
1926430,scikit-learn/scikit-learn,python,2363,1376435047,1376435395,1376435395,5,5,github,false,false,false,26,1,1,0,0,0,0,0,0,0,0,3,3,3,0,0,0,0,3,3,3,0,0,2,0,2,0,8.07612581753569,0.19764571951888382,95,vanderplas@astro.washington.edu,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,89,0.05410628019323672,0,0,false,FIX: release reference to X in _treesplitter Not releasing X in _treesplitter seems to lead to memory issues I am merging this if Travis is happy,,1326,0.7865761689291101,0.58743961352657,33555,422.8281925197437,34.182685143793776,106.15407539859932,2343,30,1092,97,travis,glouppe,glouppe,true,glouppe,38,0.9736842105263158,110,26,1006,true,true,false,false,15,130,29,67,29,0,-1
2016511,scikit-learn/scikit-learn,python,2361,1376407055,1376407958,1376407958,15,15,github,false,false,false,8,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,[DOC] Remove that RBMs are not implemented yet ,,1325,0.7864150943396226,0.5880077369439072,33584,422.4928537398761,34.15316817532158,106.06241067174845,2343,30,1092,97,travis,rolisz,glouppe,false,glouppe,3,1.0,24,12,1044,false,true,false,false,0,0,0,0,0,0,14
1905591,scikit-learn/scikit-learn,python,2357,1376210376,,1376571182,6013,,unknown,false,false,false,27,3,2,3,1,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,13,16,13,13.010721938438362,0.3184295421583186,13,vanderplas@astro.washington.edu,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,11,0.010617760617760617,0,1,false,Custom vocabulary checks for CountVectorizer  added check for repeating indices and gaps in custom vocabulary for CountVectorizeradded test with wrong custom vocabsthis commit fixes #2353 ,,1324,0.7870090634441088,0.584942084942085,33591,422.40481081242,34.146051025572326,106.04030841594475,2342,30,1090,97,travis,alemagnani,larsmans,false,,0,0,1,2,1255,false,false,false,false,0,3,0,0,1,0,75
1905081,scikit-learn/scikit-learn,python,2355,1376123697,,1420854428,745452,,unknown,false,true,false,121,12,4,17,23,0,40,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,152,0,351,106,16.679912935387705,0.408235532843321,21,vlad@vene.ro,sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,21,0.020289855072463767,0,2,false,[MRG] Issue #2185: Fixed MinibatchKMeans bad center reallocation which caused  Issue #2185The PR addresses a problem in kmeans_minibatch The problem involves wrong random reassignments of centroids Currently we choose n centroids out of k samples with given probabilities However the problem is that we do not check or control if we pick one centroid more than once To correctly pick n unique labels out of k samples with some given probability we need to repeat n times:1) choose one sample 2) add it to set of chosen samples3) make it unavailable for following pickings4) repeatLet me know what you think about this Let me know if it is correct and I will add tests Thanks,,1323,0.7876039304610734,0.5835748792270531,33587,422.45511656295594,34.150117605025756,106.05293714830142,2341,30,1089,260,travis,kpysniak,amueller,false,,2,0.5,0,0,799,true,false,false,false,1,4,2,0,4,0,270
1903670,scikit-learn/scikit-learn,python,2350,1375896683,1379437837,1379437837,59019,59019,github,false,false,false,44,4,1,0,12,0,12,0,3,1,0,2,4,0,0,2,1,0,3,4,0,1,2,0,0,0,0,9.387212875047574,0.22975374766000273,81,vanderplas@astro.washington.edu,doc/images/FNRS-logo.png|doc/index.rst|doc/themes/scikit-learn/static/nature.css_t,68,0.035398230088495575,0,3,false,DOC: contributors logos added to index screen footer for #2333 They also show tooltips upon mouse-over (currently just the sponsors name for in case the logo isnt clear)Fullscreen:[screenshot at 2013-08-07 17 26 15](https://fcloudgithubcom/assets/1378870/925255/36808da2-ff76-11e2-9add-f84a9a186f35png)Zoomed[screenshot at 2013-08-07 17 26 23](https://fcloudgithubcom/assets/1378870/925257/3dcf89f0-ff76-11e2-90a5-70117929d689png)Feedback welcome,,1322,0.7874432677760969,0.5801376597836775,33628,421.4642559771619,34.04900678006423,106.01284643749257,2337,30,1086,108,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,63,0.9206349206349206,11,13,560,true,true,false,false,10,167,34,42,31,0,12
1900618,scikit-learn/scikit-learn,python,2349,1375885176,1375954456,1375954456,1154,1154,merged_in_comments,false,false,false,42,1,1,6,6,0,12,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,108,0,108,0,9.111296958503097,0.22300191744629985,16,vlad@vene.ro,sklearn/cluster/_hierarchical.c|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/hierarchical.py,14,0.0068694798822374874,1,0,false,FIX integer types in Ward clustering Use npnpy_intp aka npintp in Python for all indices Also added modec declarations where NumPy would permit and made some small improvements to the docs and code styleI hope this fixes #2322 @vene please check,,1321,0.7872823618470856,0.57801766437684,33623,420.81313386669837,34.02432858459983,105.70145436159771,2337,30,1086,97,travis,larsmans,larsmans,true,larsmans,78,0.7307692307692307,117,36,1116,true,true,false,false,21,220,49,101,114,0,98
1901530,scikit-learn/scikit-learn,python,2348,1375872200,1375893266,1375893266,351,351,github,false,false,false,53,3,3,0,3,0,3,0,4,0,0,9,9,6,0,0,0,0,9,9,6,0,0,80,72,80,72,48.25445568284227,1.18104652749646,215,vlad@vene.ro,sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_grid_search.py,76,0.026600985221674877,0,2,false,[MRG] Fix consistency issue between scorer string and metrics function This pr fix the last consistency issue between scorer and metrics function (#2096)I finally chose to rename auc_score to roc_auc_score auc_scorer to roc_auc_scorerto have the most explicit name I havent created an alias since I think that is a bad option,,1320,0.7871212121212121,0.5773399014778325,33623,420.81313386669837,34.02432858459983,105.70145436159771,2337,30,1086,96,travis,arjoly,ogrisel,false,ogrisel,34,0.8529411764705882,17,22,596,true,true,true,false,28,326,36,241,98,0,6
1899972,scikit-learn/scikit-learn,python,2347,1375863559,,1405793763,498776,,unknown,false,false,false,26,7,2,11,6,0,17,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,226,0,714,9.589872265380661,0.2347158449492081,11,vanderplas@astro.washington.edu,sklearn/tests/test_multiclass.py|sklearn/tests/test_multiclass.py,11,0.010880316518298714,0,0,false,Extended unit tests for multiclass and multilabel Commit for Issue #2022 TST Need tests for multilabel format issues: https://githubcom/scikit-learn/scikit-learn/issues/2022  Coverage of test_multiclass increased to 100%,,1319,0.7877179681576952,0.5786350148367952,33623,420.81313386669837,34.02432858459983,105.70145436159771,2337,30,1086,207,travis,kpysniak,arjoly,false,,1,1.0,0,0,796,true,false,false,false,0,1,1,0,3,0,70
1898300,scikit-learn/scikit-learn,python,2346,1375823268,,1375826524,54,,unknown,false,false,false,14,2,1,0,5,0,5,0,3,0,0,2,2,0,1,1,0,0,2,2,0,1,1,0,0,0,0,9.381321988422771,0.2309618997892695,84,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t,60,0.06018054162487462,1,0,false,ENH added an orange cite us button on the front page [sklearn_cite_us](https://fcloudgithubcom/assets/184798/919540/7273a2c8-fecb-11e2-9026-0d18598e73dcpng)@GaelVaroquaux Varoquaux,,1318,0.7883156297420334,0.5777331995987964,33603,421.0635955123054,34.04457935303395,105.76436627682052,2337,30,1085,95,travis,NelleV,NelleV,true,,33,0.8787878787878788,38,13,1296,true,true,false,false,17,84,33,97,51,0,3
1892426,scikit-learn/scikit-learn,python,2345,1375727690,1375729569,1375729569,31,31,github,false,false,false,7,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.863532111856714,0.1195137913670507,5,vanderplas@astro.washington.edu,README.rst,5,0.005065856129685917,0,0,false,Converted Markdown style link to restructured text ,,1317,0.7881548974943052,0.5734549138804458,33628,420.75056500535266,34.04900678006423,105.74521232306411,2336,29,1084,97,travis,rgbkrk,jaquesgrobler,false,jaquesgrobler,0,0,205,235,790,false,true,false,false,0,0,0,0,0,0,31
1889089,scikit-learn/scikit-learn,python,2343,1375659397,1375786312,1375786312,2115,2115,merged_in_comments,false,false,false,5,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,43,0,43,0,4.117011884971244,0.10116920957026006,3,nelle.varoquaux@gmail.com,sklearn/__init__.py,3,0.0031185031185031187,0,0,false,MAINT: remove sklearntest() Fixes #2342,,1316,0.7879939209726444,0.5821205821205822,33628,420.75056500535266,34.04900678006423,105.74521232306411,2336,29,1083,96,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,34,0.7058823529411765,385,3,1259,true,true,false,false,24,421,38,223,127,0,532
1888799,scikit-learn/scikit-learn,python,2341,1375651861,1375952486,1375952486,5010,5010,commit_sha_in_comments,false,false,false,42,4,2,1,4,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,82,0,140,9.820388189730371,0.24132087508780126,3,nelle.varoquaux@gmail.com,sklearn/tests/test_isotonic.py|sklearn/tests/test_isotonic.py,3,0.003125,0,0,false,Added unit tests for parameters in isotonic regression Commit for Issue #2091 TST add test for sample_weight parameter to isotonic regression Its my first commit to scikit-learn so please let me know if there is anything I am doing wrong thanks :),,1315,0.7878326996197719,0.5822916666666667,33628,420.75056500535266,34.04900678006423,105.74521232306411,2335,29,1083,98,travis,kpysniak,agramfort,false,agramfort,0,0,0,0,793,false,false,false,false,0,0,0,0,1,0,632
1884366,scikit-learn/scikit-learn,python,2340,1375496741,1445287116,1445287116,1163112,1163112,merged_in_comments,false,true,false,488,7,1,19,8,0,27,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,672,0,728,0,4.422102470916083,0.10866671939432052,8,vlad@vene.ro,sklearn/gaussian_process/gaussian_process.py,8,0.008307372793354102,0,3,true,[WIP] Add Gaussian Process Classification (GPC) with Laplace approximation This is almost entirely based on Gaussian Processes for Machine Learning by Rassmussen and WilliamsPlease let me know where I should add test methods and anything else I need to do (not sure but cannot subscribe to the mailing list to ask my questions there)I can add real benchmarks as soon as I get some feedback and hopefully some pointersCodeI followed the newton formulation described in the text the method directly computes the inverse Hessian so optimizefmin_ncg was not used it has the advantage that kernel does not need to be inverted and avoids numerical instability (as described in the book) I have an implementation that does use fmin_ncg but requires the invert it can be added as a separate method to the classComparisonIt seems Laplace approximation to find the posterior distribution is not well-received in the literature but it should be a good referencesince the Expectation Propagation (EP) method described there is for binary classification only I just implemented the Laplace approximation method for now if there is any good reference for EP or other methods please let me know Another option is to use EP in a ovR classificationVariational methods also mentioned in the book and seem to be easier to implement than EP for multiple classesIf integrating with PyMC is an option an MCMC approach would be a nice addition tooRationaleGaussian Process Classification (GPC) is a kernel-based non-parametric method One main advantage is flexibility and noise tolerance It is a natural generalization of linear logistic regression similar to the transition from linear regression to GP regression The training learns a set of functionsIt is also the starting point to some powerful dimensionality reduction (DR) techniques that are based on GP-LVM and learn the manifold of high-dimensional data with few number of samplesTODOI wanted to get some feedback before I continue but here are my TODO list:1- Hyper parameter theta can be optimized in a straight forward approach similar to GPR2- Add numerical integration for prediction instead of Monte Carlo (MC) sampling (I wanted to follow the text book)3- Add a variational method or EP method if I find good references within my level of mathMy plan is to continue by implementing latent variable model (GP-LVM) and make it supervised (SGP-LVM) It will be based on these papers:Supervised Gussian Process LVM for Dimensionality Reduction Xinbo GaoSupervised Latent Linear Gaussian Process LVM Based Classification Hou Feng ZouSupervised Latent Linear Gaussian Process LVM for Dimensionality Reduction Jiang Gao Wong ZhengDiscriminative Gaussian Process LVM for Classification Urtasun and DarrellBenchmarksI ran the standard plot_classifier_comparison against all classifiers the last one if GPC (without MLE of theta)[comparison](https://fcloudgithubcom/assets/873905/905821/5a0216dc-fc52-11e2-9a6e-37c6bb4cba9apng)I will add one more comparison as soon as I finish the MLE of theta,,1314,0.7876712328767124,0.5825545171339563,33627,420.6441252564903,34.0500193297053,105.62940494245696,2334,29,1081,370,travis,dashesy,glouppe,false,glouppe,1,0.0,1,1,770,false,true,false,false,2,16,1,0,0,0,2006
1881438,scikit-learn/scikit-learn,python,2339,1375455153,1375809372,1375809372,5903,5903,commit_sha_in_comments,false,false,false,117,10,1,0,45,0,45,0,8,3,0,2,13,0,1,5,7,8,2,17,0,1,9,0,0,0,0,9.571009251855596,0.23519359479618318,74,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/banner_example1.png|doc/themes/scikit-learn/static/banner_example2.png|doc/themes/scikit-learn/static/banner_example3.png|doc/themes/scikit-learn/static/banner_example4.png|doc/themes/scikit-learn/static/nature.css_t,51,0.0,0,18,true,ENH: add a carousel with examples for the front page Adds an examples-carousel as discussed in #2333Heres an [online build](http://jaquesgroblergithubio/online-sklearn-build/indexhtml) to see it in actionSome things I need feedback on * delay time between switching (or leave it static with option to switch with arrows)* images to include in this (the ones I added are just for example purposes - will also use thumbnail versions thereof (smaller)* colours of the slider-buttons:Some options:----------------------------[screenshot at 2013-08-02 14 32 13](https://fcloudgithubcom/assets/1378870/900889/fad9dc70-fb71-11e2-96b9-dc2e5ac1ea19png)-------------------------------[screenshot at 2013-08-02 14 31 54](https://fcloudgithubcom/assets/1378870/900892/0cb2f0e4-fb72-11e2-8487-960322693531png)-------------------------------[screenshot at 2013-08-02 14 31 34](https://fcloudgithubcom/assets/1378870/900895/14f4e564-fb72-11e2-95cf-15050d4e720epng)-------------------------------Send me any links of specific examples images you think will be good hereLooking forward to feedback,,1313,0.7875095201827875,0.58298755186722,33627,420.6441252564903,34.0500193297053,105.62940494245696,2334,29,1081,96,travis,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,62,0.9193548387096774,11,13,555,true,true,true,false,12,178,46,45,29,0,17
1880911,scikit-learn/scikit-learn,python,2338,1375446732,1375522683,1375522683,1265,1265,github,false,false,false,5,15,12,0,4,0,4,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,240,24,250,24,61.896416979556896,1.5210152440475169,27,vlad@vene.ro,sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/factor_analysis.py|sklearn/decomposition/pca.py,19,0.014553014553014554,0,0,true,PCA cleanup see commit log,,1312,0.7873475609756098,0.5821205821205822,33627,420.6441252564903,34.0500193297053,105.62940494245696,2334,29,1081,96,travis,agramfort,NelleV,false,NelleV,33,0.8787878787878788,120,181,1339,true,true,true,true,8,97,53,102,84,0,5
1869734,scikit-learn/scikit-learn,python,2337,1375279180,1375451761,1375451761,2876,2876,github,false,false,false,32,2,1,3,1,0,4,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.10651682817844,0.12548752457465012,2,vanderplas@astro.washington.edu,doc/tutorial/machine_learning_map/index.rst,2,0.00211864406779661,0,0,false,fix cut-off ML map Stops the ML from being sliced due to new jquery version and map_highlight  fightingAlso fixed the Back button to point to Documentation not Getting Started as before,,1311,0.7871853546910755,0.590042372881356,33618,419.80486644059727,33.9698970789458,105.62793741448034,2333,29,1079,96,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,61,0.9180327868852459,11,13,553,true,true,false,false,13,177,41,45,27,0,0
1869441,scikit-learn/scikit-learn,python,2336,1375270011,1375370976,1375370976,1682,1682,github,false,false,false,41,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.213146100857734,0.10353383581359298,5,amueller@ais.uni-bonn.de,doc/user_guide.rst,5,0.005296610169491525,0,0,false,DOC: fix for tons of whitespace on the user_guide page of new dev-webpage fix sidebar button size staying constant and leaving a mountain of white-space due to expanding/collapsing toctreeAs mentioned in #2335------------------Compare [current User Guide](http://scikit-learnorg/dev/user_guidehtml) to [my fix](http://jaquesgroblergithubio/online-sklearn-build/user_guidehtml),,1310,0.7870229007633588,0.590042372881356,33618,419.80486644059727,33.9698970789458,105.62793741448034,2333,29,1079,95,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,60,0.9166666666666666,11,13,553,true,true,false,false,13,179,40,45,27,0,653
1869115,scikit-learn/scikit-learn,python,2334,1375264547,1378816783,1378816783,59203,59203,merged_in_comments,false,true,false,21,6,2,0,20,1,21,0,7,0,0,3,9,3,0,0,0,0,9,9,9,0,0,24,78,100,78,26.734173879439147,0.656965016384492,49,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/testing.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/utils/testing.py,36,0.018027571580063628,0,9,false,[MRG] MAINT deprecate indicesFalse for cross validation generators As per ML: adding some lines to remove many more in the future,,1309,0.7868601986249045,0.5896076352067868,33618,419.80486644059727,33.9698970789458,105.62793741448034,2333,29,1079,113,travis,jnothman,larsmans,false,larsmans,47,0.5531914893617021,17,1,1555,true,true,false,false,16,394,69,197,48,0,16
1864874,scikit-learn/scikit-learn,python,2331,1375200968,1375264872,1375264872,1065,1065,github,false,false,false,22,2,1,3,3,0,6,0,3,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.834809658768059,0.11881053021189027,45,vanderplas@astro.washington.edu,doc/themes/scikit-learn/static/nature.css_t,45,0.048025613660619,0,1,false,DOC: few small doc fixes to layout bugs on new website Simple changes that fix both#2316 and #2330Online build [here](http://jaquesgroblergithubio/online-sklearn-build/indexhtml),,1308,0.786697247706422,0.5912486659551761,33618,419.80486644059727,33.9698970789458,105.62793741448034,2333,29,1078,94,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,59,0.9152542372881356,11,13,552,true,true,false,false,12,172,37,44,25,0,8
1863228,scikit-learn/scikit-learn,python,2329,1375177296,1375181412,1375181412,68,68,github,false,false,false,31,2,1,0,1,0,1,0,1,0,0,2,3,1,0,1,0,0,3,3,1,1,1,8,0,8,0,10.087213809730711,0.2478891203104157,0,,doc/Makefile|doc/README,0,0.0,0,0,false,DOC now building to html/stable closes #2314The links work on my computer Im just not sure whats going to happen when going on scikit-learnorg/dev and clicking on a link there,,1307,0.7865340474368784,0.5922330097087378,33617,419.81735431478126,33.97090757652378,105.63107951334148,2333,29,1078,96,travis,NelleV,jaquesgrobler,false,jaquesgrobler,32,0.875,38,13,1289,true,true,false,false,15,72,30,97,40,0,68
1862020,scikit-learn/scikit-learn,python,2327,1375148797,,1406140826,516473,,unknown,false,false,false,540,3,1,12,15,0,27,0,6,1,0,3,4,4,0,0,1,0,3,4,4,0,0,555,118,633,146,17.697644369143177,0.43491231344788966,11,vlad@vene.ro,sklearn/linear_model/__init__.py|sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/utils/optimize.py,8,0.005387931034482759,1,8,false,[WIP] Implementation of logistic_regression_path and LogisticRegressionCV This code implements a function logistic_regression_path and an object LogisticRegressionCVThis is mostly the code from @GaelVaroquaux logistic_cv branch My work is only some minor cleanup some added tests and benchmarking## RationaleWe currently use LIBLINEAR for LogisticRegression and while it is very fast on most problems it cannot be warm-restarted which means that fitting a LogisticRegression on a grid of parameters takes as much time as fitting it independently on each parameter separately In this pull request we implement the code to perform an efficient fit of logistic regression models across a sequence of regularization parameters by using the solution to the previous model as initialization point for the current optimization problem We also get (almost for free) an efficient implementation of a cross-validated Logistic Regression estimator in LogisticRegressionCV## CodeThe important functions are logistic_regression_path and LogisticRegressionCV both in linear_models/logisticpy Our benchmarks have shown that the most competitive method is either lbfgs or the newton-cg solver adapted from the SciPy codebase depending on the problem The changes made to the newton-cg code are essentially to allow the function to minimize to provide the gradient and hessian in the same call thus being able to reuse several computations These changes are extremely important for this model but unfortunately these changes are incompatible with the SciPy API and extending the SciPy API to support our changes is not straightforward That is why Ive added the solver to sklearnutilsoptimizeThe available solvers are (Ive tried to use as much the vocabulary from scipyoptimize):   * **lbfgs**: this uses scipyoptimizefmin_lbfgs_b   * **newton-cg**: this uses our modificed version of scipys newton-cg implementation   * **liblinear**: this uses the LIBLINEAR implementation which in turn is a trust-region newton type algorithmThe solvers make use of the private functions _logistic* defined in logisticpy## BenchmarksI have run benchmarks on real and synthetic datasets Each vertical line corresponds to the convergence of the method for a particular parameter and each axis is done with a different method The important quantity is the timing for the last line that is when the algorithm finishes to optimize all models in the path (ie total time)### Real datasetsThis if from the haxby dataset from https://githubcom/nilearn/nilearn 216 samples x 6222 features Ill upload this data somewhere as soon as I have decent wifi connection The notebook to run the benchmarks can be found [here](http://nbvieweripythonorg/url/fabianpnet/tmp/2013/logistic_path_bench_PRipynb)[image](https://fcloudgithubcom/assets/277639/875580/365ac6d4-f8a6-11e2-824c-f0628d95cc58png)### Synthetic datasetFrom a linear model with gaussian noise (see notebook) 10K samples x 100 features [image](https://fcloudgithubcom/assets/277639/875643/fd2b3d38-f8a7-11e2-877c-b492fc18fa1bpng)## TODO* [ ] I think the some of the private functions in logisticpy can be simplified In particular it is my intuition that we could squash some of the functions _logistic* and _logistic_*_intercept without sacrificing a significant amount of performance* [x] Compatibility with old scipy that do not have keyword max_iter in the lbfgs solver (Travis build failure)* [ ] sparse matrix support (any help from the people who actually use sparse matrices would be great here)* [ ] implement multinomial logistic or raise a meaningful error for multiclass pointing to OvA OvO meta-estimators* [ ] More tests for LogisticCV object (and some are failing)* [ ] Document tolerance and stopping criterions,,1306,0.7871362940275651,0.59375,33617,419.81735431478126,33.97090757652378,105.63107951334148,2333,29,1077,206,travis,fabianp,agramfort,false,,31,0.7096774193548387,164,24,1171,true,true,true,true,0,10,4,4,16,0,781
1860107,scikit-learn/scikit-learn,python,2326,1375125697,1375136964,1375136964,187,187,merged_in_comments,false,false,false,23,1,1,0,3,0,3,0,2,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,4.663112364017374,0.11459113155708096,47,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html,47,0.050483351235230935,0,0,false,DOC added link from banner to example The image in the banner from the front page now links to the examplecloses #2325,,1305,0.7869731800766283,0.5929108485499462,33581,419.4633870343349,33.97754682707484,105.56564724099937,2333,29,1077,95,travis,NelleV,GaelVaroquaux,false,GaelVaroquaux,31,0.8709677419354839,38,13,1288,true,true,true,false,14,70,29,97,39,0,0
1859293,scikit-learn/scikit-learn,python,2324,1375116589,1375124653,1375124653,134,134,github,false,false,false,39,2,2,2,9,0,11,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9.718019293263776,0.2388102853243024,74,vlad@vene.ro,doc/whats_new.rst|doc/whats_new.rst,74,0.07956989247311828,0,5,false,[MRG] Missing contributions This pr aim to add missing contributions for 014:   - improvement of doc/module/multiclassrst documentation related pr #2207 and #2269  - better error handling in the metrics module related pr  #1750 #1985 #2002 #2015 #2024 and  #2147,,1304,0.7868098159509203,0.5935483870967742,33581,419.4633870343349,33.97754682707484,105.56564724099937,2333,29,1077,95,travis,arjoly,NelleV,false,NelleV,33,0.8484848484848485,17,22,587,true,true,true,false,27,318,39,196,91,0,46
1859065,scikit-learn/scikit-learn,python,2323,1375114179,1375843745,1375843745,12159,12159,merged_in_comments,false,false,false,51,15,2,1,28,0,29,0,7,0,0,2,5,0,0,1,3,0,5,8,0,1,4,0,0,0,0,18.904977824717918,0.46457030101802177,58,vanderplas@astro.washington.edu,doc/testimonials/testimonials.rst|doc/themes/scikit-learn/static/nature.css_t|doc/testimonials/testimonials.rst|doc/themes/scikit-learn/static/nature.css_t,57,0.061488673139158574,2,12,false,DOC added testimonials from INRIA and evernoteTODO list- [x] Check with evernote they are OK- [x] Get 4 testimonials (current 3 / 4)- [x] Activate the testimonials on the main page- [ ] Set the carrousel to display two logosPing @GaelVaroquaux and @ogrisel refs #1370,,1303,0.7866462010744436,0.5943905070118662,33581,419.4633870343349,33.97754682707484,105.56564724099937,2333,29,1077,98,travis,NelleV,GaelVaroquaux,false,GaelVaroquaux,30,0.8666666666666667,38,13,1288,true,true,true,false,13,68,26,97,33,0,4
1858280,scikit-learn/scikit-learn,python,2321,1375104428,,1375200652,1603,,unknown,false,false,false,136,2,2,0,10,0,10,0,3,1,0,1,2,0,0,0,1,0,1,2,0,0,0,0,0,0,0,4.151834510812654,0.102027056414536,0,,doc/out-of-core.rst|doc/out-of-core.rst,0,0.0,0,6,false,[WIP] Write narrative documentation for out-of-core learning This is a WIP to track progress of this [issue](https://githubcom/scikit-learn/scikit-learn/issues/2204)Here is a copy of the mission statement:- [ ] Update the out of core example to plot the behavior of Perceptron PassiveAggressiveClassifier and MultinomialNB on the Reuters dataset Handle separatly in this PR: #2222- [ ] Write narrative documentation to explain the motivation explain the partial_fit method usage in general and refer to the example in particular and give hints implementation constraints (stateless feature extraction knowing all the classes ahead of time for classification)- [ ] Models to be linked to in this section:For classification:PerceptronMultinomialNBBernoulliNBSGDClassifierPassiveAggressiveClassifierFor regression:SGDRegressorPassiveAggressiveRegressorFor out of core clustering / feature extraction:MinibatchKMeansFor out of core decomposition / feature extraction:MinibatchDictionaryLearning,,1302,0.7872503840245776,0.5952380952380952,33581,419.4633870343349,33.97754682707484,105.56564724099937,2333,29,1077,96,travis,oddskool,oddskool,true,,3,0.6666666666666666,2,2,798,true,true,false,false,1,22,2,16,3,0,248
1856561,scikit-learn/scikit-learn,python,2317,1375072900,,1375106844,565,,unknown,false,false,false,25,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.444705447351775,0.10922597818017812,12,vlad@vene.ro,sklearn/svm/classes.py,12,0.013100436681222707,0,1,false,DOC fix comment on svm probability param Needs to be set before fit not predict_probaI assume this sort of change can go into 014,,1301,0.7878554957724827,0.5960698689956332,33581,419.4633870343349,33.97754682707484,105.56564724099937,2333,29,1077,94,travis,jnothman,larsmans,false,,46,0.5652173913043478,17,1,1553,true,true,false,false,15,395,68,199,49,0,-1
1856327,scikit-learn/scikit-learn,python,2315,1375065998,1375067304,1375067304,21,21,merged_in_comments,false,false,false,15,2,2,0,0,4,4,0,1,2,0,3,5,5,0,0,2,0,3,5,5,0,0,39,60,39,60,27.855359611123657,0.6860895612634786,29,vanderplas@astro.washington.edu,sklearn/preprocessing/__init__.py|sklearn/preprocessing/_weights.py|sklearn/preprocessing/tests/test_weights.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/tests/test_weights.py|sklearn/preprocessing/weights.py|sklearn/tree/tests/test_tree.py,24,0.0,0,0,false,Pr 2305: continuation of FIX backward compatibility was broken This is a continuation of #2305,,1300,0.7876923076923077,0.5969331872946331,33566,419.7997974140499,33.99273073943872,105.58303044747662,2333,29,1076,94,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,33,0.696969696969697,384,3,1252,true,true,false,false,21,357,34,199,108,0,-1
1856054,scikit-learn/scikit-learn,python,2313,1375058410,,1377298494,37334,,unknown,false,false,false,26,1,1,0,5,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,14,0,4.625272440426485,0.1139230236972518,21,vanderplas@astro.washington.edu,sklearn/neighbors/binary_tree.pxi,21,0.023204419889502764,0,0,false,Use npuintp_t for offset calculation  Pointers should not be casted to a signed integer npuintp_t is an alias for npy_uintp typedefed to Py_uintptr_t in NumPy npy_commonh ,,1299,0.7882986913010007,0.5933701657458563,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,110,travis,sturlamolden,sturlamolden,true,,1,0.0,10,0,380,false,false,false,false,0,16,1,5,1,0,5
1855466,scikit-learn/scikit-learn,python,2310,1375045958,1375056709,1375056692,178,179,github,false,false,false,9,2,2,0,3,0,3,0,2,0,0,4,4,0,0,0,0,0,4,4,0,0,0,0,0,0,0,36.162612186737476,0.8907055396435656,94,vlad@vene.ro,doc/modules/cross_validation.rst|doc/modules/grid_search.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|doc/modules/cross_validation.rst|doc/modules/grid_search.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst,68,0.02983425414364641,0,1,false,[MRG] DOC Remove deprecated reference + acknowledge @larsman FIX#2309 ,,1298,0.788135593220339,0.5933701657458563,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,98,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,32,0.84375,17,22,586,true,true,true,false,26,311,38,196,90,0,0
1855210,scikit-learn/scikit-learn,python,2308,1375039272,1377385248,1377385248,39099,39099,github,false,false,false,9,3,2,1,5,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,34,24,68,18.286908939347608,0.4504168839110583,37,vanderplas@astro.washington.edu,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,25,0.02759381898454746,0,3,false,GBRT checks if loss is in selfsupported_loss addresses #1085 ,,1297,0.7879722436391673,0.5938189845474614,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,112,travis,pprett,larsmans,false,larsmans,38,0.868421052631579,107,28,1454,true,true,true,true,14,83,12,4,13,0,57
1855208,scikit-learn/scikit-learn,python,2307,1375039263,1375064615,1375064615,422,422,merged_in_comments,false,false,false,25,1,1,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.323315900285167,0.10648570966412037,34,vlad@vene.ro,sklearn/cross_validation.py,34,0.037527593818984545,0,0,false,[MRG] rearrange permutation_score parameters to match previous ones This fixes #2303Not sure if it is strictly necessary People shouldnt rely on argument orders imho,,1296,0.7878086419753086,0.5938189845474614,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,98,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,184,0.8586956521739131,688,34,1010,true,true,true,false,34,473,40,170,117,0,33
1855129,scikit-learn/scikit-learn,python,2306,1375037785,1376568030,1376568030,25504,25504,merged_in_comments,false,false,false,12,1,1,0,6,0,6,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,17,0,17,0,8.81374610133925,0.2170875388353134,17,vanderplas@astro.washington.edu,doc/modules/density.rst|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx,13,0.012141280353200883,0,3,false,[MRG] DOC: fix metrics documentation format Just a small sphinx formatting fix,,1295,0.7876447876447876,0.5938189845474614,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,105,travis,jakevdp,jakevdp,true,jakevdp,35,0.8857142857142857,1064,0,809,true,true,false,false,5,88,11,10,21,0,16
1855110,scikit-learn/scikit-learn,python,2305,1375037379,1375067319,1375067319,499,499,merged_in_comments,false,true,false,28,1,1,0,3,0,3,0,2,2,0,2,4,4,0,0,2,0,2,4,4,0,0,28,52,28,52,18.60270741776921,0.45819517859532033,29,vanderplas@astro.washington.edu,sklearn/preprocessing/__init__.py|sklearn/preprocessing/tests/test_weights.py|sklearn/preprocessing/weights.py|sklearn/tree/tests/test_tree.py,24,0.0055248618784530384,3,5,false,FIX backward compatibility was broken balance_weights disappeared from the preprocessing module when it was refactored I put it backThis is a roadblock for 014@mblondel @GaelVaroquaux  @amueller,,1294,0.7874806800618238,0.594475138121547,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,97,travis,NelleV,GaelVaroquaux,false,GaelVaroquaux,29,0.8620689655172413,38,13,1287,true,true,true,false,11,62,26,98,33,0,458
1855070,scikit-learn/scikit-learn,python,2304,1375036715,1375059782,1375059782,384,384,github,false,false,false,29,3,1,2,5,0,7,0,3,0,0,5,6,5,0,0,0,0,6,6,6,0,0,5,12,29,12,22.18420793011104,0.5464095567521362,65,vlad@vene.ro,sklearn/cluster/bicluster/spectral.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/base.py|sklearn/tests/test_common.py|sklearn/utils/testing.py,30,0.015486725663716814,0,1,false,WIP: explicitly mark all base classes as ABC with abstractmethod inits The goal is to avoid having the base classes end up in all_estimators in test_common in python 3,,1293,0.7873163186388245,0.5940265486725663,33557,419.6441875018625,33.972047560866585,105.58154781416694,2333,29,1076,98,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,42,0.8095238095238095,782,121,1523,true,true,true,true,15,247,26,178,126,0,14
1855107,scikit-learn/scikit-learn,python,2302,1375035804,,1405514678,507921,,unknown,false,true,false,16,1,1,0,7,0,7,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,4.158450694044704,0.10242661042769344,51,vanderplas@astro.washington.edu,sklearn/tests/test_common.py,51,0.0565410199556541,0,1,false,FIX dont test CCA in test_transformers  as the fit crashes on some platforms ie my laptop,,1292,0.7879256965944272,0.5931263858093127,33555,419.3115780062584,33.97407241841752,105.52823722247057,2333,29,1076,205,travis,amueller,amueller,true,,183,0.8633879781420765,688,34,1010,true,true,false,false,33,465,39,169,117,0,25
1854962,scikit-learn/scikit-learn,python,2300,1375034294,1405593388,1405593388,509258,509258,merged_in_comments,false,false,false,63,9,3,0,13,0,13,0,5,5,0,0,42,22,0,0,5,0,37,42,25,0,0,290,0,302,0,62.75732592912958,1.5457794624012255,0,,examples/classification/README.txt|examples/classification/plot_classification_probability.py|examples/classification/plot_classifier_comparison.py|examples/classification/plot_digits_classification.py|examples/classification/plot_lda_qda.py|examples/feature_selection/README.txt|examples/feature_selection/feature_selection_pipeline.py|examples/feature_selection/plot_feature_selection.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_digits.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_train_error_vs_test_error.py|examples/model_selection/randomized_search.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/text/README.txt|examples/text/document_classification_20newsgroups.py|examples/text/document_clustering.py|examples/text/hashing_vs_dict_vectorizer.py|examples/text/mlcomp_sparse_document_classification.py|examples/classification/README.txt|examples/classification/plot_classification_probability.py|examples/classification/plot_classifier_comparison.py|examples/classification/plot_digits_classification.py|examples/classification/plot_lda_qda.py|examples/feature_selection/README.txt|examples/feature_selection/feature_selection_pipeline.py|examples/feature_selection/plot_feature_selection.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_digits.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_train_error_vs_test_error.py|examples/model_selection/randomized_search.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/text/README.txt|examples/text/document_classification_20newsgroups.py|examples/text/document_clustering.py|examples/text/hashing_vs_dict_vectorizer.py|examples/text/mlcomp_sparse_document_classification.py|examples/classification/README.txt|examples/classification/plot_classification_probability.py|examples/classification/plot_classifier_comparison.py|examples/classification/plot_digits_classification.py|examples/classification/plot_lda_qda.py|examples/feature_selection/README.txt|examples/feature_selection/feature_selection_pipeline.py|examples/feature_selection/plot_feature_selection.py|examples/feature_selection/plot_permutation_test_for_classification.py|examples/feature_selection/plot_rfe_digits.py|examples/feature_selection/plot_rfe_with_cross_validation.py|examples/model_selection/README.txt|examples/model_selection/grid_search_digits.py|examples/model_selection/grid_search_text_feature_extraction.py|examples/model_selection/plot_confusion_matrix.py|examples/model_selection/plot_precision_recall.py|examples/model_selection/plot_roc.py|examples/model_selection/plot_roc_crossval.py|examples/model_selection/plot_train_error_vs_test_error.py|examples/model_selection/randomized_search.py|examples/neural_networks/plot_rbm_logistic_classification.py|examples/text/README.txt|examples/text/document_classification_20newsgroups.py|examples/text/document_clustering.py|examples/text/hashing_vs_dict_vectorizer.py|examples/text/mlcomp_sparse_document_classification.py,0,0.0,0,1,false,WIP move around examples for better structure This is supposed to enhance the structure of the examplesIt will break external links (actually I still have to adjust the links inside the docs)I dont see how we can sort the gallery without breaking links or heavy hacking thoughThis is not intended for 014-rc as there is other stuff to worry about,,1291,0.7877614252517429,0.5922222222222222,33553,419.33657199058206,33.97609751736059,105.53452746401217,2333,29,1076,205,travis,amueller,amueller,true,amueller,182,0.8626373626373627,688,34,1010,true,true,false,false,32,464,38,169,117,0,5
1854861,scikit-learn/scikit-learn,python,2299,1375031922,1375033287,1375033287,22,22,github,false,false,false,14,1,1,0,1,0,1,0,2,0,0,12,12,12,0,0,0,0,12,12,12,0,0,60,38,60,38,53.64402156120404,1.3213073956154842,71,vlad@vene.ro,doc/datasets/mldata_fixture.py|examples/covariance/plot_sparse_cov.py|examples/grid_search_digits.py|examples/plot_rfe_with_cross_validation.py|examples/randomized_search.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_scale_c.py|sklearn/covariance/graph_lasso_.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,42,0.0033482142857142855,0,1,false,Rename cv_scores(_) back to grid_scores(_) to keep the name free for a future refactoring,,1290,0.7875968992248062,0.5915178571428571,33558,419.2740926157697,33.97103522259968,105.51880326598724,2333,29,1076,93,travis,ogrisel,ogrisel,true,ogrisel,41,0.8048780487804879,782,121,1523,true,true,false,false,15,244,23,178,125,0,22
1854823,scikit-learn/scikit-learn,python,2298,1375031252,1375036361,1375036361,85,85,merged_in_comments,false,false,false,8,4,1,0,9,0,9,0,3,0,0,3,6,3,0,0,0,0,6,6,6,0,0,22,0,75,0,4.836914396408698,0.11913817379709461,23,vanderplas@astro.washington.edu,sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c,23,0.0234375,0,1,false,[WIP] Neighbors Struct Dtype Following discussion at #2289,,1289,0.7874321179208689,0.5915178571428571,33558,419.2740926157697,33.97103522259968,105.51880326598724,2333,29,1076,94,travis,jakevdp,GaelVaroquaux,false,GaelVaroquaux,34,0.8823529411764706,1064,0,809,true,true,false,false,5,75,10,8,17,0,20
1854806,scikit-learn/scikit-learn,python,2297,1375030751,,1375063733,549,,unknown,false,true,false,34,7,1,7,31,0,38,0,6,4,0,6,15,10,0,0,7,1,10,18,16,0,0,45,0,1327,0,31.14231447998231,0.7670672190755906,43,vanderplas@astro.washington.edu,sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/kd_tree.c|sklearn/utils/fminmax.pxd|sklearn/utils/fminmax.pyx|sklearn/utils/setup.py|sklearn/utils/src/fminmax.c|sklearn/utils/src/fminmax.h,28,0.012290502793296089,0,11,false,[WIP] fix MSVC build in neighbors and tree Some parts of the new neighbors code doesnt build under MSVCAlso the tree moduleIf anybody knows a better way to do this please help,,1288,0.7880434782608695,0.5910614525139665,33558,419.2740926157697,33.97103522259968,105.51880326598724,2333,29,1076,98,travis,vene,GaelVaroquaux,false,,44,0.8409090909090909,51,30,1204,true,true,true,false,5,73,21,86,117,0,63
1854784,scikit-learn/scikit-learn,python,2295,1375030172,1375036132,1375036132,99,99,github,false,false,false,151,1,1,0,3,0,3,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,434,16,434,16,18.8483561964961,0.46425438870594343,13,vlad@vene.ro,sklearn/utils/_logistic_sigmoid.c|sklearn/utils/_logistic_sigmoid.pyx|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,13,0.004464285714285714,3,1,false,[MRG] speed up logistic_sigmoid (using less code) I tried to optimize the RBMs which didnt work but I did optimize the logistic_sigmoid in the process As I suspected with the right shifting and scaling logistic sigmoid can be reduced to tanh to within 16 decimals precision (there is a test for this)I did not yet optimize log_logistic_sigmoid which is suffering from strides checking in a double for loop and (probably) branch prediction failures in the 0 test Ping @vene @amueller @fabianpTimings before: x  nplinspace(-1000 1000 10000) %timeit logistic_sigmoid(x)1000 loops best of 3: 570 us per loopAfter: %timeit logistic_sigmoid(x)1000 loops best of 3: 286 us per loopFor comparison: %timeit nptanh(x)10000 loops best of 3: 117 us per loop %timeit 5 * (1 + nptanh(x/2))1000 loops best of 3: 301 us per loop,,1287,0.7878787878787878,0.5915178571428571,33558,419.2740926157697,33.97103522259968,105.51880326598724,2333,29,1076,94,travis,larsmans,larsmans,true,larsmans,77,0.7272727272727273,113,34,1106,true,true,false,false,24,224,55,94,113,0,18
1854755,scikit-learn/scikit-learn,python,2294,1375029221,1375029350,1375029350,2,2,github,false,false,false,7,2,2,1,0,0,1,0,2,0,0,2,2,0,1,0,0,0,2,2,0,1,0,0,0,0,0,14.026506709883977,0.3454902109916977,50,vanderplas@astro.washington.edu,doc/documentation.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html,45,0.05027932960893855,0,0,false,Website loading A few fixes for #2286,,1286,0.7877138413685848,0.5910614525139665,33547,419.3817628998122,33.98217426297433,105.55340268876502,2333,29,1076,91,travis,glouppe,amueller,false,amueller,37,0.972972972972973,107,26,990,true,true,true,true,14,120,26,73,20,0,1
1854690,scikit-learn/scikit-learn,python,2293,1375027192,1375029823,1375029823,43,43,github,false,false,false,35,3,1,1,3,0,4,0,3,0,0,12,13,12,0,0,0,0,13,13,12,0,0,52,4,156,13,53.447695751610226,1.3164828609272894,155,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/lda.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/neighbors/base.py|sklearn/neighbors/nearest_centroid.py|sklearn/preprocessing/label.py|sklearn/qda.py|sklearn/svm/base.py|sklearn/tests/test_common.py|sklearn/tree/tree.py,55,0.01899441340782123,0,2,false,[MRG] Fix tests on master better input validation much better input validation test that warning is raised on (n_samples1) yLets wait for travis opinion on this I also want to add a whatsnew entry,,1285,0.7875486381322957,0.5910614525139665,33547,419.3817628998122,33.98217426297433,105.55340268876502,2333,29,1076,91,travis,amueller,ogrisel,false,ogrisel,181,0.861878453038674,688,34,1010,true,true,true,false,32,460,35,168,116,0,6
1854684,scikit-learn/scikit-learn,python,2292,1375026976,,1375028707,28,,unknown,false,false,false,7,2,2,0,1,0,1,0,1,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,9.671580655482678,0.2382229952473727,45,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html,45,0.05027932960893855,0,0,false,Website loading A few fixes for #2286 ,,1284,0.7881619937694704,0.5910614525139665,33547,419.3817628998122,33.98217426297433,105.55340268876502,2333,29,1076,91,travis,glouppe,glouppe,true,,36,1.0,107,26,990,true,true,false,false,14,120,24,73,20,0,0
1854384,scikit-learn/scikit-learn,python,2290,1375019061,1375198871,1375198871,2996,2996,github,false,false,false,40,22,4,21,10,0,31,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,240,92,403,92,36.3100872056683,0.8944056314969331,50,vlad@vene.ro,sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py,46,0.051685393258426963,2,9,false,[MRG]: improve logcosh function + tests This PR inclides the additions and edits from #2248 that were independent from the fast_dot Besides STY/DOC/COSMITS the _logcosh functions memory footprint was improved via chunking and and tests got fixedcc @agramfort @ogrisel,,1283,0.7879968823070927,0.5887640449438202,33549,419.356761751468,33.980148439595816,105.54711019702525,2333,29,1076,97,travis,dengemann,agramfort,false,agramfort,7,0.8571428571428571,29,29,393,false,true,true,true,2,48,6,29,24,0,28
1854442,scikit-learn/scikit-learn,python,2288,1375017931,1377439635,1377439635,40361,40361,github,false,false,false,79,104,35,21,76,4,101,0,4,0,0,3,7,3,0,0,0,0,7,7,5,0,0,347,448,827,923,258.04619632414585,6.356303199909783,23,vlad@vene.ro,sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py,15,0.012401352874859075,0,48,false,[MRG]: add fast_dot function that directly calls BLAS with appropriate data input In this new PR Ive isolated the fast_dot from the #2248 ICA related PRI improved exception handling and tests If data input is not appropriate a DataConversionWarning is thrown and the function falls back on the regular npdot Once were happy with tests and the performance we can issues specific PRs implementing the fast_dot as desiredFor convenience the current test-gist can be found here:https://gistgithubcom/dengemann/6094449,,1282,0.7878315132605305,0.5873731679819617,33547,419.3817628998122,33.98217426297433,105.55340268876502,2333,29,1076,107,travis,dengemann,ogrisel,false,ogrisel,6,0.8333333333333334,29,29,393,false,true,false,false,2,47,5,29,24,0,51
1852456,scikit-learn/scikit-learn,python,2284,1374974299,1375008291,1375008291,566,566,merged_in_comments,false,false,false,40,5,5,0,0,3,3,0,2,0,0,4,4,3,0,1,0,0,4,4,3,0,1,3,0,3,0,13.737393965144406,0.35191328319771026,2,vanderplas@astro.washington.edu,setup.cfg|sklearn/svm/setup.py|sklearn/metrics/pairwise_fast.c|sklearn/_hmmc.c|setup.cfg,2,0.0011494252873563218,0,1,false,Windows fixes The following fixes make it possible to build under Windows 7 32bits  w/ Python 27 and MinGW and the scipy-stack package from Christoph Gohlke at http://wwwlfduciedu/~gohlke/pythonlibs/#scipy-stackThere are still broken tests but at least the tests pass now,,1280,0.78828125,0.5793103448275863,33532,418.7343433138495,34.027197900512945,105.48133126565669,2333,29,1075,94,travis,ogrisel,GaelVaroquaux,false,GaelVaroquaux,40,0.8,781,121,1522,true,true,true,true,14,237,20,178,122,0,-1
1851559,scikit-learn/scikit-learn,python,2283,1374967949,,1375061446,1558,,unknown,false,false,false,5,2,1,10,14,0,24,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,4,0,4.990879683761383,0.1278522593159652,3,jaquesgrobler@gmail.com,sklearn/neighbors/typedefs.pxd,3,0.003468208092485549,0,2,false,Update typedefspxd with correct ITYPECODE ,,1279,0.7888975762314308,0.576878612716763,33532,418.7343433138495,34.027197900512945,105.48133126565669,2333,29,1075,103,travis,sturlamolden,GaelVaroquaux,false,,0,0,10,0,379,false,false,false,false,0,0,0,0,1,0,661
1848913,scikit-learn/scikit-learn,python,2281,1374950854,1374951864,1374951864,16,16,github,false,false,false,9,2,2,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,28,0,28,0,7.83867778995103,0.20079374963830224,3,vanderplas@astro.washington.edu,setup.py|setup.py,3,0.0035460992907801418,0,0,false,Add supported python versions to the classifiers + fixes ,,1278,0.7887323943661971,0.5709219858156028,33518,418.879408079241,34.04141058535713,105.52538934303956,2332,29,1075,95,travis,ogrisel,NelleV,false,NelleV,39,0.7948717948717948,781,121,1522,true,true,false,false,14,236,19,178,117,0,10
1848727,scikit-learn/scikit-learn,python,2280,1374949524,1374951156,1374951156,27,27,github,false,false,false,23,4,4,0,2,0,2,0,1,0,0,5,5,1,0,0,0,0,5,5,1,0,0,33,0,33,0,27.843231039535496,0.7132257393767455,50,vlad@vene.ro,sklearn/grid_search.py|doc/modules/decomposition.rst|doc/modules/lda_qda.rst|doc/modules/neural_networks.rst|doc/modules/sgd.rst|sklearn/grid_search.py,32,0.010650887573964497,0,0,false,[MRG] Randomized search should use new scorer API Since its new to this release it should not start off with a deprecated API,,1277,0.7885669537979639,0.570414201183432,33518,418.879408079241,34.04141058535713,105.52538934303956,2332,29,1075,95,travis,larsmans,NelleV,false,NelleV,76,0.7236842105263158,113,34,1105,true,true,false,false,24,217,54,89,107,0,25
1848060,scikit-learn/scikit-learn,python,2279,1374947303,1374954756,1374954756,124,124,merged_in_comments,false,false,false,20,14,8,6,14,0,20,0,5,0,0,14,25,13,0,1,0,0,25,25,20,0,1,349,46,783,133,181.70539088219846,4.654519416684609,21,vlad@vene.ro,sklearn/svm/src/libsvm/LIBSVM_CHANGES|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/svm/src/libsvm/LIBSVM_CHANGES|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py|sklearn/svm/src/libsvm/LIBSVM_CHANGES,11,0.0035714285714285713,0,7,false,[MRG] Controlling randomness in libsvm probability estimation Adds support for random_state parameter in libsvm classes and allows deterministic probability estimation,,1276,0.7884012539184952,0.5678571428571428,33518,418.879408079241,34.04141058535713,105.52538934303956,2332,29,1075,93,travis,vene,vene,true,vene,43,0.8372093023255814,51,30,1203,true,true,false,false,4,66,19,85,111,0,7
1847107,scikit-learn/scikit-learn,python,2278,1374942309,1378897937,1378897937,65927,65927,github,false,false,false,75,12,3,37,31,0,68,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,1231,1158,1277,1627,25.55538641000598,0.65465670802829,112,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,87,0.10494571773220748,0,12,false,[MRG] faster flatter precision_recall_fscore_support * Rewritten for efficiency and neatness Note the handling of multilabel-sequences format by binarizing is in response to benchmarks Under the simple benchmarks at https://gistgithubcom/jnothman/5734967 this new version is about 13x faster than master (for multiclass and label indicators about 2x faster for sequence of sequences)* Also fixed some warning handling in tests* Also reverted the recent reimplementation of mathews correlation coefficient using LabelEncoder to handle non-int labels instead,,1275,0.788235294117647,0.5621230398069964,33504,419.024594078319,34.05563514804202,105.5396370582617,2332,29,1075,114,travis,jnothman,ogrisel,false,ogrisel,45,0.5555555555555556,17,1,1551,true,true,false,false,14,393,66,199,47,0,6731
1846957,scikit-learn/scikit-learn,python,2277,1374940894,1374941397,1374941397,8,8,github,false,false,false,21,6,6,2,2,0,4,0,4,0,0,5,5,5,0,0,0,0,5,5,5,0,0,30,21,30,21,31.019717845413922,0.7946384963304635,116,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/ensemble/partial_dependence.py|sklearn/ensemble/tests/test_gradient_boosting.py,86,0.02666666666666667,0,0,false,FIX for #965 This solves discrepancies issues in the decision tree module between 32 and 64 bits architectureIt solves #965,,1274,0.7880690737833596,0.56,33504,419.38276026743074,33.995940783190065,105.62917860553964,2332,29,1075,95,travis,glouppe,glouppe,true,glouppe,35,1.0,107,26,989,true,true,false,false,13,111,20,72,18,0,5
1846888,scikit-learn/scikit-learn,python,2276,1374940686,1375008668,1375008668,1133,1133,merged_in_comments,false,false,false,8,22,10,5,6,0,11,0,6,0,0,6,9,6,0,0,0,0,9,9,8,0,0,312,64,599,64,88.30080406638513,2.2620198712883184,100,vlad@vene.ro,sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/metrics/scorer.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/metrics/scorer.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/metrics/scorer.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/metrics/scorer.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/linear_model/tests/test_ridge.py,52,0.02181818181818182,0,4,false,[MRG] Deprecation of the loss_func and score_func parameters ,,1273,0.7879025923016496,0.56,33504,419.38276026743074,33.995940783190065,105.62917860553964,2332,29,1075,97,travis,NicolasTr,NelleV,false,NelleV,8,0.875,3,0,1102,false,true,false,false,0,24,5,41,22,0,5
1846849,scikit-learn/scikit-learn,python,2275,1374940129,1374952031,1374952031,198,198,merged_in_comments,false,false,false,8,13,12,15,7,0,22,0,4,0,0,18,19,18,0,0,0,0,19,19,19,0,0,1205,6,1205,16,119.18750919910663,3.0532357293442467,74,vlad@vene.ro,sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/neighbors/typedefs.pxd|sklearn/neighbors/typedefs.pyx|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pxd|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/neighbors/base.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/neighbors/kd_tree.pyx|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c|sklearn/cross_decomposition/pls_.py|sklearn/utils/arpack.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/tests/test_imputation.py,22,0.008526187576126675,0,2,false,[MRG] Re-establish np 13 and py 26 compat ,,1272,0.7877358490566038,0.5578562728380024,33504,419.1439828080229,33.995940783190065,105.56948424068769,2332,29,1075,97,travis,GaelVaroquaux,NelleV,false,NelleV,32,0.6875,383,3,1251,true,true,false,true,17,285,21,182,78,0,0
1846785,scikit-learn/scikit-learn,python,2273,1374937739,1375001509,1375001509,1062,1062,merged_in_comments,false,false,false,48,15,2,12,11,0,23,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,70,169,271,9.445041526604422,0.24195187757359218,6,vanderplas@astro.washington.edu,sklearn/decomposition/tests/test_pca.py|sklearn/decomposition/tests/test_pca.py,6,0.007371007371007371,3,7,false,MRG: add failing test exposing bug in RandomizedPCA This test exposes a bug probably in all classes that center twice in fit and transform while not overwriting the fit_transform inherited from TransformerMixin This should be fixed Any clues which other classes might be affectedcc @agramfort @GaelVaroquaux @vene,,1271,0.7875688434303698,0.5540540540540541,33489,419.42130251724444,34.011167846158436,105.64663023679417,2332,29,1075,99,travis,dengemann,agramfort,false,agramfort,5,0.8,29,29,392,false,true,true,true,1,38,3,25,13,0,0
1846482,scikit-learn/scikit-learn,python,2272,1374933421,1374941592,1374941592,136,136,merged_in_comments,false,false,false,31,3,1,1,10,0,11,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,19,31,19,9.22814134178164,0.23639785545486441,7,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,7,0.008610086100861008,0,0,false,[MRG] Use projected gradient solver in transform to support sparse matrices Instead of using the scipyoptimizennls (as until now) or the l-bfgs solver (as I am experimenting in #2263Fixes #2124,,1270,0.7874015748031497,0.5535055350553506,33448,418.88902176512795,33.9930638603205,105.44726142071275,2332,29,1075,97,travis,vene,amueller,false,amueller,42,0.8333333333333334,51,30,1203,true,true,true,true,4,60,18,84,106,0,69
1846465,scikit-learn/scikit-learn,python,2271,1374932778,1374938418,1374938418,94,94,github,false,false,false,39,3,1,5,1,1,7,0,3,0,0,3,4,3,0,0,0,0,4,4,3,0,0,12,65,30,130,13.128552945838333,0.33631493565990744,23,vanderplas@astro.washington.edu,sklearn/preprocessing/data.py|sklearn/preprocessing/tests/test_data.py|sklearn/utils/validation.py,15,0.00865265760197775,0,0,false,[MRG] make StandardScaler convert int input to float Make StandardScaler convert int input to float and warn about it instead of warning and rounding for dense and crashing for sparseWill add an entry in changelog nowCloses #1709,,1269,0.7872340425531915,0.5512978986402967,33448,418.88902176512795,33.9930638603205,105.44726142071275,2332,29,1075,95,travis,amueller,amueller,true,amueller,180,0.8611111111111112,688,34,1009,true,true,false,false,28,425,29,153,101,0,78
1846391,scikit-learn/scikit-learn,python,2269,1374929631,1375008915,1375008915,1321,1321,merged_in_comments,false,false,false,20,9,4,17,8,0,25,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,18.854687146994927,0.4830045158568648,6,vlad@vene.ro,doc/modules/multiclass.rst|doc/modules/multiclass.rst|doc/modules/multiclass.rst|doc/modules/multiclass.rst,6,0.00749063670411985,0,4,false,[MRG] Documentation improvement multilabel / multioutput-multiclass / multi-target multiclass This pr continue and finishes #2207 and is related to #1781,,1268,0.7870662460567823,0.5468164794007491,33446,418.1964958440471,33.96519763200383,105.21437541111045,2332,29,1075,97,travis,arjoly,GaelVaroquaux,false,GaelVaroquaux,31,0.8387096774193549,17,21,585,true,true,true,false,25,294,36,190,84,0,0
1846315,scikit-learn/scikit-learn,python,2268,1374926577,1375065031,1375065031,2307,2307,commit_sha_in_comments,false,false,false,46,4,4,3,2,0,5,0,3,0,0,8,8,8,0,0,0,0,8,8,8,0,0,25,47,25,47,43.74096186737515,1.1205209270940815,119,vlad@vene.ro,sklearn/linear_model/ridge.py|sklearn/preprocessing/label.py|sklearn/tests/test_common.py|sklearn/neighbors/base.py|sklearn/tests/test_common.py|sklearn/preprocessing/tests/test_label.py|sklearn/ensemble/weight_boosting.py|sklearn/naive_bayes.py|sklearn/tests/test_common.py|sklearn/utils/validation.py,49,0.023779724655819776,1,1,false,[WIP] Ravel warning ping @gvaroquauxI think this is fairly tricky and it is not entirely obvious to me which estimators should raise a warningAs the multi-output ones also consistently ravel I guess all shouldFor some reason only the first warning is raised currently,,1267,0.7868981846882399,0.5456821026282853,33446,418.1964958440471,34.0847933983137,105.21437541111045,2332,29,1075,103,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,179,0.8603351955307262,688,34,1009,true,true,true,false,27,417,29,153,101,0,471
1846203,scikit-learn/scikit-learn,python,2266,1374922410,1374932585,1374932585,169,169,commit_sha_in_comments,false,false,false,27,4,2,4,10,0,14,0,3,0,0,4,5,4,0,0,0,0,5,5,5,0,0,8,8,10,42,17.282561548286772,0.4427308193945054,80,vlad@vene.ro,sklearn/neighbors/base.py|sklearn/linear_model/ridge.py|sklearn/preprocessing/label.py|sklearn/tests/test_common.py,49,0.028930817610062894,0,4,false,[MRG] Fix ravel stuff This should fix the current test issues and make everything consistentWarnings and tests for warnings will come in a separate PR soon,,1266,0.7867298578199052,0.5433962264150943,33446,418.1964958440471,34.0847933983137,105.21437541111045,2332,29,1075,95,travis,amueller,ogrisel,false,ogrisel,178,0.8595505617977528,688,34,1009,true,true,true,false,26,410,28,153,98,0,1
1843312,scikit-learn/scikit-learn,python,2265,1374871006,,1374942021,1183,,unknown,false,true,false,54,10,1,0,12,0,12,0,2,0,0,6,12,6,0,0,0,0,12,12,12,0,0,20,0,1166,0,10.058040988711932,0.25765737514033865,16,vanderplas@astro.washington.edu,sklearn/neighbors/ball_tree.c|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/kd_tree.c|sklearn/neighbors/typedefs.c|sklearn/neighbors/typedefs.pxd|sklearn/neighbors/typedefs.pyx,15,0.007633587786259542,1,3,true,[WIP] neighbors module numpy 13 compatibility Working on addressing the issue in #2245I havent been able to set up a working numpy13/14 build  Because we know the buffer interface is causing problems though Im going to work on it as much as I can  Hopefully @GaelVaroquaux can test the progress on his build,,1265,0.7873517786561265,0.5381679389312977,33435,418.33408105278903,34.096007178106774,105.24899057873485,2332,29,1074,98,travis,jakevdp,jakevdp,true,,33,0.9090909090909091,1064,0,807,true,true,false,false,4,56,8,3,9,0,93
1843200,scikit-learn/scikit-learn,python,2264,1374870364,1374870462,1374870462,1,1,github,false,false,false,24,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,0,4.579905352439808,0.11732363434271789,6,vlad@vene.ro,sklearn/setup.py,6,0.007653061224489796,0,0,true,BUG: add new preprocessing module to setuppy Preprocessing was made a module it needs to be added to setuppy in order to install correctly,,1264,0.7871835443037974,0.5369897959183674,33434,418.34659328826945,34.09702697852485,105.25213854160435,2332,29,1074,93,travis,jakevdp,jakevdp,true,jakevdp,32,0.90625,1064,0,807,true,true,false,false,4,54,5,3,7,0,1
1842711,scikit-learn/scikit-learn,python,2263,1374867375,,1440862893,1099865,,unknown,false,true,false,55,2,1,0,15,0,15,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,314,38,327,38,9.05318286293607,0.23191657532545693,20,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,20,0.02554278416347382,1,4,true,[WIP] Add L-bfgs solver for NMF Addresses #2124 Actually not yet because I need to make the l-bfgs solver efficient on sparse data in the way @mblondel suggestedI will benchmark and see in what cases and with what settings which solver is better and implement autoI will also try profiling it a bit,,1263,0.7878068091844814,0.5363984674329502,33420,417.89347695990426,34.021543985637344,105.08677438659485,2332,29,1074,361,travis,vene,mblondel,false,,41,0.8536585365853658,50,30,1202,true,true,true,false,3,57,17,82,105,0,1074
1842393,scikit-learn/scikit-learn,python,2262,1374864535,1374866058,1374866058,25,25,github,false,false,false,7,2,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,21,0,23,0,3.3692061947529672,0.08630947640728821,3,amueller@ais.uni-bonn.de,sklearn/preprocessing/imputation.py,3,0.0038412291933418692,0,0,true,Imputer: selfstatistics_ souldnt be set if axis1 ,,1262,0.7876386687797148,0.5352112676056338,33419,417.905981627218,34.02256201561986,105.0899189084054,2332,29,1074,93,travis,NicolasTr,glouppe,false,glouppe,7,0.8571428571428571,3,0,1101,false,true,false,true,0,22,4,41,17,0,-1
1842155,scikit-learn/scikit-learn,python,2261,1374862370,,1374862987,10,,unknown,false,false,false,9,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,53,0,53,0,4.1607299241345945,0.10658582103190506,20,vanderplas@astro.washington.edu,doc/sphinxext/gen_rst.py,20,0.02560819462227913,0,0,true,DOC ignoring gen_rsts parsing errors closes #1759And pep8,,1261,0.7882632831086439,0.5364916773367477,33420,417.89347695990426,34.021543985637344,105.08677438659485,2332,29,1074,93,travis,NelleV,amueller,false,,28,0.8928571428571429,38,13,1285,true,true,false,true,6,44,19,76,14,0,9
1841995,scikit-learn/scikit-learn,python,2260,1374861681,1374861787,1374861787,1,1,merged_in_comments,false,false,false,14,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.612677361850207,0.11816349021068531,5,vlad@vene.ro,doc/developers/index.rst,5,0.006402048655569782,0,1,true,[MRG] classifier template in roll your own estimator Lets show them how its done,,1260,0.7880952380952381,0.5364916773367477,33420,417.89347695990426,34.021543985637344,105.08677438659485,2332,29,1074,93,travis,larsmans,larsmans,true,larsmans,75,0.72,113,34,1104,true,true,false,false,25,213,51,88,103,0,1
1840930,scikit-learn/scikit-learn,python,2258,1374854195,1374854880,1374854880,11,11,github,false,false,false,22,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.555484763185442,0.11721712310045404,10,vanderplas@astro.washington.edu,.gitignore,10,0.01297016861219196,0,0,true,Add pycharm files in gitignore Im using pycharm and it would be nice if the files of the pycharm projects were ignored,,1259,0.7879269261318507,0.5304798962386511,33330,415.78157815781583,34.233423342334234,104.77047704770477,2332,29,1074,97,travis,NicolasTr,agramfort,false,agramfort,6,0.8333333333333334,3,0,1101,false,true,false,false,0,22,3,41,17,0,4
1840445,scikit-learn/scikit-learn,python,2257,1374851192,1374853856,1374853857,44,44,github,false,false,false,18,6,5,4,8,0,12,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,40,21,40,32.51403015732039,0.8365882906807214,5,nelle.varoquaux@gmail.com,sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_decomposition/tests/test_pls.py|sklearn/cross_decomposition/pls_.py|sklearn/cross_decomposition/tests/test_pls.py,5,0.006527415143603133,1,0,true,The PLSSVD now returns the correct number of components fixes #1312(and it is probably faster)ping @gaelvaroquaux,,1258,0.787758346581876,0.5274151436031331,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,99,travis,NelleV,amueller,false,amueller,27,0.8888888888888888,38,13,1285,true,true,false,true,5,34,18,74,13,0,5
1840369,scikit-learn/scikit-learn,python,2256,1374850745,1374881840,1374881840,518,518,merged_in_comments,false,false,false,18,3,2,0,3,0,3,0,3,0,0,13,14,12,0,0,0,0,14,14,13,0,0,314,67,471,109,117.17656043051053,3.0149611698116843,46,vanderplas@astro.washington.edu,doc/modules/density.rst|examples/mixture/plot_gmm_pdf.py|examples/neighbors/plot_kde_1d.py|examples/neighbors/plot_species_kde.py|sklearn/hmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/neighbors/kde.py|sklearn/neighbors/tests/test_kde.py|sklearn/neural_network/rbm.py|sklearn/tests/test_hmm.py|doc/modules/density.rst|examples/mixture/plot_gmm_pdf.py|examples/neighbors/plot_kde_1d.py|examples/neighbors/plot_species_kde.py|sklearn/hmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/neighbors/kde.py|sklearn/neighbors/tests/test_kde.py|sklearn/neural_network/rbm.py|sklearn/tests/test_hmm.py,23,0.00522875816993464,0,2,true,ENH rename eval / pseudolikelihood to score_samples Closes #2218Makes interface consistent and gives a more telling name,,1257,0.7875894988066826,0.526797385620915,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,99,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,177,0.8587570621468926,688,34,1008,true,true,true,false,23,366,17,147,91,0,0
1839236,scikit-learn/scikit-learn,python,2253,1374844371,,1374941451,1618,,unknown,false,true,false,46,3,2,0,11,0,11,0,4,0,0,3,4,3,0,0,0,0,4,4,4,0,0,4,12,4,14,8.375310596898611,0.2154988333487327,101,vanderplas@astro.washington.edu,sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,85,0.06692913385826772,0,7,true,Tree 32bit discrepancy fix There was a discrepancy in the results on 32bit and 64bit architectures - it was due to impurity ties - we now only check for differences in impurity up to 1e-7 precision and for ties we take the first encountered split point,,1255,0.7888446215139442,0.5275590551181102,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,104,travis,pprett,glouppe,false,,37,0.8918918918918919,107,28,1452,true,true,true,true,14,81,11,4,13,0,0
1839213,scikit-learn/scikit-learn,python,2252,1374844025,1374858614,1374858614,243,243,merged_in_comments,false,false,false,6,7,4,5,9,0,14,0,4,7,1,7,15,9,0,0,7,1,7,15,9,0,0,3702,1227,4216,1461,76.12670867247186,1.9587592263947102,46,vlad@vene.ro,sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/tests/test_label.py|sklearn/preprocessing.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/data.py|sklearn/preprocessing/imputation.py|sklearn/preprocessing/label.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/label.py|sklearn/preprocessing/tests/__init__.py|sklearn/preprocessing/tests/test_data.py|sklearn/preprocessing/tests/test_imputation.py|sklearn/preprocessing/tests/test_label.py,46,0.0,0,0,true,[WIP] Preprocessing refactoring Fix issue #2238,,1254,0.7886762360446571,0.5275590551181102,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,97,travis,NicolasTr,amueller,false,amueller,5,0.8,3,0,1101,false,true,false,false,0,18,2,38,15,0,2
1839197,scikit-learn/scikit-learn,python,2251,1374843744,1374859027,1374859027,254,254,github,false,false,false,21,6,3,6,18,0,24,0,4,0,0,9,13,9,0,0,0,0,13,13,12,0,0,162,87,247,146,119.77618753780101,3.0818712188313615,148,vlad@vene.ro,sklearn/ensemble/weight_boosting.py|sklearn/linear_model/ridge.py|sklearn/naive_bayes.py|sklearn/neighbors/kde.py|sklearn/neighbors/nearest_centroid.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/ensemble/weight_boosting.py|sklearn/linear_model/ridge.py|sklearn/naive_bayes.py|sklearn/neighbors/kde.py|sklearn/neighbors/nearest_centroid.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/utils/__init__.py|sklearn/utils/validation.py|sklearn/ensemble/weight_boosting.py|sklearn/linear_model/ridge.py|sklearn/naive_bayes.py|sklearn/neighbors/kde.py|sklearn/neighbors/nearest_centroid.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/utils/__init__.py|sklearn/utils/validation.py,49,0.026281208935611037,0,5,true,MRG allow (n_samples 1) input for all classifiers Fixes issue #604 makes all estimators behave in a uniform and predictable way,,1253,0.7885075818036712,0.5269382391590013,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,98,travis,amueller,amueller,true,amueller,176,0.8579545454545454,688,34,1008,true,true,false,false,23,354,16,146,91,0,11
1839030,scikit-learn/scikit-learn,python,2250,1374842012,,1457463114,1376958,,unknown,false,false,false,37,5,4,4,1,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,210,6,232,6,26.72879001524253,0.6877386673691309,55,vanderplas@astro.washington.edu,sklearn/naive_bayes.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/naive_bayes.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,38,0.05013192612137203,0,0,true,WIP fix Naive Bayes coefficient stuff This fixes #2237 and #2240Not yet MRG and 014-rc because Im not sure if theres a good way to preserve backward compat and some more refactoring needs to be done,,1252,0.7891373801916933,0.525065963060686,33313,415.45342659022003,34.220874733587486,104.64383273797016,2332,29,1074,487,travis,larsmans,larsmans,true,,74,0.7297297297297297,113,34,1104,true,true,false,false,25,205,50,84,101,0,7
1838533,scikit-learn/scikit-learn,python,2249,1374835959,,1374851279,255,,unknown,false,false,false,48,3,1,2,2,0,4,0,2,0,0,1,5,1,0,0,0,0,5,5,4,0,0,54,0,78,8,4.936039103900911,0.1270053047389766,0,,examples/svm/plot_weighted_samples.py,0,0.0,0,1,true,[MRG] DOC improve svm sample weight example Fixes issue #2191The sample weights doc and example were not terribly informative imhoI tried to improve itThe example looked like this:[plot_weighted_samples_1](https://fcloudgithubcom/assets/449558/861421/a03f839c-f5d0-11e2-95cb-f24bfaa40252png)Now it looks like this:[class_weight_example](https://fcloudgithubcom/assets/449558/861516/1e523b2e-f5d3-11e2-9f43-ffb9045271c4png)Ill add another sentence to the docs in a minute,,1251,0.7897681854516387,0.5290148448043185,33313,415.06318854501245,34.190856422417674,104.61381442680035,2332,28,1074,98,travis,amueller,amueller,true,,175,0.8628571428571429,688,34,1008,true,true,false,false,23,341,15,146,85,0,214
1838468,scikit-learn/scikit-learn,python,2248,1374834761,,1375015357,3009,,unknown,false,false,false,169,45,10,47,16,1,64,0,5,0,0,5,5,4,0,0,0,0,5,5,4,0,0,432,294,708,350,133.16308605886053,3.426271358364233,112,vlad@vene.ro,sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|doc/whats_new.rst|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,64,0.035182679296346414,3,13,true,[MRG]: add fast_dot function calling BLAS directly and consume only twice the memory of your data Hi there I finally got it runningThis implements a feature advocated on this scipy page (section on large arrays and linalg):http://wikiscipyorg/PerformanceTipsWhen directly calling blass instead of npdot its possible to avoid copying when data are passed in F-contiguous order In addition Ive added chunking to the _logcosh function which avoids an extra copyThis is now how it looks on 1GB testing data:[fast_dot_chunking_logcosh](https://fcloudgithubcom/assets/1908618/861320/cb8e7c78-f5cc-11e2-8e1e-59727b66f4b2png)This was how the same test would have looked on the current master (plot from the last memory PR):[memory_ica_par_w1_computation_del_gwx](https://fcloudgithubcom/assets/1908618/861329/f0928b36-f5cc-11e2-82b6-01d17d79190apng) To make this functionality available for other use cases Ive added a fast_dot function to utilsextmath with almost stupid but explicit tests that exemplify the mapping between npdot and fast_dot which can be a hellFinally Ive made sure that down-stream applications are still workin For example with this local branch the mne-python ICA looks as good as it had looked beforecc @agramfort @GaelVaroquaux @mblondel  ,,1250,0.7904,0.5277401894451962,33309,411.7205560058843,33.98480891050467,104.05596085142153,2332,28,1074,99,travis,dengemann,dengemann,true,,4,1.0,29,29,391,false,true,false,false,1,27,2,11,12,0,16
1838467,scikit-learn/scikit-learn,python,2247,1374834757,1374835509,1374835509,12,12,github,false,false,false,21,4,3,1,0,0,1,0,1,0,0,3,3,1,0,0,0,0,3,3,1,0,0,0,2,0,2,13.334474436776913,0.3430945406399942,61,vlad@vene.ro,sklearn/manifold/spectral_embedding_.py|doc/modules/clustering.rst|doc/whats_new.rst,55,0.010825439783491205,0,0,true,Doc fixes Added some information to whatsnew fixed some typos and spelling errorsCan be useful to merge before version 014,,1249,0.7902321857485989,0.5277401894451962,33309,411.7205560058843,33.98480891050467,104.05596085142153,2332,28,1074,92,travis,pgervais,agramfort,false,agramfort,3,1.0,1,0,144,false,true,false,false,2,20,3,7,2,0,3
1838142,scikit-learn/scikit-learn,python,2246,1374831117,1374834255,1374834248,52,52,github,false,false,false,10,3,1,3,9,0,12,0,4,0,0,4,4,3,0,0,0,0,4,4,3,0,0,85,6,102,11,18.029573670546867,0.46389972875648694,37,vlad@vene.ro,doc/modules/feature_extraction.rst|sklearn/datasets/base.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,18,0.020435967302452316,0,2,true,MAINT charset is deprecated in favor of encoding closes #2107,,1248,0.7900641025641025,0.5245231607629428,33160,412.4849215922798,34.077201447527145,104.13148371531966,2332,28,1074,95,travis,NelleV,larsmans,false,larsmans,26,0.8846153846153846,38,13,1285,true,true,false,false,4,29,17,73,13,0,3
1835082,scikit-learn/scikit-learn,python,2243,1374779863,,1377083046,38386,,unknown,false,false,false,24,27,3,34,22,0,56,0,6,0,0,3,4,3,0,0,0,0,4,4,3,0,0,237,0,952,0,43.10629744888893,1.1288236962618186,4,vlad@vene.ro,examples/plot_precision_recall.py|examples/plot_roc.py|examples/plot_roc_crossval.py|examples/plot_precision_recall.py|examples/plot_roc.py|examples/plot_roc_crossval.py|examples/plot_precision_recall.py|examples/plot_roc.py|examples/plot_roc_crossval.py,4,0.0027210884353741495,0,11,false,[MRG] Fixes for #1958 - Precision recall and roc curve example are suboptimal Updated with more explanatory text Also changed to use train_test_split function,,1247,0.7906976744186046,0.5251700680272109,32780,405.64368517388647,34.350213544844415,104.54545454545453,2331,28,1073,115,travis,kastnerkyle,arjoly,false,,2,1.0,165,61,490,true,false,false,false,2,7,2,2,8,0,2
1834712,scikit-learn/scikit-learn,python,2241,1374778023,1374834821,1374834821,946,946,github,false,false,false,32,4,3,1,5,0,6,0,3,0,0,5,5,5,0,0,0,0,5,5,5,0,0,40,62,40,68,21.77811334086939,0.5703020440600907,37,vlad@vene.ro,sklearn/tests/test_cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py,29,0.015047879616963064,0,2,false,[MRG] Make sure that cross_validation_score and grid_search_cv support multi-output / multi-target output This should solve an issue but I cant find itThis should also support multilabel data in label indicator form,,1246,0.790529695024077,0.5239398084815321,32780,405.64368517388647,34.350213544844415,104.54545454545453,2331,28,1073,100,travis,arjoly,glouppe,false,glouppe,30,0.8333333333333334,17,21,583,true,true,true,true,16,246,32,160,75,0,10
1834494,scikit-learn/scikit-learn,python,2239,1374776145,1374778851,1374778851,45,45,merged_in_comments,false,false,false,14,2,1,0,6,0,6,0,3,0,0,5,5,4,0,0,0,0,5,5,4,0,0,12,7,12,7,22.363234641789,0.585624583156276,36,vlad@vene.ro,doc/modules/model_evaluation.rst|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py,18,0.015089163237311385,0,2,false,[MRG] consistency mse mean_squared_error ari  adjusted_rand_score Should be more consistent and close #1711,,1245,0.7903614457831325,0.522633744855967,32780,405.61317876754117,34.350213544844415,104.54545454545453,2331,28,1073,98,travis,arjoly,arjoly,true,arjoly,29,0.8275862068965517,17,21,583,true,true,false,false,16,245,31,160,75,0,34
1834328,scikit-learn/scikit-learn,python,2236,1374775486,1374776656,1374776656,19,19,merged_in_comments,false,false,false,15,2,2,4,4,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,8.60630388651532,0.2253685251868418,8,vanderplas@astro.washington.edu,sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_hierarchical.py,8,0.011019283746556474,0,1,false,FIX: Newer numpy gives more than one warning Change check to  0 warn ings,,1244,0.7901929260450161,0.5206611570247934,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,99,travis,erg,amueller,false,amueller,13,0.7692307692307693,20,4,1756,true,true,false,false,2,20,4,12,2,0,2
1834325,scikit-learn/scikit-learn,python,2235,1374775141,,1374776081,15,,unknown,false,false,false,10,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.095677453877691,0.10725124276260604,3,vlad@vene.ro,sklearn/linear_model/tests/test_sgd.py,3,0.004137931034482759,0,1,false,FIX: Newest numpy doesnt like addressing dimensions that dont exist ,,1243,0.7908286403861625,0.52,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,98,travis,erg,erg,true,,12,0.8333333333333334,20,4,1756,true,true,false,false,2,20,3,12,2,0,4
1834216,scikit-learn/scikit-learn,python,2232,1374774312,1374800148,1374800148,430,430,merged_in_comments,false,false,false,18,117,116,0,0,0,0,0,1,20,4,37,62,43,0,0,20,4,38,62,44,0,0,1849,3896,1849,3903,865.2824172098316,22.65866442761508,103,vlad@vene.ro,sklearn/cluster/__init__.py|sklearn/metrics/__init__.py|sklearn/metrics/cluster/__init__.py|doc/modules/biclustering.rst|examples/bicluster/bicluster_newsgroups.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|sklearn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/base.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/__init__.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/unsupervised_learning.rst|examples/bicluster/README.txt|examples/bicluster/spectral_biclustering.py|examples/bicluster/spectral_coclustering.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/base.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|examples/bicluster/spectral_biclustering.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/utils/extmath.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|doc/modules/classes.rst|examples/bicluster/README.txt|examples/cluster/spectral_biclustering.py|examples/cluster/spectral_coclustering.py|sklearn/bicluster/tests/test_spectral.py|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/__init__.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|sklearn/cluster/bicluster/spectral.py|examples/cluster/plot_cocluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|doc/modules/biclustering.rst|examples/cluster/bicluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|sklearn/base.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/metrics/__init__.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/__init__.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/datasets/samples_generator.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|doc/modules/biclustering.rst|examples/cluster/plot_bicluster_newsgroups.py|sklearn/base.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|doc/modules/biclustering.rst|sklearn/cluster/bicluster/spectral.py|examples/cluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|examples/bicluster/README.txt|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/classes.rst|doc/modules/classes.rst|examples/bicluster/README.txt|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/spectral.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/datasets/samples_generator.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/setup.py|AUTHORS.rst|doc/whats_new.rst|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/utils/linear_assignment_.py|sklearn/utils/tests/test_hungarian.py|sklearn/utils/tests/test_linear_assignment.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/utils/tests/test_linear_assignment.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/utils/linear_assignment_.py,53,0.0,0,0,false,MRG spectral biclustering Ready to merge but on my system it introduces a random seed-related failure in sklearn/cluster/tests/test_spectralpy:test_affinities,,1242,0.7906602254428341,0.5200553250345782,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,98,travis,vene,vene,true,vene,40,0.85,50,30,1201,true,true,false,false,2,48,14,82,103,0,-1
1834781,scikit-learn/scikit-learn,python,2231,1374774158,,1374778649,74,,unknown,false,false,false,4,2,1,0,1,0,1,0,2,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.421106714898943,0.1157730371716059,4,olivier.grisel@ensta.org,.travis.yml,4,0.005532503457814661,0,0,false,Enable Python3 on travis ,,1241,0.7912973408541499,0.5200553250345782,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,98,travis,ogrisel,ogrisel,true,,38,0.8157894736842105,781,121,1520,true,true,false,false,10,216,16,149,107,0,28
1834191,scikit-learn/scikit-learn,python,2230,1374774068,1374833472,1374833472,990,990,github,false,false,false,53,4,3,0,9,0,9,0,3,0,0,5,5,5,0,0,0,0,5,5,5,0,0,44,0,48,0,18.453743886215413,0.4832378213568968,13,vanderplas@astro.washington.edu,sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/kd_tree.pyx|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/ball_tree.c|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/kd_tree.c,13,0.012448132780082987,0,7,false,Neighbors segfault fix addresses #2190 it sets wraparoundTrue in the cython headerIve also replaced Xshape[-1] with Xshape[Xndim - 1] I havent done it for  Xshape[:-1] though - if we do that than we can set wraparoundFalse again (at least I think so)compiled kd tree and ball tree with cython 0191,,1240,0.7911290322580645,0.5200553250345782,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,97,travis,pprett,larsmans,false,larsmans,36,0.8888888888888888,107,28,1451,true,true,true,true,12,70,12,4,11,0,73
1834176,scikit-learn/scikit-learn,python,2229,1374773828,1374805108,1374805108,521,521,github,false,false,false,40,3,2,0,4,0,4,0,2,0,0,1,2,0,0,0,0,0,2,2,1,0,0,0,0,12,0,9.306482279459672,0.243702787787703,14,vlad@vene.ro,doc/modules/preprocessing.rst|doc/modules/preprocessing.rst,14,0.019444444444444445,1,1,false,[MRG] minimal kernel centering narrative docs I know very little about this stuff so this is just rephrasing the docstring in simpler terms to get the TODO gone Someone else may want to write proper docs someday Maybe @mblondel ),,1239,0.7909604519774012,0.5180555555555556,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,97,travis,larsmans,mblondel,false,mblondel,73,0.726027397260274,113,34,1103,true,true,true,true,25,196,43,83,98,0,11
1836478,scikit-learn/scikit-learn,python,2228,1374772808,1374773840,1374773841,17,17,github,false,false,false,14,1,1,0,2,0,2,0,2,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.475491074263797,0.11719670628939946,0,,.travis.yml,0,0.0,0,0,false,use new virtualenv features of travis  so we dont have to kill the virtualenv,,1238,0.7907915993537964,0.5167130919220055,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,95,travis,amueller,ogrisel,false,ogrisel,174,0.8620689655172413,686,34,1007,true,true,true,false,19,294,12,139,83,0,15
1833943,scikit-learn/scikit-learn,python,2226,1374771900,1374775615,1374775615,61,61,merged_in_comments,false,false,false,22,2,1,0,7,0,7,0,3,0,0,2,2,1,0,0,0,0,2,2,1,0,0,236,0,236,0,9.74381512416386,0.2551548020755881,72,vlad@vene.ro,doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py,63,0.08774373259052924,0,1,false,[MRG] Remove some bad example I remove some bad example of the metrics module that bring too few things for their length,,1237,0.7906224737267583,0.5167130919220055,32780,405.61317876754117,36.11958511287371,107.71812080536914,2331,28,1073,98,travis,arjoly,amueller,false,amueller,28,0.8214285714285714,17,21,583,true,true,true,false,16,240,30,160,73,0,5
1832907,scikit-learn/scikit-learn,python,2224,1374765146,1374832542,1374832542,1123,1123,merged_in_comments,false,false,false,16,10,3,0,2,0,2,0,2,0,0,3,7,3,0,0,0,0,7,7,7,0,0,305,28,817,72,22.573378001589287,0.5918075830750276,7,vlad@vene.ro,sklearn/linear_model/__init__.py|sklearn/linear_model/omp.py|sklearn/linear_model/tests/test_omp.py|sklearn/linear_model/tests/test_omp.py|sklearn/linear_model/omp.py,6,0.004225352112676056,0,0,false,[WIP] OrthogonalMatchingPursuitCV As discussed in issue #1930 Todo:  - [ ] narratives  - [ ] coverage,,1236,0.790453074433657,0.5112676056338028,32594,403.08032153157023,36.110940663925874,107.99533656501197,2331,28,1073,100,travis,vene,agramfort,false,agramfort,39,0.8461538461538461,50,30,1201,true,true,true,true,2,47,13,82,103,0,1112
1832540,scikit-learn/scikit-learn,python,2223,1374763391,1374770464,1374770464,117,117,github,false,false,false,5,3,2,0,2,0,2,0,2,0,0,2,3,1,0,0,0,0,3,3,2,0,0,11,0,12,8,12.670843475474218,0.33219221136144944,22,vanderplas@astro.washington.edu,sklearn/datasets/__init__.py|doc/modules/classes.rst|sklearn/datasets/__init__.py,21,0.029661016949152543,0,1,false,[MRG] Doc datasets Update reference,,1235,0.7902834008097166,0.5098870056497176,32593,403.0926886141196,36.11204859939251,107.99865001687479,2331,28,1073,98,travis,arjoly,agramfort,false,agramfort,27,0.8148148148148148,17,21,583,true,true,false,false,11,214,27,159,69,0,13
1832521,scikit-learn/scikit-learn,python,2222,1374763177,1379499515,1379499515,78938,78938,github,false,false,false,36,34,3,38,26,0,64,0,5,0,0,1,5,1,0,0,1,0,5,6,1,0,0,519,0,830,0,14.55338402098058,0.38154688202720344,33,vanderplas@astro.washington.edu,examples/applications/plot_out_of_core_classification.py|examples/applications/plot_out_of_core_classification.py|examples/applications/plot_out_of_core_classification.py,33,0.046610169491525424,1,20,false,[MRG] Added comparison of other classifiers using partial fit methods Following on @ogrisel PR I modified the plot_out_of_core_classificationpy example to work with every classifier that supports a partial fit methodPR is now ready to merge,,1234,0.7901134521880064,0.5098870056497176,32593,403.0926886141196,36.11204859939251,107.99865001687479,2331,28,1073,118,travis,FedericoV,ogrisel,false,ogrisel,3,0.6666666666666666,2,1,885,false,false,false,false,1,0,2,0,0,0,101
1831649,scikit-learn/scikit-learn,python,2220,1374753266,1374762763,1374762763,158,158,merged_in_comments,false,false,false,179,7,7,3,9,0,12,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,464,152,464,152,48.72369978472134,1.277370487240634,76,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|doc/whats_new.rst|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,51,0.03188405797101449,0,3,false,[MRG] Gbrt verbose output I refactored the verbose output of GBRT The output contains less boilerplate text and is nicer alignedIt now also reports OOB improvement and estimated time (in seconds or minutes)Here is an example on covertype using verbose1:      Iter       Train Loss   Estimated Time          1           12980            789m         2           12814            782m         3           12378            795m         4           12234            810m         5           11893            794m         6           11820            792m         7           11756            795m         8           11660            774m         9           11394            781m        10           11157            785m        20           10111            763m        30           09602            670m        40           09354            566m        50           09124            464m        60           08979            365m        70           08859            272m        80           08762            181m        90           08691           5382s       100           08599            000sthe old output for verbose1 was just a  for each iteration Using verbose1 on **master** gives the following output::    built tree 1 of 100 train score  1297973e+00    built tree 2 of 100 train score  1281386e+00    built tree 3 of 100 train score  1237790e+00For verbose1 it will print iterations with modulo increasing power of 10 is zeroFor verbose1 it will print each iterationThere are now test for verbose output,,1233,0.7899432278994323,0.4971014492753623,32580,400.64456721915286,35.880908532842234,106.93677102516881,2331,28,1073,98,travis,pprett,pprett,true,pprett,35,0.8857142857142857,107,28,1451,true,true,false,false,11,60,10,4,10,0,6
1831336,scikit-learn/scikit-learn,python,2217,1374748309,1374750680,1374750680,39,39,github,false,false,false,266,5,3,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,61,12,65,14,13.609618215965558,0.35803324280563037,41,vlad@vene.ro,sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/fastica_.py,39,0.05743740795287187,0,0,false,MRG backwards compatibility in FastICA estimator After some thinking I think this is the cleanest way to change the FastICA estimator according to the new compute_sources parameterIdeally (if there were no users) the estimator wouldnt have a compute_sources init argument because the use case is different: either you use FastICAfit_transform in which case you call the underlying function with compute_sourcesTrue or you use FastICAfit in which case its more effective not to compute the sources because presumably the user will subsequently call transform on some other data  (If they will use the same data its their fault for misusing the API)In order to get there we need to deprecate the FastICAsources_ attribute Because its computation depends on the initial data passed to fit I couldnt compute it lazily so until the deprecation I propose to just compute it all the time  (this is identical to what FastICA did yesterday night before my nocturnal bugfix anyway)Alternatively if we want to make the estimator be able to save the memory we could introduce a deprecated-since-birth init attribute compute_sources True by default  However I think this is just confusing for users if people want to use fastica and save memory they should simply call the function directlyIn general I think the API can be rethought in many places many estimators have tons of parameters that are only useful for speedup/optimization and all they do is expose the underlying functions with complicated logic  I think that users who really work at the limit of their resources should go ahead and use the underlying public functionsOpinions,,1232,0.7897727272727273,0.4889543446244477,32554,400.8723966332862,35.90956564477484,106.991460342815,2331,28,1073,101,travis,vene,agramfort,false,agramfort,38,0.8421052631578947,50,30,1201,true,true,true,true,1,42,9,82,103,0,35
1831166,scikit-learn/scikit-learn,python,2216,1374745439,1374746012,1374746012,9,9,github,false,false,false,33,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,47,0,47,0,8.557334909056213,0.22512008954107457,85,vanderplas@astro.washington.edu,sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,82,0.12148148148148148,2,0,false,[MRG] Additional documentation for the Tree cython structure This adds docstrings for the Tree structure This solves #2215Ping @arjoly @pprett - nothing controversial here I think we can merge this very quickly,,1231,0.7896019496344435,0.48592592592592593,32554,400.8723966332862,35.90956564477484,106.991460342815,2331,28,1073,101,travis,glouppe,glouppe,true,glouppe,34,1.0,107,26,987,true,true,false,false,11,94,13,63,9,0,6
1829250,scikit-learn/scikit-learn,python,2214,1374716111,,1375010983,4914,,unknown,false,false,false,35,3,1,2,9,0,11,0,3,0,0,4,5,4,0,0,0,0,5,5,5,0,0,129,0,144,0,12.778992975019108,0.33618037313656424,102,vanderplas@astro.washington.edu,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py,82,0.07566765578635015,0,0,false,[WIP] COSMIT refactor tree prediction Probability or best class label is now calculated for each tree node rather than each sample and Treepredict_leaf() returns only node indices replacing Treepredict()Yes more deletions than additions :),,1230,0.7902439024390244,0.48516320474777447,32554,400.8723966332862,35.90956564477484,106.991460342815,2331,28,1072,110,travis,jnothman,jnothman,true,,44,0.5681818181818182,17,1,1548,true,true,false,false,12,385,64,203,47,0,12
1827859,scikit-learn/scikit-learn,python,2213,1374702152,1374702340,1374702340,3,3,github,false,false,false,5,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.464723658372794,0.11745461863401811,5,vanderplas@astro.washington.edu,sklearn/cluster/affinity_propagation_.py,5,0.007462686567164179,0,0,false,DOC/FIX affinity_propagation damping default value ,,1229,0.790073230268511,0.4880597014925373,32551,400.9093422629105,35.912875180486004,107.00132100396301,2331,28,1072,100,travis,cmd-ntrf,agramfort,false,agramfort,1,1.0,10,3,756,false,true,false,false,0,0,0,0,0,0,-1
1826612,scikit-learn/scikit-learn,python,2212,1374691084,1374696328,1374696328,87,87,github,false,false,false,101,7,2,0,2,0,2,0,2,0,0,2,4,2,0,0,0,0,4,4,2,0,0,168,8,202,8,18.30351672811466,0.4815208969772313,35,vanderplas@astro.washington.edu,sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py,33,0.04962406015037594,5,2,false,WIP/ENH: improve ICA memory profile by 40% Using the new memory profiler by @fabianp @garvais I was able to track down the memory issues I had witnessed during the last weeksThis proposal cuts down memory consumption by 50-40% - that is instead of 5-6 GB of ram on 1GB data input we now end up consuming 3 GBI pushed things into inplace operations where possible and removed local variables which previously had been ratcheting up across cycles of iteration Finally cost functions now return a mean vector as second value which further cuts memory consumptioncc @GaelVaroquaux @agramfort @mblondel,,1228,0.7899022801302932,0.48721804511278194,32546,400.90948196398944,35.91839242917717,107.01775947889142,2331,28,1072,101,travis,dengemann,agramfort,false,agramfort,3,1.0,29,29,389,false,true,true,true,0,25,1,11,12,0,4
1826468,scikit-learn/scikit-learn,python,2211,1374689620,1374752210,1374752210,1043,1043,merged_in_comments,false,false,false,25,5,2,45,6,0,51,0,5,0,0,5,5,4,0,0,0,0,5,5,4,0,0,14,7,22,7,21.787829432526323,0.5731846686829369,61,vlad@vene.ro,doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py,32,0.016591251885369532,2,0,false,[MRG] Scoring doc and tweaks Minor usability and doc tweaks for the new scoring API@larsman @arjoly you guys would be good candidates for reviewing,,1227,0.7897310513447433,0.48717948717948717,32546,400.90948196398944,35.91839242917717,107.01775947889142,2331,28,1072,101,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,31,0.6774193548387096,383,3,1248,true,true,false,false,10,228,18,150,53,0,3
1826160,scikit-learn/scikit-learn,python,2210,1374686317,1377873348,1377873348,53117,53117,github,false,false,false,139,37,4,63,32,1,96,0,8,0,0,3,6,3,0,0,0,0,6,6,6,0,0,294,26,1579,313,44.72360187878803,1.1894416836704744,28,vlad@vene.ro,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/__init__.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/__init__.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,16,0.01818181818181818,0,7,false,[MRG] Euclidean pairwise distances argmin The function metricseuclidean_distances_argmin has been created It is equivalent to:    def euclidean_distances_argmin(X YNone axis0):         return sklearnmetricseuclidean_distances(X YY squaredTrue)argmin(axisaxis)But much more memory efficient It computes the output array chunk-by-chunkOnly the argmin has been implemented so far argmax is trivial to add but the API must be discussed Eg should we create another function or rename this one to something like euclidean_distances_extrema and add an optionThis is an implementation of part of what was suggested in issue #325 As a side effect the current implementation is faster than the naive one above for some inputs (usually those for which the first array is much bigger than the second with axis0)An improvement in the memory usage of check_pairwise_arrays has also been done and several similar improvements can probably be done elsewhere (see #2206),,1226,0.7895595432300163,0.48333333333333334,32353,401.16836151206996,35.916298334003024,107.03798720365963,2331,28,1072,127,travis,pgervais,larsmans,false,larsmans,2,1.0,1,0,142,false,true,false,false,0,4,2,4,2,0,21
1826158,scikit-learn/scikit-learn,python,2209,1374686275,,1374686290,0,,unknown,false,false,false,2,212,212,0,0,0,0,0,0,21,3,39,63,41,0,0,21,3,39,63,41,0,0,1852,3814,1852,3814,1198.763385595026,31.881581084392423,68,vlad@vene.ro,sklearn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/bicluster/spectral.py|sklearn/base.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/__init__.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/__init__.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/datasets/samples_generator.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/unsupervised_learning.rst|examples/bicluster/README.txt|sklearn/bicluster/spectral.py|examples/bicluster/spectral_biclustering.py|examples/bicluster/spectral_coclustering.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/base.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|examples/bicluster/spectral_biclustering.py|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|examples/bicluster/spectral_biclustering.py|sklearn/bicluster/spectral.py|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/utils/extmath.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/__init__.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|doc/modules/classes.rst|examples/bicluster/README.txt|examples/cluster/spectral_biclustering.py|examples/cluster/spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|sklearn/cluster/bicluster/spectral.py|examples/cluster/plot_cocluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|doc/modules/biclustering.rst|examples/cluster/bicluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|doc/modules/biclustering.rst|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|sklearn/base.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/metrics/__init__.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/datasets/samples_generator.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|doc/modules/biclustering.rst|examples/cluster/plot_bicluster_newsgroups.py|sklearn/base.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/cluster/bicluster/spectral.py|doc/modules/biclustering.rst|examples/cluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|examples/bicluster/README.txt|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/classes.rst|sklearn/cluster/bicluster/spectral.py|doc/modules/classes.rst|sklearn/cluster/__init__.py|examples/bicluster/README.txt|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|sklearn/datasets/samples_generator.py|sklearn/setup.py|sklearn/setup.py|AUTHORS.rst|doc/whats_new.rst|sklearn/datasets/samples_generator.py|doc/whats_new.rst|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/utils/linear_assignment_.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/utils/linear_assignment_.py|sklearn/utils/tests/test_hungarian.py|sklearn/utils/tests/test_linear_assignment.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/utils/tests/test_linear_assignment.py,41,0.0,0,0,false,Spectral biclustering ,,1225,0.7902040816326531,0.48333333333333334,32353,401.16836151206996,35.916298334003024,107.03798720365963,2331,28,1072,100,travis,vene,vene,true,,37,0.8648648648648649,50,30,1200,true,true,false,false,1,40,6,81,100,0,-1
1825696,scikit-learn/scikit-learn,python,2208,1374681664,,1374686221,75,,unknown,false,false,false,54,212,211,0,0,0,0,0,0,21,3,38,63,41,0,0,21,3,39,63,41,0,0,1849,3814,1852,3814,1194.3262018781827,31.763572448030324,68,vlad@vene.ro,sklearn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/bicluster/spectral.py|sklearn/base.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/__init__.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/__init__.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/datasets/samples_generator.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/util.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/samples_generator.py|doc/datasets/index.rst|doc/modules/biclustering.rst|doc/unsupervised_learning.rst|examples/bicluster/README.txt|sklearn/bicluster/spectral.py|examples/bicluster/spectral_biclustering.py|examples/bicluster/spectral_coclustering.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/base.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|examples/bicluster/spectral_biclustering.py|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|examples/bicluster/spectral_biclustering.py|sklearn/bicluster/spectral.py|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/utils/extmath.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/spectral.py|sklearn/bicluster/tests/test_spectral.py|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/__init__.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|doc/modules/classes.rst|examples/bicluster/README.txt|examples/cluster/spectral_biclustering.py|examples/cluster/spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|doc/modules/biclustering.rst|sklearn/cluster/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|sklearn/cluster/bicluster/spectral.py|examples/cluster/plot_cocluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|doc/modules/biclustering.rst|examples/cluster/bicluster_newsgroups.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|sklearn/base.py|doc/modules/biclustering.rst|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|sklearn/base.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/__init__.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/metrics/__init__.py|sklearn/metrics/cluster/bicluster/__init__.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/tests/test_bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/datasets/tests/test_samples_generator.py|doc/modules/biclustering.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/datasets/samples_generator.py|examples/cluster/plot_spectral_biclustering.py|examples/cluster/plot_spectral_coclustering.py|doc/modules/biclustering.rst|examples/cluster/plot_bicluster_newsgroups.py|sklearn/base.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/utils.py|sklearn/cluster/bicluster/tests/test_utils.py|sklearn/cluster/bicluster/utils.py|doc/modules/biclustering.rst|doc/modules/biclustering.rst|doc/modules/biclustering.rst|sklearn/cluster/bicluster/spectral.py|doc/modules/biclustering.rst|examples/cluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|examples/bicluster/README.txt|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_spectral_coclustering.py|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|sklearn/cluster/bicluster/tests/test_spectral.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/classes.rst|sklearn/cluster/bicluster/spectral.py|doc/modules/classes.rst|sklearn/cluster/__init__.py|examples/bicluster/README.txt|examples/bicluster/plot_bicluster_newsgroups.py|examples/bicluster/plot_bicluster_newsgroups.py|doc/modules/biclustering.rst|sklearn/datasets/samples_generator.py|sklearn/setup.py|sklearn/setup.py|AUTHORS.rst|doc/whats_new.rst|sklearn/datasets/samples_generator.py|doc/whats_new.rst|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|examples/bicluster/plot_spectral_biclustering.py|examples/bicluster/plot_spectral_coclustering.py|sklearn/cluster/bicluster/spectral.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/cluster/bicluster/spectral.py|sklearn/cluster/bicluster/spectral.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/metrics/cluster/bicluster/bicluster_metrics.py|sklearn/utils/linear_assignment_.py|sklearn/utils/tests/test_hungarian.py|sklearn/utils/tests/test_linear_assignment.py|examples/bicluster/plot_bicluster_newsgroups.py|sklearn/utils/tests/test_linear_assignment.py,41,0.0,0,0,false,[MRG] Spectral biclustering reloaded As I think this is ready Im submitting a PR straight to sklearn instead of to Kemal  Maybe we can merge it soonI addressed most of the comments except the one-import-per-line (which is debatable) and the variable names in Hungarian since Im not gonna start refactoring that file now,,1224,0.7908496732026143,0.48244274809160304,32353,401.16836151206996,35.916298334003024,107.03798720365963,2331,28,1072,100,travis,vene,vene,true,,36,0.8888888888888888,50,30,1200,true,true,false,false,1,37,3,81,98,0,-1
1825567,scikit-learn/scikit-learn,python,2207,1374680230,,1374929745,4158,,unknown,false,true,false,21,7,4,14,6,0,20,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,17.88808248767284,0.47574054991007575,6,vlad@vene.ro,doc/modules/multiclass.rst|doc/modules/multiclass.rst|doc/modules/multiclass.rst|doc/modules/multiclass.rst,6,0.009216589861751152,0,3,false,[MRG] Multi class label documentation Re-organized the documentation added some real simple examples for the definitions of multiclass and multilabel classification,,1223,0.7914963205233033,0.4823348694316436,32353,401.16836151206996,35.916298334003024,107.03798720365963,2331,28,1072,112,travis,schwarty,arjoly,false,,3,0.3333333333333333,6,1,1136,false,true,false,false,0,0,1,0,0,0,86
1824440,scikit-learn/scikit-learn,python,2205,1374665209,1374680045,1374680045,247,247,github,false,false,false,100,3,2,0,5,0,5,0,3,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,9.929936154428269,0.2644327855103447,10,vanderplas@astro.washington.edu,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html,10,0.01589825119236884,0,0,false,Enh rel canonical This adds **relcanonical** to all the pages in stableturns out it was a lot simpler than I thought ----Just to be clear this will add a (for example)link relcanonical hrefhttp://scikit-learnorg/stable/modules/ensemblehtmlto that pageand will do so for every auto-generated page on the **stable** docsThis will appear in the head tag of each page I Tested it on an online build tooApparently it will take a bit of time before we see any effect as Googles crawlers need to update thisFor more info on what this is for see [this article](https://supportgooglecom/webmasters/answer/139394hlen),,1222,0.7913256955810147,0.47217806041335453,32257,401.89726260966614,35.9921877421955,107.13953560467496,2331,28,1072,104,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,58,0.9137931034482759,11,13,546,true,true,false,false,7,156,33,44,22,0,9
1823764,scikit-learn/scikit-learn,python,2203,1374660760,1374675585,1374675585,247,247,github,false,false,false,10,4,2,12,4,0,16,0,3,0,0,7,7,7,0,0,0,0,7,7,7,0,0,819,0,857,0,35.64888015570046,0.9492951972906613,9,vlad@vene.ro,sklearn/covariance/graph_lasso_.py|examples/covariance/plot_robust_vs_empirical_covariance.py|sklearn/covariance/__init__.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py,6,0.004807692307692308,0,0,false,doc fixes in covariance module doc fixes taken from #2081,,1221,0.7911547911547911,0.47115384615384615,32246,401.78626806425603,35.973454071822864,107.14507225702413,2331,28,1072,107,travis,agramfort,agramfort,true,agramfort,32,0.875,119,181,1330,true,true,false,false,4,62,29,46,47,0,52
1823254,scikit-learn/scikit-learn,python,2202,1374658050,1374664005,1374664005,99,99,github,false,false,false,71,3,2,1,1,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,41,11,41,14,14.185364232640627,0.3777425287708839,8,vanderplas@astro.washington.edu,sklearn/tests/test_isotonic.py|examples/plot_isotonic_regression.py|sklearn/isotonic.py,6,0.006430868167202572,2,1,false,New option to the isotonic regression: now can fit an isotonic regression on a decreasing function HelloHere is a small pull request that adds a new feature to the isotonic regression So far we could only fit an isotonic regression on an increasing function Now when setting the increasing to False it fits the IR on a decreasing function@agramfort @fabianp maybe you can have a look at this PR,,1220,0.7909836065573771,0.47106109324758844,32246,401.78626806425603,35.973454071822864,107.14507225702413,2331,28,1072,106,travis,NelleV,agramfort,false,agramfort,25,0.88,38,13,1283,true,true,true,true,1,18,16,42,5,0,14
1823053,scikit-learn/scikit-learn,python,2201,1374657298,1375010450,1375010450,5885,5885,github,false,false,false,130,117,9,0,45,0,45,0,7,20,0,6,36,3,5,13,25,1,15,41,7,5,15,6858,0,6882,0,254.95026086729607,6.789078847140854,18,vanderplas@astro.washington.edu,doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap-responsive.css|doc/themes/scikit-learn/static/css/bootstrap-responsive.min.css|doc/themes/scikit-learn/static/css/bootstrap.css|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/img/forkme.png|doc/themes/scikit-learn/static/img/glyphicons-halflings-white.png|doc/themes/scikit-learn/static/img/glyphicons-halflings.png|doc/themes/scikit-learn/static/img/google.png|doc/themes/scikit-learn/static/img/inria.jpg|doc/themes/scikit-learn/static/img/plot_classifier_comparison_1.png|doc/themes/scikit-learn/static/img/plot_manifold_sphere_1.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-notext.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-small.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.svg|doc/themes/scikit-learn/static/img/telecom.jpg|doc/themes/scikit-learn/static/js/bootstrap.js|doc/themes/scikit-learn/static/js/bootstrap.min.js|doc/themes/scikit-learn/static/js/jquery.js|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap-responsive.css|doc/themes/scikit-learn/static/css/bootstrap-responsive.min.css|doc/themes/scikit-learn/static/css/bootstrap.css|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/img/forkme.png|doc/themes/scikit-learn/static/img/glyphicons-halflings-white.png|doc/themes/scikit-learn/static/img/glyphicons-halflings.png|doc/themes/scikit-learn/static/img/google.png|doc/themes/scikit-learn/static/img/inria.jpg|doc/themes/scikit-learn/static/img/plot_classifier_comparison_1.png|doc/themes/scikit-learn/static/img/plot_manifold_sphere_1.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-notext.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-small.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.svg|doc/themes/scikit-learn/static/img/telecom.jpg|doc/themes/scikit-learn/static/js/bootstrap.js|doc/themes/scikit-learn/static/js/bootstrap.min.js|doc/themes/scikit-learn/static/js/jquery.js|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap-responsive.css|doc/themes/scikit-learn/static/css/bootstrap-responsive.min.css|doc/themes/scikit-learn/static/css/bootstrap.css|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/img/forkme.png|doc/themes/scikit-learn/static/img/glyphicons-halflings-white.png|doc/themes/scikit-learn/static/img/glyphicons-halflings.png|doc/themes/scikit-learn/static/img/google.png|doc/themes/scikit-learn/static/img/inria.jpg|doc/themes/scikit-learn/static/img/plot_classifier_comparison_1.png|doc/themes/scikit-learn/static/img/plot_manifold_sphere_1.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-notext.png|doc/themes/scikit-learn/static/img/scikit-learn-logo-small.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.png|doc/themes/scikit-learn/static/img/scikit-learn-logo.svg|doc/themes/scikit-learn/static/img/telecom.jpg|doc/themes/scikit-learn/static/js/bootstrap.js|doc/themes/scikit-learn/static/js/bootstrap.min.js|doc/themes/scikit-learn/static/js/jquery.js|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/documentation.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/css/bootstrap.min.css|doc/themes/scikit-learn/static/nature.css_t|doc/user_guide.rst,14,0.0,0,26,false,[WIP] Redo website yet again early stageNow with online build: http://amuellergithubio/scikit-learn/[asdf](https://fcloudgithubcom/assets/449558/851757/6aa92000-f4a2-11e2-834d-05e2e0d244e7png)Todo:- [x] Fix text on front-page- [x] Fix links to sections on front page- [x] Fix text on documentation page- [x] Fix links on documentation page- [ ] link to older versions of docs- [x] remove old version warning on install- [x] fix blue bar on front page- [x] fix sponsor images- [x] fix lowest part: community etc- [x] fix copyright notice- [x] fix collabsible sidebare- [x] fix collabsible toctree- [x] fix prev / next buttons- [x] layout on the first page- [x] drop-down documentation link in menu- [x] fix broken link on the first page (testimonials donations read more changelog),,1219,0.7908121410992617,0.47106109324758844,32246,401.78626806425603,35.973454071822864,107.14507225702413,2331,28,1072,116,travis,amueller,amueller,true,amueller,173,0.861271676300578,685,34,1006,true,true,false,false,14,198,9,47,42,0,18
1817744,scikit-learn/scikit-learn,python,2199,1374621722,1394013628,1394013628,323138,323138,commits_in_master,false,false,false,106,242,90,100,57,1,158,0,9,4,0,6,36,7,0,0,7,0,32,39,28,0,0,9720,396,14257,2145,799.680809470978,21.29472646772378,25,vlad@vene.ro,sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.pyx|sklearn/cluster/fast_dict.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/test_fast_dict.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/hierarchical.py|examples/cluster/plot_hierarchical_clustering.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/fast_dict.cpp,17,0.0,1,14,false,[WIP] Hierarchical Agglomerative Clustering Introduces complete and average linkage with structureTODO:* [x] Narrative documentation* [x] docstring* [x] Increase test coverage* [x] Implement other distances for average and complete linkage (half done only callable and precompute are still needed)* [x] Probably deprecate Ward* classes* [x] Probably move the FloatIntDict to the utils module* [x] Finish the exampleSingle linkage will come in a later PR as it should use a different algorithm based on minimum spanning tree@vene : do I remember right that you had expressed interest in working with me on having different distances in this PR,,1218,0.7906403940886699,0.46859903381642515,32246,401.78626806425603,35.973454071822864,107.14507225702413,2331,28,1071,182,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,30,0.6666666666666666,381,3,1247,true,true,false,false,10,221,17,134,51,0,613
1814568,scikit-learn/scikit-learn,python,2198,1374600380,1377023968,1377023968,40393,40393,commits_in_master,false,false,false,91,32,12,2,11,0,13,0,6,2,0,5,11,5,0,0,2,2,7,11,7,0,0,1122,230,2151,582,80.5246609792932,2.1443001296476423,2,g.louppe@gmail.com,sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/tests/test_bagging.py|sklearn/ensemble/__init__.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py|sklearn/ensemble/random_patches.py|sklearn/ensemble/tests/test_random_patches.py,2,0.0,0,1,false,[WIP] Bagging ensemble meta-estimator HiThis is a very early PR for a meta-estimator ensemble implementing ensemble averaging/voting The idea is to make a meta-estimator that can take as input any type of base estimator (not only trees) and make an ensemble out of it This should work quite well for estimators with high variance (trees gbrt neural networks typically)--- TODO list:- [x] rename to BaggingClassifier and BaggingRegressor- [x] add subsampling hyper-parameter- [x] add subsampling_features hyper-parameter- [ ] documentation- [x] tests- [ ] examples,,1217,0.790468364831553,0.46416938110749184,32246,401.78626806425603,35.973454071822864,107.14507225702413,2331,28,1071,124,travis,glouppe,glouppe,true,glouppe,33,1.0,107,26,985,true,true,false,false,11,91,12,39,6,0,47
1814491,scikit-learn/scikit-learn,python,2197,1374599672,,1374833235,3892,,unknown,false,false,false,49,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,23,0,0.0,0,,,0,0.0,0,2,false,Test: Added a test for naive_bayes which is currently broken  Lars work Added a test that makes sure that the _coef attitude as implemented in naive bayes works the same way as _coefs coming from linear models  The test is currently brokenCurrently the test is skipped by default,,1216,0.7911184210526315,0.46416938110749184,32328,400.73620390992323,35.882207374412275,106.87329868844346,2331,28,1071,114,travis,FedericoV,larsmans,false,,2,1.0,2,1,883,false,false,false,false,0,0,1,0,0,0,3889
1813856,scikit-learn/scikit-learn,python,2196,1374595155,1374598031,1374598031,47,47,github,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.331941738880044,0.11534677391860429,26,vlad@vene.ro,sklearn/preprocessing.py,26,0.04304635761589404,0,0,false,DOC: Fix backwards docs on thresholds for preprocessing See issue #2066,,1215,0.7909465020576132,0.4552980132450331,32328,400.73620390992323,35.882207374412275,106.87329868844346,2331,28,1071,103,travis,erg,agramfort,false,agramfort,11,0.8181818181818182,20,4,1754,true,true,false,false,2,16,2,12,2,0,-1
1813822,scikit-learn/scikit-learn,python,2195,1374594777,1374605890,1374605891,185,185,github,false,false,false,10,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.474182036045548,0.11913421160549854,2,normalesup.org,sklearn/utils/sparsetools/setup.py,2,0.003316749585406302,0,0,false,FIX: Finish package rename from mst - sparsetools Fixes #2189 ,,1214,0.7907742998352554,0.45439469320066334,32328,400.73620390992323,35.882207374412275,106.87329868844346,2331,28,1071,105,travis,erg,NelleV,false,NelleV,10,0.8,20,4,1754,true,true,false,false,2,16,1,12,2,0,-1
1813435,scikit-learn/scikit-learn,python,2194,1374590924,1374760320,1374760320,2823,2823,github,false,false,false,101,4,2,9,4,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,12,12,12,8.712249474028186,0.2319911380412452,4,vanderplas@astro.washington.edu,sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py,4,0.006711409395973154,0,1,false,[MRG] as_float_array doesnt copy The function as_float_array doesnt copy as expected when the matrix was created with sparse_random_matrix This code should fix thatExample:pythonimport numpy as npimport scipysparse as spfrom sklearnrandom_projection import sparse_random_matrixfrom sklearnutils import as_float_arraymatrices  [    npmatrix(nparange(5))    spcsc_matrix(nparange(5))todense()    sparse_random_matrix(10 10 density010)todense()]for M in matrices:    print type(M) # class numpymatrixlibdefmatrixmatrix    N  as_float_array(M copyTrue)    print type(N) # type numpyndarray    print npisnan(M)any() # False    N[0 0]  npnan    print npisnan(N)any() # True    # False in the first and the second case    # True in the third    print npisnan(M)any()    assert npisnan(M)any()  False,,1213,0.7906018136850783,0.44798657718120805,32314,400.13616389181163,35.866806956737015,106.67203069876834,2331,28,1071,110,travis,NicolasTr,agramfort,false,agramfort,4,0.75,3,0,1098,false,true,false,false,0,12,1,12,1,0,9
1807933,scikit-learn/scikit-learn,python,2188,1374518121,1374666805,1374666805,2478,2478,github,false,false,false,58,57,9,24,10,0,34,0,5,1,0,3,5,3,0,0,1,0,4,5,3,0,0,591,36,1038,132,56.51281745432712,1.504832227862261,16,vanderplas@astro.washington.edu,examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_gradient_boosting_oob.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_gradient_boosting_oob.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|examples/ensemble/plot_gradient_boosting_oob.py|examples/ensemble/plot_gradient_boosting_oob.py,15,0.005154639175257732,1,5,false,[MRG] Gradient boosting OOB improvement This PR adresses issue #1802 by @yanirsIt sets oob_score_ deprecated and introduces oob_improvement_ which gives the relative improvement of adding the i-th tree on the out-of-bag examples The PR includes an example that shows how oob_improvement_ can be used to estimate the optimal number of iterations (basically an alternative to cross-validation)[gbrt_oob_example](https://fcloudgithubcom/assets/111730/836289/9aa0c888-f2ec-11e2-8a03-12b3a337a416png),,1212,0.7904290429042904,0.43470790378006874,32314,400.13616389181163,35.866806956737015,106.67203069876834,2330,28,1070,110,travis,pprett,pprett,true,pprett,34,0.8823529411764706,107,28,1448,true,true,false,false,8,44,5,0,6,0,16
1807719,scikit-learn/scikit-learn,python,2187,1374515896,1374515976,1374515976,1,1,github,false,false,false,34,1,0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,5,5,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,cosmetic: Changed all instances of nonlinear to non-linear Purely trivial changes to documentation in order to maintain consistency  I avoided changing uses of nonlinear when they were referring to paper abstracts or variable names,,1211,0.7902559867877786,0.4330434782608696,32314,400.13616389181163,35.866806956737015,106.67203069876834,2330,28,1070,103,travis,FedericoV,NelleV,false,NelleV,1,1.0,2,1,882,false,false,false,false,0,0,0,0,0,0,-1
1807495,scikit-learn/scikit-learn,python,2186,1374513823,1374600231,1374600231,1440,1440,merged_in_comments,false,false,false,28,23,22,5,4,0,9,0,4,0,0,9,9,8,0,0,0,0,9,9,8,0,0,3709,203,3709,203,212.3680032333775,5.660541034510039,52,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|doc/whats_new.rst|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_lasso_model_selection.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|doc/whats_new.rst|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_sparse_coordinate_descent.py,37,0.005235602094240838,0,1,false,Refactor linear model coordinate descent code to work as LARS for cv there is a million dollar bug cf plot_model_lasso_model_selectionpytests pass but the example output is wrong ,,1210,0.7900826446280992,0.43106457242582896,32314,398.00086649749335,35.89775329578511,106.67203069876834,2330,28,1070,109,travis,agramfort,fabianp,false,fabianp,31,0.8709677419354839,119,181,1328,true,true,true,true,4,51,24,30,24,0,69
1807399,scikit-learn/scikit-learn,python,2184,1374513169,1374513716,1374513716,9,9,merged_in_comments,false,false,false,7,2,2,0,0,0,0,0,1,0,0,4,4,4,0,0,0,0,4,4,4,0,0,42,0,42,0,17.849517811339503,0.4757683920914382,29,vlad@vene.ro,sklearn/cross_validation.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/datasets/samples_generator.py,28,0.008726003490401396,0,0,false,Doc fixes Small doc and typo fixes,,1209,0.7899090157154673,0.43106457242582896,32309,398.06245937664426,35.903308675601224,106.68853879723916,2330,28,1070,106,travis,pgervais,GaelVaroquaux,false,GaelVaroquaux,1,1.0,1,0,140,false,true,false,false,0,4,1,0,2,0,-1
1807364,scikit-learn/scikit-learn,python,2183,1374512840,1374692531,1374692529,2994,2994,github,false,false,false,60,7,4,13,4,0,17,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,173,28,183,32,36.59973792946016,0.9755444740700555,9,vlad@vene.ro,sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/tests/test_graph_lasso.py,9,0.015706806282722512,0,0,false,Graph lasso cv fixes Fixes all problems raised in issue #2159 This is a minor code modification**However** there will be some changes in the output values when a cross-validation object is provided as input: **every fold is now taken into account** which wasnt the case before Execution time will therefore increase for a number of folds greater than 3 ,,1208,0.7897350993377483,0.43106457242582896,32309,398.06245937664426,35.903308675601224,106.68853879723916,2330,28,1070,114,travis,pgervais,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,140,false,true,false,false,0,4,0,0,2,0,3
1807240,scikit-learn/scikit-learn,python,2182,1374511596,1374750922,1374750922,3988,3988,github,false,false,false,53,54,22,10,28,0,38,0,7,2,0,4,16,5,0,0,7,3,11,21,15,0,0,6134,162,11548,265,180.857174083448,4.820642225127511,21,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/pls/pls_.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/pls_.py|sklearn/tests/test_common.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/pls_.py|sklearn/tests/test_common.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/pls_.py|sklearn/tests/test_common.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/pls_.py|sklearn/tests/test_common.py|sklearn/pls.py|sklearn/pls/__init__.py|sklearn/pls/pls_.py|sklearn/pls/tests/test_pls.py|sklearn/pls/pls_.py|sklearn/tests/test_common.py,17,0.0,0,5,false,[MRG] Small refactoring of the PLS module Here is a PR that splits the PLS module into two modules: the pls and the cca The only classifier that moved is the CCA classifier: it has been deprecated in the pls moduleTODO List- [x] update the documentation- [x] update examplesThanks,,1207,0.7895608947804474,0.43106457242582896,32307,398.0871018664686,35.905531309004246,106.69514346736001,2330,28,1070,113,travis,NelleV,agramfort,false,agramfort,24,0.875,38,13,1281,true,true,true,true,0,12,14,22,4,0,98
1806974,scikit-learn/scikit-learn,python,2181,1374509687,1374515551,1374515551,97,97,merged_in_comments,false,false,false,18,6,4,0,8,0,8,0,5,1,0,1,4,2,0,0,1,0,3,4,3,0,0,54,0,82,0,16.425582583728794,0.43781975647359717,2,vanderplas@astro.washington.edu,sklearn/utils/random.pyx|sklearn/utils/random.pxd|sklearn/utils/random.pxd|sklearn/utils/random.pyx,2,0.0034965034965034965,0,1,false,[MRG] Add missing pxd random When I created the utilsrandom module I forgot to add a pxd file,,1206,0.7893864013266998,0.43006993006993005,32288,397.2063924677899,36.14345887016848,106.88181367690783,2330,28,1070,107,travis,arjoly,glouppe,false,glouppe,26,0.8076923076923077,16,21,580,true,true,true,true,5,192,22,114,64,0,11
1806709,scikit-learn/scikit-learn,python,2180,1374507564,,1374515295,128,,unknown,false,false,false,13,10,4,0,3,0,3,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2417,0,2684,18.77964297910527,0.5005663236322292,17,vanderplas@astro.washington.edu,sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py,17,0.02992957746478873,2,1,false,MAINT Set random_state modernize tests I modernize a bit test_treepyPing @pprett  @glouppe,,1205,0.7900414937759336,0.426056338028169,32287,397.21869483073687,36.144578313253014,106.88512404373276,2330,28,1070,107,travis,arjoly,arjoly,true,,25,0.84,16,21,580,true,true,false,false,5,190,19,114,61,0,29
1805778,scikit-learn/scikit-learn,python,2178,1374495096,1374594605,1374594605,1658,1658,merged_in_comments,false,false,false,35,46,25,27,14,1,42,0,4,0,0,3,4,3,0,0,0,0,4,4,3,0,0,282,176,445,263,179.79205002367559,4.79942369785481,15,vanderplas@astro.washington.edu,sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py,10,0.014414414414414415,2,14,false,FIX: reissue FastICA improvements #2113 This is a cleanup of #2113 including all commits by@mblondel @larsmans and myselfTests pass on my box but it would be good if someone could confirm this before merging,,1204,0.7898671096345515,0.4126126126126126,32240,398.5732009925558,36.259305210918114,107.47518610421835,2330,28,1070,113,travis,dengemann,,false,,2,1.0,29,29,387,false,true,false,false,0,13,0,3,1,0,3
1805296,scikit-learn/scikit-learn,python,2177,1374486318,1374489110,1374489110,46,46,commit_sha_in_comments,false,false,false,8,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.718014067665778,0.12465293345129869,3,vlad@vene.ro,doc/datasets/twenty_newsgroups.rst,3,0.0054446460980036296,0,0,false,Update twenty_newsgroupsrst Vectorizer() changed to TfidfVectorizer()#2173https://githubcom/scikit-learn/scikit-learn/issues/2173,,1203,0.7896924355777224,0.40834845735027225,32342,396.04848185022576,36.14495083791974,107.25990971492178,2330,28,1070,109,travis,har777,larsmans,false,larsmans,0,0,5,23,313,false,true,false,false,1,0,0,0,1,0,-1
1802522,scikit-learn/scikit-learn,python,2175,1374436872,1374753739,1374753739,5281,5281,github,false,false,false,97,66,8,28,16,0,44,0,6,0,0,4,10,4,0,0,0,0,10,10,7,0,0,534,368,1549,710,72.34346055148865,1.9113602554862466,32,vlad@vene.ro,sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,29,0.027124773960216998,0,4,false,[MRG] partial_fit for the discrete naive Bayes models Early PR to review an implementation of partial_fit for the discrete naive Bayes modelsTODO:- [x] check test coverage and add more tests if necessary- [x] check on a non-toy datasets such as the out-of-core Reuters example- [x] identify ways to factorize common code- [x] update documentation to mention the new method where applicable- [x] fix broken test: a rebase on master highlighted a regression introduced in this PRNote generic topical documentation for out-of-core learning in general will be dealt with in #2204,,1201,0.7901748542880933,0.40687160940325495,32342,396.04848185022576,36.14495083791974,107.25990971492178,2330,28,1069,117,travis,ogrisel,ogrisel,true,ogrisel,37,0.8108108108108109,778,121,1516,true,true,false,false,8,189,10,135,99,0,813
1801842,scikit-learn/scikit-learn,python,2170,1374228111,1374771525,1374771525,9056,9056,merged_in_comments,false,false,false,35,18,6,3,12,0,15,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,81,377,101,513,54.392904052105244,1.4371039881079688,73,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,58,0.10526315789473684,0,3,true,[MRG] Ensure that classification metrics support string label Since most estimators accept string as input I make sure that most metrics will doThis pr doesnt include metrics with y_scoreThis should fix #2168 #1989,,1199,0.7906588824020017,0.4010889292196007,32342,395.7392863768475,36.114031290581906,107.22899016758394,2330,28,1067,117,travis,arjoly,amueller,false,amueller,24,0.8333333333333334,16,21,577,true,true,true,false,5,188,18,114,59,0,320
1804250,scikit-learn/scikit-learn,python,2167,1374169636,1407880062,1407880062,561780,561780,commit_sha_in_comments,false,false,false,99,12,3,5,19,0,24,0,5,0,0,2,4,0,0,0,0,0,4,4,2,0,0,0,0,16,120,18.40039695834371,0.4861548702908923,32,vlad@vene.ro,doc/modules/feature_selection.rst|doc/modules/feature_selection.rst|doc/tutorial/index.rst|doc/modules/feature_selection.rst,22,0.040145985401459854,0,9,false,Feature Selection ease-of-use Improvements As mentioned on the mailing list this is a PR to improve the ease of use of feature selection in sklearn :1) Documentation- put a bit more detailed explanations in the narrative docs2) GradientBoostingClassifier - should implement transform() to perform feature selection based on computed features importanceFor 1) I would basically touch a few areas of the docs pointed in the MLFor 2) Im considering a simple strategy : make GradientBoostingClassifier derive from _LearntSelectorMixin and see if the tests pass If anyone has more clever ideas let me know :) ,,1198,0.7904841402337228,0.39963503649635035,32331,395.87392904642604,36.12631839411092,107.2654727660759,2330,28,1066,216,travis,oddskool,larsmans,false,larsmans,2,0.5,2,2,787,true,true,true,false,0,14,1,16,1,0,5
1785097,scikit-learn/scikit-learn,python,2165,1374078667,,1438540177,1074298,,unknown,false,false,false,94,5,3,43,18,0,61,0,10,0,0,5,5,4,0,0,0,0,5,5,4,0,0,465,46,465,46,32.02260967354787,0.8460682065773795,108,vlad@vene.ro,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|examples/linear_model/plot_polynomial_interpolation.py|doc/modules/linear_model.rst|sklearn/linear_model/base.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,63,0.04267161410018553,0,10,false,WIP: Kernel ridge This PR adds kernel support to the 4 classes in the ridge module: Ridge RidgeClassifier RidgeCV and RidgeClassifierCV The basic functionality is there but the the class hierarchy became a bit messy and probably needs some clean upI am not sure yet how to handle fit_interceptTrue in the kernel case so I introduced fit_incerceptauto which sets the value to False when a non-linear kernel is used Note that the auto mode will have to be introduced to fix #1389 anywayI rewrote the polynomial interpolation example using the new functionality,,1197,0.7911445279866333,0.3821892393320965,32328,395.91066567681264,36.129670873546154,107.27542687453601,2330,27,1065,375,travis,mblondel,mblondel,true,,24,0.8333333333333334,262,30,1205,true,true,false,false,6,108,3,53,17,0,261
1785043,scikit-learn/scikit-learn,python,2164,1374077939,1374266154,1374266154,3136,3136,merged_in_comments,false,false,false,25,2,1,2,3,0,5,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,16,7,18,9.029809237168427,0.23857626173852398,9,vlad@vene.ro,sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,9,0.016697588126159554,0,1,false,BUG: use correct algorithm for callable metric Fixes bug reported in #2151  The neighbors base object was choosing  the wrong algorithm given a callable metric,,1196,0.7909698996655519,0.3821892393320965,32328,395.91066567681264,36.129670873546154,107.27542687453601,2330,27,1065,107,travis,jakevdp,ogrisel,false,ogrisel,31,0.9032258064516129,1056,0,798,true,true,false,false,2,47,4,3,6,0,36
1784663,scikit-learn/scikit-learn,python,2163,1374073067,1374150607,1374150607,1292,1292,github,false,false,false,42,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,16,0,4.743003932760716,0.1253147345606613,3,nelle.varoquaux@gmail.com,examples/ensemble/plot_forest_iris.py,3,0.0055762081784386614,0,0,false,Updated docs to fix formatting errors Docstring changes for the formatting bug listed here:https://githubcom/scikit-learn/scikit-learn/issues/2162No significant change to the code I cant see any reason why the recent modifications (in the previous PR) would disrupt Sphinx from building a new image,,1195,0.7907949790794979,0.38104089219330856,32328,395.91066567681264,36.129670873546154,107.27542687453601,2330,27,1065,105,travis,ianozsvald,NelleV,false,NelleV,1,1.0,173,6,1163,false,false,false,false,0,2,1,0,0,0,-1
1784174,scikit-learn/scikit-learn,python,2161,1374065637,1374854571,1374854571,13148,13148,commit_sha_in_comments,false,false,false,46,94,52,26,13,0,39,0,5,1,1,13,18,11,0,0,4,4,13,21,14,0,0,1572,1000,2498,1016,371.98915651365894,9.828311987255784,82,vlad@vene.ro,sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|doc/modules/neighbors.rst|doc/modules/tree.rst|examples/ensemble/plot_forest_multioutput.py|examples/plot_multioutput_face_completion.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|doc/modules/multiclass.rst|doc/modules/multiclass.rst|examples/plot_multioutput_face_completion.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|doc/modules/neighbors.rst|doc/modules/tree.rst|examples/ensemble/plot_forest_multioutput.py|examples/plot_multioutput_face_completion.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|doc/modules/multiclass.rst|doc/modules/multiclass.rst|examples/plot_multioutput_face_completion.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|doc/modules/neighbors.rst|doc/modules/tree.rst|examples/ensemble/plot_forest_multioutput.py|examples/plot_multioutput_face_completion.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|doc/modules/multiclass.rst|doc/modules/multiclass.rst|examples/plot_multioutput_face_completion.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|doc/modules/neighbors.rst|doc/modules/tree.rst|examples/ensemble/plot_forest_multioutput.py|examples/plot_multioutput_face_completion.py|sklearn/datasets/samples_generator.py|sklearn/datasets/tests/test_samples_generator.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_cross_validation.py|doc/modules/multiclass.rst|doc/modules/multiclass.rst|examples/plot_multioutput_face_completion.py,31,0.01858736059479554,0,4,false,[MRG] Add multioutput support to nearest neighbors This pr add multi-output support for knn estimatorsSince more and more estimators support multi-output data I have added test to check that GridsearchCV RandomSearchCV and cross_val_score works with multi-output data Finally I re-work the face completion example[completion](https://fcloudgithubcom/assets/1274722/862503/5c5788dc-f5f3-11e2-8074-3490096d8fd9png),,1194,0.7906197654941374,0.38104089219330856,32328,395.91066567681264,36.129670873546154,107.27542687453601,2330,27,1065,122,travis,arjoly,arjoly,true,arjoly,23,0.8260869565217391,16,21,575,true,true,false,false,5,182,17,112,53,0,17
1776998,scikit-learn/scikit-learn,python,2158,1373963790,,1374017420,893,,unknown,false,false,false,105,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.4440336567011665,0.117275087704695,36,vlad@vene.ro,sklearn/metrics/metrics.py,36,0.07058823529411765,0,0,false,Use float64 inside metricsr2_score() to preserve FP accuracy Without this if the input arrays are of type npfloat32 their sums may be computed with an large accumulated error resulting in the wrong score  This can often be seen with very long arrays (millions of elements)  The 1 - numerator / denominator calculation at the very end produces a float64 when the inputs are float32 anyway so the returned type does not change--only the accuracyThe existing metrics tests pass after this change someone may with to further review other uses of sum() in this file and elsewhere to see if similar improvements can be made,,1193,0.7912824811399832,0.34509803921568627,32328,395.91066567681264,36.129670873546154,107.27542687453601,2330,27,1064,102,travis,jzwinck,larsmans,false,,0,0,7,0,601,false,false,false,false,0,0,0,0,1,0,77
1768929,scikit-learn/scikit-learn,python,2155,1373777315,1373812939,1373812939,593,593,merged_in_comments,false,false,false,28,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,2,13,2,9.160620870903568,0.24703059749958925,24,vlad@vene.ro,sklearn/grid_search.py|sklearn/manifold/tests/test_spectral_embedding.py,22,0.043824701195219126,0,0,false,Fix two remaining python3 bugs 2 More bugs that surfacedOne with dictionary orders being different between python version the other is another case of DeprecationWarnings being dropped,,1192,0.7911073825503355,0.33067729083665337,32101,383.1344817918445,35.79327746799165,106.91255724120744,2330,28,1062,104,travis,justinvf,GaelVaroquaux,false,GaelVaroquaux,3,1.0,5,1,727,true,false,false,false,1,8,3,9,2,0,593
1760317,scikit-learn/scikit-learn,python,2151,1373575924,1375183074,1375183074,26785,26785,merged_in_comments,false,true,false,79,46,16,18,43,0,61,0,6,0,0,6,6,5,0,0,0,0,6,6,5,0,0,279,111,576,186,103.61463305553983,2.7941374432180393,98,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|examples/tree/plot_tree_regression.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|examples/tree/plot_tree_regression.py|sklearn/cluster/dbscan_.py|examples/tree/plot_tree_regression.py|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|examples/tree/plot_tree_regression.py|sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/dbscan_.py|doc/modules/clustering.rst|sklearn/cluster/dbscan_.py|sklearn/cluster/tests/test_dbscan.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,69,0.02536997885835095,0,6,false,[WIP] Dbscan balltrees DBSCAN modified to use balltrees for neighborhood calculations for the case of a Minkowski metric The restriction to Minkowski metrics is a direct result of a limitation of the balltree implementation  Additional optional parameter added to DBSCAN for the power of Minkowski metric Precomputed metrics or other metrics still be useableTODO:- [x] wait for #1732 to be completed and merged to update this branch on top of the latest change in the neighbors module,,1191,0.7909319899244333,0.3382663847780127,32126,382.4628027143124,35.73429620867833,106.798231961651,2328,28,1059,138,travis,cleverless,GaelVaroquaux,false,GaelVaroquaux,0,0,0,2,224,false,false,false,false,2,5,0,0,0,0,104
1752744,scikit-learn/scikit-learn,python,2150,1373494414,1373727753,1373727754,3889,3888,github,false,false,false,190,16,9,6,5,0,11,0,3,0,0,13,13,8,0,0,0,0,13,13,8,0,0,47,6,68,12,102.6132166298126,2.767076591075763,93,vlad@vene.ro,doc/developers/utilities.rst|doc/modules/feature_extraction.rst|sklearn/base.py|sklearn/grid_search.py|doc/conf.py|doc/developers/utilities.rst|doc/modules/feature_extraction.rst|doc/modules/preprocessing.rst|doc/modules/tree.rst|doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_ridge.py|sklearn/metrics/metrics.py|sklearn/preprocessing.py|doc/modules/feature_extraction.rst|doc/modules/preprocessing.rst|doc/modules/tree.rst|sklearn/base.py|sklearn/linear_model/stochastic_gradient.py|sklearn/base.py|sklearn/tests/test_cross_validation.py|sklearn/grid_search.py,31,0.015521064301552107,0,3,false,Last of the Python3 Bugs() I seem to have all tests passing locally for python3 nowThis required changing a lot of doc tests to no longer assert on the output of their values ie changing{python} foo()[ua ub]To{python} foo()  [a b]TrueThis is because unicode strings do not print with the preceding u in python3Two changes in this PR I would like special attention of:sklearn/basepy:206This logic required that DeprecationWarnings would show up after calling getattr This for some reason was getting broken in python3 Adding this here seems a little bit like a hack We already make a call like that in utils/__init__py but it was not sticking aroundsklearn/tests/test_cross_validationpy:71We were checking the dimensions relative to the global variable y instead of the local variable Y I switched it to Y that makes more sense relative to the code in cross_validationpy:1035 which resizes the fit parameters relative to the cross validation set sizeLook forward to getting validation of this on the jenkins build so I know I didnt break any python2 stuff in the process,,1190,0.7907563025210084,0.3303769401330377,32112,382.4115595416044,35.74987543597409,106.84479322371699,2328,29,1058,108,travis,justinvf,ogrisel,false,ogrisel,2,1.0,5,1,723,true,false,false,false,1,5,2,8,1,0,20
1752655,scikit-learn/scikit-learn,python,2148,1373481212,1374767930,1374767930,21445,21445,merged_in_comments,false,true,false,226,140,23,196,61,1,258,0,11,1,0,6,14,5,0,0,3,2,11,16,9,0,1,1189,1040,5026,3898,166.71683228835002,4.495700058089484,65,vlad@vene.ro,sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/classes.rst|doc/modules/preprocessing.rst|examples/imputation.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/tests/test_preprocessing.py|sklearn/utils/validation.py,35,0.03881278538812785,0,24,false,[MRG] Missing values imputation Heres some code for the data imputation Its still a work in progress so *Travis* will probably be mad at meIll work on the documentation tomorrow Meanwhile you can look at the tests if you want to see how it works***Whats new***I added four classes:* MeanImputer: replace the missing values with the mean of the row/column* MedianImputer: replace the missing values with the median of the row/column* MostFrequentImputer: replace the missing values with the most frequent value in the row/column* RandomImputer: replace the missing values with one of the values of the row/column***Behaviour**** Any value can represent a missing value (not only npnan for dense matrices and 0 for sparse matrices *any* value)* Different missing values can be imputed separately in a pipeline* Support dense and sparse matrices* **To determine:** What should the imputer do if he has nothing to work with For example: What should be done if theres no median value because the whole row/column is missing   * 0 for the mean and npnan for the median/most frequent/random    * always npnan   * Let the user choose with a parameter***Whats missing***- [x] **Documentation**- [x] Delete the rows/columns where npisnan(imputation_data_)- [x] Add tests with copy- [x] Add tests with grid search- [x] Test pickling,,1188,0.7904040404040404,0.3105022831050228,32112,382.4115595416044,35.74987543597409,106.84479322371699,2328,29,1058,122,travis,NicolasTr,amueller,false,amueller,3,0.6666666666666666,3,0,1085,false,true,false,false,0,1,1,0,1,0,206
1753585,scikit-learn/scikit-learn,python,2147,1373470393,1374673633,1374673633,20054,20054,github,false,false,false,9,25,5,7,8,1,16,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,175,206,232,271,44.02068654302596,1.1870738250697763,68,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,55,0.13095238095238096,0,1,false,[MRG] Better error message for _check_clf_target Should fix #2137 ,,1187,0.7902274641954508,0.3119047619047619,32112,382.4115595416044,35.74987543597409,106.84479322371699,2328,29,1058,123,travis,arjoly,arjoly,true,arjoly,22,0.8181818181818182,16,20,568,true,true,false,false,5,174,15,99,34,0,1397
1753416,scikit-learn/scikit-learn,python,2146,1373467181,1373561473,1373561473,1571,1571,github,false,false,false,579,2,1,0,4,1,5,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,92,0,111,0,4.563518057951083,0.12306107110647102,0,,examples/ensemble/plot_forest_iris.py,0,0.0,0,3,false,clearer decision surface plots and classifier final predictions for the  Ive made some changes to plot_forest_irispy to make the demo more useful when teaching this affects the image shown here:http://scikit-learnorg/dev/auto_examples/ensemble/plot_forest_irishtmlThis is connected with this issue https://githubcom/scikit-learn/scikit-learn/issues/2133Ive attached a preview image of the output to this report Roughly in order of significance I discuss the changes I have *not* changed the AdaBoost plot with respect to estimator weights - see below1) Changed estimator_alpha from a hardcoded 01 to 1/nbr_estimators At 01 it was common for the red channel to dominate the plot leading to the erroneous visual output in the original diagram (seen at the link above) where eg the bottom-right plot in the region -11 to 325 looks like it has a strong red decision surface but has yellow samples plotted on top The model is correctly predicting yellow in this region but the 01 alpha and the red channel obscure this After this change the same region is still coloured Red but with a much lighter shade (but this still looks wrong) A benefit is that if the user experiments with the size of the ensembles the plot will still be drawn sensibly2) Changed the colour map from Paired (yielding BlueRedOrange) to RedYellowBlue The distinction between red/orange is weaker than red/yellow the new colour map shows the erroneous region mentioned in 1) with its correct colour (now blue) The side effect is that the original Blue/Red/Orange colours for the 3 classes change to Red/Yellow/Blue I dont know if the colour map change is acceptable Maybe theres a standard for the Iris set that Im not aware of This choice does not conflict with the other examples in the /examples/ensemble/ directory3) Added a regular coarse grid overlay of predictions in 2 dimensions using a scatter plot these show as small dots in a grid over each plot The purpose is to show how the predictor behaves at each plot location which might *not* be the same as the average colour that comes through via the decision surfaces and the alpha values An example is plot 8 (right column second row) where the yellow region is not a uniform rectangle but is narrower near the red points (on the left) and wider in the blue points (on the right) although to my eye the alpha-blended decision surfaces do not look very different In the same plot there is a yellow spur from 005 up to 025 which runs through the blue region this region would probably be missed by a student4) I did *not* vary the AdaBoost columns alpha value with respect to the underlying weights Visually I could see very little difference to the output when using the weights to vary the alpha value I found the regular coarse grid overlay to be far more informative (see 3)5) Changed AdaBoost to use a tree max_depth of 3 rather than the usual None (which in this dataset is equivalent to a max_depth of 5) to show that the method combats increased bias by combining votes from weak classifiers (as per OGs note in the forum on 7th July)6) Added column titles rather than using a long suptitle7) Added more output to the console to show the number of estimators (noting that AdaBoost can use fewer estimators if it fits well early on - a student might experiment and not realise this) and the score on the dataset[plot_forest_iris_new](https://fcloudgithubcom/assets/273210/774555/405f873a-e95d-11e2-9f5b-9271572c8a5cpng)ensembles,,1186,0.7900505902192243,0.31026252983293556,32112,382.4115595416044,35.74987543597409,106.84479322371699,2328,29,1058,107,travis,ianozsvald,NelleV,false,NelleV,0,0,170,6,1156,false,false,false,false,0,0,0,0,0,0,9
1752992,scikit-learn/scikit-learn,python,2145,1373459265,1373462841,1373462841,59,59,github,false,false,false,8,3,3,0,0,0,0,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,38,0,38,0,12.746249477226133,0.3437186200066279,6,jaquesgrobler@gmail.com,sklearn/multiclass.py|sklearn/kernel_approximation.py|sklearn/lda.py,4,0.007159904534606206,0,0,false,Minor numpydoc reformatting Small typos and numpydoc formatting,,1185,0.789873417721519,0.31026252983293556,32112,382.4115595416044,35.74987543597409,106.84479322371699,2328,29,1058,104,travis,NelleV,agramfort,false,agramfort,23,0.8695652173913043,36,13,1269,true,true,true,true,0,11,9,5,3,0,-1
1751679,scikit-learn/scikit-learn,python,2144,1373430031,,1373648708,3644,,unknown,false,false,false,81,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,3.9916660890963116,0.10764022792922724,18,vlad@vene.ro,sklearn/grid_search.py,18,0.043478260869565216,1,0,false,ENH print number of fits in BaseSearchCV_fit As mentioned on the ML (@robertlayton) this makes the grid search a little bit friendlier by warning you how big your grids areIt might be nice to have a method that returns the fit count in advance but it doesnt fit with the current class model and I dont have the time to change that nowIt is also only smoke-tested (and the case where parameter_iterator does not have __len__ is not tested),,1184,0.7905405405405406,0.30193236714975846,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,29,1058,108,travis,jnothman,larsmans,false,,43,0.5813953488372093,16,1,1534,true,true,false,false,11,390,66,222,52,0,-1
1751241,scikit-learn/scikit-learn,python,2143,1373420463,1373777422,1373777422,5949,5949,merged_in_comments,false,false,false,6,6,6,1,0,0,1,0,2,0,0,6,6,3,0,0,0,0,6,6,3,0,0,11,0,11,0,26.785183125157186,0.7222956911635714,43,vlad@vene.ro,examples/applications/plot_stock_market.py|doc/developers/performance.rst|doc/modules/cross_validation.rst|doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/pls.py|examples/applications/plot_stock_market.py|examples/applications/plot_stock_market.py,29,0.01699029126213592,0,0,false,Yahoo bugfix bugfix for issue 2116/2117,,1183,0.790363482671175,0.29854368932038833,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,29,1057,107,travis,ccoovrey,GaelVaroquaux,false,GaelVaroquaux,1,1.0,0,0,11,false,false,false,false,1,2,2,0,1,0,3777
1742760,scikit-learn/scikit-learn,python,2142,1373398508,1373455840,1373455841,955,955,github,false,false,false,6,9,3,9,4,0,13,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,24,0,36,15,13.52042863258074,0.3645984876018382,1,gael.varoquaux@normalesup.org,sklearn/qda.py|sklearn/qda.py|sklearn/qda.py,1,0.002457002457002457,0,0,false, Adding covariance regularization to QDA ,,1182,0.7901861252115059,0.29484029484029484,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,28,1057,107,travis,sergeyf,agramfort,false,agramfort,1,1.0,8,0,384,false,true,false,false,0,2,1,0,0,0,192
1760536,scikit-learn/scikit-learn,python,2141,1373391588,1373391884,1373391884,4,4,commits_in_master,false,false,false,19,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,14,0,4.542962069182117,0.12250773586163716,1,gael.varoquaux@normalesup.org,sklearn/qda.py,1,0.002457002457002457,0,0,false,Adding covariance regularization to QDA Class-specific covariance estimation can numerical issues  This adds a simple shrinkage towards the identity,,1181,0.790008467400508,0.29484029484029484,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,28,1057,104,travis,sergeyf,,false,,0,0,8,0,384,false,true,false,false,0,1,0,0,0,0,-1
1750237,scikit-learn/scikit-learn,python,2140,1373385857,1373404979,1373404979,318,318,github,false,false,false,18,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.153426940905006,0.13896983000174956,5,larsmans@gmail.com,doc/about.rst,5,0.012254901960784314,1,0,false,@arjoly and @glouppe thank their funding for the paris sprint We acknowledge our funding for the sprint (@NelleV),,1180,0.7898305084745763,0.29411764705882354,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,28,1057,104,travis,arjoly,NelleV,false,NelleV,21,0.8095238095238095,16,20,567,true,true,true,false,5,172,14,95,32,0,318
1726392,scikit-learn/scikit-learn,python,2139,1373339925,,1373470504,2176,,unknown,false,true,false,9,1,1,0,6,0,6,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.17812299686849,0.11266867522576561,33,vlad@vene.ro,sklearn/metrics/metrics.py,33,0.0812807881773399,0,0,false,FIX error message for mixed metric input (for #2137) ,,1179,0.7905004240882103,0.29310344827586204,32110,382.15509187169107,35.72095920274058,106.7891622547493,2327,28,1056,106,travis,jnothman,arjoly,false,,42,0.5952380952380952,16,1,1532,true,true,false,true,11,388,65,220,51,0,311
1720354,scikit-learn/scikit-learn,python,2138,1373313341,1373327882,1373327882,242,242,github,false,false,false,18,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,31,0,31,0,4.562298170702256,0.1230284599428768,5,vlad@vene.ro,sklearn/pls.py,5,0.012345679012345678,0,0,false,DOC fixed small mistakes in the pls module There were small mistakes and typo in the PLS module,,1178,0.7903225806451613,0.291358024691358,32110,382.15509187169107,35.72095920274058,106.7891622547493,2326,28,1056,102,travis,NelleV,agramfort,false,agramfort,22,0.8636363636363636,36,13,1267,true,true,true,true,0,10,6,5,2,0,-1
1714566,scikit-learn/scikit-learn,python,2135,1373264708,1373284819,1373284819,335,335,github,false,false,false,16,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.398630755078986,0.11874695943991823,6,vlad@vene.ro,sklearn/cluster/mean_shift_.py,6,0.015345268542199489,0,0,false,DOC fix docstring typos in cluster/mean_shift_ Leaving the rest of docstring fixes for pull request #2084,,1177,0.7901444350042481,0.2864450127877238,32089,381.0963258437471,35.744336065318336,106.2981083860513,2326,28,1056,103,travis,fhs,agramfort,false,agramfort,0,0,5,9,1269,false,true,false,false,0,0,0,0,0,0,-1
1714036,scikit-learn/scikit-learn,python,2134,1373251772,,1374489327,20625,,unknown,false,false,false,112,9,9,6,2,0,8,0,4,3,11,9,23,30,0,2,3,11,9,23,30,0,2,1630,65,1630,65,65.14574696544354,1.758697149938376,20,vlad@vene.ro,sklearn/cluster/hierarchical.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/__init__.py|sklearn/utils/graph.py|sklearn/utils/setup.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_graph_tools.pyx|sklearn/utils/sparsetools/_graph_validation.py|sklearn/utils/sparsetools/_min_spanning_tree.c|sklearn/utils/sparsetools/_min_spanning_tree.pyx|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/sparsetools/_traversal.pyx|sklearn/utils/sparsetools/setup.py|sklearn/utils/sparsetools/tests/__init__.py|sklearn/utils/sparsetools/tests/test_spanning_tree.py|sklearn/utils/sparsetools/tests/test_traversal.py|sklearn/feature_extraction/tests/test_image.py|sklearn/utils/graph.py|sklearn/utils/mst/_traversal.c|sklearn/utils/mst/_traversal.pyx|sklearn/utils/mst/setup.py|sklearn/utils/mst/tests/test_traversal.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/mst/__init__.py|sklearn/cluster/hierarchical.py|sklearn/utils/_csgraph.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/complex_ops.h|sklearn/utils/sparsetools/csgraph.h|sklearn/utils/sparsetools/csgraph.i|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/sparsetools/csgraph_wrap.cxx|sklearn/utils/sparsetools/npy_3kcompat.h|sklearn/utils/sparsetools/py3k.h|sklearn/utils/sparsetools/setup.py|sklearn/utils/setup.py|doc/developers/utilities.rst,12,0.0,0,0,false,[MRG]: Connected components update to sparsetools backport The cs_graph_components that was in util was out of date and incorrectIve updated the backport and cleaned up the previous csgraph backport at the same timeIn addition Ive renamed the mst folder to sparsetools which is where one would expect these functions to beWhile this PR now affects more than just the proposed EAC algorithm (See PR#1830) this update is required for that PR to happen(This is me fixing the mess I made in issue #2037)TODO (ALL DONE)- [x] Have to run tests with an older version of scipy - [x] Testing for connected_componentspy- [x] pep8 for test_spanning_treepy,,1176,0.7908163265306123,0.2864450127877238,32089,381.0963258437471,35.744336065318336,106.2981083860513,2326,28,1055,112,travis,robertlayton,GaelVaroquaux,false,,22,0.8181818181818182,9,10,779,true,true,true,false,0,36,8,13,16,0,15160
1706536,scikit-learn/scikit-learn,python,2132,1373159701,1373648223,1373648223,8142,8142,commit_sha_in_comments,false,false,false,19,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,20,0,20,0,4.3885105673178755,0.11847375134573909,14,stefano.lattarini@gmail.com,sklearn/feature_extraction/text.py,14,0.037037037037037035,0,0,false,COSMIT refactor document frequency implementations this emplys the csc_matrixindptrdiff() trick in TfIdfTransformer and gives it a name for clarity,,1175,0.7906382978723404,0.2962962962962963,32089,381.0963258437471,35.744336065318336,106.2981083860513,2326,28,1054,109,travis,jnothman,larsmans,false,larsmans,41,0.5853658536585366,16,1,1530,true,true,false,false,10,388,65,222,53,0,-1
1705369,scikit-learn/scikit-learn,python,2131,1373125664,1374497041,1374497041,22856,22856,github,false,false,false,473,622,216,35,107,0,142,0,14,0,0,10,24,10,0,0,1,0,24,25,22,0,0,29670,1860,57281,2196,1600.1585100785192,43.19843339301206,17,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,9,0.013550135501355014,4,33,false,[MRG] Complete rewrite of the tree module  Here are some good news for Kaggle competitors :) In this PR I propose a complete rewrite of the core tree module (_treepyx) and of all tree-dependent estimators In particular this new implementation factorizes out the splitting strategy at the core of the construction process of a tree Such a strategy is now implemented in a Splitter object as specified in the new interface in _treepxd As of now this PR provides two splitting strategies BestSplitter for finding the best split in a node (this is CART) and RandomSplitter for finding the best random split (this is Extra-Tree) The PR adresses 3 issues with the module: modularity space and speed1) Modularity: It is now more convenient to write a new Splitting strategy For example one could now easily write a splitting strategy based on binning and plug it in the module I think it is also possible to reimplement our old X_argsorted strategy in a shared Splitter object We will have to carry out some benchmarks though to see if it is worth it 2) Space: no more X_argsorted no more sample_mask no more min_density stuff No more dupplicates of X A tree is now directly built on the original version of X3) Speed: Both Random Forest and Extra-Trees have been speeded up The most significant improvement benefits to Extra-Trees which are now properly implemented ie without any sorting As a small benchmark I did a little experiment on mnist3vs8 (around ~12000 samples 784 features 2 classes):Parameters:n_estimators10max_features5bootstrapFalserandom_state0all other defaultsmaster    RandomForestClassifier        In [4]: %timeit -n 5 clffit(X_train y_train)        5 loops best of 3: 222 s per loop    ExtraTreesClassifier        In [7]: %timeit -n 5 clffit(X_train y_train)        5 loops best of 3: 258 s per looptrees-v2    RandomForestClassifier        In [5]: %timeit -n 5 clffit(X_train y_train)        5 loops best of 3: 133 s per loop         Speedup  167x    ExtraTreesClassifier        In [9]: %timeit -n 5 clffit(X_train y_train)        5 loops best of 3: 757 ms per loop         Speedup  3408xI think the figures speak for themselves :-)---This is still a work in progress and lot of work still needs to be done However all tests in test_treepy and test_forestpy already pass On my todo list:* [x] Fix the GBRT module for the new interface* [x] Fix the AdaBoost module for the new interface* [x] Depecrate removed parameters* [x] Update documentation* [x] More benchmarks* [x] PEP8 / Flake* [ ] Check the impact of this refactoring on the RandomState unpickling perf issue from #1622 Once all of this is done Ill call for reviews My goal is to have this beast merged in during the sprint I think it is shaping up very well :)CC: @pprett @bdholt1 @ndawe @arjoly,,1174,0.7904599659284497,0.3008130081300813,32089,381.0963258437471,35.744336065318336,106.2981083860513,2326,28,1054,112,travis,glouppe,glouppe,true,glouppe,32,1.0,106,26,968,true,true,false,false,7,29,8,17,3,0,136
1687430,scikit-learn/scikit-learn,python,2128,1372859321,1372943634,1372943634,1405,1405,commit_sha_in_comments,false,false,false,46,1,1,0,3,0,3,0,2,0,0,5,5,3,0,0,0,0,5,5,3,0,0,38,4,38,4,23.028625416776453,0.6216987046936944,36,vlad@vene.ro,doc/modules/feature_extraction.rst|doc/whats_new.rst|sklearn/datasets/base.py|sklearn/datasets/tests/test_base.py|sklearn/datasets/twenty_newsgroups.py,25,0.016666666666666666,0,0,false,[MRG] charset - encoding in load_files Fixes #2107 Very quick PR to see if I interpreted the :+1:s rightRemoved the deprecation of charse_error this isnt going to be pickled anyway Renamed charset_error to decode_error which I think is more suggestive of its meaning than encoding_error,,1173,0.7902813299232737,0.31666666666666665,32043,375.214555441126,35.577193146709114,105.38963268108479,2323,28,1051,104,travis,larsmans,larsmans,true,larsmans,72,0.7222222222222222,110,34,1081,true,true,false,false,25,174,40,51,52,0,47
1667681,scikit-learn/scikit-learn,python,2123,1372702090,,1374677513,32923,,unknown,false,false,false,152,17,6,45,38,0,83,0,8,0,0,10,10,8,0,0,0,0,10,10,8,0,0,1237,124,1844,144,225.35864962103,6.083923856305686,77,vlad@vene.ro,doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py|doc/modules/model_evaluation.rst|examples/grid_search_text_feature_extraction.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py,34,0.03151862464183381,2,15,false,WIP new simpler scorer API Heres a simpler scorer API Sorry for being so late to the game I was too busy to bother with the discussions I hope no-one takes offense (esp @amueller who worked hard on this) Ping @jnothman Anyway the highlights:* Scorers are just callables that take estimator X y **kwargs* No more greater_is_better A scorer made from a loss function returns minus the loss* No more Scorer class Theres a factory function make_scorer but it produces objects of an internal class hierarchy that do their best to hide as callables* Theres a currently unused scorer class for probabilistic classification in anticipation of #2013* Scorers can return tuples to report additional information beyond a simple score The first element of such a tuple should be the score (see #1850)TODO:* Docs needs copy-editing see commit message* Should make_scorer be public at all,,1172,0.7909556313993175,0.3037249283667622,32042,374.88296610698455,35.57830347668685,105.39292179015044,2322,27,1049,121,travis,larsmans,larsmans,true,,71,0.7323943661971831,110,34,1079,true,true,false,false,25,177,39,46,53,0,57
1665273,scikit-learn/scikit-learn,python,2122,1372644024,1373055457,1373055457,6857,6857,merged_in_comments,false,false,false,22,17,12,17,4,0,21,0,5,0,0,12,12,12,0,0,0,0,12,12,12,0,0,90,68,94,83,76.50146809844863,2.0652856341903956,57,vlad@vene.ro,sklearn/feature_extraction/text.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|sklearn/datasets/tests/test_svmlight_format.py|sklearn/tests/test_common.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_common.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/utils/fixes.py|sklearn/utils/fixes.py|sklearn/feature_selection/tests/test_base.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/decomposition/pca.py|sklearn/tests/test_grid_search.py,22,0.011527377521613832,0,0,false,More Python 3 changes Some more python 3 changes mainly limited to the tests themselves Some None  0 type fixes too,,1171,0.7907771135781383,0.30547550432276654,32045,374.84787018255577,35.574972694648146,105.38305507879545,2322,27,1048,104,travis,justinvf,larsmans,false,larsmans,1,1.0,5,1,713,false,false,false,false,0,0,1,0,0,0,461
1665259,scikit-learn/scikit-learn,python,2120,1372635306,1392922912,1392922912,338066,338066,commits_in_master,false,true,false,145,148,3,286,215,24,525,0,21,2,0,1,26,2,0,0,16,7,17,40,11,0,3,1863,0,18987,2948,27.385131181692223,0.7393076162562604,3,larsmans@gmail.com,sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py|sklearn/neural_network/__init__.py|sklearn/neural_network/mlp.py,3,0.008746355685131196,0,96,false,[MRG] Multi-layer perceptron (MLP) h1Multi-layer perceptron (MLP)/h1[mlp](https://fcloudgithubcom/assets/3382128/727899/5112c4dc-e1cb-11e2-8eaa-fe9b87bd9724jpg)noteThis is an extention to larsmans code/noteA multilayer perceptron (MLP) is a feedforward artificial neural network model that tries to learn a function f(X)y where y is the output and X is the input An MLP consists of multiple layers usually of one hidden layer an input layer and an output layer where each layer is fully connected to the next one This is a classic algorithm that has been extensively used in Neural Networksh3Code Check out :/h31) git clone https://githubcom/scikit-learn/scikit-learn2) cd scikit-learn/3) git fetch origin refs/pull/2120/head:mlp4) git checkout mlp h3Tutorial link:/h3- http://easymachinelearningblogspotcom/p/multi-layer-perceptron-tutorialhtmlh3Sample Benchmark:/h3- MLP on the scikits Digits dataset gives - Score for tanh-based sgd: 0981 -  Score for logistic-based sgd: 0987 -  Score for tanh-based l-bfgs: 0994 -  Score for logistic-based l-bfgs: 1000h3TODO:/h3- Review,,1170,0.7905982905982906,0.30903790087463556,32045,374.84787018255577,35.574972694648146,105.38305507879545,2322,27,1048,225,travis,IssamLaradji,IssamLaradji,true,IssamLaradji,1,0.0,11,1,156,true,false,false,false,0,5,1,0,10,0,577
1670379,scikit-learn/scikit-learn,python,2118,1372553462,1372580023,1372580023,442,442,github,false,false,false,19,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.302667303767818,0.11616970471011584,9,vlad@vene.ro,sklearn/cluster/hierarchical.py,9,0.02710843373493976,0,0,false,DOC fix in the hierarchical clustering Im pretty sure that n_components is not a sparse matrix but an integer,,1169,0.7904191616766467,0.3132530120481928,32019,375.40210500015615,35.57262875167869,105.56232237109215,2322,26,1047,107,travis,NelleV,agramfort,false,agramfort,21,0.8571428571428571,36,13,1258,true,true,true,true,0,7,5,5,2,0,-1
1669964,scikit-learn/scikit-learn,python,2117,1372542558,1373419313,1373419313,14612,14612,commit_sha_in_comments,false,false,false,15,18,7,0,5,2,7,0,2,0,0,25,40,24,0,0,0,0,40,40,35,0,1,251,26,517,36,121.58525772491717,3.2827257668540297,47,vlad@vene.ro,examples/applications/plot_stock_market.py|doc/about.rst|sklearn/linear_model/randomized_l1.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/tests/test_base.py|sklearn/metrics/tests/test_metrics.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_fixes.py|benchmarks/bench_glm.py|benchmarks/bench_glmnet.py|benchmarks/bench_lasso.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_plot_lasso_path.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_nmf.py|benchmarks/bench_plot_omp_lars.py|benchmarks/bench_plot_parallel_pairwise.py|benchmarks/bench_plot_svd.py|benchmarks/bench_plot_ward.py|benchmarks/bench_random_projections.py|benchmarks/bench_sample_without_replacement.py|benchmarks/bench_sgd_regression.py|benchmarks/bench_tree.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_fastkmeans.py,22,0.0,0,2,false,example yahoo stock issue fix bug fix to issue 2116: incorrect ticker for yahoo quotes,,1168,0.7902397260273972,0.31097560975609756,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1047,115,travis,ccoovrey,ccoovrey,true,ccoovrey,0,0,0,0,1,false,false,false,false,0,0,0,0,1,0,23
1669145,scikit-learn/scikit-learn,python,2115,1372521308,,1372601123,1330,,unknown,false,false,false,21,1,1,0,1,0,1,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.115731139307035,0.13812153497503443,3,gael.varoquaux@normalesup.org,doc/about.rst,3,0.009345794392523364,0,3,false,Added sponsors for the Paris sprint Here is  a pull request that adds the sponsors to the aboutrst pageThanksN,,1167,0.790916880891174,0.30218068535825543,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1047,107,travis,NelleV,larsmans,false,,20,0.9,36,13,1258,true,true,false,false,0,7,4,5,2,0,764
1664208,scikit-learn/scikit-learn,python,2113,1372488991,,1374495468,33441,,unknown,false,false,false,4,8,3,3,25,0,28,0,6,0,0,2,3,2,0,0,0,0,3,3,3,0,0,68,23,87,69,27.179490063714983,0.7338317186915874,3,peter.prettenhofer@gmail.com,sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py,3,0.00949367088607595,0,25,false,FastICA improvements Fixes #2101,,1166,0.7915951972555746,0.2974683544303797,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1047,120,travis,mblondel,larsmans,false,,23,0.8695652173913043,259,30,1187,true,true,true,true,5,78,1,31,13,0,346
1664868,scikit-learn/scikit-learn,python,2112,1372466908,1374502954,1374502954,33934,33934,merged_in_comments,false,false,false,181,3,2,3,3,0,6,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,110,0,110,0,13.53436586478818,0.36542065140572877,2,jaquesgrobler@gmail.com,examples/document_classification_20newsgroups.py|sklearn/datasets/twenty_newsgroups.py|examples/document_classification_20newsgroups.py,2,0.00641025641025641,0,0,true,[MRG] Add filters on newsgroup text for the 20newsgroups evaluation It is easy to overfit on the 20newsgroups dataset by letting classifiers learn from metadata that commonly appears in newsgroup texts but would be useless for identifying topics outside of this set of newsgroups in 1993For example many classifiers will tell you that three of the most informative features are nntp posting and host because the NNTP-Posting-Host header appears with different frequency in different groupsThe fetch_20newsgroups function now allows you to ask for any of the following kinds of text to be removed:- Newsgroup headers (which contain lots of NNTP metadata that can identify the group)- Signature blocks (which often contain multiple terms that uniquely identify the person posting which in turn identifies the group)- Quote blocks (which contain peoples e-mail addresses and large amounts of text from another post in the same newsgroup)The 20newsgroups classification example takes the --filtered flag which will remove all of these This noticeably decreases the accuracy of all classifiers leaving room for a better method to improve the accuracy,,1165,0.7914163090128755,0.2916666666666667,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1046,118,travis,rspeer,larsmans,false,larsmans,1,1.0,34,2,1185,false,false,false,false,1,2,1,0,4,0,2420
1664074,scikit-learn/scikit-learn,python,2110,1372464164,1374503264,1374503264,33985,33985,merged_in_comments,false,false,false,9,5,1,11,11,0,22,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,19,10,62,56,9.442838991981116,0.25495161059200155,5,joel.nothman@gmail.com,sklearn/multiclass.py|sklearn/tests/test_multiclass.py,4,0.012903225806451613,0,2,true,Adding decision_function to OneVsRestClassifier Fixed #2012 ovr_single_label_decision_function in test_multiclasspy,,1164,0.7912371134020618,0.2870967741935484,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1046,117,travis,kastnerkyle,GaelVaroquaux,false,GaelVaroquaux,1,1.0,160,61,463,true,false,true,false,0,0,1,0,3,0,425
1665122,scikit-learn/scikit-learn,python,2108,1372461917,1372603034,1372603034,2351,2351,merged_in_comments,false,false,false,12,6,1,0,0,0,0,0,2,0,0,1,7,1,0,0,0,0,7,7,7,0,0,0,3,88,25,4.322666861742147,0.1167096970931851,21,peter.prettenhofer@gmail.com,sklearn/metrics/tests/test_metrics.py,21,0.06818181818181818,0,0,true,WIP: Python3 conversion There are my changes to make scikit-learn python2/3 compatible,,1163,0.7910576096302665,0.2857142857142857,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1046,107,travis,justinvf,larsmans,false,larsmans,0,0,5,1,711,false,false,false,false,0,0,0,0,0,0,-1
1666648,scikit-learn/scikit-learn,python,2106,1372449556,1372543163,1372543163,1560,1560,merged_in_comments,false,false,false,49,4,4,0,2,0,2,0,2,0,0,2,2,1,0,0,0,0,2,2,1,0,0,30,0,30,0,17.892757188585122,0.4830932315488585,8,vlad@vene.ro,doc/modules/feature_extraction.rst|doc/modules/feature_extraction.rst|sklearn/datasets/base.py|sklearn/datasets/base.py,7,0.022727272727272728,0,1,true,[MRG] Text dataset loader / vectorizer fixes This branch contains some straightforward revisions to text handling stuff that I talked about with ogrisel at the SciPy sprintMost of the changes are in documentation The only code change here is to rename the parameter charse_errors to charset_errors in load_files,,1162,0.7908777969018933,0.2857142857142857,32012,375.4841934274647,35.58040734724478,105.58540547294764,2322,26,1046,108,travis,rspeer,ogrisel,false,ogrisel,0,0,34,2,1185,false,false,false,false,1,2,0,0,2,0,160
1666180,scikit-learn/scikit-learn,python,2104,1372442666,1372444941,1372444941,37,37,github,false,false,false,17,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.6400213232708625,0.12527834731070076,2,stefano.lattarini@gmail.com,examples/neighbors/plot_classification.py,2,0.006557377049180328,0,0,true,Fixes #2039 Image in tutorial misaligned [figure_2](https://fcloudgithubcom/assets/1563421/722960/606f5f96-e00c-11e2-982a-3d5b14b43c47png)() plaxis(tight) appears to be adding whitespace around the colormesh,,1161,0.7906976744186046,0.28852459016393445,32011,375.4959232763738,35.58151885289432,105.5887038830402,2322,25,1046,102,travis,kastnerkyle,jakevdp,false,jakevdp,0,0,160,61,463,true,false,true,false,0,0,0,0,1,0,37
1657342,scikit-learn/scikit-learn,python,2103,1372345855,1372519711,1372519711,2897,2897,merged_in_comments,false,false,false,11,10,1,6,1,0,7,0,1,0,0,11,102,11,0,0,0,0,102,102,75,0,0,30,0,292,32,48.14431268627738,1.299873320731795,4,stefano.lattarini@gmail.com,examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_stock_market.py|examples/cluster/plot_adjusted_for_chance_measures.py|examples/cluster/plot_lena_segmentation.py|examples/covariance/plot_covariance_estimation.py|examples/covariance/plot_lw_vs_oas.py|examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_robust_vs_empirical_covariance.py|examples/datasets/plot_digits_last_image.py|examples/decomposition/plot_pca_vs_lda.py|examples/ensemble/plot_partial_dependence.py,3,0.0033333333333333335,0,0,false,Fix for Some spelling errors in the documentation For issue #2102,,1160,0.7905172413793103,0.2833333333333333,32011,375.4959232763738,35.58151885289432,105.5887038830402,2320,24,1045,106,travis,jaquesgrobler,vene,false,vene,56,0.9107142857142857,10,13,519,true,true,false,false,6,146,31,48,19,0,1949
1638085,scikit-learn/scikit-learn,python,2093,1372071494,,1407683618,593475,,unknown,false,false,false,175,4,3,1,5,0,6,0,3,2,0,8,11,10,0,0,2,0,9,11,10,0,0,1621,447,1621,501,108.05392641426316,2.9173989312806334,119,vanderplas@astro.washington.edu,sklearn/feature_extraction/text.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/by_score.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_by_score.py|sklearn/feature_selection/univariate_selection.py|sklearn/linear_model/randomized_l1.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/by_score.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_by_score.py|sklearn/feature_selection/univariate_selection.py|sklearn/linear_model/randomized_l1.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/by_score.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_by_score.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/linear_model/randomized_l1.py|sklearn/tests/test_common.py,51,0.04666666666666667,0,0,false,[MRG] refactor feature selection by score A number of places across the package perform feature selection by score bounding the scores (specified absolutely or relatively) and/or limiting the number of features (specified absolutely or relatively)While I think a mask_by_score utility could be useful for anyone playing with feature selection I have particularly used it to assure correctness and common functionality in the many places this selection appears (univariate selection learnt selector mixin randomized l1 feature extraction)I am not sure whether mask_by_score or its realisation as a transformer SelectByScore should be part of the public API or how they might come into examples or narrative documentation and opinions are certainly welcome It may be confusing that univariate_selection is also selection by score but there the score_func returns both scores and p-values and here we dont care what the scores are as long as they are orderableAnd I apologize for not having a negative total line count (but I think we lose a couple of lines if we remove comments tests and blank lines),,1158,0.7918825561312608,0.2966666666666667,32007,375.54285000156216,35.585965570031554,105.6018995844659,2320,24,1042,216,travis,jnothman,jnothman,true,,40,0.6,16,1,1518,true,true,false,false,11,367,64,195,52,0,2
1637575,scikit-learn/scikit-learn,python,2092,1372056775,,1372601885,9085,,unknown,false,false,false,8,2,1,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,36,0,4.260344631666516,0.11502756361998388,9,stefano.lattarini@gmail.com,sklearn/linear_model/randomized_l1.py,9,0.030303030303030304,0,0,false,added some documentation in randomized_l1py See Issue #1134,,1157,0.7925669835782195,0.2996632996632997,32008,375.5311172206948,35.58485378655337,105.59860034991252,2320,24,1042,105,travis,kanielc,larsmans,false,,2,1.0,2,0,525,false,false,false,false,1,9,2,0,1,0,5
1637329,scikit-learn/scikit-learn,python,2090,1372048601,1372049160,1372049160,9,9,github,false,false,false,51,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,67,0,67,0,4.437190153871284,0.11979967944555599,1,gael.varoquaux@normalesup.org,sklearn/isotonic.py,1,0.003367003367003367,0,0,false,renamed weight to sample_weight in sklearn/isotonicpy I ended up making the change all throughout the file to remain consistentA few other minor edits were done for pep8See Issue #2041 for reasoningI didnt address the iterable issue in the documentation Ill leave that for someone more senior to decide,,1156,0.7923875432525952,0.30303030303030304,31987,375.77765967424267,35.60821583768406,105.6679275955857,2320,24,1042,98,travis,kanielc,jnothman,false,jnothman,1,1.0,2,0,525,false,false,false,false,1,6,1,0,1,0,9
1635859,scikit-learn/scikit-learn,python,2087,1372011508,1372022688,1372022688,186,186,github,false,false,false,7,1,1,0,0,0,0,0,1,0,0,4,4,4,0,0,0,0,4,4,4,0,0,7,0,7,0,18.0180857725903,0.48647269806750454,1,joel.nothman@gmail.com,examples/plot_hmm_sampling.py|examples/plot_isotonic_regression.py|examples/plot_johnson_lindenstrauss_bound.py|examples/randomized_search.py,1,0.0,0,0,false,Add missing doc string printing for examples ,,1155,0.7922077922077922,0.30405405405405406,31984,375.8129064532266,35.611555777888945,105.67783891945973,2320,24,1041,98,travis,ahojnnes,agramfort,false,agramfort,4,0.5,8,2,1360,true,true,false,false,0,12,7,23,42,0,-1
2016515,scikit-learn/scikit-learn,python,2085,1371938445,1371945807,1371945807,122,122,github,false,false,false,6,0,0,0,1,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,FIX : missing yNone in FactorAnalysis ,,1153,0.792714657415438,0.3050847457627119,31984,375.8129064532266,35.611555777888945,105.67783891945973,2320,24,1040,97,travis,agramfort,jnothman,false,jnothman,30,0.8666666666666667,114,181,1298,true,true,true,false,5,38,13,20,8,0,13
1633488,scikit-learn/scikit-learn,python,2084,1371928226,1405792469,1405792469,564344,564344,merged_in_comments,false,false,false,22,7,1,13,10,5,28,0,7,0,0,2,28,2,0,0,1,0,28,29,29,0,0,22,0,442,118,8.667250657874963,0.23400926896844795,116,vlad@vene.ro,sklearn/linear_model/bayes.py|sklearn/metrics/metrics.py,115,0.391156462585034,0,4,false,DOC fix some docstring/parameter list mismatches  (particularly in metrics and bayes)See #2062 for a list and a script to fetch them,,1152,0.7925347222222222,0.30952380952380953,31981,375.8481598449079,35.58362777899378,105.62521497138925,2320,24,1040,226,travis,kanielc,arjoly,false,arjoly,0,0,2,0,523,false,false,false,false,1,0,0,0,1,0,9
1633674,scikit-learn/scikit-learn,python,2083,1371918165,,1371936731,309,,unknown,false,false,false,19,1,1,0,2,0,2,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,4,4,4,4,18.142714042509056,0.4898396754482618,23,peter.prettenhofer@gmail.com,doc/whats_new.rst|sklearn/cluster/__init__.py|sklearn/manifold/__init__.py|sklearn/manifold/spectral_embedding_.py|sklearn/manifold/tests/test_spectral_embedding.py,23,0.0,0,0,false,FIX avoid spectral_embedding naming conflict Without this patch nosetests sklearn/manifold/ sklearn/cluster/ has errors (while nosetests sklearn/cluster/ sklearn/manifold/ does not),,1151,0.7932232841007819,0.3127147766323024,31981,375.8481598449079,35.58362777899378,105.62521497138925,2320,24,1040,98,travis,jnothman,larsmans,false,,38,0.631578947368421,16,1,1516,true,true,false,false,10,354,58,186,49,0,292
1631742,scikit-learn/scikit-learn,python,2082,1371860321,1371935839,1371935839,1258,1258,github,false,false,false,8,5,3,0,10,0,10,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,34,17,67,17,25.53609555620666,0.6894548970020167,17,stefano.lattarini@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,16,0.05555555555555555,0,3,true,MRG allow empty grid in ParameterGrid Fixes #2048,,1150,0.7930434782608695,0.3229166666666667,31980,375.85991244527827,35.58474046278924,105.62851782363977,2320,24,1039,98,travis,larsmans,larsmans,true,larsmans,70,0.7285714285714285,108,34,1069,true,true,false,false,28,170,39,56,40,0,5
1630122,scikit-learn/scikit-learn,python,2081,1371837426,,1374660780,47055,,unknown,false,true,false,25,25,5,44,14,2,60,0,4,0,0,2,7,2,0,0,0,0,7,7,7,0,0,131,0,1382,0,22.67937326341397,0.6123255970350124,1,gael.varoquaux@normalesup.org,sklearn/covariance/__init__.py|sklearn/covariance/__init__.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py,1,0.003424657534246575,0,5,true,M-estimator for the robust covariance matrix estimation Example and unit tests are in the works In the meanwhile feel free to comment on the code,,1149,0.793733681462141,0.3184931506849315,31980,375.85991244527827,35.58474046278924,105.62851782363977,2320,24,1039,121,travis,ahojnnes,agramfort,false,,3,0.6666666666666666,8,2,1358,true,true,false,false,0,9,6,5,40,0,1080
1625192,scikit-learn/scikit-learn,python,2080,1371757276,1371757568,1371757568,4,4,github,false,false,false,8,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.516856997146612,0.1221069648245057,8,stefano.lattarini@gmail.com,sklearn/metrics/pairwise.py,8,0.02909090909090909,0,0,false,Remove invalid todo comment As discussed in #2078,,1148,0.7935540069686411,0.3490909090909091,31910,376.55907239109996,35.66280162958321,105.73487934816671,2320,24,1038,98,travis,ahojnnes,agramfort,false,agramfort,2,0.5,8,2,1357,true,true,false,false,0,8,4,5,25,0,-1
1609643,scikit-learn/scikit-learn,python,2078,1371523493,,1371757285,3896,,unknown,false,false,false,46,5,2,0,7,0,7,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,29,0,253,0,13.476304731779164,0.36431963128958705,8,stefano.lattarini@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx,8,0.009868421052631578,0,0,false,Refactor euclidean distance metric in Cython Not a major performance gain but timeit on 1000x3 feature set reports a speedup from 15ms to 10ms on my systemSince this function is used quite a lot in scikit-learn it should speedup the whole package quite a bit,,1146,0.7949389179755671,0.40131578947368424,31910,376.55907239109996,35.66280162958321,105.73487934816671,2319,25,1035,102,travis,ahojnnes,ahojnnes,true,,1,1.0,7,2,1354,true,true,false,false,0,4,1,5,25,0,866
1608379,scikit-learn/scikit-learn,python,2076,1371505077,1371545603,1371545603,675,675,github,false,false,false,61,2,2,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.706423813316039,0.12723686150033056,2,joel.nothman@gmail.com,doc/modules/clustering.rst,2,0.006644518272425249,1,1,false,MRG: Dbscan doc enh some minor modifications to the DBScan narrative documentation: a) getting rid of min_points b) our dbscan works on distances but the documentation is written in terms of similarity - this is confusing (at least is was confusing for me)PS: killed some trailing white space along the way@robertlayton please check if you agree with the modifications,,1145,0.7947598253275109,0.4053156146179402,31910,376.55907239109996,35.66280162958321,105.73487934816671,2319,25,1035,100,travis,pprett,robertlayton,false,robertlayton,33,0.8787878787878788,104,28,1413,true,true,true,false,3,20,6,1,5,0,591
1604643,scikit-learn/scikit-learn,python,2075,1371450932,1371484590,1371484590,560,560,github,false,false,false,10,2,1,0,3,0,3,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,13,0,13,2,4.235977997514384,0.11451864248816408,0,,sklearn/cluster/_feature_agglomeration.py,0,0.0,0,0,false,COSMIT much simpler agglomeration inverse_transform do it the easy way,,1144,0.7945804195804196,0.4215686274509804,31918,376.40203020239363,35.653863023999,105.64571715019738,2319,25,1035,99,travis,jnothman,agramfort,false,agramfort,36,0.6388888888888888,15,1,1511,true,true,false,true,10,325,55,173,48,0,547
1606266,scikit-learn/scikit-learn,python,2074,1371445966,1371508294,1371508294,1038,1038,github,false,false,false,18,10,2,2,4,0,6,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,48,0,151,0,8.711453126018304,0.23551203209182908,2,stefano.lattarini@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py,2,0.006535947712418301,0,0,false,DOC minor fixes to Ward docstrings I think these changes are accurate but if someone could please check,,1143,0.7944006999125109,0.4215686274509804,31918,376.40203020239363,35.653863023999,105.64571715019738,2319,25,1035,99,travis,jnothman,agramfort,false,agramfort,35,0.6285714285714286,15,1,1511,true,true,false,true,10,325,54,173,48,0,594
1601247,scikit-learn/scikit-learn,python,2071,1371331870,1371333354,1371333354,24,24,github,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.458844229457369,0.12054377540509333,3,joel.nothman@gmail.com,sklearn/feature_selection/rfe.py,3,0.00967741935483871,0,0,false,Fix wrong argument name in RFECV docstring ,,1141,0.7949167397020158,0.432258064516129,31918,376.40203020239363,35.653863023999,105.64571715019738,2318,25,1033,97,travis,djv,agramfort,false,agramfort,0,0,56,554,1139,false,true,false,false,0,0,0,0,0,0,-1
1595580,scikit-learn/scikit-learn,python,2068,1371208789,1372942098,1372942098,28888,28888,commits_in_master,false,false,false,129,30,6,8,15,2,25,0,3,0,0,2,5,2,0,0,0,0,5,5,4,0,0,107,63,818,125,30.297365493874377,0.8190810942814928,14,larsmans@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py,13,0.040880503144654086,0,6,true,Multi target ridge regression with individual penalties Hi everybodyas indicated a few days ago in https://githubcom/scikit-learn/scikit-learn/pull/1333 here is a minimal version of a multiple target ridge regression with the possibility of individual penalties per targetIt is a small increment on Fabians recent contribution https://githubcom/scikit-learn/scikit-learn/pull/1914It is available within the function ridge_regression and in the Ridge estimator For individual penalties with the ridge estimator one must pass a 1d array like object as the penalties with length corresponding to the number of targets Otherwise an exception will be thrownThis PR is supposed to be small but self-contained As far as I can see it is - please let me know your opinion on this and what to add if necessaryThanks very much for your timeMichael,,1140,0.7947368421052632,0.44025157232704404,31915,376.1554128152906,35.65721447595175,105.56164812783958,2317,25,1032,112,travis,eickenberg,mblondel,false,mblondel,3,0.6666666666666666,7,2,526,false,true,false,false,0,3,1,0,0,0,258
1594226,scikit-learn/scikit-learn,python,2067,1371177967,1371247740,1371247740,1162,1162,github,false,false,false,19,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,21,5,21,8.93616209945409,0.24158861043740493,8,stefano.lattarini@gmail.com,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,8,0.02531645569620253,0,0,false,TST additional tests for preprocessingBinarizer In particular the method does not work for sparse matrices with threshold  0,,1139,0.7945566286215979,0.4430379746835443,31946,375.79039629374574,35.62261315970701,105.45921242096037,2317,24,1031,98,travis,jnothman,larsmans,false,larsmans,34,0.6176470588235294,15,1,1507,true,true,false,false,10,310,56,153,47,0,613
1593987,scikit-learn/scikit-learn,python,2066,1371173371,1372801053,1372801053,27128,27128,commit_sha_in_comments,false,false,false,66,2,2,2,4,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,8.817621034757563,0.2383838598094419,8,stefano.lattarini@gmail.com,sklearn/preprocessing.py|sklearn/preprocessing.py,8,0.025396825396825397,0,3,false,DOC: Clarify docs on preprocessingBinarizer The current docs seem to imply that only zeros remain zero and that any other value is set to 10Clarify that- Values below the threshold also remain 0- Values above the threshold are replaced by integer 1 not floating point 10The default is clearly in the parameter explanation and the constructor signature but could be mentioned again,,1138,0.7943760984182777,0.4444444444444444,31946,375.79039629374574,35.62261315970701,105.45921242096037,2317,23,1031,111,travis,erg,larsmans,false,larsmans,9,0.7777777777777778,20,4,1714,false,true,false,false,2,11,1,2,0,0,11
1591948,scikit-learn/scikit-learn,python,2063,1371147159,1374513700,1374513700,56109,56109,commit_sha_in_comments,false,false,false,9,4,1,0,7,0,7,0,3,0,0,1,4,0,0,1,0,0,4,4,3,0,1,0,0,39,0,4.375844430700068,0.1183044859069278,0,,MANIFEST.in,0,0.0,0,4,false,Adding pxd to manifest Failed attempt to address #2057,,1137,0.7941952506596306,0.4444444444444444,31977,375.36354254620505,35.58807893173218,105.32570284892266,2317,23,1031,120,travis,TimSC,glouppe,false,glouppe,3,1.0,9,6,661,false,false,false,false,0,0,0,0,0,0,1
1583531,scikit-learn/scikit-learn,python,2056,1371012489,1371037097,1371037097,410,410,github,false,false,false,32,3,1,0,2,0,2,0,2,0,0,1,2,1,0,0,0,0,2,2,1,0,0,17,0,24,0,4.4602190539444075,0.12089033998463662,13,stefano.lattarini@gmail.com,sklearn/cross_validation.py,13,0.04075235109717868,0,0,false,DOC clarify LeavePOuts combinatoric explosion LeavePOuts analogy to LeaveOneOut needs to be clearer: the former makes *more* iterations than the latter not fewer This clarification intends to avoid confusions like in #2049,,1136,0.7940140845070423,0.46394984326018807,31902,375.932543414206,35.671744718199484,105.51062629302238,2316,23,1030,97,travis,jnothman,jaquesgrobler,false,jaquesgrobler,33,0.6060606060606061,15,1,1506,true,true,false,false,9,304,57,148,48,0,298
1583274,scikit-learn/scikit-learn,python,2055,1371004669,1371197454,1371197454,3213,3213,github,false,false,false,36,8,1,7,5,0,12,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,136,0,448,0,4.398411732465323,0.11921504129072741,12,stefano.lattarini@gmail.com,sklearn/cross_validation.py,12,0.03773584905660377,0,8,false,[MRG] refactor partitioning cross-validation strategies There was a lot of repeated code in cross validation __iter__ methodsThe refactor aims to make the differences between individual strategies clearer by not having them handle masking negation etc,,1135,0.7938325991189428,0.46540880503144655,31902,375.932543414206,35.671744718199484,105.51062629302238,2316,23,1029,99,travis,jnothman,robertlayton,false,robertlayton,32,0.59375,15,1,1505,true,true,false,false,6,298,56,147,47,0,2085
1579024,scikit-learn/scikit-learn,python,2054,1370949275,1371780496,1371780496,13853,13853,github,false,false,false,52,7,3,8,19,0,27,0,5,0,0,3,6,2,0,0,0,0,6,6,3,0,0,55,24,193,28,40.83039811398243,1.1066716563042067,47,stefano.lattarini@gmail.com,doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/whats_new.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,25,0.04643962848297214,0,4,false,[MRG] Enforce n_folds  2 for k-fold cross-validation Users might be confused if they set n_folds1 in KFold or StratifiedKFold as they would get empty training sets that can often results in model fit weird error messagesThis can happen if they naively use cv1 in GridSearchCV for instance See: #2048 ,,1134,0.7936507936507936,0.47368421052631576,31902,375.932543414206,35.671744718199484,105.51062629302238,2316,23,1029,100,travis,ogrisel,jnothman,false,jnothman,36,0.8055555555555556,756,121,1476,true,true,false,false,7,72,6,43,41,0,188
1577477,scikit-learn/scikit-learn,python,2052,1370908776,,1374800280,64858,,unknown,false,true,false,81,336,36,128,38,0,166,0,8,8,0,5,52,9,0,0,31,4,40,75,43,0,9,1543,913,3752,7723,209.56367742785326,5.680037244624909,11,vanderplas@astro.washington.edu,sklearn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/bicluster/spectral.py|sklearn/base.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|scikits/learn/utils/tests/test_hungarian.py|scikits/learn/utils/hungarian.py|sklearn/utils/hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/utils/tests/test_hungarian.py|sklearn/base.py|sklearn/bicluster/__init__.py|sklearn/bicluster/spectral.py|sklearn/bicluster/util.py|sklearn/cluster/bicluster/tests/test_spectral.py,11,0.0,0,13,false,[MRG] Spectral biclustering An implementation of the spectral biclustering algorithms This pull request is ready for review- [x] sklearnutilsarpacksvds sometimes returns NaN This causes test_spectral_biclustering() to sometimes fail- [x] Adapt Hungarian algorithm and use it for bicluster scoring- [x] documentation and examples (Still need to expand Spectral Biclustering documentation and adapt blog examples)- [x] get tests to full code coverage- [x] better array validation in fit()- [x] support sparse matrices- [x] allow randomized SVD,,1133,0.794351279788173,0.4801223241590214,31902,375.932543414206,35.671744718199484,105.51062629302238,2315,23,1028,128,travis,kemaleren,vene,false,,3,0.6666666666666666,5,0,806,true,true,false,false,0,3,3,14,8,0,9
1559101,scikit-learn/scikit-learn,python,2042,1370534962,1370844110,1370844110,5152,5152,commit_sha_in_comments,false,false,false,50,11,1,23,15,0,38,0,5,1,0,2,4,3,0,0,1,0,3,4,3,0,0,164,64,261,73,13.503847300755446,0.36646844176100063,31,stefano.lattarini@gmail.com,examples/randomized_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,25,0.044642857142857144,1,7,false,MRG: fix inconsistent cv_scores_ generation for randomized search and re-add example The RandomizedSearchCVcv_scores_ was completely inconsistent with the best_params_ / best_score_ attributeAlso for some reason the merge of this feature dropped an example along the way @amueller I would appreciate it if you could have a look at it,,1131,0.7948717948717948,0.5148809523809523,31833,383.46998397888984,35.74906543524016,109.91738133383595,2313,22,1024,95,travis,ogrisel,jnothman,false,jnothman,35,0.8,755,121,1471,true,true,false,false,5,58,7,40,41,0,470
1543728,scikit-learn/scikit-learn,python,2037,1370501986,1373251014,1373251014,45817,45817,commits_in_master,false,false,false,101,94,25,0,11,0,11,0,4,3,11,28,83,48,0,2,3,11,69,83,79,0,3,3643,226,5611,847,241.9562418617841,6.566222814392139,35,vlad@vene.ro,sklearn/cluster/hierarchical.py|sklearn/manifold/spectral_embedding.py|sklearn/utils/__init__.py|sklearn/utils/graph.py|sklearn/utils/setup.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_graph_tools.pyx|sklearn/utils/sparsetools/_graph_validation.py|sklearn/utils/sparsetools/_min_spanning_tree.c|sklearn/utils/sparsetools/_min_spanning_tree.pyx|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/sparsetools/_traversal.pyx|sklearn/utils/sparsetools/setup.py|sklearn/utils/sparsetools/tests/__init__.py|sklearn/utils/sparsetools/tests/test_spanning_tree.py|sklearn/utils/sparsetools/tests/test_traversal.py|sklearn/feature_extraction/tests/test_image.py|sklearn/utils/graph.py|sklearn/utils/mst/_traversal.c|sklearn/utils/mst/_traversal.pyx|sklearn/utils/mst/setup.py|sklearn/utils/mst/tests/test_traversal.py|sklearn/manifold/spectral_embedding.py|sklearn/utils/mst/__init__.py|sklearn/cluster/hierarchical.py|sklearn/utils/_csgraph.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/complex_ops.h|sklearn/utils/sparsetools/csgraph.h|sklearn/utils/sparsetools/csgraph.i|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/sparsetools/csgraph_wrap.cxx|sklearn/utils/sparsetools/npy_3kcompat.h|sklearn/utils/sparsetools/py3k.h|sklearn/utils/sparsetools/setup.py|sklearn/utils/setup.py|doc/developers/utilities.rst|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|benchmarks/bench_glm.py|benchmarks/bench_glmnet.py|benchmarks/bench_lasso.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_plot_lasso_path.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_nmf.py|benchmarks/bench_plot_omp_lars.py|benchmarks/bench_plot_parallel_pairwise.py|benchmarks/bench_plot_svd.py|benchmarks/bench_plot_ward.py|benchmarks/bench_random_projections.py|benchmarks/bench_sample_without_replacement.py|benchmarks/bench_sgd_regression.py|benchmarks/bench_tree.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_fastkmeans.py|sklearn/utils/mst/_traversal.c|sklearn/utils/mst/_traversal.pyx|sklearn/utils/mst/setup.py|sklearn/utils/mst/tests/test_traversal.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/mst/__init__.py|sklearn/cluster/hierarchical.py|sklearn/utils/_csgraph.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/complex_ops.h|sklearn/utils/sparsetools/csgraph.h|sklearn/utils/sparsetools/csgraph.i|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/sparsetools/csgraph_wrap.cxx|sklearn/utils/sparsetools/npy_3kcompat.h|sklearn/utils/sparsetools/py3k.h|sklearn/utils/sparsetools/setup.py|sklearn/utils/setup.py|doc/developers/utilities.rst|sklearn/feature_extraction/tests/test_image.py|sklearn/utils/graph.py|sklearn/cluster/hierarchical.py|sklearn/manifold/spectral_embedding_.py|sklearn/utils/__init__.py|sklearn/utils/graph.py|sklearn/utils/setup.py|sklearn/utils/sparsetools/README|sklearn/utils/sparsetools/__init__.py|sklearn/utils/sparsetools/_graph_tools.c|sklearn/utils/sparsetools/_graph_tools.pyx|sklearn/utils/sparsetools/_graph_validation.py|sklearn/utils/sparsetools/_min_spanning_tree.c|sklearn/utils/sparsetools/_min_spanning_tree.pyx|sklearn/utils/sparsetools/_traversal.c|sklearn/utils/sparsetools/_traversal.pyx|sklearn/utils/sparsetools/setup.py|sklearn/utils/sparsetools/tests/__init__.py|sklearn/utils/sparsetools/tests/test_spanning_tree.py|sklearn/utils/sparsetools/tests/test_traversal.py,15,0.0,0,5,false,[FAIL]: Connected components update to sparsetools backport The cs_graph_components that was in util was out of date and incorrectIve updated the backport and cleaned up the previous csgraph backport at the same timeIn addition Ive renamed the mst folder to sparsetools which is where one would expect these functions to beWhile this PR now affects more than just the proposed EAC algorithm (See PR#1830) this update is required for that PR to happenTODO (ALL DONE)- [x] Have to run tests with an older version of scipy - [x] Testing for connected_componentspy- [x] pep8 for test_spanning_treepy,,1130,0.7946902654867256,0.5194029850746269,31816,383.6748805632386,35.76816696002012,109.97611264772442,2312,22,1024,112,travis,robertlayton,robertlayton,true,robertlayton,21,0.8095238095238095,9,10,748,false,true,false,false,0,20,3,10,14,0,11
1556685,scikit-learn/scikit-learn,python,2036,1370466162,,1372609485,35722,,unknown,false,false,false,114,1,1,0,11,0,11,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,17,0,4.479200453118627,0.12140923788827279,13,stefano.lattarini@gmail.com,sklearn/cross_validation.py,13,0.03987730061349693,0,1,false,Added unique identifier to cloned estimators in cross_validationcross_val_score() Currently all estimators used in cross-validation are identical because they are clones of a single proto-estimator I added an extra attribute to each of the clones so that they are aware that they are clone and are uniquely identifiableMotivation: I often find I need to write some debugging information to disk and I want the output of each cross-validation fold to go to a separate file The example below illustrates a possible use of the newly added cv_number attribute:pythonclass MyVectorizer(CountVectorizer):    def fit_transform(self X y):            if hasattr(self cv_id):                with open(debug_file_%d%selfcv_number) as outfile:                    outfilewrite(Estimator %d fitting %d data points%(selfcv_number len(y))            return super(MyVectorizer self)fit_transform(X y),,1129,0.7953941541186891,0.50920245398773,31816,384.0834800100578,35.79959768669852,109.97611264772442,2312,22,1023,110,travis,mbatchkarov,larsmans,false,,4,0.25,11,17,681,false,false,false,false,0,0,0,0,0,0,123
1551433,scikit-learn/scikit-learn,python,2031,1370408273,1370415572,1370415572,121,121,github,false,false,false,77,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,15,0,15,0,4.372662928960148,0.11866577761857704,4,gael.varoquaux@normalesup.org,sklearn/tree/_tree.pyx,4,0.012195121951219513,0,0,false,DOC Detail on parent-child relationship in tree Some explanation of the Tree object seemed lacking (especially as the class docstream boasts that you can find a detailed description of all arrays below) It seems to me that the parent index always precedes its children and that the left child is the x  t case and right is x  t  But I need to confirm that these are true and that Ive described them clearly enough,,1128,0.7952127659574468,0.5152439024390244,31817,383.2856648961247,35.735613037055664,109.97265612722758,2311,23,1023,95,travis,jnothman,glouppe,false,glouppe,31,0.5806451612903226,13,1,1499,true,true,false,false,5,250,54,120,39,0,121
1546187,scikit-learn/scikit-learn,python,2030,1370365614,,1405789397,590336,,unknown,false,false,false,51,4,3,4,3,0,7,0,2,0,0,4,4,4,0,0,0,0,4,4,4,0,0,13,43,13,47,17.533630372623165,0.4758294696954428,50,vanderplas@astro.washington.edu,sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py|sklearn/multiclass.py|sklearn/tests/test_multiclass.py,22,0.06790123456790123,0,0,false,#1979 Added Hamming loss and Jaccard similarity to Scorers and make Hamming loss default Fixed scikit-learn/scikit-learn#1979Hi this is my first contribution to the scikit-learn project I added the Hamming loss to SCORERS and made it default score for OneVsRestClassifierIf you have any comments please let me knowBestAndreas ,,1127,0.7959183673469388,0.5246913580246914,31817,383.2856648961247,35.735613037055664,109.97265612722758,2311,23,1022,225,travis,andreas-hjortgaard,arjoly,false,,0,0,1,0,210,false,false,false,false,0,0,0,0,0,0,1131
1544354,scikit-learn/scikit-learn,python,2027,1370353249,1373055575,1373055575,45038,45038,github,false,false,false,113,6,1,22,42,0,64,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,84,27,214,98,9.242322646489987,0.2508191053519723,14,stefano.lattarini@gmail.com,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,14,0.043209876543209874,2,6,false,[MRG] Non-categorical variables in OneHotEncoder I added a categorical_features option to OneHotEncoder It lets the user specify whether features / variables are categorical or not Non-categorical features are not touched and are stacked as is at the end of the matrix I was hesitating between using an array of indices or a boolean mask I went for the latter as it seems easier to handle for the user but I would be glad to change if other people think otherwiseIm thinking of adding a similar feature to DictVectorizer but I would like us to agree on whether I should use an array of indices or a boolean mask firstCC @amueller @larsmans,,1126,0.7957371225577264,0.5246913580246914,31817,383.2856648961247,35.735613037055664,109.97265612722758,2311,23,1022,112,travis,mblondel,larsmans,false,larsmans,22,0.8636363636363636,249,29,1162,true,true,true,true,5,88,2,25,10,0,8
1545231,scikit-learn/scikit-learn,python,2026,1370306065,1370331858,1370331858,429,429,merged_in_comments,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.296132578951532,0.11789299229553965,1,amueller@ais.uni-bonn.de,sklearn/mixture/dpgmm.py,1,0.0030959752321981426,0,0,false,Documentation ERROR: mixtureDPGMMprecs_  in the code: mixtureDPGMMprecs_ in the doc: mixtureDPGMMprecisions_,,1125,0.7955555555555556,0.5356037151702786,31795,383.3621638622425,35.76033967604969,110.04874980342821,2311,23,1021,95,travis,nzer0,GaelVaroquaux,false,GaelVaroquaux,0,0,0,1,316,false,true,false,false,0,0,0,0,1,0,-1
1543880,scikit-learn/scikit-learn,python,2025,1370293972,1382287204,1382287204,199887,199887,merged_in_comments,false,false,false,54,414,56,152,89,0,241,0,11,3,0,4,16,4,0,0,4,1,12,17,10,0,0,914,440,5333,1732,306.8904949566848,8.421583387524915,14,vlad@vene.ro,sklearn/utils/ransac.py|sklearn/utils/__init__.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|examples/plot_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|examples/plot_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/__init__.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|examples/plot_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|examples/plot_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/__init__.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|examples/plot_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|examples/plot_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/__init__.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|sklearn/utils/ransac.py|examples/plot_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py|examples/plot_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/ransac.py|sklearn/utils/tests/test_ransac.py|sklearn/utils/tests/test_ransac.py,14,0.0,0,48,false,MRG: RANSAC algorithm This is my first contribution to scikit-learn Please let me know if I meet all you coding conventions I hope you find the implementation usefulI am not aware of all the different estimator implementations so I am not sure if this function is universally applicable to all the different estimators,,1124,0.7953736654804271,0.5496894409937888,31795,383.3621638622425,35.76033967604969,110.04874980342821,2311,22,1021,153,travis,ahojnnes,arjoly,false,arjoly,0,0,7,2,1340,false,true,false,false,0,0,0,0,14,0,454
1553906,scikit-learn/scikit-learn,python,2024,1370254419,,1370450752,3272,,unknown,false,false,false,47,1,1,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,29,24,29,24,8.424849097894164,0.23119181066602887,45,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,31,0.09935897435897435,0,0,false,[MRG] ENH remove mix of multilabel input format Remove support of mix of mulitlabel input format in metricspyThe support of mix input label might lead to ambiguity problemA stronger checker for mix of multilabel input type is at #1985This pr is ready for reviews,,1123,0.7960819234194123,0.5801282051282052,31795,383.3621638622425,35.76033967604969,110.04874980342821,2310,22,1021,96,travis,arjoly,arjoly,true,,20,0.85,15,20,531,true,true,false,false,4,168,16,93,24,0,36
1562844,scikit-learn/scikit-learn,python,2023,1370245096,1372612027,1372612027,39448,39448,merged_in_comments,false,false,false,112,15,14,0,3,0,3,0,4,0,0,14,15,14,0,0,0,0,15,15,15,0,0,123,0,185,0,175.28482289864033,4.810110211490987,2,gael.varoquaux@normalesup.org,benchmarks/bench_plot_neighbors.py|benchmarks/bench_glmnet.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_glm.py|benchmarks/bench_glmnet.py|benchmarks/bench_lasso.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_plot_lasso_path.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_nmf.py|benchmarks/bench_plot_omp_lars.py|benchmarks/bench_plot_parallel_pairwise.py|benchmarks/bench_plot_svd.py|benchmarks/bench_plot_ward.py|benchmarks/bench_sample_without_replacement.py|benchmarks/bench_sgd_regression.py|benchmarks/bench_tree.py|benchmarks/bench_glmnet.py|benchmarks/bench_lasso.py|benchmarks/bench_plot_ward.py|benchmarks/bench_glm.py|benchmarks/bench_lasso.py|benchmarks/bench_plot_lasso_path.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_plot_nmf.py|benchmarks/bench_plot_parallel_pairwise.py|benchmarks/bench_plot_svd.py|benchmarks/bench_sample_without_replacement.py|benchmarks/bench_tree.py|benchmarks/bench_glm.py|benchmarks/bench_tree.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_lasso.py|benchmarks/bench_glm.py|benchmarks/bench_sgd_regression.py|benchmarks/bench_plot_neighbors.py|benchmarks/bench_glm.py|benchmarks/bench_plot_fastkmeans.py|benchmarks/bench_plot_lasso_path.py,1,0.0,0,1,false,WIP: Cleanup benchmark charts [WIP] In first poking around the scikit-learn code I started by running all of the benchmarks Not all ran and I thought the output could be cleaned up a bit What Ive done should be pretty clear from my commit messagesTo do:* get feedback* make sure that plots are spaced properly not to overlap* consider not using tight axis boundaries on time axes because it obscures startup time* consider changing some axis labels from stating the measure to stating the dimension eg Samples becomes Dataset size (samples)* figure out the intention for the neighbors plot currently two sets of plots are overlaid,,1122,0.7959001782531194,0.5801282051282052,31798,385.1500094345556,35.75696584690861,110.13271274922951,2310,22,1021,113,travis,kgeis,larsmans,false,larsmans,1,1.0,8,0,279,false,true,false,false,0,0,1,0,2,0,212
1537296,scikit-learn/scikit-learn,python,2019,1369884619,1392212991,1392212991,372079,372079,commit_sha_in_comments,false,false,false,147,12,8,0,3,0,3,0,3,1,0,3,6,3,0,0,1,0,5,6,5,0,0,740,872,847,1298,75.66514588028986,2.076367609834558,27,vlad@vene.ro,sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/tests/test_pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/pipeline.py|sklearn/tests/test_pipeline.py|sklearn/pipeline.py|sklearn/tests/test_metaestimators.py|sklearn/tests/test_pipeline.py|sklearn/tests/test_metaestimators.py,21,0.05396825396825397,0,0,false,[MRG] Ensure delegated ducktyping in MetaEstimators This intends to solve #1805 ensuring that appropriate metaestimators have hasattr(metaest method)  True iff the sub-estimator does for the set of standard methods: inverse_transform transform predict predict_proba predict_log_proba decision_function scoreI posted this as a WIP so that you can:* check the testing is reasonable* confirm the set of methods is correct* contribute patches to relevant estimatorsI currently take the approach of testing for attributes only after fitting There may be problems if ducktyping is used before fitting on say a GridSearchCV but I guess thats just something well have to be aware of when working with ducktypingI also am not currently dealing with metaestimators that delegate their fit methods (Im not sure if they exist but I have considered some)(I have pulled in the relevant commit from #1769 and removed its Pipeline-particular testing),,1121,0.7957181088314005,0.5968253968253968,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1016,177,travis,jnothman,jnothman,true,jnothman,30,0.5666666666666667,12,1,1492,true,true,false,false,5,221,53,97,37,0,15947
1537294,scikit-learn/scikit-learn,python,2018,1369884537,,1369884580,0,,unknown,false,false,false,74,2,2,0,0,0,0,0,1,3,0,10,13,13,0,0,3,0,10,13,13,0,0,448,117,448,117,58.676566215411064,1.6101749375960563,64,stefano.lattarini@gmail.com,doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/feature_selection/base.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/tests/test_base.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/feature_selection/univariate_selection.py|sklearn/linear_model/logistic.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/classes.py|sklearn/tree/tree.py,28,0.01904761904761905,0,0,false,[WIP] Ensure delegated ducktyping in MetaEstimators This intends to solve #1805 ensuring that appropriate metaestimators have hasattr(metaest method)  True iff the sub-estimator does for the set of standard methods: inverse_transform transform predict predict_proba predict_log_proba decision_function scoreI am posting this as a WIP so that you can:* check the testing is reasonable* offer patches to relevant estimators (I have pulled in the relevant commit from #1769 and removed its Pipeline-particular testing),,1120,0.7964285714285714,0.5968253968253968,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1016,93,travis,jnothman,jnothman,true,,29,0.5862068965517241,12,1,1492,true,true,false,false,5,220,51,97,37,0,-1
1536494,scikit-learn/scikit-learn,python,2017,1369871494,1374665170,1374665170,79894,79894,github,false,false,false,33,21,5,4,20,0,24,0,5,0,0,3,36,2,0,1,0,0,36,36,35,0,1,165,0,355,0,27.191649613634297,0.7461805545851453,13,vanderplas@astro.washington.edu,doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/jquery.js|doc/themes/scikit-learn/static/nature.css_t|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py,10,0.022222222222222223,0,3,false,MRG: Google-images style effect for Example Gallery This is nearly done - will put up an online build soon as I prettified itWorks well for now Will be ready for review soon,,1119,0.7962466487935657,0.5968253968253968,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1016,124,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,55,0.9090909090909091,10,13,490,true,true,false,false,9,164,33,54,18,0,629
1534141,scikit-learn/scikit-learn,python,2016,1369843705,1369992374,1369992374,2477,2477,github,false,false,false,60,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,4.136752440596523,0.11351882927904522,3,stefano.lattarini@gmail.com,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx,3,0.00946372239747634,0,3,false,Fix SGD L2 regularization order This pull request addresses #1965 Because the change is in a pyx file I already ran CythonWhen testing the change with make_classification and make_regression both the classifier performance metrics and regressor predictions remained identical However I assume this is not always the case so someone with more knowledge of SGD implementations should review this,,1118,0.7960644007155635,0.5993690851735016,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1016,93,travis,norbert,pprett,false,pprett,0,0,51,73,1926,false,true,false,false,0,0,0,0,0,0,49
1533670,scikit-learn/scikit-learn,python,2015,1369837647,1373291817,1373291817,57569,57569,github,false,false,false,113,67,6,14,21,1,36,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,420,574,1329,1078,53.36467554462582,1.464408513604083,10,mathieu@mblondel.org,sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py|sklearn/utils/multiclass.py|sklearn/utils/tests/test_multiclass.py,8,0.025236593059936908,0,8,false,[MRG] FIX corner cases with unique_labels Bugs were discovered in unique_labels in #1985:  - Mix of multilabel and multiclass format doesnt raise any errors   - Mix of input data type doesnt raise any error (eg [1 2] and [a])  - Mix of indicator matrix with different number of labels doesnt raise   any error~~In my opinion the implementation could be greatly simplify and improve using type_of_target in  #1985 (see https://gistgithubcom/arjoly/5665632)~~~~However this pr is not merged yet ~~Remaining bugs that wont be treated in this pr: - mix of multioutput multiclass format and multiclass / multilabel format- mix of unknown / continuous / continuous multi-output format and multiclass / multilabel format,,1117,0.7958818263205013,0.5993690851735016,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1016,113,travis,arjoly,arjoly,true,arjoly,19,0.8421052631578947,15,20,526,true,true,false,false,4,162,14,92,24,0,39
1528046,scikit-learn/scikit-learn,python,2013,1369753305,,1374867687,85239,,unknown,false,false,false,20,12,7,34,34,0,68,0,7,0,0,6,8,3,0,0,0,0,8,8,5,0,0,372,173,479,246,138.34850110331718,3.7964945847375904,202,vlad@vene.ro,doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,78,0.1746031746031746,0,9,false,MRG add log loss (cross-entropy loss) to metrics Reissue of #1125 with a clearer implementation more documentation and more tests,,1116,0.796594982078853,0.6,31796,384.85973078374633,35.75921499559693,110.07673921247955,2307,22,1015,119,travis,larsmans,larsmans,true,,69,0.7391304347826086,106,32,1045,true,true,false,false,47,291,68,87,107,0,12
1526314,scikit-learn/scikit-learn,python,2011,1369719058,,1369731489,207,,unknown,false,false,false,18,2,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,5,0,4.174044360312899,0.11454219499153427,3,larsmans@gmail.com,sklearn/utils/multiclass.py,3,0.009584664536741214,0,0,false,ENH faster unique_labels for big sequences of sequences master redundantly converts each sequence to a set before union,,1115,0.7973094170403587,0.597444089456869,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,22,1015,89,travis,jnothman,jnothman,true,,28,0.6071428571428571,12,1,1491,true,true,false,false,4,197,49,92,36,0,11
1524725,scikit-learn/scikit-learn,python,2010,1369690452,1369703380,1369703377,215,215,github,false,false,false,4,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.281436017412563,0.11748918761911123,28,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py,28,0.08945686900958466,0,1,false,Fixed typo in metricspy ,,1114,0.7971274685816876,0.597444089456869,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,23,1014,89,travis,kmike,jnothman,false,jnothman,10,0.9,346,0,1404,true,true,false,false,3,27,8,3,2,0,214
1523490,scikit-learn/scikit-learn,python,2008,1369671557,,1446773162,1284966,,unknown,false,false,false,118,32,14,22,38,0,60,0,8,4,0,9,14,7,0,1,4,0,10,14,7,0,1,921,84,2637,320,108.22888998093462,2.9699671579963907,49,vlad@vene.ro,sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/k_means_elkan.py|sklearn/cluster/k_means_elkan.py|sklearn/cluster/tests/test_k_means_elkan.py|sklearn/cluster/k_means_elkan.py|sklearn/cluster/tests/test_k_means_elkan.py|sklearn/cluster/k_means_elkan.py|sklearn/cluster/k_means_elkan.py|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/setup.py|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/tests/test_k_means.py|.gitattributes|doc/tutorial/statistical_inference/unsupervised_learning.rst|sklearn/cluster/_k_means_elkan.c|sklearn/cluster/_k_means_elkan.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/tests/test_k_means.py,30,0.006389776357827476,0,8,false,WIP Elkan k-means This is an early pr for an accelerated K-Means implementationUnfortunately it is still slower than the current versionProfiling tells me all time is spent in the inner distance computationI am not sure if there is a coding way to speed this up more As the speed improvement should be significant it might be that I made a mistake somewhereAny help welcome )PS: I started off doing everything with typed memory views but there slicing is more expensive (checks if the index is out of bounds) and each indexing operation is more expensive as it takes possible strides into account Does any one know if it is possible to circumvent this,,1113,0.7978436657681941,0.597444089456869,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,23,1014,387,travis,amueller,MechCoder,false,,172,0.8662790697674418,645,33,948,true,true,false,false,34,386,29,75,52,0,32450
1523164,scikit-learn/scikit-learn,python,2006,1369667090,,1371612972,32431,,unknown,false,false,false,35,2,1,0,7,0,7,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,12,9,12,9,8.904500436853457,0.2443531838908998,11,stefano.lattarini@gmail.com,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,11,0.03514376996805112,0,0,false,ENH Special-case where LabelEncoder need not copy If the classes are a range starting at 0 no operation needs to be done by LabelEncoder This makes it more reasonable to use LabelEncoder in validation-type transformations,,1111,0.7992799279927992,0.597444089456869,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,23,1014,106,travis,jnothman,jnothman,true,,27,0.6296296296296297,12,1,1490,true,true,false,false,4,190,46,92,35,0,577
1522874,scikit-learn/scikit-learn,python,2005,1369662385,1369665295,1369665295,48,48,github,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.469315088943357,0.12264487534571936,2,stefano.lattarini@gmail.com,sklearn/tests/test_pipeline.py,2,0.0064516129032258064,0,0,false,Fixed test_pipeline_methods_preprocessing_svm: pca was unused  ,,1110,0.7990990990990992,0.6,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,23,1014,86,travis,kmike,jnothman,false,jnothman,8,1.0,346,0,1404,true,true,false,false,3,27,6,3,2,0,48
1522512,scikit-learn/scikit-learn,python,2004,1369656474,1371817675,1371817675,36020,36020,github,false,false,false,27,33,2,54,43,0,97,0,8,2,0,0,8,2,0,0,3,2,4,9,3,0,1,516,0,2477,0,18.714273567115953,0.513548430385396,0,,examples/out_of_core_classification.py|examples/reuters_parser.py|examples/out_of_core_classification.py|examples/reuters_parser.py,0,0.0,0,26,false,out-of-core learning example As discussed on the ML here is a full-fledged example of out-of-core learning using:- HashingVectorizer- SGDClassifier (partial_fit)on the Reuters 21578 dataset,,1109,0.7989179440937781,0.6006493506493507,31795,384.8718351942129,35.76033967604969,110.08020128951094,2307,23,1014,104,travis,oddskool,ogrisel,false,ogrisel,1,0.0,2,2,735,false,true,false,false,0,0,0,0,0,0,33
1520016,scikit-learn/scikit-learn,python,2002,1369593237,,1374509894,81944,,unknown,false,false,false,33,81,8,25,53,1,79,0,3,0,0,2,3,2,0,0,0,0,3,3,3,0,0,787,160,2472,870,66.79150978804707,1.8328641657206886,63,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,47,0.15309446254071662,1,11,false,[MRG] Remove _check_1d_array and _is_1d private function Improve the test coverage and test formally this utilitiesI have also improved the ValueError messageedit : Thanks to @GaelVaroquaux I ended remove those two,,1108,0.7996389891696751,0.5993485342019544,31795,384.8718351942129,35.76033967604969,110.08020128951094,2305,23,1013,118,travis,arjoly,arjoly,true,,18,0.8888888888888888,15,20,523,true,true,false,false,3,117,13,67,14,0,8
1519322,scikit-learn/scikit-learn,python,2001,1369571441,1370250859,1370250859,11323,11323,commit_sha_in_comments,false,false,false,39,4,4,0,18,0,18,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,126,202,126,202,27.19678950485011,0.7463208577374975,46,tadej.janez@tadej.hicsalta.si,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,32,0.10526315789473684,2,5,false,[MRG] Remove pos_label argument with multilabel binary indicator format As suggested by @GaelVaroquaux and @jnothman I remove the pos_label argument with the multilabel indicator formatThis will simplify the interface of several metrics and will reduce the maintenance effort,,1107,0.7994579945799458,0.6019736842105263,31778,385.0462584177733,35.77947007363585,110.139089936434,2305,23,1013,95,travis,arjoly,arjoly,true,arjoly,17,0.8823529411764706,15,20,523,true,true,false,false,3,116,12,67,13,0,18
1518962,scikit-learn/scikit-learn,python,2000,1369556897,1369567859,1369567859,182,182,merged_in_comments,false,false,false,602,5,5,0,8,0,8,0,2,1,0,5,6,6,0,0,1,0,5,6,6,0,0,251,0,251,0,34.25331009242573,0.9399624085765245,58,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/base.py|sklearn/grid_search.py|sklearn/linear_model/coordinate_descent.py|sklearn/base.py|sklearn/feature_selection/univariate_selection.py|sklearn/pipeline.py|sklearn/utils/iter_fits.py,27,0.046357615894039736,2,0,false,[WIP] Estimatoriter_fits for efficient parameter search This implements an approach to generalised cross validation (#1626) by allowing each estimator to control its fitting over a sequence of parameter settings estimatoriter_fits returns an iterator (usually a generator) over parameters and models fit with those parameters*This PR is to open comment rather than to finalize the implementation in the near term*The default implementation allows an estimator to specify parameters which when changed do not require fit to be called again Estimators where it is possible to warm start from the model attributes learnt with the previous parameters may utilise that fact in particular this approach emphasises that the same data is being used where multiple calls to fit does not A Pipeline resolves its ordering through a depth-first traversal of the parameters affecting each pipeline step (ordered by that steps estimator) and the use of generators means the output of transform from higher levels is retained in the stack which adds a memory cost (it may be possible to optionally reduce this cost later) but cuts a lot of repeated work The model iterator approach means regularization paths can be easily incorporated if rewritten as generators[Grid search](https://gistgithubcom/jnothman/5624440) (cv3 n_jobs1) over a simple pipeline of (StandardScaler SelectKBest ElasticNet) ran in 67% of the baseline time (repeated calls to Pipelinefit) using this implementation with the following call counts:| Method                     | calls@master | calls@iter_fits ||----------------------------|--------------|-----------------||StandardScalerfit        |241           |7                ||StandardScalertransform  |481           |247              ||SelectKBestfit           |241           |7                ||SelectKBesttransform     |481           |265              ||ElasticNetfit            |241           |241              |(Note: many calls to transform are at predict time which is not affected by this change)Caveats and comments:* iter_fits will often overwrite attributes of the same object in each iteration Its therefore necessary to *copy* any data off the models or call any of their methods *while iterating* This may make it a dangerous method for public use and it may be worthwhile to require that each iteration yields a clone of the estimator at some added expense and complication* a common pattern for implementing iter_fits is to group the list of parameter settings by the values of some higher-order parameters (those requiring substantial work to be done at each change) within which groups lower-order parameters are iterated Thus: * the parameter settings need to be grouped by each estimator making the dict form in which parameters are transmitted cumbersome * parameter values may be objects that are unorderable and/or unhashable and/or un-boolean-comparable (ndarray is all of these) making the grouping operation and other generic parameter equality evaluations not trivial to code * in many cases it should be possible instead to order merely on the basis of parameter names and some simple ordering rules for each parameter resulting in less repeated reordering work (and on the scale of number of parameter names not number of parameter value combinations) Setting a pipeline step (#1769) presents a case where parameter names are not sufficient information for an estimator to return an ordering* user-intended candidate ordering and the manner of their generation is ignored This makes the solution generic but its harder to harness the properties of a grid for example The reordering also means parameter search may return a different best candidate with the same top score* since the ordering is performed at fit time each fold of a cv-search process may operate in parallel To recombine the outputs it is easiest if the reordering is deterministic and therefore the same across all folds Also because the reordering is coupled with fitting  parallelisation beyond one-process-per-fold may require arbitrary splits in the candidate sequence,,1106,0.7992766726943942,0.6059602649006622,31778,385.0462584177733,35.77947007363585,110.139089936434,2305,23,1013,85,travis,jnothman,jnothman,true,jnothman,26,0.6153846153846154,12,1,1489,true,true,false,false,4,154,41,81,34,0,120
2016673,scikit-learn/scikit-learn,python,1998,1369382191,1369385413,1369385413,53,53,github,false,false,false,23,0,0,0,1,0,1,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,true,Changed the setup instructions in the README Changed the setup instructions in the README to properly install the package in the user home,,1105,0.7990950226244344,0.592948717948718,31777,385.05837555464643,35.780596028574124,110.14255593668376,2303,23,1011,86,travis,kgeis,glouppe,false,glouppe,0,0,8,0,269,false,true,false,false,0,0,0,0,0,0,53
1516698,scikit-learn/scikit-learn,python,1997,1369352399,1369583947,1369583947,3859,3859,merged_in_comments,false,false,false,37,3,2,1,5,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,35,5,37,5,13.246703821085305,0.3635061765410704,7,stefano.lattarini@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,7,0.022508038585209004,1,1,false,MRG: Chunking in manhattan distance manhattan_distance can use too much memory due to a large temporary This PR introduces a mode to chunk and use less memory@AlexandreAbraham this is related to your PR on silouhette coefficient,,1104,0.7989130434782609,0.594855305466238,31777,385.05837555464643,35.780596028574124,110.14255593668376,2303,23,1010,86,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,29,0.6551724137931034,365,3,1186,true,true,false,false,12,140,10,39,26,0,110
2016652,scikit-learn/scikit-learn,python,1996,1369345512,1369406809,1369406809,1021,1021,github,false,false,false,43,0,0,0,3,0,3,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,2,1,false,add optional banner to index page to advertise code sprints As discussed with @NelleV and @GaelVaroquaux This is just to add it to the dev master so longIll push it once approved to the stable version tooOnline build available [here](http://jaquesgroblergithubio/online-sklearn-build/indexhtml),,1103,0.7987307343608341,0.5967741935483871,31777,385.05837555464643,35.780596028574124,110.14255593668376,2303,23,1010,85,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,54,0.9074074074074074,10,13,484,true,true,false,false,9,160,30,55,16,0,12
1547081,scikit-learn/scikit-learn,python,1991,1369286413,1370354542,1370354542,17802,17802,commit_sha_in_comments,false,true,false,38,4,2,0,7,0,7,0,5,6,0,2,8,8,0,0,10,0,2,12,17,0,1,716,66,756,66,26.209821263380896,0.719230162829656,1,larsmans@gmail.com,sklearn/utils/_graph_tools.c|sklearn/utils/_graph_tools.pyx|sklearn/utils/_graph_validation.py|sklearn/utils/_min_spanning_tree.c|sklearn/utils/_min_spanning_tree.pyx|sklearn/utils/setup.py|sklearn/utils/__init__.py|sklearn/utils/tests/test_spanning_tree.py,1,0.0,0,3,false,MRG: Minimal spanning tree (backport from scipy 013) Backported (with test) so that I can use it in PR1830I dont believe docs etc are needed for this type of PR but let me know if they are,,1102,0.7985480943738656,0.6064516129032258,31777,385.05837555464643,35.780596028574124,110.14255593668376,2303,23,1010,96,travis,robertlayton,amueller,false,amueller,20,0.8,8,10,734,false,true,true,true,0,20,3,12,3,0,92
1508040,scikit-learn/scikit-learn,python,1990,1369236560,1374940200,1374940200,95060,95060,merged_in_comments,false,false,false,276,36,13,0,3,0,3,0,1,0,0,6,8,5,0,0,0,0,8,8,7,0,0,1231,65,2006,596,80.64642639209654,2.213029173685722,62,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/utils/multiclass.py|sklearn/utils/multiclass.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/utils/multiclass.py|sklearn/utils/multiclass.py|sklearn/metrics/metrics.py|sklearn/utils/multiclass.py|sklearn/preprocessing.py|sklearn/utils/multiclass.py|sklearn/linear_model/logistic.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/preprocessing.py|sklearn/utils/multiclass.py|sklearn/metrics/metrics.py,29,0.041935483870967745,0,0,false,[WIP] rewrite precision_recall_fscore_support precision_recall_fscore_support was getting gargantuan because the similarities between the different input formats and metric variations werent being exploited rather almost everything was special-cased As a result some bugs and inconsistencies crept in (admittedly on my watch as a reviewer)This implementation:* is much smaller and less nested in code and should be faster in some cases taking advantage of LabelEncoder to use bincount (and building upon vectorized multilabel support in #1985 #1987)* deprecates pos_label and introduces neg_label which allows micro-averaging to be interesting in the multiclass case (see #1983)* neg_label has not yet been tested* to do so will assume that multilabel indicator matrices are represented with 1 and 1 (or False and True)* ~~has not yet implemented support for the labels argument largely because I dont know what it means (see #1989) Its not hard to implement but~~ I wish labels were deprecated in favour of stating a convention regarding label ordering in the averageNone case* ~~has not yet fixed some broken tests for multilabel averagesamples~~* has not yet updated the documentation or signatures of precision_score etc derivative functions regarding pos/neg_label* currently assumes P and R go to 0 when their denominators are 0 ~~I think this is incorrect behaviour but it is backward-compatible (except with the averagesamples implementation again whoops): precision should be perfect when recalling nothing recall should be perfect when there are no instances to retrieve (And not realising that scikit-learn had adopted the 0 approach I suggested in the documentation that 1 should be used)~~ The decision made elsewhere is to use 0 and warn This is not yet implemented,,1101,0.7983651226158038,0.6161290322580645,31777,387.5129810869497,35.780596028574124,110.14255593668376,2303,23,1009,118,travis,jnothman,jnothman,true,jnothman,25,0.6,12,1,1485,true,true,false,false,3,130,40,78,32,0,49
1507897,scikit-learn/scikit-learn,python,1988,1369224523,1374937326,1374937326,95213,95213,merged_in_comments,false,true,false,22,84,8,15,49,0,64,0,6,0,0,2,8,2,0,0,0,0,8,8,7,0,0,51,520,1675,4851,69.95172845352864,1.9195545635804094,105,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,81,0.26129032258064516,0,13,false,[MRG] FIX remaining bug in precision recall and fscore with multilabel data Fix in precision recall and fscore as pointed in #1945,,1100,0.7981818181818182,0.6161290322580645,31777,387.5129810869497,35.780596028574124,110.14255593668376,2303,23,1009,117,travis,arjoly,amueller,false,amueller,16,0.875,15,20,519,true,true,true,false,3,95,11,52,8,0,50
1510506,scikit-learn/scikit-learn,python,1987,1369222810,,1375354117,102188,,unknown,false,false,false,68,9,1,14,15,0,29,0,3,0,0,2,5,2,0,0,0,0,5,5,5,0,0,87,0,335,84,9.222705071487962,0.25308180668519864,12,stefano.lattarini@gmail.com,sklearn/preprocessing.py|sklearn/utils/multiclass.py,11,0.03571428571428571,0,7,false,ENH support multilabel targets in LabelEncoder Also support 1d-array of arrays (with the outer array having dtypeobject) as a multilabel format and their npvectorized processingThis makes it easy to support multilabel data with string (or non-consecutive / negative-inclusive) labels In particular bincount and related operations that can be used on LabelEncoded multiclass targets can also be used with multilabel dataAlso added inheritance of LabelBinarizer from LabelEncoder,,1099,0.7989080982711556,0.6168831168831169,31776,387.5251762336355,35.78172205438066,110.14602215508559,2303,23,1009,119,travis,jnothman,jnothman,true,,24,0.625,12,1,1485,true,true,false,false,3,128,39,77,32,0,1329
1708587,scikit-learn/scikit-learn,python,1986,1369222123,1369222840,1369222840,11,11,github,false,false,false,45,2,2,0,1,0,1,0,2,0,0,5,5,5,0,0,0,0,5,5,5,0,0,23,0,23,0,27.14906405767419,0.7450020496448381,27,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py,21,0.012987012987012988,0,0,false,Doc reference fixes This is for #1980 * Replaced removed/renamed examples with current ones (please review these choices and tell me if they are incorrect or if you have a better example to mention) * Fixed broken reference paths to examples that have been moved,,1098,0.7987249544626593,0.6168831168831169,31776,387.5251762336355,35.78172205438066,110.14602215508559,2303,23,1009,81,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,53,0.9056603773584906,10,13,483,true,true,false,false,8,153,27,50,15,0,2
1507299,scikit-learn/scikit-learn,python,1985,1369211976,1370786856,1370786856,26248,26248,commit_sha_in_comments,false,true,false,21,76,2,37,77,0,114,0,5,0,0,2,5,2,0,0,0,0,5,5,5,0,0,168,0,2208,1024,17.68223159314683,0.4852211019578562,34,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py|sklearn/utils/multiclass.py|sklearn/metrics/metrics.py|sklearn/utils/multiclass.py,33,0.10714285714285714,1,8,false,FIX helper to check multilabel types Fixes [bugs](https://githubcom/scikit-learn/scikit-learn/pull/1945/files#r4332200) related to simply checking type() equality and refactors@arjoly does this seem reasonable,,1097,0.7985414767547858,0.6201298701298701,31776,387.5251762336355,35.78172205438066,110.14602215508559,2303,23,1009,99,travis,jnothman,jnothman,true,jnothman,23,0.6086956521739131,12,1,1485,true,true,false,false,3,124,38,72,32,0,1
2016650,scikit-learn/scikit-learn,python,1984,1369187777,,1420467055,854594,,unknown,false,true,false,73,34,2,4,23,0,27,0,13,2,0,0,10,1,0,1,6,3,5,14,5,0,2,188,0,1681,63,9.368948626648452,0.25709490076868013,0,,sklearn/cluster/optics.py|examples/cluster/plot_optics,0,0.0,0,29,false,Implementation of OPTICS Equivalent results to DBSCAN but allows execution on arbitrarily large datasets After initial construction allows multiple scans to quickly extract DBSCAN clusters at variable epsilon distancesAlgorithm is tested and validated but not yet optimized Tested on 70K points (successful) currently testing on 2 x 10^9 LiDAR points (pending)First commit would appreciate any advice on changes to help code conform to the scikit-learn APIExample usage in examples folder,,1096,0.7992700729927007,0.6221498371335505,31776,387.5251762336355,35.78172205438066,110.14602215508559,2303,23,1008,256,travis,espg,espg,true,,0,0,1,1,64,false,false,false,false,0,0,0,0,0,0,686
2016609,scikit-learn/scikit-learn,python,1982,1369155254,1369156550,1369156550,21,21,merged_in_comments,false,false,false,15,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,BUG: set random state in LogisticRegression random_state from LogisticRegression in the constructor was not set,,1095,0.7990867579908676,0.6209150326797386,31776,387.5251762336355,35.78172205438066,110.14602215508559,2303,23,1008,78,travis,schwarty,,false,,2,0.0,6,1,1072,false,true,false,false,0,0,0,0,0,0,19
1499917,scikit-learn/scikit-learn,python,1974,1369028259,,1369144626,1939,,unknown,false,false,false,38,5,1,2,5,1,8,0,4,0,0,2,2,1,0,0,0,0,2,2,1,0,0,121,0,148,0,9.099957559282672,0.2506399173198538,30,stefano.lattarini@gmail.com,doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py,27,0.08852459016393442,0,0,false,DOC rewrite descriptions of P/R/F averages and define support Following on from #1945 this is an attempt to explain the averages a different way Im not sure if the reiteration of the descriptions in notation is helpful though,,1093,0.8005489478499542,0.6262295081967213,31758,387.46142704200514,35.80200264500284,110.14547515586624,2300,22,1007,79,travis,jnothman,jnothman,true,,22,0.6363636363636364,12,1,1483,true,true,false,false,3,117,35,69,29,0,503
1491420,scikit-learn/scikit-learn,python,1971,1368802858,1389807262,1389807262,350013,350013,github,false,false,false,13,171,123,26,99,7,132,0,11,96,1,33,202,39,2,39,96,53,53,202,57,4,61,4778,794,7745,810,996.6722561300224,27.451879036183925,448,vlad@vene.ro,solutions/exercise_04_face_recognition.py|tutorial/index.rst|solutions/exercise_02_sentiment.py|solutions/exercise_01_language_train_model.py|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|skeletons/exercise_02_sentiment.py|solutions/exercise_02_sentiment.py|skeletons/exercise_02_sentiment.py|solutions/exercise_02_sentiment.py|tutorial/conf.py|tutorial/index.rst|skeletons/exercise_01_language_train_model.py|skeletons/exercise_02_sentiment.py|skeletons/exercise_04_face_recognition.py|solutions/exercise_01_language_train_model.py|solutions/exercise_02_sentiment.py|solutions/exercise_04_face_recognition.py|tutorial/general_concepts.rst|tutorial/setup.rst|tutorial/working_with_text_data.rst|skeletons/exercise_01_language_train_model.py|skeletons/exercise_02_sentiment.py|solutions/exercise_01_language_train_model.py|solutions/exercise_02_sentiment.py|tutorial/exercises.rst|tutorial/working_with_text_data.rst|skeletons/exercise_04_face_recognition.py|solutions/exercise_04_face_recognition.py|skeletons/exercise_02_language_train_model.py|solutions/exercise_02_language_train_model.py|skeletons/exercise_01_sentiment.py|skeletons/exercise_02_language_train_model.py|skeletons/exercise_04_face_recognition.py|solutions/exercise_02_language_train_model.py|solutions/exercise_04_face_recognition.py|skeletons/exercise_01_sentiment.py|solutions/exercise_01_sentiment.py|tutorial/setup.rst|tutorial/setup.rst|README.rst|tutorial/index.rst|tutorial/setup.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|README.rst|README.rst|tutorial/conf.py|tutorial/themes/scikit-learn/layout.html|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/which_algorithm.rst|tutorial/working_with_text_data.rst|skeletons/exercise_01_sentiment.py|skeletons/exercise_02_language_train_model.py|solutions/exercise_01_sentiment.py|solutions/exercise_02_language_train_model.py|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/sphinxtoghpages.py|tutorial/working_with_text_data.rst|tutorial/conf.py|tutorial/Makefile|tutorial/conf.py|tutorial/sphinxtoghpages.py|tutorial/exercises.rst|tutorial/finding_help.rst|tutorial/general_concepts.rst|tutorial/working_with_text_data.rst|solutions/exercise_02_language_train_model.py|tutorial/general_concepts.rst|tutorial/images/supervised_scikit_learn.png|README.rst|skeletons/exercise_01_sentiment.py|solutions/exercise_01_sentiment.py|skeletons/exercise_02_language_train_model.py|solutions/exercise_02_language_train_model.py|skeletons/exercise_02_language_train_model.py|solutions/exercise_02_language_train_model.py|tutorial/which_algorithm.rst|.gitignore|tutorial/exercises.rst|tutorial/exercises.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/images/decision_boundary.png|tutorial/working_with_text_data.rst|tutorial/setup.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/exercises.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/images/linearly_separable_data.png|tutorial/images/non_linearly_separable_data.png|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/images/supervised_scikit_learn.png|tutorial/working_with_text_data.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/images/iris_pca_2d_kmeans.png|README.rst|.gitignore|.gitignore|tutorial/general_concepts.rst|tutorial/themes/scikit-learn/static/nature.css_t|tutorial/general_concepts.rst|tutorial/themes/scikit-learn/static/nature.css_t|tutorial/working_with_text_data.rst|skeletons/exercise_01_language_train_model.py|skeletons/exercise_02_sentiment.py|skeletons/exercise_04_face_recognition.py|solutions/exercise_01_language_train_model.py|solutions/exercise_02_sentiment.py|solutions/exercise_04_face_recognition.py|solutions/generate_skeletons.py|tutorial/general_concepts.rst|tutorial/images/iris_pca_2d.png|tutorial/conf.py|tutorial/general_concepts.rst|tutorial/general_concepts.rst|skeletons/exercise_04_face_recognition.py|solutions/exercise_04_face_recognition.py|.gitignore|data/labeled_faces_wild/fetch_data.py|tutorial/general_concepts.rst|tutorial/general_concepts.rst|skeletons/exercise_02_language_train_model.py|solutions/exercise_02_language_train_model.py|skeletons/exercise_01_language_train_model.py|solutions/exercise_01_language_train_model.py|data/movie_reviews/fetch_data.py|data/twenty_newsgroups/fetch_data.py|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/general_concepts.rst|tutorial/images/supervised.png|tutorial/images/unsupervised.png|tutorial/finding_help.rst|tutorial/general_concepts.rst|tutorial/setup.rst|tutorial/bigger_toc_css.rst|tutorial/exercises.rst|tutorial/general_concepts.rst|tutorial/index.rst|tutorial/which_algorithm.rst|tutorial/finding_help.rst|tutorial/general_concepts.rst|tutorial/images/Virginia_Iris.png|tutorial/working_with_text_data.rst|README.rst|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|data/twenty_newsgroups/fetch_data.py|.gitignore|data/twenty_newsgroups/fetch_data.py|tutorial/exercices.rst|tutorial/finding_help.rst|tutorial/general_concepts.rst|tutorial/index.rst|tutorial/setup.rst|tutorial/which_algorithm.rst|tutorial/working_with_text_data.rst|README.rst|.gitignore|README.rst|data/languages/fetch_data.py|data/movie_reviews/fetch_data.py|solutions/languages.py|tutorial/Makefile|tutorial/big_toc_css.rst|tutorial/bigger_toc_css.rst|tutorial/conf.py|tutorial/images/google-logo.png|tutorial/images/inria-logo.jpg|tutorial/images/inria-small.jpg|tutorial/index.rst|tutorial/logos/favicon.ico|tutorial/logos/scikit-learn-logo-small.png|tutorial/logos/scikit-learn-logo.bmp|tutorial/logos/scikit-learn-logo.png|tutorial/logos/scikit-learn-logo.svg|tutorial/make.bat|tutorial/themes/scikit-learn/layout.html|tutorial/themes/scikit-learn/static/nature.css_t|tutorial/themes/scikit-learn/theme.conf|tutorial/working_with_text_data.rst|tutorial/working_with_text_data.rst|doc/tutorial/text_analytics/README.rst|doc/tutorial/text_analytics/data/labeled_faces_wild/fetch_data.py|doc/tutorial/text_analytics/data/languages/fetch_data.py|doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py|doc/tutorial/text_analytics/data/twenty_newsgroups/fetch_data.py|doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py|doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py|doc/tutorial/text_analytics/skeletons/exercise_04_face_recognition.py|doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py|doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py|doc/tutorial/text_analytics/solutions/exercise_04_face_recognition.py|doc/tutorial/text_analytics/solutions/generate_skeletons.py|doc/tutorial/text_analytics/tutorial/Makefile|doc/tutorial/text_analytics/tutorial/big_toc_css.rst|doc/tutorial/text_analytics/tutorial/conf.py|doc/tutorial/text_analytics/tutorial/exercises.rst|doc/tutorial/text_analytics/tutorial/finding_help.rst|doc/tutorial/text_analytics/tutorial/general_concepts.rst|doc/tutorial/text_analytics/tutorial/images/Virginia_Iris.png|doc/tutorial/text_analytics/tutorial/images/decision_boundary.png|doc/tutorial/text_analytics/tutorial/images/google-logo.png|doc/tutorial/text_analytics/tutorial/images/inria-logo.jpg|doc/tutorial/text_analytics/tutorial/images/inria-small.jpg|doc/tutorial/text_analytics/tutorial/images/iris_pca_2d.png|doc/tutorial/text_analytics/tutorial/images/iris_pca_2d_kmeans.png|doc/tutorial/text_analytics/tutorial/images/linearly_separable_data.png|doc/tutorial/text_analytics/tutorial/images/non_linearly_separable_data.png|doc/tutorial/text_analytics/tutorial/images/supervised.png|doc/tutorial/text_analytics/tutorial/images/supervised_scikit_learn.png|doc/tutorial/text_analytics/tutorial/images/unsupervised.png|doc/tutorial/text_analytics/tutorial/index.rst|doc/tutorial/text_analytics/tutorial/logos/favicon.ico|doc/tutorial/text_analytics/tutorial/logos/scikit-learn-logo-small.png|doc/tutorial/text_analytics/tutorial/logos/scikit-learn-logo.bmp|doc/tutorial/text_analytics/tutorial/logos/scikit-learn-logo.png|doc/tutorial/text_analytics/tutorial/logos/scikit-learn-logo.svg|doc/tutorial/text_analytics/tutorial/make.bat|doc/tutorial/text_analytics/tutorial/setup.rst|doc/tutorial/text_analytics/tutorial/sphinxtoghpages.py|doc/tutorial/text_analytics/tutorial/themes/scikit-learn/layout.html|doc/tutorial/text_analytics/tutorial/themes/scikit-learn/static/nature.css_t|doc/tutorial/text_analytics/tutorial/themes/scikit-learn/theme.conf|doc/tutorial/text_analytics/tutorial/which_algorithm.rst|doc/tutorial/text_analytics/tutorial/working_with_text_data.rst|sklearn/ensemble/weight_boosting.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/tests/test_multiclass.py,180,0.0,0,37,true,MRG - Merge old tutorial into docs This addresses #1601 Reviews welcome,,1092,0.8003663003663004,0.6496815286624203,31665,369.1457445128691,33.69651034264962,102.32117479867361,2299,22,1004,158,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,52,0.9038461538461539,10,13,478,true,true,false,false,8,148,26,44,14,0,143
1484481,scikit-learn/scikit-learn,python,1970,1368675648,,1374527641,97533,,unknown,false,false,false,50,2,1,1,13,0,14,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,12,0,14,0,4.203312380865643,0.11577344054463047,4,peter.prettenhofer@gmail.com,sklearn/mixture/gmm.py,4,0.012539184952978056,0,7,false,GMM to take advantage of labels if provided to fit(X y  None) to be useful for prediction in pipeline GMM to take advantage of labels if provided to be useful for final/prediction stage of a pipelineHere is the gist:https://gistgithubcom/dashesy/5577051And here is the issue I created:https://githubcom/scikit-learn/scikit-learn/issues/1957,,1091,0.8010999083409716,0.6520376175548589,31665,369.1457445128691,33.69651034264962,102.32117479867361,2299,22,1002,119,travis,dashesy,agramfort,false,,0,0,0,1,691,false,true,false,false,1,4,0,0,0,0,473
1483957,scikit-learn/scikit-learn,python,1969,1368668173,1368688786,1368688786,343,343,merged_in_comments,false,false,false,14,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.756484353127182,0.13100966774799339,3,peter.prettenhofer@gmail.com,examples/linear_model/plot_lasso_coordinate_descent_path.py,3,0.009404388714733543,0,0,false,Tiny fix in lasso example DOC rename lambda to alpha in plot_lasso_coordinate_descent_path (Re)-Closes #903,,1090,0.8009174311926606,0.6520376175548589,31665,369.1457445128691,33.69651034264962,102.32117479867361,2299,22,1002,80,travis,jmmcd,GaelVaroquaux,false,GaelVaroquaux,0,0,3,1,1247,false,false,false,false,0,1,0,0,1,0,-1
1473523,scikit-learn/scikit-learn,python,1962,1368499955,1369115689,1369115689,10262,10262,commit_sha_in_comments,false,false,false,28,12,8,11,4,0,15,0,4,3,0,12,15,14,0,0,3,0,12,15,14,0,0,281,120,286,125,131.19702746838533,3.628318330391685,50,stefano.lattarini@gmail.com,sklearn/feature_selection/base.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/base.py|sklearn/feature_selection/base.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/base.py|sklearn/feature_selection/tests/test_base.py|sklearn/ensemble/forest.py|sklearn/feature_selection/__init__.py|sklearn/feature_selection/base.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/tests/test_base.py|sklearn/feature_selection/univariate_selection.py|sklearn/linear_model/logistic.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/feature_selection/from_model.py|sklearn/feature_selection/tests/test_from_model.py|sklearn/linear_model/logistic.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/classes.py|sklearn/tree/tree.py|sklearn/feature_selection/selector_mixin.py,18,0.015432098765432098,0,0,false,MRG Centralise feature selection transformations in a mixin This creates sklearnfeature_selectionSelectorMixin which provides robust transform and inverse_transform implementations given a _get_support_mask implementationIt also renames sklearnfeature_selectionselector_mixinSelectorMixin to sklearnfeature_selectionfrom_model_LearntSelectorMixin,,1089,0.800734618916437,0.6666666666666666,30209,386.93766758250854,35.320599821245324,107.25280545532787,2297,22,1000,81,travis,jnothman,jnothman,true,jnothman,21,0.6190476190476191,12,1,1476,true,true,false,false,3,109,32,60,27,0,6
1470776,scikit-learn/scikit-learn,python,1961,1368463121,1368477261,1368477261,235,235,commit_sha_in_comments,false,false,false,47,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,23,0,23,0,4.391762729197455,0.12145627233844682,21,stefano.lattarini@gmail.com,sklearn/cross_validation.py,21,0.06422018348623854,0,1,false,ENH: add pre_dispatch parameter to cross_val_score function This adds a new parameter pre_dispatch to cross_val_score The same parameter is used in GridSearchCV  https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/grid_searchpy#L524Tests are passing docs arre build correctly and I checked for PEP8Feel free to disregard this PR if not of any use,,1088,0.8005514705882353,0.6666666666666666,30196,387.1042522188369,35.33580606702875,107.29897999735064,2297,22,1000,78,travis,mekman,mekman,true,mekman,0,0,7,4,1308,false,true,false,false,0,0,0,0,1,0,3
1466619,scikit-learn/scikit-learn,python,1956,1368361538,,1369641515,21332,,unknown,false,false,false,10,4,1,0,10,0,10,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.111128188136003,0.14134282473378945,2,amueller@ais.uni-bonn.de,doc/modules/clustering.rst,2,0.006024096385542169,0,9,false,Corrected a few things on the Mutual Information doc pages ,,1087,0.8012879484820608,0.6626506024096386,30187,387.2196640938152,35.346341140225924,107.33097028522212,2295,22,999,90,travis,serch,jnothman,false,,1,1.0,2,16,1276,false,true,false,false,0,0,0,0,0,0,39
1466198,scikit-learn/scikit-learn,python,1955,1368344022,1368707839,1368707839,6063,6063,commit_sha_in_comments,false,false,false,95,2,1,10,3,0,13,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.776081805087241,0.13207761258148085,4,peter.prettenhofer@gmail.com,doc/modules/grid_search.rst,4,0.012121212121212121,0,0,false,DOC clarification of parameter search The grid_search documentation is too centered on grid search and does not describe what parameter space can be tuned (ie mentioning get_params) This patches up the documentation by firstly outlining the general idea of parameter search and by making it more consistent in its use of terminology It refers to _search candidates_ being one setting of parameters sampled from a _parameter space_ by the _search method_ It eschews use of _hyperparameters_ because of the use of get_params set_params and the _Parameters_ heading in Estimator docstrings but it explains the term,,1086,0.8011049723756906,0.6666666666666666,30231,385.76295855247923,35.195660084019714,106.81088948430418,2295,22,999,85,travis,jnothman,jnothman,true,jnothman,20,0.6,12,1,1475,true,true,false,false,2,96,26,60,25,0,3093
1455385,scikit-learn/scikit-learn,python,1954,1368116528,,1374687410,109514,,unknown,false,false,false,95,122,75,65,19,12,96,11,12,8,2,14,30,10,0,1,11,2,20,33,16,0,1,1252,190,2273,322,370.01956823830733,10.232507360058106,63,vlad@vene.ro,sklearn/rbm.py|sklearn/tests/test_rbm.py|sklearn/neural_networks/rbm.py|sklearn/neural_networks/tests/test_rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|sklearn/rbm.py|examples/rbm_logistic_classification.py|examples/rbm_logistic_classification.py|examples/rbm_logistic_classification.py|examples/rbm_logistic_classification.py|examples/rbm_logistic_classification.py|sklearn/neural_networks/__init__.py|sklearn/neural_networks/rbm.py|sklearn/tests/test_rbm.py|sklearn/neural_networks/tests/test_rbm.py|sklearn/tests/test_rbm.py|examples/rbm_logistic_classification.py|doc/modules/neural_networks.rst|doc/images/rbm_graph.png|doc/whats_new.rst|doc/modules/classes.rst|doc/modules/neural_networks.rst|doc/modules/classes.rst|doc/modules/classes.rst|examples/rbm_logistic_classification.py|sklearn/neural_networks/__init__.py|sklearn/neural_networks/rbm.py|sklearn/neural_networks/tests/test_rbm.py|sklearn/neural_networks/rbm.py|doc/whats_new.rst|sklearn/neural_networks/rbm.py|sklearn/neural_networks/rbm.py|doc/images/rbm_graph.png|doc/modules/neural_networks.rst|doc/unsupervised_learning.rst|doc/modules/classes.rst|doc/modules/neural_networks.rst|doc/unsupervised_learning.rst|doc/whats_new.rst|examples/rbm_logistic_classification.py|sklearn/neural_network/__init__.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/neural_networks/__init__.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/neural_network/tests/test_rbm.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|doc/unsupervised_learning.rst|sklearn/neural_network/rbm.py|examples/plot_rbm_logistic_classification.py|sklearn/neural_network/rbm.py|sklearn/neural_network/tests/test_rbm.py|examples/plot_rbm_logistic_classification.py|examples/plot_rbm_logistic_classification.py|examples/plot_rbm_logistic_classification.py|sklearn/neural_network/rbm.py|doc/modules/neural_networks.rst|examples/plot_rbm_logistic_classification.py,56,0.0,1,9,false,[MRG] Restricted Boltzmann Machines: directors cut This is the new life of PR #1200I grabbed @GaelVaroquauxs branch fixed the random state issue made most of the API private changed the example description and added the image to the narrative docsUp for debate:- I left gibbs free_energy and pseudo_likelihood as public functions - I feel like in some places the tests check the implementation against itselfie if sample_bernoulli had some bug then test_gibbs would pass because it verifies the bug against itself  Could reviewers please take a look at the tests too ,,1085,0.8018433179723502,0.6512968299711815,30231,385.76295855247923,35.195660084019714,106.81088948430418,2294,22,996,121,travis,vene,larsmans,false,,35,0.9142857142857143,48,30,1124,false,true,false,false,1,12,1,0,0,0,315
1466437,scikit-learn/scikit-learn,python,1953,1368092106,1368811371,1368811371,11987,11987,merged_in_comments,false,false,false,10,1,1,0,5,0,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,42,0,42,0,4.232348004284917,0.11704109775146423,13,peter.prettenhofer@gmail.com,sklearn/ensemble/weight_boosting.py,13,0.037463976945244955,0,1,false,[MRG] AdaBoost: use estimator weights in predict_proba This fixes #1897,,1084,0.801660516605166,0.6512968299711815,30231,385.76295855247923,35.195660084019714,106.81088948430418,2294,23,996,86,travis,ndawe,larsmans,false,larsmans,4,0.75,23,48,1182,false,true,false,false,2,4,1,0,1,0,4381
1452992,scikit-learn/scikit-learn,python,1952,1368063904,,1407683836,660272,,unknown,false,false,false,164,1,1,0,1,0,1,0,1,0,0,3,3,2,0,0,0,0,3,3,2,0,0,9,22,9,22,13.49811656519289,0.37327610554896296,29,vlad@vene.ro,doc/modules/pipeline.rst|sklearn/pipeline.py|sklearn/tests/test_pipeline.py,17,0.03488372093023256,0,0,false,ENH store per-transformer index into feature space in FeatureUnion FeatureUnion provided no simple way to tell which parts of the stack belonged to which transformer This provides a feature_ptr_ attribute (of the form of csr_matrixindptr) to solve thatCaveats:* it can only be determined when fit_transform not fit is called* it will be incorrect if one of the sub-transformers has a change of parameters that affects the output size at transform time without a refitBoth these caveats would be solved if each transformer in sklearn provided a way to get the number of output features I suggest a transformed_width_ attribute or get_transformed_width() method (better name) the latter making it more clear that the output can be affected by set_params I also suggest this be posed as an Easy Issue for someone to tackleFinally feature_ptr_ is compact and versatile but it might be more usable if I add a method to get this data as a dict from transformer-name to slices,,1083,0.8024007386888273,0.6540697674418605,30231,385.76295855247923,35.195660084019714,106.81088948430418,2294,23,995,205,travis,jnothman,jnothman,true,,19,0.631578947368421,11,1,1471,true,true,false,false,2,94,25,59,23,0,660332
1452181,scikit-learn/scikit-learn,python,1951,1368052611,1368351763,1368351763,4985,4985,commit_sha_in_comments,false,false,false,4,15,1,3,9,0,12,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,16,0,40,47,4.233317889147892,0.11706791888926767,26,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py,26,0.07492795389048991,0,0,false,[MRG] MAINT refactor metricsauc ,,1082,0.8022181146025879,0.6484149855907781,30231,385.76295855247923,35.195660084019714,106.81088948430418,2294,23,995,84,travis,Jim-Holmstroem,jnothman,false,jnothman,13,0.46153846153846156,16,39,672,false,true,false,false,0,33,16,8,4,0,24
1447736,scikit-learn/scikit-learn,python,1949,1367981033,,1368058304,1287,,unknown,false,false,false,72,1,1,2,9,0,11,0,4,3,0,4,7,6,0,1,3,0,4,7,6,0,1,252,2,252,2,26.557219272946455,0.7344117545919164,37,stefano.lattarini@gmail.com,examples/datasets/dataframe_titanic_dataset.py|sklearn/datasets/titanic/__init__.py|sklearn/datasets/titanic/titanic3.csv|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/utils/multiclass.py|sklearn/utils/validation.py,24,0.008645533141210375,1,3,false,Tentative support for Pandas Dataframe with example using Titanic dataset * Modified check_arrays function to leave Pandas data frames alone* Modified is_multilabel function to support Pandas Series* Added new DataFrameMapper class (inspired by @benhamner & mostly copied from https://githubcom/paulgb/sklearn-pandas)* Added new Titanic dataset to sklearndatasets* Added Titanic example to demonstrate DataFrameMapper use caseLooking for feedback since it is my first contribution to sklearnThank you in advance,,1081,0.8029602220166513,0.6484149855907781,30231,385.76295855247923,35.195660084019714,106.81088948430418,2293,23,994,81,travis,arnaudsj,jnothman,false,,0,0,59,93,1823,false,true,false,false,0,0,0,0,0,0,35
1447638,scikit-learn/scikit-learn,python,1948,1367978884,1368452207,1368452207,7888,7888,merged_in_comments,false,false,false,46,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.340133519183531,0.12002179295306646,24,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py,24,0.069164265129683,0,0,false,MRG: Fixed precompute issue (again) in ElasticNet and enet_path  The previous fix (https://githubcom/scikit-learn/scikit-learn/commit/41ae851643625a2dc933fd0864a20bbd23a4e8cb) didnt change things (https://githubcom/scikit-learn/scikit-learn/issues/1734) in the n_samples  n_features case as the value of precompute was unchanged and remained truthyAlso the same issue existed in enet_path so I applied the same fix,,1080,0.8027777777777778,0.6484149855907781,30231,385.76295855247923,35.195660084019714,106.81088948430418,2293,23,994,84,travis,jamestwebber,agramfort,false,agramfort,1,1.0,1,0,640,false,false,false,false,1,6,1,0,1,0,-1
1470336,scikit-learn/scikit-learn,python,1947,1367949848,,1374513886,109400,,unknown,false,true,false,42,21,7,14,16,0,30,0,3,0,0,5,6,5,0,0,0,0,6,6,5,0,0,716,100,1628,200,62.576812281405864,1.7304906902440484,30,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/__init__.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,25,0.023255813953488372,0,2,false,MRG - ENH change lasso_path output to lars_path style Addresses Issue #32 ~~Almost done~~~~Just an example update check some docstrings and go over tests left - ThenIll change to MRG~~Unless Im missing anything Id say this guy is ready,,1079,0.8035217794253939,0.6540697674418605,30231,385.76295855247923,35.195660084019714,106.81088948430418,2293,23,994,121,travis,jaquesgrobler,agramfort,false,,51,0.9215686274509803,10,13,468,true,true,true,true,7,135,25,27,13,0,0
1444654,scikit-learn/scikit-learn,python,1945,1367934146,1369027344,1369027344,18219,18219,commit_sha_in_comments,false,false,false,95,49,8,8,15,3,26,0,5,0,0,3,5,2,0,0,0,0,5,5,3,0,0,1900,1772,3323,3199,108.96957340430825,3.0134438175182194,43,tadej.janez@tadej.hicsalta.si,doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,29,0.06158357771260997,0,4,false,[MRG] Add multilabel support to precision recall fscore and classification report This pull request adds support for mulltilabel output support for the following functions:-   classification_report-   f1_score-   fbeta_score-   precision_recall_fscore_support-   precision_score-   recall_scoreOne more pull request to tackle the issue #558Two questions: - What should we do when the computation of the precision recall or fscore lead to a nan Currently if the computation lead to a nan the related  measure is set to 00 - Which tests should I change to use the new precision recall  and fscore function,,1078,0.8033395176252319,0.6598240469208211,30252,385.0323945524263,35.17122834853894,106.37313235488563,2292,23,994,87,travis,arjoly,jnothman,false,jnothman,15,0.8666666666666667,15,20,504,true,true,true,false,1,80,10,67,5,0,29
1443111,scikit-learn/scikit-learn,python,1944,1367901493,1367930630,1367930630,485,485,github,false,false,false,57,3,3,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,54,17,54,13.271135820426817,0.3669984924433115,6,peter.prettenhofer@gmail.com,sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,6,0.01775147928994083,0,1,false,FIX SelectPercentile limit bug SelectPercentile had a tie breaker mechanism that just took the first percentile / 100 * len(features) selected features So if scores were [0 0 1] selecting the top third would mask [F F T] but the top two-thirds would mask [T F F] ie missing the best score entirelyThis is a fix,,1077,0.8031569173630455,0.665680473372781,30246,385.10877471401176,35.178205382529924,106.39423394829069,2292,23,994,81,travis,jnothman,jaquesgrobler,false,jaquesgrobler,18,0.6111111111111112,11,1,1470,true,true,false,false,2,81,20,51,17,0,192
1442271,scikit-learn/scikit-learn,python,1943,1367884536,1367914146,1367914146,493,493,merged_in_comments,false,false,false,28,2,2,0,0,0,0,0,1,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.680246136370144,0.2676964343217592,2,larsmans@gmail.com,doc/modules/tree.rst|doc/modules/svm.rst,2,0.0058823529411764705,0,0,false,Doc latex tweaks This makes a few small changes to latex in the docs for svm and tree learners thanks again for all your work on this package,,1076,0.8029739776951673,0.6617647058823529,30246,385.10877471401176,35.178205382529924,106.39423394829069,2292,24,993,81,travis,aflaxman,GaelVaroquaux,false,GaelVaroquaux,1,1.0,71,9,1554,false,false,false,false,0,1,1,0,2,0,-1
1442059,scikit-learn/scikit-learn,python,1942,1367881703,,1368018354,2277,,unknown,false,false,false,29,3,3,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,59,0,59,13.436171173356005,0.37156236147037974,18,tadej.janez@tadej.hicsalta.si,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,18,0.05325443786982249,0,0,false,TST: General label type tests for confusion matrix More generalized and refactored tests for general label types in confusion_matrixAlso factored out the matthew_corrcoef from confusion_matrixAdd-on to https://githubcom/scikit-learn/scikit-learn/pull/1911,,1075,0.8037209302325582,0.665680473372781,30246,385.10877471401176,35.178205382529924,106.39423394829069,2292,24,993,83,travis,Jim-Holmstroem,Jim-Holmstroem,true,,12,0.5,16,39,670,false,true,false,false,0,19,14,6,3,0,-1
1440056,scikit-learn/scikit-learn,python,1941,1367854383,,1367863334,149,,unknown,false,false,false,1885,2,2,0,2,0,2,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,36,0,36,0,7.681435556953252,0.2124214473231344,25,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,25,0.07396449704142012,0,2,false,FIX compatibility with np 13 py 26 Fix the following jenkins failure See https://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/1800/changesChanges:[arnaudvjoly] ENH more pythonic way to treat list of list of labels[arnaudvjoly] ENH add jaccard similarity score metrics------------------------------------------[truncated 2265 lines]sklearnteststest_preprocessingtest_scaler_without_centering  okCheck that StandardScalerfit does not change input  oksklearnteststest_preprocessingtest_scale_sparse_with_mean_raise_exception  oksklearnteststest_preprocessingtest_scale_function_without_centering  okCheck warning when scaling integer data  oksklearnteststest_preprocessingtest_normalizer_l1  oksklearnteststest_preprocessingtest_normalizer_l2  okCheck that invalid arguments yield ValueError  oksklearnteststest_preprocessingtest_binarizer  oksklearnteststest_preprocessingtest_label_binarizer  oksklearnteststest_preprocessingtest_label_binarizer_set_label_encoding  oksklearnteststest_preprocessingtest_label_binarizer_multilabel  okCheck that invalid arguments yield ValueError  okTest OneHotEncoders fit and transform  okTest LabelEncoders transform and inverse_transform methods  okTest fit_transform  okTest LabelEncoders transform and inverse_transform methods with  okCheck that invalid arguments yield ValueError  oksklearnteststest_preprocessingtest_label_binarizer_iris  okCheck that LabelBinarizer can handle an unlabeled sample  okTest that KernelCenterer is equivalent to StandardScaler  oksklearnteststest_preprocessingtest_fit_transform  oksklearnteststest_preprocessingtest_add_dummy_feature  oksklearnteststest_preprocessingtest_add_dummy_feature_coo  oksklearnteststest_preprocessingtest_add_dummy_feature_csc  oksklearnteststest_preprocessingtest_add_dummy_feature_csr  oksklearnteststest_preprocessingtest_balance_weights  okQDA classification  oksklearnteststest_qdatest_qda_priors  oksklearnteststest_qdatest_qda_store_covariances  oksklearnteststest_random_projectiontest_invalid_jl_domain  oksklearnteststest_random_projectiontest_input_size_jl_min_dim  okCheck basic properties of random matrix generation  okCheck some statical properties of Gaussian random matrix  okCheck some statical properties of sparse random matrix  oksklearnteststest_random_projectiontest_sparse_random_projection_transformer_invalid_density  oksklearnteststest_random_projectiontest_random_projection_transformer_invalid_input  oksklearnteststest_random_projectiontest_try_to_transform_before_fit  oksklearnteststest_random_projectiontest_too_many_samples_to_find_a_safe_embedding  oksklearnteststest_random_projectiontest_random_projection_embedding_quality  oksklearnteststest_random_projectiontest_SparseRandomProjection_output_representation  oksklearnteststest_random_projectiontest_correct_RandomProjection_dimensions_embedding  oksklearnteststest_random_projectiontest_warning_n_components_greater_than_n_features  okDoctest: sklearn_NoseTestertest  okFAIL: sklearnmetricsteststest_metricstest_multilabel_representation_invariance----------------------------------------------------------------------Traceback (most recent call last): File /home/slave/virtualenvs/cpython-26/lib/python26/site-packages/nose/casepy line 197 in runTest   selftest(*selfarg) File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/ws/sklearn/metrics/tests/test_metricspy line 947 in test_multilabel_representation_invariance   % name) File /home/slave/virtualenvs/cpython-26/lib/python26/site-packages/numpy/testing/utilspy line 265 in assert_almost_equal   raise AssertionError(msg)AssertionError: Items are not equal:jaccard_similarity_score failed representation invariance  between list of list of labels format and dense binary indicator formatACTUAL: 03562091503267974DESIRED: 029738562091503268raise AssertionError(\nItems are not equal:\njaccard_similarity_score failed representation invariance  between list of list of labels format and dense binary indicator format\n ACTUAL: 03562091503267974\n DESIRED: 029738562091503268)FAIL: sklearnmetricsteststest_metricstest_multilabel_jaccard_similarity_score----------------------------------------------------------------------Traceback (most recent call last): File /home/slave/virtualenvs/cpython-26/lib/python26/site-packages/nose/casepy line 197 in runTest   selftest(*selfarg) File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/ws/sklearn/metrics/tests/test_metricspy line 1103 in test_multilabel_jaccard_similarity_score   assert_equal(1 jaccard_similarity_score(y1 y2 pos_label10))AssertionError: 1  00raise selffailureException \         (None or %r  %r % (1 00))Name                                             Stmts   Miss  Cover   Missing------------------------------------------------------------------------------sklearn                                             34      4    88%   27 52 64-65sklearn__check_build                               18      3    83%   24 45-46sklearn__check_buildsetup                          9      2    78%   17-18sklearn_build_utils                                17      4    76%   18 22 27-28sklearnbase                                       136      3    98%   58 76 84sklearncluster                                     11      0   100%   sklearncluster_feature_agglomeration              22      2    91%   64 70sklearnclusteraffinity_propagation_               90      6    93%   145-146 167-169 264sklearnclusterdbscan_                             46      0   100%   sklearnclusterhierarchical                       154      0   100%   sklearnclusterk_means_                           381      5    99%   95 360 953 1234 1237sklearnclustermean_shift_                         78      5    94%   103 120 154-156sklearnclustersetup                               18      2    89%   40-41sklearnclusterspectral                           117     14    88%   140-142 154 253-256 258-261 408-411 413-416 442 476sklearncovariance                                   6      0   100%   sklearncovarianceempirical_covariance_            62      0   100%   sklearncovariancegraph_lasso_                    201     18    91%   134 156 172-176 202 310 335 341 344 436-437 480 499-501 503-505sklearncovarianceoutlier_detection                38      2    95%   71 100sklearncovariancerobust_covariance               211     19    91%   124 134 139-141 147 228 322-323 331 380-386 566 574-579 645sklearncovarianceshrunk_covariance_              127      5    96%   182 184-188sklearncross_validation                           428      1    99%   1294sklearndatasets                                    47      0   100%   sklearndatasetsbase                              136     14    90%   455-464 498-505sklearndatasetscalifornia_housing                 37     24    35%   27-29 64-101sklearndatasetscovtype                            50     23    54%   19-20 69-78 84-93 100-104sklearndatasetslfw                               157    135    14%   53-55 66-105 113-163 178-208 260-276 294-334 342 402-429 439sklearndatasetsmlcomp                             47     40    15%   11-13 56-103sklearndatasetsmldata                             80      7    91%   15-19 151-153sklearndatasetsolivetti_faces                     41     26    37%   32-35 89-116sklearndatasetssamples_generator                 301     37    88%   116 120 123 524 573-593 731 1121-1124 1276-1303sklearndatasetssetup                              14      2    86%   22-23sklearndatasetsspecies_distributions              72     55    24%   46-48 69-82 98-108 125-135 210-257sklearndatasetssvmlight_format                    96      6    94%   127 238 334 339 347-348sklearndatasetstwenty_newsgroups                 121     88    27%   71-92 133-139 143 148-195 225-271sklearndecomposition                                8      0   100%   sklearndecompositiondict_learning                289     28    90%   87 91-92 305-306 308 417 445 450-451 453 476 478 481 571 581 605 647 797 1117-1133sklearndecompositionfactor_analysis               77      3    96%   138 165-166sklearndecompositionfastica_                     153     17    89%   79-83 124-128 239 264 271-275 287-288 312-314 329sklearndecompositionkernel_pca                    82      2    98%   172 258sklearndecompositionnmf                          202      8    96%   104 254 373 393-395 418 540sklearndecompositionpca                          153      5    97%   49 62-63 244 329sklearndecompositionsparse_pca                    59      0   100%   sklearndummy                                      110      0   100%   sklearnensemble                                    15      0   100%   sklearnensemblebase                               25      1    96%   76sklearnensembleforest                            307     13    96%   77 131-132 148-149 236 306 411 446-449 478 593-594sklearnensemblegradient_boosting                 348     13    96%   53 123 188 215 368 419 546 551 595 604 607-608 614sklearnensemblepartial_dependence                159    104    35%   54 56 237-388sklearnensemblesetup                              10      2    80%   16-17sklearnensembleweight_boosting                   272     24    91%   100-104 143 193 230 239-240 379 531-532 595-596 622 675 701-703 806 912 987 989 1001 1018sklearnexternals                                    1      0   100%   sklearnexternalsjoblib                            10      0   100%   sklearnexternalsjoblib_compat                     4      2    50%   7-8sklearnexternalsjoblibdisk                       51     11    78%   28 84-88 94 103-107sklearnexternalsjoblibformat_stack              227     46    80%   34-35 50 55 63 67-70 133-135 146 148 168-173 193-197 201-205 208 215-224 246-247 282-286 299-300 323 345-346 363-367 372-375 405 412sklearnexternalsjoblibfunc_inspect              117     12    90%   73 98-102 109-110 138-139 185 224 228sklearnexternalsjoblibhashing                    91     18    80%   22 54-55 71 88-99 144 155-159 195sklearnexternalsjobliblogger                     73     10    86%   29 42 68 78 94 99 115 121 138-139sklearnexternalsjoblibmemory                    235     20    91%   19-20 58 130 159-160 257 289-290 300 353 355 375-376 395-396 406 476 522 547sklearnexternalsjoblibmy_exceptions              42      3    93%   43 70-71sklearnexternalsjoblibnumpy_pickle              170     20    88%   21-27 86 115 122-125 196-197 243-246 272-273 290 298sklearnexternalsjoblibparallel                  219     21    90%   18-19 28-29 38-40 53 103 123 330-331 398-400 450 456 469-471 477 506sklearnexternalssetup                              6      0   100%   sklearnexternalssix                              170     64    62%   34-40 54-56 92-94 107-115 190 195-201 205-213 228-230 234-237 240 245 260 264 272-284 298-308 319-320sklearnfeature_extraction                           5      0   100%   sklearnfeature_extractiondict_vectorizer          98      3    97%   228 246-247sklearnfeature_extractionhashing                  39      1    97%   97sklearnfeature_extractionimage                   147      0   100%   sklearnfeature_extractionsetup                    10      0   100%   sklearnfeature_extractionstop_words                1      0   100%   sklearnfeature_extractiontext                    414     15    96%   104-105 232 400 405-406 595 601 767 832 891 900 941-944 1049sklearnfeature_selection                           13      0   100%   sklearnfeature_selectionrfe                      102      7    93%   120 129 142 149 216 219 358sklearnfeature_selectionselector_mixin            46      5    89%   45 57 88 99-102sklearnfeature_selectionunivariate_selection     180      7    96%   226 300 315 376 382 433 584sklearngaussian_process                             5      0   100%   sklearngaussian_processcorrelation_models         78     28    64%   48 53 91 96 129-148 178-185 221 227 271 277sklearngaussian_processgaussian_process          335    105    69%   21 309 318 320 324 329 344 349 355 361 373-381 426 459-467 495-520 582-586 597-598 604-610 616-623 680-682 688 722-724 742-744 750-799 812 828 834 845 848 853-856 868 871 876sklearngaussian_processregression_models          19      0   100%   sklearngrid_search                                227      6    97%   284 369 411 440 668-670sklearnhmm                                        452     34    92%   296-297 402 437-439 521 524 695 704-711 737 749 800-801 931 995 999 1003 1008 1017 1103 1160 1167 1188 1199-1201sklearnisotonic                                    54      1    98%   54sklearnkernel_approximation                       153      4    97%   247 256 454 485sklearnlda                                         93     11    88%   91-95 123 130 139 141-142 161sklearnlinear_model                                14      0   100%   sklearnlinear_modelbase                          128      2    98%   264 294sklearnlinear_modelbayes                         126      8    94%   178-184 210 423sklearnlinear_modelcoordinate_descent            345     29    92%   138-139 209-210 262 315 513 708-709 730 740-741 766-771 885 888-890 925 980-982 1192-1193 1308-1309 1363 1382sklearnlinear_modelleast_angle                   362     18    95%   165-166 285-286 293-302 403 518 787-790sklearnlinear_modellogistic                       11      0   100%   sklearnlinear_modelomp                           192     15    92%   87-88 180-181 279 288 376 379 384 541-544 547-548 557sklearnlinear_modelpassive_aggressive             25      0   100%   sklearnlinear_modelperceptron                      5      0   100%   sklearnlinear_modelrandomized_l1                 174      8    95%   69 100 122 142 573 591 600-601sklearnlinear_modelridge                         269     21    92%   134 141-158 511 516 537 575-579 591 678 885sklearnlinear_modelsetup                          19      2    89%   41-42sklearnlinear_modelstochastic_gradient           309     13    96%   64-65 136-137 307-310 350 396-401 833-836sklearnmanifold                                     5      0   100%   sklearnmanifoldisomap                             46      0   100%   sklearnmanifoldlocally_linear                    218     22    90%   45 47 147 176 271 274 283 286 289 310 333-334 360 384-389 437 482-483sklearnmanifoldmds                               102     21    79%   79-80 98-107 122 124-128 229-233 345 378 385-388sklearnmanifoldspectral_embedding                144     17    88%   197-200 270-282 306 387 459sklearnmetrics                                      8      0   100%   sklearnmetricscluster                             14      0   100%   sklearnmetricsclustersetup                       14      2    86%   22-23sklearnmetricsclustersupervised                 110      0   100%   sklearnmetricsclusterunsupervised                27      0   100%   sklearnmetricsmetrics                            387     18    95%   139 142 669-671 673-675 681-683 968-972 1847-1848sklearnmetricspairwise                           177      4    98%   173 532 636 796sklearnmetricsscorer                              33      2    94%   72-74sklearnmetricssetup                               13      2    85%   20-21sklearnmixture                                      5      0   100%   sklearnmixturedpgmm                              344     23    93%   211-216 219 222 251 266 367-370 460 465 502 685 692 737-740sklearnmixturegmm                                256     29    89%   244 261-268 299 301 303 357-358 419 421 440 474 566 590 619 621 624 627 637 640 645-648 666sklearnmulticlass                                 165      9    95%   53 79 126 258 277-281 542sklearnnaive_bayes                                124      6    95%   160 230 238 246 267 440sklearnneighbors                                    7      0   100%   sklearnneighborsbase                             225      9    96%   65 74 90 118 122 126 152 255 463sklearnneighborsclassification                    62      0   100%   sklearnneighborsgraph                             10      0   100%   sklearnneighborsnearest_centroid                  50      2    96%   90 156sklearnneighborsregression                        32      0   100%   sklearnneighborssetup                             10      0   100%   sklearnneighborsunsupervised                       7      0   100%   sklearnpipeline                                   138      6    96%   81 91 188 272 334 341sklearnpls                                        202     24    88%   63-64 98-99 227 231 238 242 244 247 250 366-372 402-404 834 837 842sklearnpreprocessing                              389      5    99%   62 430 439 456 1027sklearnqda                                         75      0   100%   sklearnrandom_projection                          114      0   100%   sklearnsemi_supervised                              2      0   100%   sklearnsemi_supervisedlabel_propagation          112      3    97%   130 135 171sklearnsetup                                       56      4    93%   68-70 83-84sklearnsvm                                          4      0   100%   sklearnsvmbase                                   278      8    97%   129 278 307 351 494 554 624 718sklearnsvmbounds                                  35      0   100%   sklearnsvmclasses                                 24      0   100%   sklearnsvmsetup                                   26      4    85%   54-55 83-84sklearntree                                         6      0   100%   sklearntreeexport                                 45      8    82%   71-76 100 127 133sklearntreesetup                                  14      2    86%   22-23sklearntreetree                                  180      5    97%   78 215 242 246 349sklearnutils                                      112      3    97%   321 354 358sklearnutils_csgraph                              21      3    86%   65-66 69sklearnutilsarpack                               627    308    51%   307 312 315 364-368 429 431 433 439-450 453 455 463-497 500 503 509 516 536 542-543 545-547 553 555 578 586 629 631 633 638-679 682 685 691 706 718 728 734-735 737 739 745-748 771 794-803 810-834 841-842 848 852 859-875 880 886-887 906 933-945 948-953 963-984 987 990 993-998 1009 1023 1032-1046 1184 1186-1191 1196 1202 1205 1214-1253 1423-1440 1443 1445-1450 1455 1462 1470-1480 1484 1494-1495 1499-1529 1563-1599 1603sklearnutilsbench                                  3      0   100%   sklearnutilsclass_weight                          19      0   100%   sklearnutilsextmath                              134     11    92%   46-49 54 113-115 190-192 306 369sklearnutilsfixes                                142     33    77%   25-26 29 33-38 47 72-73 78-83 93 108-110 116 131 141 160 167 178 196-197 207-209 217 228-229 231sklearnutilsgraph                                 75      5    93%   52 66 72 112 131sklearnutilsmulticlass                            20      0   100%   sklearnutilssetup                                 26      2    92%   70-71sklearnutilssparsetools                            1      0   100%   sklearnutilssparsetoolscsgraph                   61     34    44%   17-19 29 33-34 38-50 54 58-63 67-71 77-80sklearnutilssparsetoolssetup                     11      2    82%   16-17sklearnutilsvalidation                           105      1    99%   179------------------------------------------------------------------------------TOTAL                                            17190   1979    88%   ----------------------------------------------------------------------Ran 1855 tests in 293749sFAILED (SKIP16 failures2)https://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/ws/sklearn/cluster/k_means_py:1161: RuntimeWarning: init_size3 should be larger than k8 Setting it to 3*k init_sizeinit_size)[Parallel(n_jobs1)]: Done   1 jobs       | elapsed:    00s[Parallel(n_jobs1)]: Done   9 out of   9 | elapsed:    00s finishedhttps://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/ws/sklearn/qdapy:158: RuntimeWarning: divide by zero encountered in log + nplog(selfpriors_))make: *** [test-coverage] Error 1Build step Custom Python Builder marked build as failureArchiving artifactsSkipping Cobertura coverage report as build was not UNSTABLE or better After investigation with python 26 and numpy 13 I got : In [1]: import numpy as npIn [2]: from __future__ import divisionIn [3]: a  nparray([2 0 0])In [4]: b  nparray([4 1 0])In [5]: a / bOut[5]: array([ 05  0   0 ])instead of In [5]: a / bOut[5]: array([ 05  0   nan ])The patch solves the issue,,1074,0.8044692737430168,0.665680473372781,30245,385.121507687221,35.17936849065961,106.39775169449496,2291,24,993,79,travis,arjoly,GaelVaroquaux,false,,14,0.9285714285714286,15,20,503,true,true,true,false,1,78,9,67,4,0,0
1439741,scikit-learn/scikit-learn,python,1939,1367849183,,1385105698,287548,,unknown,false,false,false,223,20,7,2,1,0,3,0,2,2,0,4,10,5,0,0,3,0,8,11,8,0,0,862,0,2357,0,75.5700913254983,2.0898057472047453,45,vanderplas@astro.washington.edu,sklearn/feature_selection/etc.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/base.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/base.py|sklearn/feature_selection/etc.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py,25,0.03560830860534125,0,0,false,WIP Overhaul of feature_selection It seems to me there are commonalities between feature selection processes that are not being exploited and places where ad-hoc code are used instead of a librarySome of these things may be split off into a separate PR especially on request and all are up for discussion* [x] ~~create a mixin to provide [inverse_]transform given _get_support_mask() in all feature selectors (but what to call it given that SelectorMixin is taken by something else)~~ split off into #1962 * The following are included in #2093: * [x] provide SelectByScore (score_func minimum maximum scaling_func limit) for more generic score thresholding eg supporting two-sided document frequency cutoffs This implementation also reduces work at transform time by pre-transforming/scaling scores to match the threshold * [x] work out how to deal with nan   * do we want a way to force exclusion of nans * [x] documentation and testing of above * [x] hopefully use SelectByScore for Select{KBestPercentileFdrFprFweorMixin} closing #1459 * [x] use SelectByScore to handle thresholding in feature_extractiontextCountVectorizer * [x] use same in randomized_l1* [x] support dummy cases: selection by given indices selection by feature name given a feature extractor (or union thereof)* [ ] documentation and testing of above* [ ] move chi2 f_classify etc to sklearnmetricsfeature_scores or similar (and add document frequency  count_nonzero what else),,1073,0.8052190121155638,0.6676557863501483,30245,385.121507687221,35.17936849065961,106.39775169449496,2291,24,993,146,travis,jnothman,jnothman,true,,17,0.6470588235294118,11,1,1469,true,true,false,false,2,79,19,51,17,0,78
1438610,scikit-learn/scikit-learn,python,1935,1367822525,1367936818,1367936818,1904,1904,github,false,false,false,30,11,6,2,11,0,13,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,40,34,49,52,43.88931354181799,1.2137244555449869,6,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py,6,0.01791044776119403,0,2,false,Small improvements to inverse_transform used in feature_selection The lossy inverse transformation in feature selection needed some explanation and I made a couple of enhancements for completeness at the same time,,1071,0.8057889822595705,0.6716417910447762,30198,384.33008808530366,35.10166236174581,106.3646599112524,2291,24,993,83,travis,jnothman,jnothman,true,jnothman,16,0.625,11,1,1469,true,true,false,false,2,75,17,51,16,0,274
1438498,scikit-learn/scikit-learn,python,1934,1367818304,1367937554,1367937554,1987,1987,commit_sha_in_comments,false,false,false,26,1,1,1,8,0,9,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,31,0,31,0,13.230484642655757,0.36587864957612876,12,peter.prettenhofer@gmail.com,sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/univariate_selection.py,8,0.01791044776119403,0,0,false,ENH Feature selection should use CSC matrices Feature selection involves slicing over columns It should therefore be done with CSC rather than CSR for sparse X,,1070,0.805607476635514,0.6716417910447762,30198,384.33008808530366,35.10166236174581,106.3646599112524,2290,24,993,82,travis,jnothman,jnothman,true,jnothman,15,0.6,11,1,1469,true,true,false,false,2,75,16,51,16,0,252
1436814,scikit-learn/scikit-learn,python,1933,1367770041,,1367849087,1317,,unknown,false,false,false,9,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,13,0,13,4.621541310042989,0.12780496945042782,19,robert.l.marchman@dartmouth.edu,sklearn/feature_extraction/tests/test_text.py,19,0.057057057057057055,0,0,false,TST test CountVectorizerstop_words_ value Test to weed out https://githubcom/scikit-learn/scikit-learn/commit/3444bfbe8897a4738800cf3d2676c12b56d3ae08#commitcomment-3147501,,1069,0.8063610851262862,0.6756756756756757,30198,384.33008808530366,35.10166236174581,106.3646599112524,2290,24,992,81,travis,jnothman,jnothman,true,,14,0.6428571428571429,11,1,1468,true,true,false,false,2,75,15,51,15,0,1176
1436672,scikit-learn/scikit-learn,python,1932,1367765185,1367937875,1367937875,2878,2878,merged_in_comments,false,false,false,20,13,10,0,4,0,4,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,654,0,659,13,42.8440203610963,1.1848165679022102,30,stefano.lattarini@gmail.com,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,30,0.09036144578313253,0,0,false,MRG Clean up CountVectorizer The code for CountVectorizer was left with some redundant content This intends to tidy things up,,1068,0.8061797752808989,0.677710843373494,30198,384.33008808530366,35.10166236174581,106.3646599112524,2290,24,992,81,travis,jnothman,larsmans,false,larsmans,13,0.6153846153846154,11,1,1468,true,true,false,false,2,74,14,51,14,0,21
1434405,scikit-learn/scikit-learn,python,1930,1367685880,,1367687461,26,,unknown,false,false,false,23,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,18,0,18,4.5758298441965,0.126537631398139,3,peter.prettenhofer@gmail.com,sklearn/feature_extraction/tests/test_dict_vectorizer.py,3,0.009009009009009009,0,0,false,WIP DictVectorizers handling of empty features This is an uncontroversial change but Im submitting a PR to get a Travis buildSee #1903,,1067,0.8069353327085286,0.6756756756756757,30196,384.05749105841835,35.10398728308385,106.33858789243608,2289,24,991,80,travis,larsmans,larsmans,true,,68,0.75,105,32,1021,true,true,false,false,53,313,76,123,134,0,-1
1433961,scikit-learn/scikit-learn,python,1929,1367671777,1367687679,1367687679,265,265,merged_in_comments,false,false,false,36,1,1,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,1,28,1,28,8.38764337926205,0.23194705104785657,18,peter.prettenhofer@gmail.com,sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,14,0.041916167664670656,0,0,false,FIX bug in callable kernel decision function  Obvious fix added test (and tested that failed without the fix)I guess testing was not very good before ^^ still should have taken more care refactoringCloses #1894,,1066,0.8067542213883677,0.6736526946107785,30194,385.70576935815063,35.271908326157515,106.54434655891899,2289,24,991,80,travis,amueller,larsmans,false,larsmans,171,0.8654970760233918,626,33,925,true,true,true,true,49,534,39,117,68,0,-1
1431708,scikit-learn/scikit-learn,python,1928,1367615604,1368454782,1368454782,13986,13986,merged_in_comments,false,false,false,141,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,80,0,80,0,4.441237117051057,0.12396384476629893,7,peter.prettenhofer@gmail.com,setup.py,7,0.020710059171597635,0,1,true,Fix setuppy to resolve numpy requirement  If we try to install scikit with some command like: pip install numpy scipy scikit-learn or via requirementstxt which has: numpy  scipy scikit-learn  It raises from numpydistutilscore import setup ImportError: No module named numpydistutilscore because of numpy requirement There was similar issue in scipy which was resolved I adapted the patch for scikit and was able to fix it This fix is based of issue and fixes in Scipy:https://githubcom/scipy/scipy/pull/453https://githubcom/branden/scipy/commit/ad0dd3e883ca5b241dda6136eee921eb900077a0Notes: -Please note that this obviously will work with similarly patched version of scipy  [As far as I know the patch is on master but not on the stable release  Hence pip install numpy scipy scikit might not work presently unless you install   with requirements file which has the link to the scipy source or your own patched version  with above fixes in place],,1065,0.8065727699530516,0.6686390532544378,30171,385.63521262139136,35.19936362732425,106.59242318782937,2288,24,990,91,travis,abhirk,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,273,false,true,false,false,0,0,0,0,0,0,797
1430351,scikit-learn/scikit-learn,python,1926,1367599550,1367603341,1367603341,63,63,merged_in_comments,false,false,false,25,1,1,0,3,0,3,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,15,7,15,7,13.53197732275063,0.377704546711305,35,peter.prettenhofer@gmail.com,doc/whats_new.rst|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,29,0.019943019943019943,0,0,true,MRG disable memory-blowing SVD for sparse input in RidgeCV Ive just disallowed gcv_modesvd for sparse X and set auto to select eigen insteadFixes #1921,,1064,0.806390977443609,0.6467236467236467,30171,385.6020682111962,35.19936362732425,106.55927877763416,2288,25,990,83,travis,larsmans,larsmans,true,larsmans,67,0.746268656716418,105,32,1020,true,true,false,false,53,310,73,123,140,0,9
1429390,scikit-learn/scikit-learn,python,1924,1367586933,1367796767,1367796767,3497,3497,github,false,false,false,92,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.60711869725106,0.12859389560643128,0,,doc/themes/scikit-learn/static/sidebar.js,0,0.0,0,0,true,Fix for sidebar bug on index page Simple fix for #1869 Currently it would remember the last position (expanded or collapsed) which is fine while browsing the docs but it creates a problem on the home page as it uses no sidebar buttonand ie cant be re-expandedThis is a quick fix that works fine Ive added a comment to serve as a futre-TODO to make it still use cookies while not affecting the main page right now I dont have time for thatBut this works for now :),,1063,0.80620884289746,0.6438746438746439,30171,385.6020682111962,35.19936362732425,106.55927877763416,2288,25,990,85,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,49,0.9387755102040817,10,13,464,true,true,false,false,6,109,17,24,10,0,2956
1429059,scikit-learn/scikit-learn,python,1923,1367581137,,1367603463,372,,unknown,false,false,false,35,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,22,0,22,0,4.149091776471546,0.11580944833990009,18,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py,18,0.05128205128205128,0,0,true,REV: reverted back to python3 style division instead of npdivide Reverted back to using python3 style division since older numpy doesnt support dtype argument in npdivision(I really hope this one is appended to https://githubcom/scikit-learn/scikit-learn/pull/1913),,1062,0.8069679849340866,0.6438746438746439,30171,385.6020682111962,35.19936362732425,106.55927877763416,2288,25,990,82,travis,Jim-Holmstroem,larsmans,false,,11,0.5454545454545454,16,39,667,false,true,false,false,0,17,13,5,3,0,66
1429046,scikit-learn/scikit-learn,python,1922,1367580843,1367687871,1367687871,1783,1783,merged_in_comments,false,false,false,10,1,1,0,2,0,2,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.48623349528553,0.12521974788777296,1,jaquesgrobler@gmail.com,doc/tutorial/index.rst,1,0.002849002849002849,0,0,true,DOC add scipy lecture notes link to tutorials For #1915,,1061,0.8067860508953817,0.6438746438746439,30171,385.6020682111962,35.19936362732425,106.55927877763416,2288,25,990,82,travis,jaquesgrobler,larsmans,false,larsmans,48,0.9375,10,13,464,true,true,true,false,6,107,16,23,10,0,1384
1428907,scikit-learn/scikit-learn,python,1920,1367578371,1367764746,1367764746,3106,3106,commit_sha_in_comments,false,false,false,11,2,1,2,1,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,16,0,35,0,4.527916331706761,0.12638319920465133,9,vlad@vene.ro,sklearn/linear_model/least_angle.py,9,0.025714285714285714,0,0,true,[MRG] document return_path argument in lars_path The argument return_path wasnt documented,,1060,0.8066037735849056,0.6457142857142857,30171,385.6020682111962,35.19936362732425,106.55927877763416,2288,25,990,81,travis,arjoly,amueller,false,amueller,13,0.9230769230769231,15,20,500,true,true,true,false,1,82,6,79,5,0,2
1424426,scikit-learn/scikit-learn,python,1918,1367509741,1367664292,1367664292,2575,2575,merged_in_comments,false,false,false,11,21,1,6,4,0,10,0,3,0,0,1,19,1,0,0,0,0,19,19,19,0,0,23,0,130,9,3.2315134502931775,0.09019800239262954,16,peter.prettenhofer@gmail.com,sklearn/svm/base.py,16,0.043596730245231606,0,1,false,FIX: pep8 fixes for Violations Report (build 2161) for issue #1917,,1058,0.8071833648393195,0.6321525885558583,30171,385.6020682111962,35.19936362732425,106.55927877763416,2287,25,989,83,travis,jaquesgrobler,amueller,false,amueller,47,0.9361702127659575,10,13,463,true,true,true,true,6,100,13,20,8,0,6
1424363,scikit-learn/scikit-learn,python,1916,1367508941,1367510443,1367510443,25,25,github,false,false,false,29,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.044377520319139,0.143460256295706,0,,doc/about.rst,0,0.0,0,0,false,sponsor update by NelleVaroquaux squished and turned into one commit Already updated the 013 earlier the week This is just to update the 013 code as wellFrom #1906 ,,1057,0.8070009460737938,0.6311475409836066,29356,386.63305627469686,35.29091156833356,105.2595721487941,2287,25,989,78,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,46,0.9347826086956522,10,13,463,true,true,false,false,6,98,12,20,7,0,0
1423504,scikit-learn/scikit-learn,python,1914,1367495216,1370520775,1370520775,50425,50425,github,false,false,false,121,5,1,4,19,1,24,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,37,4,111,4,8.998751140383725,0.2511731389567234,7,peter.prettenhofer@gmail.com,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,7,0.018970189701897018,0,1,false,MRG - Add SVD-based solver to ridge regression I added a solver based on the thin SVD of X to compute the ridgecoefficients This is much more stable than the cholesky decompositionfor singular matricesWhile in the Ridge regression the Gram matrix becomes nonsingularbecause of the regularization Ive come across problems when theregularization parameter is very small This pull request remedies itmaking the algorithm defined and stable even for the border casealpha0 (useful for cross validating over a grid of parameters)I took the liberty to change the default algorithm of to this one(when there are no sample weights) I havent done any benchmarksIt should be slower than dense_cholesky but not much ,,1056,0.8068181818181818,0.6287262872628726,30171,385.6020682111962,35.19936362732425,106.55927877763416,2287,25,989,105,travis,fabianp,fabianp,true,fabianp,30,0.7,145,23,1083,true,true,false,false,0,4,0,0,2,0,67
1416402,scikit-learn/scikit-learn,python,1913,1367368411,1367432403,1367432403,1066,1066,github,false,false,false,9,1,1,0,2,0,2,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,41,0,41,0,4.2615988265578775,0.11894945366227914,16,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py,16,0.04155844155844156,0,3,false,MAINT: Faster preallocation and count with integer-type in precision_recall ,,1055,0.8066350710900474,0.6155844155844156,30150,385.6716417910448,35.124378109452735,106.50082918739635,2285,26,987,77,travis,Jim-Holmstroem,ogrisel,false,ogrisel,10,0.5,16,39,664,false,true,false,false,0,10,11,2,3,0,898
1416389,scikit-learn/scikit-learn,python,1912,1367368226,1371052320,1371052320,61401,61401,commits_in_master,false,false,false,6,21,2,0,13,0,13,0,6,0,0,1,12,1,0,0,0,0,12,12,9,0,0,6,0,350,114,8.73438593145332,0.2437935800402491,26,stefano.lattarini@gmail.com,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,26,0.06753246753246753,0,1,false,[MRG] Added input checks in confusion_matrix ,,1054,0.8064516129032258,0.6155844155844156,30150,385.6716417910448,35.124378109452735,106.50082918739635,2285,26,987,108,travis,Jim-Holmstroem,jnothman,false,jnothman,9,0.4444444444444444,16,39,664,false,true,false,false,0,10,10,2,3,0,899
1416386,scikit-learn/scikit-learn,python,1911,1367368170,1367838974,1367838974,7846,7846,github,false,false,false,8,5,3,3,2,0,5,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,64,16,76,13.009467477037873,0.3631193624296976,24,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,16,0.04155844155844156,0,0,false,BUG: Generalize label type for confusion_matrix Fixes: https://githubcom/scikit-learn/scikit-learn/issues/1835,,1053,0.8062678062678063,0.6155844155844156,30150,385.6716417910448,35.124378109452735,106.50082918739635,2285,26,987,84,travis,Jim-Holmstroem,jaquesgrobler,false,jaquesgrobler,8,0.375,16,39,664,false,true,false,false,0,10,9,2,3,0,896
1416379,scikit-learn/scikit-learn,python,1910,1367368107,,1374924409,125938,,unknown,false,true,false,15,11,4,5,27,0,32,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,42,134,52,404,17.675381454004896,0.4933540327925349,37,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py,26,0.06753246753246753,0,7,false,WIP: Generalize label type for precision_recall_fscore_support Fixes the comment in https://githubcom/scikit-learn/scikit-learn/issues/1835Should add DOC changes,,1052,0.8070342205323194,0.6155844155844156,30150,385.6716417910448,35.124378109452735,106.50082918739635,2285,26,987,117,travis,Jim-Holmstroem,arjoly,false,,7,0.42857142857142855,16,39,664,false,true,false,false,0,10,8,2,3,0,1010
1410670,scikit-learn/scikit-learn,python,1909,1367280453,,1367368601,1469,,unknown,false,false,false,25,10,10,7,0,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,101,198,101,198,43.557324916168156,1.2157433862096096,23,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,15,0.03722084367245657,0,0,false,Generalized the label type for some methods in metrics Response to: https://githubcom/scikit-learn/scikit-learn/issues/1835Added support for non-integer labels in confusion_matrix precision_recall_fscore_support f1_score fbeta_score precision_score and recall_score,,1051,0.807802093244529,0.5930521091811415,30148,385.69722701340055,35.12670823935253,106.50789438768741,2283,26,986,78,travis,Jim-Holmstroem,Jim-Holmstroem,true,,6,0.5,16,39,663,false,true,false,false,0,10,7,0,3,0,112
1406049,scikit-learn/scikit-learn,python,1908,1367225688,1367310981,1367310981,1421,1421,merged_in_comments,false,false,false,53,17,6,5,7,0,12,0,3,2,0,185,225,185,0,0,2,2,221,225,221,0,0,578,99,1054,196,894.1575854183516,24.957418382544713,289,vlad@vene.ro,sklearn/cluster/eac.py|sklearn/tests/test_common.py|sklearn/cluster/eac.py|sklearn/cluster/tests/test_eac.py|sklearn/tests/test_common.py|sklearn/cluster/__init__.py|sklearn/cluster/eac.py|sklearn/cluster/tests/test_eac.py|sklearn/cluster/eac.py|sklearn/cluster/eac.py|benchmarks/bench_covertype.py|benchmarks/bench_plot_parallel_pairwise.py|benchmarks/bench_sgd_regression.py|examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_species_distribution_modeling.py|examples/applications/plot_stock_market.py|examples/applications/plot_tomography_l1_reconstruction.py|examples/applications/svm_gui.py|examples/applications/topics_extraction_with_nmf.py|examples/applications/wikipedia_principal_eigenvector.py|examples/cluster/plot_adjusted_for_chance_measures.py|examples/cluster/plot_cluster_iris.py|examples/cluster/plot_color_quantization.py|examples/cluster/plot_digits_agglomeration.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/cluster/plot_lena_compress.py|examples/cluster/plot_lena_segmentation.py|examples/cluster/plot_lena_ward_segmentation.py|examples/cluster/plot_segmentation_toy.py|examples/cluster/plot_ward_structured_vs_unstructured.py|examples/covariance/plot_sparse_cov.py|examples/datasets/plot_digits_last_image.py|examples/datasets/plot_iris_dataset.py|examples/decomposition/plot_faces_decomposition.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_kernel_pca.py|examples/decomposition/plot_pca_3d.py|examples/decomposition/plot_pca_iris.py|examples/document_classification_20newsgroups.py|examples/document_clustering.py|examples/ensemble/plot_adaboost_hastie_10_2.py|examples/ensemble/plot_adaboost_multiclass.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/ensemble/plot_gradient_boosting_regularization.py|examples/feature_stacker.py|examples/grid_search_text_feature_extraction.py|examples/hashing_vs_dict_vectorizer.py|examples/linear_model/plot_iris_logistic.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_lasso_lars.py|examples/linear_model/plot_lasso_model_selection.py|examples/linear_model/plot_logistic.py|examples/linear_model/plot_logistic_l1_l2_sparsity.py|examples/linear_model/plot_logistic_path.py|examples/linear_model/plot_multi_task_lasso_support.py|examples/linear_model/plot_ols.py|examples/linear_model/plot_ols_3d.py|examples/linear_model/plot_ols_ridge_variance.py|examples/linear_model/plot_polynomial_interpolation.py|examples/linear_model/plot_ridge_path.py|examples/linear_model/plot_sgd_comparison.py|examples/linear_model/plot_sparse_recovery.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|examples/manifold/plot_swissroll.py|examples/mixture/plot_gmm_classifier.py|examples/mlcomp_sparse_document_classification.py|examples/neighbors/plot_regression.py|examples/plot_classification_probability.py|examples/plot_classifier_comparison.py|examples/plot_digits_classification.py|examples/plot_digits_pipe.py|examples/plot_kernel_approximation.py|examples/plot_multilabel.py|examples/plot_permutation_test_for_classification.py|examples/plot_train_error_vs_test_error.py|examples/svm/plot_svm_iris.py|examples/svm/plot_svm_kernels.py|examples/svm/plot_svm_margin.py|examples/svm/plot_svm_scale_c.py|sklearn/__check_build/setup.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/datasets/california_housing.py|sklearn/datasets/covtype.py|sklearn/datasets/lfw.py|sklearn/datasets/mlcomp.py|sklearn/datasets/mldata.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/twenty_newsgroups.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_sparse_pca.py|sklearn/dummy.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/partial_dependence.py|sklearn/ensemble/tests/test_base.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/weight_boosting.py|sklearn/externals/joblib/disk.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/hashing.py|sklearn/externals/joblib/logger.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/test/test_disk.py|sklearn/externals/joblib/test/test_format_stack.py|sklearn/externals/joblib/test/test_func_inspect.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_logger.py|sklearn/externals/joblib/test/test_memory.py|sklearn/externals/joblib/test/test_parallel.py|sklearn/feature_extraction/dict_vectorizer.py|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_dict_vectorizer.py|sklearn/feature_extraction/tests/test_image.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/grid_search.py|sklearn/isotonic.py|sklearn/kernel_approximation.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/passive_aggressive.py|sklearn/linear_model/perceptron.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_base.py|sklearn/linear_model/tests/test_bayes.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/spectral_embedding.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/metrics.py|sklearn/metrics/pairwise.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/graph.py|sklearn/neighbors/nearest_centroid.py|sklearn/neighbors/regression.py|sklearn/pls.py|sklearn/preprocessing.py|sklearn/qda.py|sklearn/random_projection.py|sklearn/svm/__init__.py|sklearn/tests/test_base.py|sklearn/tests/test_common.py|sklearn/tree/export.py|sklearn/tree/tree.py|sklearn/utils/class_weight.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/graph.py|sklearn/utils/multiclass.py|sklearn/utils/testing.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_fixes.py|sklearn/utils/tests/test_graph.py|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/validation.py,38,0.005025125628140704,0,1,false,WIP: Consistent licensing across all files I havent yet checked every one yet nor checked that I got them all (I left that script on my other computer)*edit*: Precision is high recall unknown It appears that all of those Ive done are fixed Just got to get check I havent missed any,,1050,0.8076190476190476,0.5904522613065326,30151,385.6588504527213,35.123213160425856,106.49729693874167,2283,26,986,78,travis,robertlayton,GaelVaroquaux,false,GaelVaroquaux,19,0.7894736842105263,8,10,710,false,true,true,false,0,11,2,5,0,0,2
1405295,scikit-learn/scikit-learn,python,1907,1367205973,1367241106,1367241106,585,585,github,false,false,false,28,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.419942466727942,0.12336791094524442,29,stefano.lattarini@gmail.com,sklearn/cross_validation.py,29,0.0728643216080402,0,0,false,DOC: add random_state parameter to StratifiedShuffleSplit doc string Thanks for this amazing project I added to a doc string about an undocumented parameter that I wanted to use,,1049,0.8074356530028599,0.5904522613065326,30151,385.6588504527213,35.123213160425856,106.49729693874167,2283,26,985,77,travis,aflaxman,agramfort,false,agramfort,0,0,66,9,1546,false,false,false,false,0,0,0,0,0,0,-1
1404619,scikit-learn/scikit-learn,python,1906,1367178076,1367259831,1367259831,1362,1362,commit_sha_in_comments,false,false,false,36,2,1,7,3,0,10,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,5.040736807154348,0.1406953086391182,0,,doc/about.rst,0,0.0,1,3,false,Added sponsors to the aboutrst page We forgot to add the sponsors of the Grenada sprint on the aboutrst page@GaelVaroquaux if that is OK with you Ill do a PR on the latest stable too,,1048,0.8072519083969466,0.5875,30151,385.6588504527213,35.123213160425856,106.49729693874167,2283,26,985,76,travis,NelleV,GaelVaroquaux,false,GaelVaroquaux,19,0.8947368421052632,35,13,1196,true,true,true,false,0,15,5,1,3,0,213
1684632,scikit-learn/scikit-learn,python,1904,1367002000,1367076302,1367076302,1238,1238,merged_in_comments,false,false,false,8,3,2,0,1,0,1,0,2,0,0,5,5,5,0,0,0,0,5,5,5,0,0,24,2,30,2,19.683265627307975,0.5493898255375609,91,stefano.lattarini@gmail.com,sklearn/grid_search.py|sklearn/mixture/dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/cross_validation.py|sklearn/ensemble/weight_boosting.py,40,0.0665083135391924,0,0,true,for i cleanup Cleaned up some unused indices,,1047,0.8070678127984718,0.5581947743467933,30162,385.0871958092965,35.11040381937537,106.22637756116967,2283,26,983,77,travis,Jim-Holmstroem,amueller,false,amueller,5,0.4,16,39,660,false,true,false,false,0,9,6,0,2,0,1143
1746468,scikit-learn/scikit-learn,python,1899,1366978360,,1366979206,14,,unknown,false,false,false,7,2,2,0,0,0,0,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,19,0,19,9.074139572101295,0.2532738960974242,11,peter.prettenhofer@gmail.com,sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/tests/test_weight_boosting.py,11,0.02570093457943925,0,0,true,Fix random state in test weight boosting ,,1046,0.8078393881453155,0.5467289719626168,30161,385.0668081297039,35.11156791883558,106.22989953913995,2283,26,983,77,travis,Jim-Holmstroem,larsmans,false,,4,0.5,16,39,660,false,true,false,false,0,8,5,0,1,0,-1
1746428,scikit-learn/scikit-learn,python,1898,1366977698,1366977917,1366977917,3,3,merged_in_comments,false,false,false,7,1,1,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,17,0,17,4.538308596316157,0.1266719994372686,9,peter.prettenhofer@gmail.com,sklearn/ensemble/tests/test_weight_boosting.py,9,0.02107728337236534,0,0,true,Added random_state0 for AdaBoostRegressor Fix for https://githubcom/scikit-learn/scikit-learn/issues/1896,,1045,0.8076555023923445,0.5456674473067916,30161,384.96734193163354,35.11156791883558,106.22989953913995,2283,26,983,77,travis,Jim-Holmstroem,GaelVaroquaux,false,GaelVaroquaux,3,0.3333333333333333,16,39,660,false,true,false,false,0,7,4,0,1,0,2
1403122,scikit-learn/scikit-learn,python,1895,1366936323,,1367255092,5312,,unknown,false,false,false,12,2,1,0,4,0,4,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,11,0,4.2510990345820145,0.11865542753102158,28,stefano.lattarini@gmail.com,sklearn/feature_extraction/text.py,28,0.06557377049180328,0,3,false,Small modification to TfidftTransformer that seems to make a big difference [tfidf](https://fcloudgithubcom/assets/786402/428012/eccb9b4a-adf7-11e2-8b73-8f9e4722531bpng),,1044,0.8084291187739464,0.5386416861826698,30161,384.96734193163354,35.11156791883558,106.22989953913995,2283,26,982,78,travis,lqdc,larsmans,false,,2,0.5,10,1,713,false,true,false,false,0,27,5,10,0,0,3755
1744995,scikit-learn/scikit-learn,python,1893,1366913088,1366913891,1366913891,13,13,github,false,false,false,71,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,43,0,43,0,8.609894283228247,0.24031573975826442,12,sergeykarayev@gmail.com,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py,12,0.028037383177570093,0,1,false,SGD documentation updates This PR makes a few documentation changes to SGDClassifier and SGDRegressor:* Documents all the loss functions added in #810* Fixes the description of epsilon which was incorrect for the Huber loss* Fixes a minor mistake in the docstring of _init_tAlso: I dont understand why but when I run make doc-noplot these changes dont show up in the generated html Am I doing something wrong,,1043,0.8082454458293384,0.5350467289719626,30068,386.1580417719835,35.22016762006119,106.55846747372621,2283,26,982,77,travis,dougalsutherland,pprett,false,pprett,1,1.0,23,19,1612,false,true,false,false,0,0,1,0,0,0,12
1403400,scikit-learn/scikit-learn,python,1892,1366907926,1367167880,1367167880,4332,4332,commit_sha_in_comments,false,false,false,29,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.413427952883095,0.12318574055350998,7,peter.prettenhofer@gmail.com,doc/modules/cross_validation.rst,7,0.016317016317016316,0,0,false,Cross validation doc update Addresses issue #1766 Open to more feedback this just expands the previous explanation a wee bit and adds a link to Wikipedia for further reading,,1042,0.8080614203454894,0.5337995337995338,30068,386.1580417719835,35.22016762006119,106.55846747372621,2283,26,982,77,travis,jaquesgrobler,larsmans,false,larsmans,45,0.9333333333333333,10,13,456,true,true,true,false,5,83,12,18,11,0,-1
1740318,scikit-learn/scikit-learn,python,1890,1366821695,1366823595,1366823595,31,31,merged_in_comments,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,7,0,7,0,4.567481692711684,0.1274852751045489,8,stefano.lattarini@gmail.com,sklearn/hmm.py,8,0.01839080459770115,0,1,false,Removed unnecessary copy() and changed to preprocessingsafe_asarray ,,1041,0.8078770413064361,0.5218390804597701,30063,386.22226657352894,35.22602534677178,106.57619000099791,2283,27,981,78,travis,Jim-Holmstroem,,false,,2,0.0,16,39,658,false,true,false,false,0,6,3,0,0,0,-1
1737765,scikit-learn/scikit-learn,python,1889,1366759332,,1393547368,446407,,unknown,false,false,false,88,1,1,0,5,0,5,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,85,0,85,0,4.659144243927603,0.1300439560785342,128,vlad@vene.ro,sklearn/metrics/metrics.py,128,0.2929061784897025,0,0,false,adding classification_table I needed to see the actual numbers for the true/false positives and negatives for a project I was working on I noticed sklearn didnt have an implementation of a table like this so I coded this up Im relatively new to coding and very new to contributing to large projects like this so the code may not be at an adequate level but I thought this would be a feature that would be useful to some people The implementation draws heavily from the code in metricsclassification_report ,,1040,0.8086538461538462,0.5194508009153318,30056,386.31221719457017,35.23422943838169,106.60101144530209,2283,27,980,174,travis,johnb30,larsmans,false,,0,0,28,9,471,false,false,false,false,0,0,0,0,0,0,195
1327942,scikit-learn/scikit-learn,python,1887,1366745219,1368350134,1368350134,26748,26748,commit_sha_in_comments,false,false,false,71,10,1,30,13,0,43,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,242,4,336,57,8.74387484113555,0.24405513464813444,23,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,15,0.033936651583710405,0,3,false,REFACTOR roc_curve and precision_recall_curve After getting caught on some of the quirks of roc_curve* I decided it and its cousin precision_recall_curve could be neater especially seeing as they share the need to calculate true positives and false negatives at each thresholdThis is my attempt at cleaning things up(* Im pretty sure that and should be or in https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/metrics/metricspy#L662 or else the following two if clauses will never get used),,1038,0.8092485549132948,0.5135746606334841,30056,386.31221719457017,35.23422943838169,106.60101144530209,2282,27,980,93,travis,jnothman,jnothman,true,jnothman,12,0.5833333333333334,10,1,1456,true,true,false,false,2,48,13,36,10,0,776
1735433,scikit-learn/scikit-learn,python,1886,1366713461,1366794199,1366794199,1345,1345,github,false,false,false,27,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.542118653290769,0.1267775899880927,14,peter.prettenhofer@gmail.com,doc/sphinxext/gen_rst.py,14,0.03017241379310345,0,0,false,FIX gen_rstpy was something using an undefined variable erow_docstring was undefinedIm not sure this patch should go in scikit learn or to another projectThanksNelle,,1037,0.8090646094503375,0.49353448275862066,30056,386.31221719457017,35.23422943838169,106.60101144530209,2280,27,980,78,travis,NelleV,NelleV,true,NelleV,18,0.8888888888888888,35,13,1191,true,true,false,false,0,14,3,1,3,0,90
1326436,scikit-learn/scikit-learn,python,1885,1366696343,,1389489443,379825,,unknown,false,false,false,28,5,2,6,25,0,31,0,6,4,1,4,10,7,0,0,4,1,5,10,9,0,0,947,133,1105,151,46.51856685405457,1.2984054899543973,25,vlad@vene.ro,doc/modules/decomposition.rst|examples/decomposition/plot_pca_vs_ccipca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/ccipca.py|sklearn/decomposition/ipca.py|sklearn/decomposition/tests/test_ipca.py|doc/modules/decomposition.rst|examples/decomposition/plot_faces_decomposition.py|examples/decomposition/plot_pca_vs_ccipca.py|sklearn/decomposition/__init__.py|sklearn/decomposition/ccipca.py|sklearn/decomposition/tests/test_ccipca.py,19,0.0,0,4,false,IPCA and CCIPCA WIP - I expect some more work will be required before this can be merged wanted to make a PR now to get some feedback,,1036,0.8098455598455598,0.4924731182795699,30056,386.31221719457017,35.23422943838169,106.60101144530209,2280,27,980,160,travis,pickle27,pickle27,true,,1,1.0,10,20,285,false,true,false,false,0,2,1,2,0,0,439
1401750,scikit-learn/scikit-learn,python,1884,1366674934,1367670620,1367670620,16594,16594,github,false,false,false,34,16,1,0,36,0,36,0,6,2,0,0,16,0,1,1,13,7,7,27,7,3,7,0,0,875,0,5.304056749432992,0.1480444662901995,0,,doc/tutorial/machine_learning_map/drop_shadows.html|doc/tutorial/machine_learning_map/drop_shadows.png,0,0.0,0,16,false,MRG - Machine Learning flowchart for Tutorials page ~~Dont review yet Need to change a thing or two but I just want to get it up~~This addresses issue #1755 Update: Ready for reviews,,1035,0.8096618357487922,0.4893162393162393,30056,386.31221719457017,35.23422943838169,106.60101144530209,2280,27,979,86,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,44,0.9318181818181818,10,13,453,true,true,false,false,5,78,11,20,13,0,958
1328008,scikit-learn/scikit-learn,python,1883,1366624308,1368350442,1368350442,28768,28768,commit_sha_in_comments,false,false,false,72,5,1,2,15,0,17,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,9,34,21,9.191570129453876,0.2565504287874212,23,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,15,0.029880478087649404,0,1,false,FIX bug where hinge_loss( neg_label1) would produce incorrect results The following is bad (consider neg_label  1 or y_truedtype  S1 for that matter):pythony_true[y_true  pos_label]  1y_true[y_true  neg_label]  -1The following are good:pythony_true  (y_true  pos_label) * 2 -1or pythonorig  y_truey_true  nprepeat(-1 origshape)y_true[orig  pos_label]  1Hence I have deprecated neg_label in hinge_loss,,1034,0.809477756286267,0.45617529880478086,30056,386.31221719457017,35.23422943838169,106.60101144530209,2280,28,979,92,travis,jnothman,jnothman,true,jnothman,11,0.5454545454545454,10,1,1455,true,true,false,false,2,45,12,35,9,0,336
1324250,scikit-learn/scikit-learn,python,1881,1366566345,1366904608,1366904608,5637,5637,merged_in_comments,false,false,false,8,3,1,19,0,0,19,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,77,0,4.424875090021548,0.12350486213863615,6,peter.prettenhofer@gmail.com,sklearn/linear_model/ridge.py,6,0.011538461538461539,0,0,false,Ridge regression now uses compute_class_weight() Closes issue #1494,,1033,0.8092933204259438,0.4519230769230769,30056,386.31221719457017,35.23422943838169,106.60101144530209,2279,27,978,79,travis,kemaleren,GaelVaroquaux,false,GaelVaroquaux,1,1.0,5,0,756,false,true,false,false,0,1,1,0,0,0,259
1395476,scikit-learn/scikit-learn,python,1880,1366559723,1368357614,1368357614,29964,29964,github,false,false,false,49,2,2,0,3,0,3,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,66,15,66,15,8.909247772597144,0.24867039080829811,3,peter.prettenhofer@gmail.com,sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_image.py,3,0.005703422053231939,0,0,false,Patch extractor doesnt support float values for max patches I got this error when using max_patches01 with PatchExtractor:ValueError: could not broadcast input array from shape (10363277) into shape (077)I solved the problem with an auxiliar function to avoid code duplication and I added a test,,1032,0.8091085271317829,0.44866920152091255,30056,386.31221719457017,35.23422943838169,106.60101144530209,2279,27,978,89,travis,NicolasTr,jnothman,false,jnothman,2,0.5,2,0,1005,false,true,false,false,0,5,2,0,1,0,17249
1395404,scikit-learn/scikit-learn,python,1879,1366555588,1367066774,1367066774,8519,8519,merged_in_comments,false,false,false,28,3,2,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,46,32,46,33,13.678294374777385,0.381781593080029,13,stefano.lattarini@gmail.com,sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,10,0.019011406844106463,0,3,false,check_pairwise_arrays() preserves type if it is float32 This pull request closes issue #1520I made the appropriate changes to check_pairwise_arrays() used it in additive_chi2_kernel() and added some tests,,1031,0.8089233753637245,0.44866920152091255,30056,386.31221719457017,35.23422943838169,106.60101144530209,2279,27,978,81,travis,kemaleren,amueller,false,amueller,0,0,5,0,756,false,true,false,false,0,0,0,0,0,0,78
1395025,scikit-learn/scikit-learn,python,1878,1366545099,,1396346088,496623,,unknown,false,false,false,53,20,13,3,3,0,6,0,3,0,0,10,12,10,0,0,0,0,12,12,11,0,0,115,86,246,125,58.364928439791804,1.6290521865675418,188,vlad@vene.ro,sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/hmm.py|sklearn/hmm.py|sklearn/hmm.py|sklearn/tests/test_hmm.py|sklearn/lda.py|sklearn/qda.py|sklearn/multiclass.py|sklearn/ensemble/forest.py|sklearn/neighbors/classification.py|sklearn/semi_supervised/label_propagation.py|sklearn/hmm.py,63,0.05303030303030303,0,0,false,[MRG] preprocessingnormalize_proba Deprecated the HMMnormalize and instead used preprocessingnormalize_probaChanged the usage of normalizations across the code to instead use preprocessingnormalize_probaAll tests passes didnt manage to get the benchmark to work properly so I dont know the performance impactIn response to: https://githubcom/scikit-learn/scikit-learn/issues/1862Not sure about the name adopted it from predict_proba,,1030,0.8097087378640777,0.44696969696969696,30056,386.31221719457017,35.23422943838169,106.60101144530209,2278,27,978,177,travis,Jim-Holmstroem,arjoly,false,,1,0.0,16,39,655,false,true,false,false,0,4,2,0,0,0,16
1322552,scikit-learn/scikit-learn,python,1875,1366380053,1366652204,1366652204,4535,4535,merged_in_comments,false,false,false,26,3,1,7,2,0,9,0,6,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.507742564684346,0.1258181143303662,11,stefano.lattarini@gmail.com,doc/developers/index.rst,11,0.02022058823529412,0,0,true,Add bit more instruction on writing docs to Contributing section Just a little bit more detail on writing documentation and things to remember when doing so,,1029,0.8095238095238095,0.43014705882352944,30031,386.6338117278812,35.263560986980124,106.68975392094836,2277,27,976,75,travis,jaquesgrobler,,false,,43,0.9302325581395349,10,13,450,true,true,false,false,7,93,17,17,28,0,208
1388523,scikit-learn/scikit-learn,python,1874,1366361656,1366847192,1366847192,8092,8092,merged_in_comments,false,false,false,25,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,204,0,204,0,9.474069683489205,0.26443603766324913,3,stefano.lattarini@gmail.com,sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx,3,0.0055248618784530384,1,0,true,Release GIL around sparse liblinear training Enhancement to #1857 Thanks to @jnothman for pointing out that Id missed one function to wrap with with nogil,,1028,0.8093385214007782,0.430939226519337,30031,386.6338117278812,35.263560986980124,106.68975392094836,2277,27,976,79,travis,ihaque,,false,,1,0.0,11,0,507,false,false,false,false,0,5,1,0,1,0,1856
1388279,scikit-learn/scikit-learn,python,1873,1366354607,1367256091,1367256091,15024,15024,merged_in_comments,false,false,false,41,1,1,0,2,0,2,0,3,0,0,3,3,3,0,0,0,0,3,3,3,0,0,99,0,99,0,8.592454733883752,0.23982879158982875,24,stefano.lattarini@gmail.com,sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx,21,0.0055248618784530384,0,1,true,REFACTOR combine train_wrap and csr_train_wrap The dense and sparse liblinear train wrappers were almost entirely identical It seems as a result that csr_train_wrap missed out on #1857 and this refactored form would make implementing parameter changes like #1867 a little simpler,,1027,0.8091528724440117,0.430939226519337,30031,386.6338117278812,35.263560986980124,106.68975392094836,2277,27,976,78,travis,jnothman,larsmans,false,larsmans,10,0.5,10,1,1452,true,true,false,false,2,42,11,24,8,0,1978
1320150,scikit-learn/scikit-learn,python,1867,1366265120,,1417650381,856361,,unknown,false,true,false,5,34,2,41,18,3,62,0,4,0,0,9,12,9,0,0,0,0,12,12,12,0,0,1623,29,2402,307,82.06089675554809,2.2826319497698893,93,vlad@vene.ro,sklearn/linear_model/base.py|sklearn/svm/__init__.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/tests/test_svm.py|sklearn/linear_model/base.py|sklearn/svm/__init__.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/tests/test_svm.py,43,0.01303538175046555,0,6,false,ENH: Add LinearSVR Fix #1473,,1026,0.8099415204678363,0.4301675977653631,30103,385.70906554164037,35.179218018137725,106.43457462711358,2275,27,975,235,travis,luoq,larsmans,false,,0,0,4,7,604,true,true,false,false,0,0,0,0,1,0,136
1383437,scikit-learn/scikit-learn,python,1866,1366260410,1366651576,1366651576,6519,6519,merged_in_comments,false,false,false,32,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.897263973968665,0.13622384906099813,0,,doc/modules/hmm.rst,0,0.0,0,0,false,small change to hmm documentation (believe predict should use model2 not model) Should use model2predict(X) instead of modelpredict(X)  The predicted states though will have a different sort order than the generating model,,1025,0.8097560975609757,0.4301675977653631,30103,385.70906554164037,35.179218018137725,106.43457462711358,2275,27,975,77,travis,stevekochscience,GaelVaroquaux,false,GaelVaroquaux,0,0,21,24,678,false,true,false,false,0,0,0,0,0,0,290
1378173,scikit-learn/scikit-learn,python,1864,1366157103,,1366192248,585,,unknown,false,false,false,16,3,3,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,2,5,2,13.265499098140026,0.36897598672033693,10,stefano.lattarini@gmail.com,sklearn/hmm.py|sklearn/hmm.py|sklearn/tests/test_hmm.py,9,0.015625,0,0,false,Hmm normalization inplace My very first pull request in response to the first point in https://githubcom/scikit-learn/scikit-learn/issues/1862,,1024,0.810546875,0.4010416666666667,30102,385.7218789449206,35.180386685270086,106.4381104245565,2274,28,973,70,travis,Jim-Holmstroem,Jim-Holmstroem,true,,0,0,16,38,650,false,true,false,false,0,1,0,0,0,0,546
1316701,scikit-learn/scikit-learn,python,1861,1366054743,1366216294,1366216294,2692,2692,commit_sha_in_comments,false,false,false,63,2,1,6,2,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,26,0,27,0,4.8765133084378025,0.13597197767189656,2,peter.prettenhofer@gmail.com,examples/decomposition/plot_pca_3d.py,2,0.00333889816360601,0,0,false,modified the plot_pca_3d example to actually use scikit-learns pca class Seems a bit silly that this example does PCA manually using scipy-linalg rather than the scikit-learn PCA class I left the manual SVD approach in the example it is labelled and commented outI also updated the description of this example to be a bit more descriptive about what exactly is being shown,,1023,0.8103616813294232,0.39232053422370616,30088,391.18585482584416,35.69529380483914,108.3820792342462,2272,27,972,76,travis,pickle27,larsmans,false,larsmans,0,0,9,20,277,false,true,false,false,0,0,0,0,0,0,7
1315563,scikit-learn/scikit-learn,python,1859,1365963436,1366064349,1366064349,1681,1681,merged_in_comments,false,false,false,69,3,3,2,1,0,3,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,33,16,33,16,13.689892371881381,0.38171580925509174,47,stefano.lattarini@gmail.com,doc/modules/cross_validation.rst|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py,31,0.03442622950819672,1,0,false,FIX ENH LeaveOneLabelOut and LeavePLabelOuts handling of labels Ive fixed LeaveOneLabelOut and LeavePLabelsOuts handling of labels as discussed with @larsmans:https://githubcom/scikit-learn/scikit-learn/pull/1775#discussion-diff-3373086Here is the summary of changes:- copying of labels is done in __init__ instead of __iter__- redundant calls to unique function are removed- a test case is added that checks if LeaveOneLabelOut and LeavePLabelsOut handle changing of labels variable properly (current master fails the test),,1022,0.8101761252446184,0.38524590163934425,30090,391.1598537720173,35.69292123629113,108.37487537387837,2272,27,971,76,travis,tjanez,agramfort,false,agramfort,9,0.7777777777777778,3,3,187,true,true,false,false,2,21,5,7,3,0,12
1368790,scikit-learn/scikit-learn,python,1858,1365955316,1366115431,1366115431,2668,2668,merged_in_comments,false,false,false,34,4,3,0,1,0,1,0,2,2,0,5,7,5,0,0,2,0,5,7,5,0,0,279,162,281,162,40.06844634902694,1.1172304365534165,29,peter.prettenhofer@gmail.com,sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/__init__.py|sklearn/tree/export.py|sklearn/tree/tests/test_export.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/export.py|sklearn/tree/tests/test_export.py,25,0.0,0,0,false,MRG: Give export_graphviz some love This solves both #1851 and #1844I also moved export_graphviz into sklearn/tree/exportpy This does not change anything for the users but I find this clearer for us as developers ,,1021,0.8099902056807052,0.38235294117647056,30090,391.1598537720173,35.69292123629113,108.37487537387837,2271,27,971,75,travis,glouppe,larsmans,false,larsmans,31,1.0,98,23,885,true,true,false,false,16,83,22,27,27,0,1329
1367738,scikit-learn/scikit-learn,python,1857,1365907541,,1366125382,3630,,unknown,false,false,false,121,7,1,0,11,0,11,0,6,0,0,3,8,3,0,0,0,0,8,8,8,0,0,28,0,963,0,9.212846047300559,0.25688227085028265,4,stefano.lattarini@gmail.com,sklearn/svm/libsvm.c|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx,4,0.0031847133757961785,0,4,false,Release GIL for time-consuming operations in libsvm Currently training an SVM model or using one to make predictions locks up the interpreter thread while libsvm chugs along in CThis patch releases the GIL around the time consuming operations svm_train svm_predict svm_predict_values and svm_predict_probability (The latter three are wrapped in libsvm_helper I release the GIL around the wrapper function)This allows Python code to continue running in my particular case it lets me maintain a GUI in a main thread run sklearn code in a compute thread and still have a responsive UI Without the patch the UI blocks waiting for libsvm operations to run (It also would let you parallelize libsvm across Threads but thats not the main point here),,1020,0.8107843137254902,0.37420382165605093,30088,391.18585482584416,35.69529380483914,108.3820792342462,2271,28,970,74,travis,ihaque,larsmans,false,,0,0,11,0,501,false,false,false,false,0,0,0,0,1,0,2
1712236,scikit-learn/scikit-learn,python,1852,1365715615,1365716110,1365716110,8,8,github,false,false,false,12,1,1,0,1,0,1,0,2,0,0,112,112,97,0,0,0,0,112,112,97,0,0,278,26,278,26,500.01545279146484,13.941957291028393,308,vlad@vene.ro,doc/datasets/index.rst|doc/developers/index.rst|doc/developers/performance.rst|doc/modules/decomposition.rst|doc/modules/dp-derivation.rst|doc/modules/ensemble.rst|doc/modules/feature_extraction.rst|doc/modules/gaussian_process.rst|doc/modules/kernel_approximation.rst|doc/modules/lda_qda.rst|doc/modules/linear_model.rst|doc/modules/model_evaluation.rst|doc/modules/outlier_detection.rst|doc/modules/sgd.rst|examples/applications/plot_outlier_detection_housing.py|examples/cluster/plot_digits_agglomeration.py|examples/cluster/plot_kmeans_digits.py|examples/cluster/plot_segmentation_toy.py|examples/covariance/plot_mahalanobis_distances.py|examples/decomposition/plot_sparse_coding.py|examples/document_clustering.py|examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_random_forest_embedding.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_iris_logistic.py|examples/linear_model/plot_multi_task_lasso_support.py|examples/linear_model/plot_sgd_iris.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_manifold_sphere.py|examples/neighbors/plot_classification.py|examples/neighbors/plot_nearest_centroid.py|examples/plot_classifier_comparison.py|examples/plot_kernel_approximation.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|examples/svm/plot_custom_kernel.py|examples/svm/plot_iris.py|examples/svm/plot_svm_iris.py|examples/svm/plot_svm_margin.py|examples/svm/plot_svm_scale_c.py|examples/tree/plot_tree_regression.py|sklearn/_hmmc.c|sklearn/_isotonic.c|sklearn/cluster/_hierarchical.c|sklearn/cluster/_k_means.c|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/shrunk_covariance_.py|sklearn/cross_validation.py|sklearn/datasets/DATASET_PROPOSAL.txt|sklearn/datasets/_svmlight_format.c|sklearn/datasets/lfw.py|sklearn/datasets/species_distributions.py|sklearn/datasets/tests/test_lfw.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/partial_dependence.py|sklearn/externals/joblib/func_inspect.py|sklearn/externals/joblib/memory.py|sklearn/externals/joblib/numpy_pickle.py|sklearn/externals/joblib/parallel.py|sklearn/externals/joblib/test/test_hashing.py|sklearn/externals/joblib/test/test_memory.py|sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/text.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/hmm.py|sklearn/linear_model/base.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/manifold/spectral_embedding.py|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/cluster/supervised.py|sklearn/metrics/metrics.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.c|sklearn/neighbors/ball_tree.c|sklearn/pls.py|sklearn/preprocessing.py|sklearn/svm/classes.py|sklearn/svm/liblinear.c|sklearn/svm/libsvm.c|sklearn/svm/libsvm_sparse.c|sklearn/svm/setup.py|sklearn/svm/src/libsvm/svm.cpp|sklearn/tests/test_pipeline.py|sklearn/tree/_tree.c|sklearn/utils/arpack.py|sklearn/utils/arraybuilder.c|sklearn/utils/arrayfuncs.c|sklearn/utils/class_weight.py|sklearn/utils/graph_shortest_path.c|sklearn/utils/murmurhash.c|sklearn/utils/random.c|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsetools/csgraph_wrap.cxx|sklearn/utils/src/gamma.c|sklearn/utils/weight_vector.c,37,0.0029850746268656717,0,0,false,COSMIT various typofixes As suggested by codespell https://githubcom/lucasdemarchi/codespellSigned-off-by: Stefano Lattarini stefanolattarini@gmailcom,,1018,0.8113948919449901,0.35074626865671643,30088,391.18585482584416,35.69529380483914,108.3820792342462,2271,29,968,72,travis,slattarini,glouppe,false,glouppe,0,0,2,0,425,true,false,false,false,0,0,0,0,1,0,8
1355162,scikit-learn/scikit-learn,python,1849,1365620385,1366114522,1366114522,8235,8235,commit_sha_in_comments,false,false,false,67,1,1,0,6,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,31,8,31,9.181750364672522,0.25601567643608614,16,peter.prettenhofer@gmail.com,sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_svmlight_format.py,13,0.018950437317784258,0,2,false,reduce size of files created by dump_svmlight_file (tests included pep8/pyflakes pass) by (1) printing integers as integers and (2) using printf type g instead of e     %16e % 987     98700000000000003e+01 # not very concise     %16g % 987    987 # nice     %16g % 1000000000000001    1000000000000001     %16g % 10000000000000001    1     %16e % 10000000000000001    10000000000000000e+00 # precision works the same way in g and e,,1017,0.8112094395280236,0.34110787172011664,30081,391.27688574183037,35.70360027924604,108.40730028921911,2271,30,967,75,travis,seamusabshere,larsmans,false,larsmans,0,0,80,63,1743,false,false,false,false,0,0,0,0,0,0,5537
1433753,scikit-learn/scikit-learn,python,1847,1365523945,1367664103,1367664103,35669,35669,merged_in_comments,false,false,false,16,2,1,0,4,0,4,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,4,13,3.828565307388033,0.10675095893447593,11,peter.prettenhofer@gmail.com,sklearn/multiclass.py,11,0.015363128491620111,0,0,false,FIX scores calculation in ovo multiclass Fixes a typo that breaks scores calculation in ovo multiclass,,1016,0.8110236220472441,0.3268156424581006,30068,391.3795397099907,35.719036849807104,108.42091259811095,2270,30,966,90,travis,flxb,amueller,false,amueller,0,0,1,1,844,false,true,false,false,0,0,0,0,1,0,44
1349809,scikit-learn/scikit-learn,python,1846,1365510094,1366216558,1366216558,11774,11774,merged_in_comments,false,false,false,12,3,1,0,5,0,5,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,28,0,4.708059789352219,0.1312737418062606,3,peter.prettenhofer@gmail.com,examples/plot_confusion_matrix.py,3,0.004201680672268907,0,1,false,DOC Add labels and some explanation to Confusion Matrix example Issue #1841,,1015,0.8108374384236453,0.32492997198879553,30068,391.3795397099907,35.719036849807104,108.42091259811095,2270,30,966,71,travis,rolisz,larsmans,false,larsmans,2,1.0,22,11,918,false,true,false,false,1,7,2,1,1,0,1403
1349722,scikit-learn/scikit-learn,python,1845,1365508838,1366066801,1366066801,9299,9299,merged_in_comments,false,false,false,10,3,3,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,7.930754560709439,0.2211313944835521,3,peter.prettenhofer@gmail.com,sklearn/linear_model/bayes.py|sklearn/linear_model/bayes.py,3,0.004207573632538569,0,0,false,ARDRegression bug fix For issue #1843 added the lambda_ attribute ,,1014,0.8106508875739645,0.32398316970546986,30068,391.3795397099907,35.719036849807104,108.42091259811095,2270,30,966,76,travis,rolisz,agramfort,false,agramfort,1,1.0,22,11,918,false,true,false,false,1,7,1,1,0,0,1292
1341921,scikit-learn/scikit-learn,python,1842,1365302655,,1407683783,706292,,unknown,false,false,false,124,4,2,0,1,0,1,0,1,0,0,2,3,2,0,0,0,0,3,3,3,0,0,83,67,95,86,18.45126773321997,0.5144758669500992,115,vlad@vene.ro,sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,90,0.11642949547218628,0,0,false,ENH method to index ParameterGrid points by parameter values This extension intends to solve #1020 for regular grids: it provides an index into the grid points output by ParameterGrid such that results from GridSearchCV (particularly when returned as an array as in #1787) can be accessed by parameter valueFor example I could get the mean test_score for each max_depth value:pythongrid_search  GridSearchCV(clf {max_depth: range(1 5) max_features: range(1 3)})grid_searchfit()grid  ParameterGrid(grid_searchparam_grid) # should we make it easier to get the ParameterGridkeys index  gridbuild_index([max_depth] ravelTrue)indexed  nparray(candidatemean_validation_score for candidate in [grid_searchcv_scores_])[index]print(indexedmean(axis1))This PR aims to account for most of the functionality of #1034 within the current parameter search framework The examples from there should be adopted,,1013,0.8114511352418559,0.3027166882276843,30069,391.3665236622435,35.71784894742093,108.41730686088663,2269,31,963,199,travis,jnothman,jnothman,true,,9,0.5555555555555556,10,1,1439,true,true,false,false,2,24,10,8,5,0,502
1334537,scikit-learn/scikit-learn,python,1840,1365113977,1367675612,1367675612,42693,42693,merged_in_comments,false,false,false,56,1,1,0,6,0,6,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,3,26,3,26,8.768918559506615,0.24450369159245208,51,peter.prettenhofer@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py,37,0.04630788485607009,0,0,false,MRG fix broken scorer add regression test Closes #1831At least the bug partI still think if a specific scoring is given this should take precedence over the score method of the base estimator The problem is that this change would not be backward compatible :-/One might consider the current behavior a bug though,,1012,0.8112648221343873,0.29411764705882354,30072,391.3274807129556,35.714285714285715,108.40649108805533,2268,31,961,88,travis,amueller,amueller,true,amueller,170,0.8647058823529412,597,33,895,true,true,false,false,66,908,73,194,194,6,800
1334982,scikit-learn/scikit-learn,python,1839,1365112318,1365122718,1365122718,173,173,github,false,false,false,35,1,1,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,27,0,27,0,9.194254773053723,0.2563633381012543,4,peter.prettenhofer@gmail.com,examples/cluster/plot_dbscan.py|sklearn/cluster/dbscan_.py,3,0.0037546933667083854,0,1,false,MRG correct / simplify dbscan examle Somehow the results with 1 - D  and D looked the same which I find quite disturbingI now switched to the more standard interface which uses D,,1011,0.811078140454995,0.29411764705882354,30072,391.3274807129556,35.714285714285715,108.40649108805533,2268,31,961,68,travis,amueller,ogrisel,false,ogrisel,169,0.863905325443787,597,33,895,true,true,true,false,65,906,72,194,194,6,0
1345918,scikit-learn/scikit-learn,python,1836,1365023359,1365429713,1365429713,6772,6772,github,false,false,false,7,1,1,0,1,0,1,0,1,0,0,16,16,16,0,0,0,0,16,16,16,0,0,85,0,85,0,75.24536628341303,2.0980659937336523,259,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/feature_selection/univariate_selection.py|sklearn/grid_search.py|sklearn/linear_model/base.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/linear_model/stochastic_gradient.py|sklearn/naive_bayes.py|sklearn/neighbors/base.py|sklearn/random_projection.py|sklearn/semi_supervised/label_propagation.py|sklearn/svm/base.py|sklearn/tree/tree.py,100,0.023227383863080684,0,0,false,Fix ABCMeta under Python 3x See #1829,,1010,0.810891089108911,0.2982885085574572,30072,391.3274807129556,35.714285714285715,108.40649108805533,2268,31,960,68,travis,kmike,larsmans,false,larsmans,7,1.0,334,0,1350,true,true,false,true,4,28,7,3,3,0,183
1304062,scikit-learn/scikit-learn,python,1833,1365012676,1366932225,1366932225,31992,31992,merged_in_comments,false,false,false,59,7,4,5,3,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,227,0,298,0,17.0772563653899,0.47616501342480977,20,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,20,0.023980815347721823,1,1,false,MRG: Better enet cv Less computations and better memory usage in ElasticNetCVThe fact that the coordinate descent cython code has to cast data into float64 combined with parallel computing puts a lot of stress on the memory footprint of this code in our applicationsThe code should be ready for merge All tests pass @agramfort  please review,,1009,0.8107036669970268,0.29256594724220625,30072,391.3274807129556,35.714285714285715,108.40649108805533,2268,31,960,81,travis,GaelVaroquaux,,false,,28,0.6428571428571429,341,3,1136,true,true,false,false,31,253,17,93,70,0,18
1375770,scikit-learn/scikit-learn,python,1828,1364925443,1366116119,1366116119,19844,19844,commit_sha_in_comments,false,false,false,73,3,3,0,6,0,6,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,126,0,126,0,9.264415962340017,0.2583174115520825,5,vlad@vene.ro,sklearn/_hmmc.c|sklearn/_hmmc.pyx|sklearn/_hmmc.pyx,5,0.005834305717619603,0,1,false,Cython alternative to #1825  Here fit and eval should be about 10-20% faster than #1825 on large HMMs and several times faster on tiny HMMs _decode_viterbi should be on par with #1825_logsum is not moved to _extmath in this pull requestFor the record: I slightly prefer #1825 because of simplicity but if there are use cases for HMMs with small number of parameters but slow training this PR should be preferred,,1007,0.8113207547169812,0.2870478413068845,30068,391.3795397099907,35.719036849807104,108.42091259811095,2266,33,959,72,travis,kmike,larsmans,false,larsmans,6,1.0,334,0,1349,true,true,false,true,3,21,6,3,3,0,25
1325072,scikit-learn/scikit-learn,python,1827,1364908775,1364909739,1364909739,16,16,merged_in_comments,false,false,false,11,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.501766555040502,0.1255219432039522,3,larsmans@gmail.com,doc/install.rst,3,0.0035046728971962616,0,0,false,Use scikit-learn instead of scikits-learn with macports PR for issue #1824,,1006,0.8111332007952287,0.2850467289719626,30066,391.4388345639593,35.721412891638394,108.428124792124,2266,33,959,64,travis,NicolasTr,larsmans,false,larsmans,1,0.0,2,0,986,false,true,false,false,0,5,1,0,1,0,-1
1324791,scikit-learn/scikit-learn,python,1825,1364903855,1366116189,1366116189,20205,20205,merged_in_comments,false,true,false,53,1,1,0,6,0,6,0,4,0,2,2,4,4,0,0,0,2,2,4,4,0,0,217,0,217,0,9.143743738162236,0.2549533540089947,13,vlad@vene.ro,sklearn/_hmmc.c|sklearn/_hmmc.pyx|sklearn/hmm.py|sklearn/setup.py,7,0.005841121495327103,0,0,false,Remove _hmmcpyx rewrite functions from this file as python+numpy On my tests this makes fit (with 100 states) 3x faster eval (with 1000 states) 20% faster decode_viterbi (with 1000 states) 7x fasterIt might be possible to write a better Cython extension so I didnt inline functions extracted from _hmmcpyx to _BaseHMM methods,,1005,0.8109452736318408,0.2850467289719626,30066,391.4388345639593,35.721412891638394,108.428124792124,2266,33,959,72,travis,kmike,larsmans,false,larsmans,5,1.0,334,0,1349,true,true,false,true,3,12,5,3,2,0,100
1301889,scikit-learn/scikit-learn,python,1823,1364837430,1364914568,1364914568,1285,1285,github,false,false,false,49,3,2,5,9,0,14,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,102,0,152,0,8.090766606923172,0.22559369554346148,28,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx,27,0.029069767441860465,0,2,false,PR for Issue #1466 Fixes the crash with addressing large datasets Does not fix having billions of rows/columnsI checked in the _treec cythonized file from cython 180 If you run make cython a lot of c files change How often is it worth pushing out new c files,,1004,0.8107569721115537,0.2837209302325581,30151,390.268979470001,35.5875427017346,108.12245033332228,2264,33,958,64,travis,erg,glouppe,false,glouppe,8,0.75,19,4,1641,true,true,false,false,5,48,3,7,6,0,1
1298842,scikit-learn/scikit-learn,python,1821,1364532037,,1366264253,28870,,unknown,false,false,false,14,16,12,7,13,0,20,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,859,17,1030,34,66.23872384834114,1.8469366836553809,63,vlad@vene.ro,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|doc/modules/feature_extraction.rst|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,34,0.02511415525114155,0,0,true,Parallel CountVectorizer [figure_1_mem](https://fcloudgithubcom/assets/786402/316331/6dabd876-9811-11e2-9f4e-21347b19c829png)[figure_1_transform](https://fcloudgithubcom/assets/786402/316332/6db4c54e-9811-11e2-926f-362e84ac8e34png)[figure_1](https://fcloudgithubcom/assets/786402/316333/6dbd6514-9811-11e2-9182-f703a5976f3bpng)Slightly higher memory usage for multiprocessing but much faster,,1003,0.8115653040877367,0.2773972602739726,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,955,73,travis,lqdc,lqdc,true,,1,1.0,10,1,686,false,true,false,false,0,10,3,6,0,0,6
1298188,scikit-learn/scikit-learn,python,1820,1364501291,1364652590,1364652590,2521,2521,commit_sha_in_comments,false,false,false,54,1,1,2,1,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.775499024599199,0.13315540848110163,6,vlad@vene.ro,doc/modules/svm.rst,6,0.00684931506849315,0,0,false,MRG describe SVM probability calibration After I caught a colleague using SVCpredict_proba where decision_function would have been fine answering some SO/MO questions and reading a bunch of stuff on how SVM probabilities work I tried to improve the docs accordinglyId like a quick check to see if Im not saying anything stupid here,,1002,0.811377245508982,0.2773972602739726,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,954,66,travis,larsmans,larsmans,true,larsmans,66,0.7424242424242424,101,31,984,true,true,false,false,53,305,68,105,189,4,29
1297490,scikit-learn/scikit-learn,python,1816,1364345105,1364379921,1364379921,580,580,github,false,false,false,8,1,1,0,4,0,4,0,3,1,0,0,1,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0.0,5,peter.prettenhofer@gmail.com,setup.py,5,0.0056753688989784334,0,2,false,Make the setuppy executable Just a minor annoyance,,1001,0.8111888111888111,0.2746878547105562,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,952,65,travis,ndawe,glouppe,false,glouppe,3,0.6666666666666666,23,48,1138,true,true,true,false,1,58,1,34,72,2,8
1295770,scikit-learn/scikit-learn,python,1814,1364311409,1364654074,1364654074,5711,5711,merged_in_comments,false,false,false,33,3,1,2,17,0,19,0,4,0,0,5,5,4,0,0,0,0,5,5,4,0,0,64,44,148,93,22.16713992857033,0.6180871479278022,94,vlad@vene.ro,doc/whats_new.rst|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,72,0.01126126126126126,1,3,false,MRG multiclass probability estimates for SGDClassifier As discussed on the ML Not done for lossmodified_huber since I didnt see how to generalize its way of producing probabilities to the multiclass caseCc @pprett,,1000,0.811,0.2713963963963964,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,952,66,travis,larsmans,larsmans,true,larsmans,65,0.7384615384615385,101,31,982,true,true,false,false,47,281,66,99,189,4,990
1294903,scikit-learn/scikit-learn,python,1813,1364299399,,1364304035,77,,unknown,false,false,false,140,1,1,0,2,0,2,0,6,1,1,1,3,2,0,1,1,1,1,3,2,0,1,19,0,19,0,7.477990083285522,0.2085090701552791,25,vlad@vene.ro,sklearn/tree/.gitignore|sklearn/tree/_tree.c|sklearn/tree/setup.py,25,0.0,1,6,false,The auto-generated c files could be removed The git repository contains many c files that could be generated by cython during the build process I did the work for one of them If you agree with the approach Ill do it for the others c filesEverything compiles fine with make I also tested with this script:bash#/bin/bashset -esource /etc/bash_completiond/virtualenvwrappervirtualenv_namesklearn_cythonize mkvirtualenv base_with_numpy_scipy_cythonpip install numpy pip install scipypip install cython deactivate cpvirtualenv base_with_numpy_scipy_cython $virtualenv_name# Test installing with pippip install -U -e git+https://githubcom/NicolasTr/scikit-learngit@cythonize#eggscikit-learn-014 python -c import sklearntree# Test building inplace from the top of the repositorycd $(virtualenvwrapper_derive_workon_home)/$virtualenv_name/src/scikit-learnrm sklearn/tree/_treesopython setuppy build_ext --inplacepython -c import sklearntree# Test building inplace from the pyx file directorycd sklearn/treerm _treesopython setuppy build_ext --inplacepython -c import sklearntree,,999,0.8118118118118118,0.2696629213483146,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,952,66,travis,NicolasTr,larsmans,false,,0,0,2,0,979,false,true,false,false,0,0,0,0,0,0,8
1295136,scikit-learn/scikit-learn,python,1812,1364273130,1364304117,1364304117,516,516,github,false,false,false,5,3,3,0,0,0,0,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,4,40,4,40,13.314258155293782,0.3712419456095246,85,vlad@vene.ro,sklearn/preprocessing.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_common.py,46,0.03258426966292135,0,0,false,Some Python 3x-related testing fixes ,,998,0.811623246492986,0.2696629213483146,30141,397.10029527885604,35.86476891941209,110.28167612222555,2259,35,952,65,travis,kmike,larsmans,false,larsmans,4,1.0,332,0,1342,true,true,false,true,1,3,4,3,2,0,-1
1363446,scikit-learn/scikit-learn,python,1806,1364108199,,1374518319,173502,,unknown,false,true,false,49,1,1,0,25,0,25,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,25,0,25,0,4.096275460511426,0.11421662225015264,19,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py,19,0.020674646354733407,0,11,false,Addressing issue #1802 (Gradient Boosting Out-of-bag Estimates) Changed oob_score_ to be calculated based only on trees where the OOB instances werent used for trainingNot entirely sure about the correctness of doing it this way but it seems to make the OOB scores more reliable (based on local testing) ,,997,0.8124373119358074,0.2589771490750816,30141,397.03394047974524,35.83159151985667,110.28167612222555,2258,35,950,128,travis,yanirs,pprett,false,,0,0,3,0,1,false,false,false,false,1,0,0,0,0,0,220
1291964,scikit-learn/scikit-learn,python,1804,1364002963,1364028336,1364028336,422,422,github,false,false,false,17,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.474271465031608,0.12475633392973975,3,peter.prettenhofer@gmail.com,sklearn/cluster/affinity_propagation_.py,3,0.0031914893617021275,0,0,true,DOC update example path The location of the example has changed so that the documentation was wrong,,996,0.8122489959839357,0.2542553191489362,30141,397.03394047974524,35.83159151985667,110.28167612222555,2258,35,948,65,travis,AlexanderFabisch,mblondel,false,mblondel,0,0,21,18,638,false,true,true,false,0,0,0,0,0,0,-1
1289384,scikit-learn/scikit-learn,python,1803,1363966256,1363984204,1363984204,299,299,github,false,false,false,12,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.322839450034266,0.12053394721512661,4,peter.prettenhofer@gmail.com,sklearn/hmm.py,4,0.004219409282700422,0,0,true,Update outdated comments in sklearnhmm See sklearn 011 release notes and https://githubcom/scikit-learn/scikit-learn/commit/f6deb73249d2ece162b6a75011080ebc1c3022b5,,995,0.8120603015075377,0.2542194092827004,30141,397.03394047974524,35.83159151985667,110.28167612222555,2256,35,948,66,travis,kmike,jaquesgrobler,false,jaquesgrobler,3,1.0,332,0,1338,true,true,false,false,1,3,3,3,2,0,299
1288434,scikit-learn/scikit-learn,python,1801,1363947363,1364898792,1364898792,15857,15857,merged_in_comments,false,false,false,53,2,2,0,7,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,38,21,38,21,17.50665222327279,0.4881388539601708,46,peter.prettenhofer@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,33,0.03477344573234984,0,5,true,Make it possible to pickle a fit GridSearchCV and RandomizedSearchCV Fix for #1789:* dont make best_estimator_s methods available by copying its methods Instead link to its attributes by properties (also added properties for decision_function and transform where predict predict_proba and score had been available)* and dont declare CVScoresTuple in a closure,,994,0.8118712273641852,0.25500526870389884,30141,397.03394047974524,35.83159151985667,110.28167612222555,2256,35,948,67,travis,jnothman,amueller,false,amueller,8,0.5,10,1,1424,true,true,false,false,0,15,9,8,5,0,78
1286122,scikit-learn/scikit-learn,python,1799,1363899190,,1434062873,1169334,,unknown,false,true,false,20,10,9,19,37,0,56,0,6,0,0,5,5,4,0,0,0,0,5,5,4,0,0,964,296,968,296,83.40791561604036,2.3256670562002606,104,vlad@vene.ro,sklearn/preprocessing.py|sklearn/utils/sparsefuncs.pyx|sklearn/utils/sparsefuncs.pyx|sklearn/utils/sparsefuncs.pyx|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/sparsefuncs.pyx|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/sparsefuncs.pyx,80,0.022175290390707498,0,11,false,Global normalization and sparse matrix support for MinMaxScaler tests working & doc in place needs feedback and merge if OK,,993,0.8126888217522659,0.25343189017951423,30141,397.03394047974524,35.83159151985667,110.28167612222555,2256,35,947,319,travis,temporaer,amueller,false,,2,1.0,4,2,1627,true,true,true,true,0,7,1,0,6,0,249
1284354,scikit-learn/scikit-learn,python,1798,1363871530,1364208597,1364208597,5617,5617,github,false,false,false,423,2,2,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,11,4,11,13.098803994090783,0.365252863986715,20,vlad@vene.ro,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,20,0.020964360587002098,0,2,false,SGDClassifier(warm_startTrue) crashes on second call to fit() on multiclass problems Minimum example is at bottom The problem is that selfcoef_ in SGDClassifier is set to F-order and is not unset from it when the chain of calls from the second fit() eventually hits fit_binary() My solution is to make sure that coef_ is set back to C-order before training Adding minimal test case that fails without my change    In [1]: from sklearnlinear_model import SGDClassifier    In [2]: X  nprandomrand(100 10)    In [3]: y  nprandomrandint(3 size100)    In [4]: clf  SGDClassifier(warm_startFalse)    In [5]: clffit(X y)    Out[5]:    SGDClassifier(alpha00001 class_weightNone epsilon01 eta000           fit_interceptTrue l1_ratio015 learning_rateoptimal           losshinge n_iter5 n_jobs1 penaltyl2 power_t05           random_stateNone rhoNone shuffleFalse verbose0           warm_startFalse)    In [6]: clffit(X y)    Out[6]:    SGDClassifier(alpha00001 class_weightNone epsilon01 eta000           fit_interceptTrue l1_ratio015 learning_rateoptimal           losshinge n_iter5 n_jobs1 penaltyl2 power_t05           random_stateNone rhoNone shuffleFalse verbose0           warm_startFalse)    In [7]: clf  SGDClassifier(warm_startTrue)    In [8]: clffit(X y)    Out[8]:    SGDClassifier(alpha00001 class_weightNone epsilon01 eta000           fit_interceptTrue l1_ratio015 learning_rateoptimal           losshinge n_iter5 n_jobs1 penaltyl2 power_t05           random_stateNone rhoNone shuffleFalse verbose0           warm_startTrue)    In [9]: clffit(X y)    ---------------------------------------------------------------------------    ValueError                                Traceback (most recent call last)    ipython-input-9-1a154f38b9fb in module()    ---- 1 clffit(X y)    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/stochastic_gradientpyc in fit(self X y coef_init intercept_init class_weight sample_weight)        522                          coef_initcoef_init intercept_initintercept_init        523                          class_weightclass_weight    -- 524                          sample_weightsample_weight)        525        526    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/stochastic_gradientpyc in _fit(self X y alpha C loss learning_rate coef_init intercept_init class_weight sample_weight)        421        422         self_partial_fit(X y alpha C loss learning_rate selfn_iter    -- 423                           classes sample_weight coef_init intercept_init)        424        425         # fitting is over we can now transform coef_ to fortran order    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/stochastic_gradientpyc in _partial_fit(self X y alpha C loss learning_rate n_iter classes sample_weight coef_init intercept_init)        374             self_fit_multiclass(X y alphaalpha CC        375                                  learning_ratelearning_rate    -- 376                                  sample_weightsample_weight n_itern_iter)        377         elif n_classes  2:        378             self_fit_binary(X y alphaalpha CC    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/stochastic_gradientpyc in _fit_multiclass(self X y alpha C learning_rate sample_weight n_iter)        454                                 n_iter self_expanded_class_weight[i] 1        455                                 sample_weight)    -- 456             for i in range(len(selfclasses_)))        457        458         for i (coef intercept) in enumerate(result):    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/externals/joblib/parallelpyc in __call__(self iterable)        512         try:        513             for function args kwargs in iterable:    -- 514                 selfdispatch(function args kwargs)        515        516             selfretrieve()    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/externals/joblib/parallelpyc in dispatch(self func args kwargs)        309                 310         if self_pool is None:    -- 311             job  ImmediateApply(func args kwargs)        312             index  len(self_jobs)        313             if not _verbosity_filter(index selfverbose):    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/externals/joblib/parallelpyc in __init__(self func args kwargs)        133         # Dont delay the application to avoid keeping the input        134         # arguments in memory    -- 135         selfresults  func(*args **kwargs)        136        137     def get(self):    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/stochastic_gradientpyc in fit_binary(est i X y alpha C learning_rate n_iter pos_weight neg_weight sample_weight)        281                      pos_weight neg_weight        282                      learning_rate_type esteta0    -- 283                      estpower_t estt_ intercept_decay)        284        285    /Users/sergeyk/virtual_envs/system/lib/python27/site-packages/sklearn/linear_model/sgd_fastso in sklearnlinear_modelsgd_fastplain_sgd (sklearn/linear_model/sgd_fastc:7362)()    ValueError: ndarray is not C-contiguous,,992,0.8125,0.25366876310272535,30162,399.5093163583317,36.33711292354618,110.039122074133,2255,36,947,65,travis,sergeyk,pprett,false,pprett,0,0,83,3,1812,false,true,false,false,0,0,0,0,0,0,166
1264282,scikit-learn/scikit-learn,python,1795,1363799526,1367847722,1367847722,67469,67469,github,false,false,false,95,32,5,13,16,0,29,0,5,0,0,1,6,1,0,0,0,0,6,6,3,0,0,80,0,1755,2885,20.151192699766373,0.5619048005954032,62,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,62,0.0641158221302999,0,2,false,[MRG] multilabel accuracy with jaccard the index This pr intends to bring multilabel accuracy and zero-one loss based on the jaccard indexFor reference see section 711 of [Mining Multi-label Data](http://lkmfriuni-ljsi/xaigor/slo/pedagosko/dr-ui/tsoumakas09-dmkdhpdf) and the [Wikipedia entry on Jaccard index](http://enwikipediaorg/wiki/Jaccard_index)TODO list:- [x] Add multilabel accuracy based on jaccard similarity score- [x] write narrative doc for accuracy based on jaccard similarity score- [x] Update whats newThis was removed of the pr scope:- Add multilabel zero-one loss based on jaccard distance- write narrative doc for multilabel zero-one loss based on jaccard distance,,991,0.8123107971745711,0.25542916235780766,30162,399.5093163583317,36.33711292354618,110.039122074133,2254,36,946,84,travis,arjoly,arjoly,true,arjoly,12,0.9166666666666666,15,20,456,true,true,false,false,9,181,14,154,138,2,20
1279812,scikit-learn/scikit-learn,python,1794,1363787707,1363789470,1363789470,29,29,github,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.628912965495858,0.12907461017570906,35,robert.l.marchman@dartmouth.edu,sklearn/feature_extraction/text.py,35,0.03597122302158273,0,0,false,Tiny PY3 hack simplification ,,990,0.8121212121212121,0.2538540596094553,30162,399.5093163583317,36.33711292354618,110.039122074133,2254,36,946,61,travis,kmike,larsmans,false,larsmans,2,1.0,332,0,1336,true,true,false,true,1,3,2,3,2,0,-1
1260185,scikit-learn/scikit-learn,python,1792,1363738616,1364690522,1364690522,15865,15865,commit_sha_in_comments,false,false,false,11,10,2,15,9,2,26,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,86,0,238,17.992113736682608,0.5016996052510385,14,peter.prettenhofer@gmail.com,sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py,10,0.01017293997965412,0,4,false,Added custom kernels to SpectralClustering Added custom kernels as per #1791 ,,989,0.8119312436804853,0.25127161749745675,30139,396.86120972825904,35.90032847805169,108.19867945187299,2253,35,945,66,travis,rolisz,larsmans,false,larsmans,0,0,22,11,897,false,true,false,false,0,2,0,0,0,0,64
1258305,scikit-learn/scikit-learn,python,1790,1363651777,1364311690,1364311690,10998,10998,commit_sha_in_comments,false,false,false,30,57,11,14,10,0,24,0,4,0,0,13,33,10,0,1,0,0,33,33,30,0,1,89,63,180,203,59.85315062838253,1.668989796495526,90,vlad@vene.ro,doc/developers/index.rst|doc/developers/utilities.rst|setup.py|sklearn/datasets/california_housing.py|sklearn/datasets/mldata.py|.gitignore|sklearn/datasets/svmlight_format.py|doc/datasets/mldata_fixture.py|sklearn/datasets/tests/test_mldata.py|sklearn/utils/testing.py|sklearn/cross_validation.py|sklearn/cluster/tests/test_k_means.py|sklearn/datasets/tests/test_base.py,40,0.003913894324853229,0,1,false,MRG: More work porting to Python 3 with single codebase / six We are currently sprinting on fixing the Python 3 supportPlease feel free to review as we go,,988,0.8117408906882592,0.22602739726027396,30138,396.87437786183557,35.90151967615635,108.2022695600239,2253,35,944,67,travis,ogrisel,larsmans,false,larsmans,34,0.7941176470588235,697,121,1391,true,true,true,true,23,273,17,155,46,1,15
1269705,scikit-learn/scikit-learn,python,1787,1363588118,,1371741185,135884,,unknown,false,true,false,273,10,0,8,36,1,45,0,3,0,0,0,6,0,0,0,0,0,6,6,6,0,0,0,0,1590,142,0,0.0,0,,,0,0.0,0,15,false,ENH extensible parameter search results GridSearch and friends need to be able to return more fields in their results (eg #1742 [composite score](http://sourceforgenet/mailarchive/forumphpthread_name513C6BDD8020001%40aisuni-bonnde&forum_namescikit-learn-general))More generally the conceivable results from a parameter search can be classified into:1 per-parameter setting per-fold (currently only the test score for each fold)2 per-parameter setting (currently the parameters and mean test score across folds)3 per-search (best_params_ best_score_ best_estimator_ however best_params_ and best_score_ are redundantly available in grid_scores_ as long as the index of the best parameters is known)Hence this patch changes the output of a parameter search to be (attribute names are open for debate):* grid_results_ (1) a structured array (a numpy array with named fields) with one record per set of parameters* fold_results_ (2) a structured array with one record per fold per set of parameters* best_index_ (3)* best_estimator_ if refitTrue (3)The structured arrays can be indexed by field name to produce an array of values alternatively they can be indexed as an array to produce a single record akin to the namedtuples introduced in 0c94b55 (not in 0131) In any case it allows numpy vectorised operations as used here when calculating the mean score for each parameter setting (in _aggregate_scores)Given this data the legacy grid_scores_ (already deprecated) best_params_ and best_scores_ are calculated as propertiesThis approach is extensible to new fields in particular new fields within fold_results_ records which are compiled from dicts returned from fit_fold (formerly fit_grid_point)This PR is cut back from #1768 there you can see this extensibility exemplified to store training scores training and test times and precision and recall together with F-score,,987,0.8125633232016211,0.21844660194174756,30122,399.4090697828829,36.219374543523,108.69132195737336,2253,35,944,109,travis,jnothman,jnothman,true,,7,0.5714285714285714,10,1,1420,true,true,false,false,0,14,7,8,3,0,25442
1269643,scikit-learn/scikit-learn,python,1786,1363585231,,1363618433,553,,unknown,false,false,false,32,5,5,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,21,11,21,11,20.453536385370576,0.5703411878471182,11,peter.prettenhofer@gmail.com,sklearn/kernel_approximation.py|sklearn/tests/test_kernel_approximation.py|sklearn/kernel_approximation.py|sklearn/tests/test_kernel_approximation.py|sklearn/tests/test_kernel_approximation.py,11,0.010679611650485437,0,0,false,Bugfix nystroem gamma +If gamma is passed then fit and transform use ** the value of gamma+Test cases were added to tests/test_kernel_approximationtest_nystrom_approximation+test_nystrom_approximation was added to if __name__ in test_kernel approximation,,986,0.8133874239350912,0.21844660194174756,30122,399.4090697828829,36.219374543523,108.69132195737336,2253,35,944,62,travis,JakeMick,larsmans,false,,0,0,8,6,532,false,true,false,false,0,1,0,0,0,0,459
1256465,scikit-learn/scikit-learn,python,1784,1363548519,1374854562,1374854562,188434,188434,merged_in_comments,false,true,false,21,5,5,8,13,0,21,0,6,1,0,4,5,0,1,0,1,0,4,5,0,1,0,0,0,0,0,47.06691188629063,1.3124477805570582,16,vlad@vene.ro,doc/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html|doc/user_guide.rst|doc/developers/contributing.rst|doc/developers/index.rst|doc/index.rst|doc/themes/scikit-learn/layout.html|doc/user_guide.rst,11,0.001941747572815534,0,6,false,WIP Website changes Work in progress on the website navigationPlease comment :)You can find the built website athttp://amuellergithubcom/,,985,0.8131979695431472,0.2174757281553398,30122,399.4090697828829,36.219374543523,108.69132195737336,2253,35,943,116,travis,amueller,amueller,true,amueller,168,0.8630952380952381,577,33,877,true,true,false,false,72,1020,95,281,290,7,31
1253434,scikit-learn/scikit-learn,python,1776,1363287280,1366406116,1366406116,51980,51980,commit_sha_in_comments,false,false,false,42,16,2,14,37,0,51,0,7,0,0,1,3,1,0,0,0,0,3,3,2,0,0,289,0,698,4,8.983730717216996,0.25050854368336145,35,robert.l.marchman@dartmouth.edu,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py,35,0.031446540880503145,0,4,false,CountVectorizer Rewrite Rewrote CountVectorizer fit transform  I did some tests and it performs about 40% faster  For example heres the plot on my dataset of 50000 English documents of 150-1000 words in length  It passes all the tests in the sklearn/feature_extraction/tests/test_textpy[countvect_1](https://fcloudgithubcom/assets/786402/259497/40bfd536-8cbf-11e2-89c6-99099cd87f50png),,984,0.8130081300813008,0.18957771787960467,30125,399.23651452282155,36.21576763485477,108.6804979253112,2245,34,940,83,travis,lqdc,larsmans,false,larsmans,0,0,10,1,671,false,true,false,false,0,0,0,0,0,0,64
1253196,scikit-learn/scikit-learn,python,1775,1363280316,1363378181,1363378181,1631,1631,merged_in_comments,false,false,false,146,3,3,3,3,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,43,0,43,0,12.530657130133479,0.3494170435312067,40,vlad@vene.ro,sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/cross_validation.py,40,0.036003600360036005,0,1,false,ENH Various ehancements to the cross_validation module One additional explanation I discovered that in some places arraysize was used and in some places len(array)So I used IPythons %timeit to compare the two and I discovered that len(array) is a bit faster:pythonIn [1]: import numpy as npIn [2]: a  npones(100)In [3]: b  npones(1000000)In [4]: c  npones((10000 10000))In [5]: %timeit asize10000000 loops best of 3: 719 ns per loopIn [6]: %timeit len(a)10000000 loops best of 3: 586 ns per loopIn [7]: %timeit bsize10000000 loops best of 3: 754 ns per loopIn [8]: %timeit len(b)10000000 loops best of 3: 612 ns per loopIn [9]: %timeit csize10000000 loops best of 3: 758 ns per loopIn [10]: %timeit len(c)10000000 loops best of 3: 609 ns per loopIn [11]:,,983,0.8128179043743642,0.18901890189018902,30093,398.03276509487256,36.154587445585356,108.43053201741269,2245,34,940,63,travis,tjanez,larsmans,false,larsmans,8,0.75,3,3,156,true,true,false,false,1,16,4,4,3,0,2
1267103,scikit-learn/scikit-learn,python,1774,1363260998,,1374764533,191725,,unknown,false,true,false,94,2,1,0,12,0,12,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,108,48,132,52,13.377628871428957,0.3730310304265559,63,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/scorer.py|sklearn/metrics/tests/test_score_objects.py,51,0.009009009009009009,0,7,false,ENH annotate metrics to simplify populating SCORERS This patch makes needs_threshold and greater_is_better an attribute of each metric rather than providing them when a Scorer constructed from them I am not sure these are the most appropriate names but their use means information isnt duplicated in scorerpy (It also would allow for composite Scorers to be easily constructed assuming their support in #1768 is granted)Perhaps there should be other machine-readable annotations on our library of metrics:* lower and upper bound* is_symmetric* multiclass/binary/regressionbut their application is not clear to me,,982,0.8136456211812627,0.1873873873873874,30125,399.23651452282155,36.21576763485477,108.6804979253112,2245,34,940,116,travis,jnothman,arjoly,false,,6,0.6666666666666666,10,1,1416,true,true,false,true,0,9,6,7,3,0,142
1255921,scikit-learn/scikit-learn,python,1773,1363253312,1363275428,1363275428,368,368,github,false,false,false,27,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,82,0,82,0,4.324441024825284,0.12058702436466247,49,vlad@vene.ro,sklearn/metrics/metrics.py,49,0.044144144144144144,0,0,false,DOC clarify relationship between pos_label and average parameters The interaction between these two parameters for the PRF family of scores is confusing Hopefully this docstring is clearer,,981,0.8134556574923547,0.1873873873873874,30087,398.11214145644294,36.161797454049925,108.45215541596039,2245,34,940,64,travis,jnothman,larsmans,false,larsmans,5,0.6,10,1,1416,true,true,false,false,0,9,5,7,3,0,99
1685137,scikit-learn/scikit-learn,python,1772,1363233802,1363259912,1363259912,435,435,github,false,false,false,26,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,15,0,15,0,4.39027094894363,0.12242269159148135,30,peter.prettenhofer@gmail.com,sklearn/grid_search.py,30,0.02702702702702703,0,0,false,DOC Allowing a list of param_grids means GridSearchCV is more than grids Its worth making clear that the user can effectively specify any sequence of parameters,,980,0.813265306122449,0.1873873873873874,30087,398.11214145644294,36.161797454049925,108.45215541596039,2245,34,940,63,travis,jnothman,agramfort,false,agramfort,4,0.5,10,1,1416,true,true,false,true,0,9,4,7,3,0,-1
1251355,scikit-learn/scikit-learn,python,1770,1363170102,1363184995,1363184995,248,248,commit_sha_in_comments,false,false,false,113,4,1,0,4,1,5,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,6,0,16,15,4.487120023541489,0.12512438368830645,8,peter.prettenhofer@gmail.com,sklearn/feature_selection/univariate_selection.py,8,0.007213706041478809,0,0,false,ENH allow SelectKBest to select all features in a parameter search Currently it is difficult to perform a grid-search over k for SelectKBest feature selection such that one parameter option is to ignore SelectKBest altogether (ie k  infty) If k  Xshape[1] it throws an error and Xshape is not always known when the parameter grid is constructed This patch allows one to use a parameter grid such as {k: [5 10 20 40 80 SelectKBestSELECT_ALL]} to include bypassing the selector as an optionAn alternative to this is to harness #1769 (together with param_grid being a list of dicts) to simply switch off the SelectKBest component Perhaps neither solution is intuitive,,979,0.8130745658835546,0.18394950405770966,30082,397.74616049464794,36.13456552090952,108.43693903330895,2243,33,939,63,travis,jnothman,larsmans,false,larsmans,3,0.3333333333333333,9,1,1415,false,true,false,false,0,2,3,5,2,0,168
1250441,scikit-learn/scikit-learn,python,1768,1363144392,,1363588130,7395,,unknown,false,true,false,95,13,9,8,4,0,12,0,3,0,0,5,7,4,0,0,0,0,7,7,6,0,0,460,86,616,150,72.06763040086209,2.009622606588417,60,peter.prettenhofer@gmail.com,doc/tutorial/statistical_inference/model_selection.rst|examples/svm/plot_rbf_parameters.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/tests/test_grid_search.py|sklearn/grid_search.py|sklearn/metrics/scorer.py|sklearn/tests/test_grid_search.py,37,0.007220216606498195,0,1,false,Restructure the output attributes of *SearchCV * BaseSearchCV now stores attribs best_index_ grid_results_ and fold_results_ from which best_params_ best_score_ and grid_scores_ are derived and best_estimator_* grid_results_ and fold_results_ are structured arrays allowing for flexible further computation* The contents of these are extensible Currently they may store training scores and times as well as test scores number of test samples parameters for each grid point etc* Multiple scores may be output such as precision and recall as well as the F1 objective* Some refactoring and additional testing includedAn alternative to #1742 ,,977,0.8147389969293757,0.1832129963898917,30082,397.74616049464794,36.13456552090952,108.43693903330895,2243,33,938,67,travis,jnothman,jnothman,true,,1,1.0,9,1,1414,false,true,false,false,0,2,1,0,1,0,10
1249851,scikit-learn/scikit-learn,python,1767,1363135071,1363138319,1363138319,54,54,github,false,false,false,40,3,3,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,0,2,0,4.413474881641889,0.12307061702691668,5,vlad@vene.ro,sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/ball_tree.c,5,0.004488330341113106,0,0,false,Small Fix: Balltree Docstring Example Has Wrong Keyword Arg The class-level docstring for BallTree contains an example that says dist ind  ball_treequery(X[0] n_neighbors3)But in fact the name of the keyword argument is k not n_neighbors,,976,0.8145491803278688,0.1813285457809695,30082,397.74616049464794,36.13456552090952,108.43693903330895,2243,33,938,61,travis,rmcgibbo,amueller,false,amueller,0,0,49,51,744,false,true,false,false,0,0,0,0,0,0,27
1248728,scikit-learn/scikit-learn,python,1765,1363121144,,1363190815,1161,,unknown,false,false,false,133,2,1,0,5,0,5,0,2,0,0,3,4,3,0,0,0,0,4,4,4,0,0,101,0,101,11,8.776823423080454,0.24474345117672108,30,vlad@vene.ro,sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py,25,0.004476275738585497,2,2,false,MRG constant-space cluster reassignment in minibatch k-means I just ran into a MemoryError while clustering batches of 10k samples with 250k features (CSR matrices of course) into 500 clusters with MiniBatchKMeans It turns out that the line    distance_to_centers  npasarray(centers[nearest_center] - X)in the random reassignment creates **two** dense arrays of size equal to my minibatches I rewrote the squared-distance-to-centers code in Cython to take only constant space apart from the size of the output and did the dense version too while I was at itThere was already a test for this which passes but I added an extra one I can now run partial_fit on 24 size-10k batches in less than 2 minutes which includes I/O feature extraction and two reassignments on a low-powered workstation without any troubleCC @pprett @ogrisel,,975,0.8153846153846154,0.18084153983885407,30082,397.74616049464794,36.13456552090952,108.43693903330895,2243,33,938,62,travis,larsmans,larsmans,true,,64,0.75,99,31,968,true,true,false,false,40,200,51,72,140,5,1067
1240331,scikit-learn/scikit-learn,python,1760,1362960735,1362961430,1362961430,11,11,commit_sha_in_comments,false,false,false,6,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.34854182973733,0.12157935239016919,53,vlad@vene.ro,sklearn/metrics/metrics.py,53,0.04431438127090301,0,0,false,Fix comment: returns fbeta_score not f1_score ,,974,0.8151950718685832,0.1605351170568562,30090,397.6071784646062,36.12495845795945,108.40810900631439,2240,34,936,61,travis,jnothman,larsmans,false,larsmans,0,0,9,1,1412,false,true,false,false,0,0,0,0,1,0,-1
1239628,scikit-learn/scikit-learn,python,1758,1362944137,1362948248,1362948248,68,68,github,false,false,false,9,2,2,2,3,0,5,0,2,0,0,4,4,2,0,0,0,0,4,4,2,0,0,36,36,36,36,32.39581501676078,0.9057499162811584,83,vlad@vene.ro,doc/whats_new.rst|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|doc/modules/pipeline.rst|doc/whats_new.rst|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py,76,0.0041946308724832215,0,0,false,MRG removal of components in kernel PCA Fixes #1690,,973,0.8150051387461459,0.15604026845637584,30081,397.36046009108736,36.102523187394034,108.3408131378611,2239,34,936,61,travis,larsmans,larsmans,true,larsmans,63,0.746031746031746,98,31,966,true,true,false,false,38,180,46,67,131,5,2
1233279,scikit-learn/scikit-learn,python,1756,1362771087,1362771892,1362771892,13,13,commits_in_master,false,false,false,42,2,2,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,23,0,23,0,17.521830254138347,0.4898961983736092,25,vlad@vene.ro,sklearn/linear_model/stochastic_gradient.py|sklearn/utils/class_weight.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/class_weight.py,18,0.014754098360655738,0,0,true,BUG fix compute_class_weights issue in SGD Fixes #1749This isnt for review I just want to see if Travis succeeds this timeMy humble apologies for leaving master in a broken state for two days Ill be more careful in the future,,972,0.8148148148148148,0.15,30072,396.21574886938015,36.04682096302208,108.14046288906624,2237,34,934,66,travis,larsmans,larsmans,true,larsmans,62,0.7419354838709677,98,31,964,true,true,false,false,36,175,40,67,127,5,4
1232710,scikit-learn/scikit-learn,python,1754,1362763251,1362961282,1362961282,3300,3300,merged_in_comments,false,false,false,67,4,2,4,11,0,15,0,4,1,0,3,5,3,0,0,2,0,4,6,4,0,0,107,0,652,0,17.59545810137527,0.49195462564981,9,robertlayton@gmail.com,sklearn/_isotonic.pyx|sklearn/_isotonic.pyx|sklearn/isotonic.py|sklearn/setup.py,7,0.003276003276003276,2,5,true,Isotonic speedups This PR aims at speeding up the isotonic regression on large datasetsIve gain a 25 speed up on the following test case::    import numpy as np    from sklearnisotonic import isotonic_regression    seed  0    random_state  nprandomRandomState(seedseed)    n  100000    y  nparange(n) + 50 * random_staterandn(n)    y_fit  isotonic_regression(y)If @agramfort and @fabianp could have a look at this PR that would be great,,971,0.8146240988671473,0.14905814905814907,30072,396.21574886938015,36.04682096302208,108.14046288906624,2237,34,934,66,travis,NelleV,larsmans,false,larsmans,17,0.8823529411764706,32,13,1145,true,true,false,false,2,11,5,3,5,0,120
1232517,scikit-learn/scikit-learn,python,1753,1362760094,1362774177,1362774177,234,234,github,false,false,false,134,1,1,0,6,0,6,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.614898491622646,0.12902878951930452,25,peter.prettenhofer@gmail.com,sklearn/datasets/samples_generator.py,25,0.020475020475020474,0,8,true,FIX make_classification now outputs integer labels Due to a recent very small change (https://githubcom/scikit-learn/scikit-learn/commit/ffad26a299a1ce4738926bac11a4cf93dc) the tests now fail with the following error (at least with a very recent numpy version)My patch fixes the test but do not fix the backward incompatibility we now have Should we tackle that too (or instead of this patch)ERROR: sklearnteststest_commontest_class_weight_auto_classifies----------------------------------------------------------------------Traceback (most recent call last):  File /bioinfo/users/nvaroqua/work/lib/python26/site-packages/nose/casepy line 197 in runTest    selftest(*selfarg)  File /bioinfo/users/nvaroqua/Projects/scikit-learn/sklearn/tests/test_commonpy line 904 in test_class_weight_auto_classifies    classifierfit(X_train y_train)  File /bioinfo/users/nvaroqua/Projects/scikit-learn/sklearn/linear_model/stochastic_gradientpy line 520 in fit    sample_weightsample_weight)  File /bioinfo/users/nvaroqua/Projects/scikit-learn/sklearn/linear_model/stochastic_gradientpy line 419 in _fit    classes sample_weight coef_init intercept_init)  File /bioinfo/users/nvaroqua/Projects/scikit-learn/sklearn/linear_model/stochastic_gradientpy line 360 in _partial_fit    selfclasses_ y)  File /bioinfo/users/nvaroqua/Projects/scikit-learn/sklearn/utils/class_weightpy line 40 in compute_class_weight    counts  bincount(y_ind minlengthlen(classes))TypeError: Cannot cast array data from dtype(float64) to dtype(int64) according to the rule safe----------------------------------------------------------------------Ran 1831 tests in 260294s,,970,0.8144329896907216,0.14905814905814907,30072,396.21574886938015,36.04682096302208,108.14046288906624,2237,34,934,66,travis,NelleV,larsmans,false,larsmans,16,0.875,32,13,1145,true,true,false,false,2,10,4,3,5,0,67
1229330,scikit-learn/scikit-learn,python,1751,1362667436,1362840243,1362840243,2880,2880,merged_in_comments,false,true,false,280,4,2,4,8,0,12,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,26,8,28,8.487633015277169,0.23730728239373244,50,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,39,0.03168155970755483,0,2,false, FIX KFold should return the same result when indicesTrue and when indicesFalse Here is an example of the problem:pythonimport numpy as npfrom sklearncross_validation import KFoldkf_mask  KFold(10 5 indicesFalse shuffleTrue random_state1)kf_ind  KFold(10 5 indicesTrue shuffleTrue random_state1)for (train_mask test_mask) (train_ind test_ind) in zip(kf_mask kf_ind):    print npwhere(train_mask  True)[0]    print train_ind    print npwhere(test_mask  True)[0]    print test_indThe output is:python[0 1 3 4 5 6 7 8][2 9 4 0 3 1 7 8][2 9][6 5][0 1 2 3 5 7 8 9][2 9 6 4 3 7 8 5][4 6][0 1][1 2 4 5 6 7 8 9][9 6 0 3 1 7 8 5][0 3][2 4][0 2 3 4 5 6 8 9][2 6 4 0 3 1 8 5][1 7][9 7][0 1 2 3 4 6 7 9][2 9 6 4 0 1 7 5][5 8][3 8]The output with the fix applied is the following:python[0 1 3 4 5 6 7 8][0 1 3 4 5 6 7 8][2 9][2 9][0 1 2 3 5 7 8 9][0 1 2 3 5 7 8 9][4 6][4 6][1 2 4 5 6 7 8 9][1 2 4 5 6 7 8 9][0 3][0 3][0 2 3 4 5 6 8 9][0 2 3 4 5 6 8 9][1 7][1 7][0 1 2 3 4 6 7 9][0 1 2 3 4 6 7 9][5 8][5 8],,969,0.8142414860681114,0.1454102355808286,30072,396.21574886938015,36.04682096302208,108.14046288906624,2234,34,933,64,travis,tjanez,amueller,false,amueller,7,0.7142857142857143,3,3,149,true,true,false,false,1,8,3,1,3,0,10
1236297,scikit-learn/scikit-learn,python,1750,1362662688,1363786606,1363786606,18731,18731,github,false,false,false,60,5,4,28,17,1,46,0,5,0,0,4,4,2,0,0,0,0,4,4,2,0,0,711,576,786,576,72.83822532547325,2.0364972513842767,163,vlad@vene.ro,doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,81,0.04552845528455285,0,2,false,[MRG] Refactor accuracy score + Fix bugs with 1d array This pull request intends to reduce the amount of redundant code between accuracy_score and zero_one_loss (see issue #1748) In order to achieve this  a normalize option has been added to accuracy_scoreFurthermore  mix representation of 1d vector are now properly handle by all the metrics that comparse y_true to y_pred,,968,0.8140495867768595,0.14471544715447154,30072,396.21574886938015,36.04682096302208,108.14046288906624,2234,34,933,70,travis,arjoly,arjoly,true,arjoly,11,0.9090909090909091,14,19,443,true,true,false,false,9,196,11,171,198,2,14
1726304,scikit-learn/scikit-learn,python,1747,1362617145,,1362618248,18,,unknown,false,false,false,13,1,1,1,2,0,3,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,9,10,9,10,13.139989951288822,0.36737633700099587,32,peter.prettenhofer@gmail.com,sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/utils/class_weight.py,23,0.006436041834271922,1,0,false,FIX weird interface for compute_class_weight Correct fix for #1746 (I think) Thanks @gatagat,,967,0.8148914167528438,0.1415929203539823,30070,395.94280013302296,36.04921849018956,108.01463252411041,2234,34,932,63,travis,amueller,amueller,true,,167,0.8682634730538922,563,33,866,true,true,false,false,72,1032,94,274,400,7,7
1226107,scikit-learn/scikit-learn,python,1746,1362604315,,1362617988,227,,unknown,false,true,false,68,1,1,0,6,0,6,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,9,2,9,9.127592421608103,0.2555378263091263,9,peter.prettenhofer@gmail.com,sklearn/tests/test_multiclass.py|sklearn/utils/class_weight.py,5,0.0040290088638195,0,3,false,Fix automatic setup of class weights Automatic setup of class weights works only when the classes are numbered by {0 1  n-1} For this case the mapping done on sklearn/svm/basepy#141 by a call to unique(y return_inverseTrue) is identityFor classes numbered differently (eg just shifting the target) the class weights setup causes division by zero and subsequent mediocre performance of the classifier (prediction of a single class),,966,0.8157349896480331,0.14020950846091862,30063,395.8686757808602,36.02434886737851,107.97325616205967,2234,34,932,64,travis,gatagat,larsmans,false,,0,0,0,0,316,false,false,false,false,0,0,0,0,0,0,145
1225623,scikit-learn/scikit-learn,python,1745,1362599826,,1363277532,11295,,unknown,false,false,false,122,5,2,1,5,1,7,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,12,0,26,12,8.407479430302903,0.23537740502876048,29,vlad@vene.ro,sklearn/preprocessing.py|sklearn/preprocessing.py,29,0.02333065164923572,0,2,false,Standard scaler sparse matrix with_std default changed to auto Solves   Issue #1724Although I am having following fail in test How should I deal with itFAIL: sklearnteststest_commontest_estimators_overwrite_params----------------------------------------------------------------------Traceback (most recent call last):  File /usr/local/lib/python27/dist-packages/nose/casepy line 187 in runTest    selftest(*selfarg)  File /home/hrishi/scikit-learn/sparse3/sklearn/tests/test_commonpy line 951 in test_estimators_overwrite_params    % (name k v new_params[k]))AssertionError: Estimator StandardScaler changes its parameter with_std from auto to True during fit    Estimator StandardScaler changes its parameter with_std from auto to True during fit  self_formatMessage(Estimator StandardScaler changes its parameter with_std from auto to True during fit %s is not false % safe_repr(True))  raise selffailureException(Estimator StandardScaler changes its parameter with_std from auto to True during fit)    ----------------------------------------------------------------------Ran 1831 tests in 185116sFAILED (SKIP14 failures1),,965,0.816580310880829,0.13998390989541432,30063,395.8686757808602,36.02434886737851,107.97325616205967,2234,34,932,66,travis,hrishikeshio,larsmans,false,,5,0.8,5,12,321,false,true,false,false,5,76,5,0,5,0,9
1226023,scikit-learn/scikit-learn,python,1741,1362508410,1362610040,1362610040,1693,1693,github,false,false,false,72,1,1,3,8,0,11,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,75,0,75,0,4.18121925453229,0.1170583129491284,49,vlad@vene.ro,sklearn/metrics/metrics.py,49,0.03904382470119522,0,3,false,[MRG] FIX numpy 13 issues with the new multilabel metrics With the support of multilabel input for the zero_one_loss accuracy_score and hamming_loss there were 2 bugs with numpy 13:     - npsetxor1d does not make unique by default (and so there were some failures with redundant labels)- there were some failures with 0-dimensionality arrayThis patch intend to solve both of these issues For more information see #1606 and [jenkins console log](https://jenkinsshiningpanda-cicom/scikit-learn/job/python-26-numpy-130-scipy-072/1656/console),,963,0.8172377985462098,0.1362549800796813,30063,395.8686757808602,36.02434886737851,107.97325616205967,2234,34,931,64,travis,arjoly,ogrisel,false,ogrisel,10,0.9,14,19,441,true,true,true,false,9,193,10,162,198,2,92
2016714,scikit-learn/scikit-learn,python,1740,1362506836,1362516790,1362516790,165,165,github,false,false,false,11,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,COSMIT Moved the test_roc_curve_one_label test where other ROC curve tests are ,,962,0.817047817047817,0.1362549800796813,30062,396.3475483999734,36.058811788969464,108.10990619386601,2234,34,931,61,travis,tjanez,amueller,false,amueller,6,0.6666666666666666,3,3,147,true,true,false,false,1,8,2,1,2,0,6
1223721,scikit-learn/scikit-learn,python,1739,1362489564,1362492036,1362492022,40,41,github,false,false,false,87,2,2,0,4,0,4,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,16,46,16,46,23.16481847752779,0.6485252937520134,115,vlad@vene.ro,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|doc/whats_new.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,77,0.0245253164556962,1,0,false,MRG set min_df in fetext back to 1 min_df2 is simply too confusing for users since it causes trivial examples with fewer than three documents (and often with a few more) to fail Many scikit-learn questions on [StackOverflow](http://stackoverflowcom/q/15220961/166749) pertain to this and the fact that Ive been recommending TfidfVectorizer for general-purpose document comparison (rather than learning) makes it worseAs @ogrisel remarked somewhere expert users will know how and when to set this parameterNot sure if there was already an issue about this on the tracker,,961,0.8168574401664932,0.13528481012658228,30062,396.38081298649456,36.058811788969464,108.10990619386601,2234,33,931,61,travis,larsmans,larsmans,true,larsmans,61,0.7377049180327869,97,31,961,true,true,false,false,35,165,36,64,122,5,2
1223703,scikit-learn/scikit-learn,python,1738,1362489297,1362842551,1362842551,5887,5887,merged_in_comments,false,false,false,14,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,3.961165963175691,0.11089732139973815,26,peter.prettenhofer@gmail.com,sklearn/grid_search.py,26,0.020569620253164556,0,0,false,Issue #1711 warnings in grid_searchpy Changed the warning to refer to the parameter scoring  ,,960,0.8166666666666667,0.13528481012658228,30062,396.38081298649456,36.058811788969464,108.10990619386601,2234,33,931,65,travis,archipleago-creature,amueller,false,amueller,0,0,0,0,124,false,false,false,false,0,0,0,0,0,0,-1
1222932,scikit-learn/scikit-learn,python,1736,1362466196,1362484250,1362484250,300,300,github,false,false,false,38,1,1,0,5,0,5,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.157049332733904,0.1163813007690308,14,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py,14,0.011049723756906077,0,1,false,Update coordinate_descentpy Reverting _dense_fit to explicitly check precompute  True rather than just looking at its truthiness (was problem because default value is auto which is meant to have a special meaning)Should prevent unwanted precomputing see https://githubcom/scikit-learn/scikit-learn/issues/1734,,959,0.8164754953076121,0.13338595106550907,30062,396.38081298649456,36.058811788969464,108.10990619386601,2234,33,931,61,travis,jamestwebber,amueller,false,amueller,0,0,1,0,577,false,false,false,false,0,4,0,0,0,0,156
1217876,scikit-learn/scikit-learn,python,1733,1362364040,1362841921,1362841921,7964,7964,merged_in_comments,false,true,false,23,40,13,3,8,0,11,0,4,0,0,18,37,12,0,0,1,0,37,38,30,0,0,808,106,1032,324,111.83573098434667,3.1309740475572148,284,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/ensemble/forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/modules/feature_selection.rst|sklearn/svm/tests/test_sparse.py|doc/modules/classes.rst|doc/modules/grid_search.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/whats_new.rst|examples/grid_search_digits.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_scale_c.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/utils/testing.py|doc/whats_new.rst|doc/modules/clustering.rst,79,0.013996889580093312,0,4,false,ENH Function auc_score should throw an error when y_true doesnt contain two unique class values This is my proposed fix for issue #1722,,957,0.8171368861024033,0.1290824261275272,30065,396.4077831365375,36.0552137036421,107.83302843838351,2232,33,929,67,travis,tjanez,amueller,false,amueller,5,0.6,3,3,145,false,true,false,false,1,6,1,1,1,0,2
1217594,scikit-learn/scikit-learn,python,1732,1362356501,1373827705,1373827705,191186,191186,commits_in_master,false,false,false,426,60,1,50,83,0,133,0,11,11,0,6,46,17,0,0,17,0,35,52,30,0,0,5003,634,10325,1193,59.48477097368384,1.6653467769576884,68,vlad@vene.ro,sklearn/neighbors/__init__.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/binary_tree.pxi|sklearn/neighbors/dist_metrics.c|sklearn/neighbors/dist_metrics.pxd|sklearn/neighbors/dist_metrics.pyx|sklearn/neighbors/kd_tree.c|sklearn/neighbors/kd_tree.pyx|sklearn/neighbors/setup.py|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/tests/test_dist_metrics.py|sklearn/neighbors/tests/test_kd_tree.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/typedefs.c|sklearn/neighbors/typedefs.pxd|sklearn/neighbors/typedefs.pyx,34,0.004640371229698376,0,26,false,MRG: Faster spatial trees with enhancements New Features--------------------- The BallTree class is enhanced to be compatible with 20 different distance metrics  There are also depth first breadth first single tree and dual tree algorithms for the queries  The interface is designed to be fully backward compatible with the old code: all test scripts from previous versions pass on this new version- The same core binary tree code which implements the Ball Tree is used to also implement a KD Tree which can be faster in low dimensions for some datasets- Both BallTree and KDTree now have a method for performing fast kernel density estimates in O[N log(N)] rather than the naive O[N^2]  Six common kernels areimplemented and each kernel can work with any of the valid distance metrics- Both BallTree and KDTree have a method for computing fast 2-point correlation functions also in O[N log(N)] rather than the naive O[N^2]  This works with any of the valid distance metrics- As a bonus there is a set of class interfaces to the various distance metrics with functions to generate a pairwise distance matrix under any of the metrics- All functions are documented and testedRemaining Tasks-------------------------These tasks can be added to this PR or saved for a later one  I wanted to get some feedback on the changes before going forward with these:- Update the NearestNeighbors classes to use the available metrics as well as to use the new KDTree class- Rework the narrative documentation for nearest neighbors to reflect these changes- Create estimator classes to wrap the kernel density estimator functionality and perhaps also the 2-point correlation functionSpeed-----------The the new code is generally faster than the old ball tree implementation and the recent enhancement to the scipy KDTree  Here are some runtimes in seconds for a 2000 pt query on my slightly old linux laptop  The points are in 3 dimensions with each dimension uniformly distributed between 0 and 1  These timings will change in hard-to-predict ways as the distribution of points is changed  Uniform point distributions in low dimensions tend to favor the KD Tree approach2000 points metric  euclidean            Build       KNN         RNN         KDE         2PT        cKDTree      000022     0014      old BallTree 000072     0023       0022      BallTree     00008      0013       0011       0043       026       KDTree       00011      0008       002        0031       031       2000 points metric  minkowski (p3)            Build       KNN         RNN         KDE         2PT        cKDTree      000021     00088     old BallTree 000095     0029       0022      BallTree     000089     0017       0012       0038       018      KDTree       00011      00095      0013       0023       02       ,,956,0.8169456066945606,0.12838360402165508,30065,396.4077831365375,36.0552137036421,107.83302843838351,2231,33,929,117,travis,jakevdp,GaelVaroquaux,false,GaelVaroquaux,30,0.9,907,0,662,false,true,false,false,6,11,0,1,1,0,40
1217077,scikit-learn/scikit-learn,python,1731,1362346577,1363350084,1363350084,16725,16725,merged_in_comments,false,false,false,34,4,1,6,4,0,10,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,6,36,28,108,8.791427607135414,0.2461293312408661,37,peter.prettenhofer@gmail.com,sklearn/grid_search.py|sklearn/tests/test_grid_search.py,26,0.020202020202020204,0,3,false,MRG FIX for iid weighting in grid-search This is a fix for the iid weighting in GridSearchCV It could be that I screwed this up before but didnt notice as there was no test,,955,0.8167539267015707,0.1258741258741259,29925,398.1620718462824,36.157059314954054,108.03675856307434,2231,33,929,70,travis,amueller,agramfort,false,agramfort,165,0.8727272727272727,558,33,863,true,true,true,false,86,1043,89,275,404,7,47
1216805,scikit-learn/scikit-learn,python,1730,1362338350,1362342084,1362342084,62,62,github,false,false,false,78,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.925808208115569,0.1379052565829463,3,peter.prettenhofer@gmail.com,doc/modules/feature_selection.rst,3,0.0023346303501945525,0,0,false,[MRG] DOC FIX foating point issue I fix the following floating point issueI am on 32 bit linux machineFAIL: Doctest: feature_selectionrst----------------------------------------------------------------------Traceback (most recent call last):  File /home/ajoly/opt/python/lib/python27/doctestpy line 2201 in runTest    raise selffailureException(selfformat_failure(newgetvalue()))AssertionError: Failed doctest test for feature_selectionrst  File /home/ajoly/git/scikit-learn/doc/modules/feature_selectionrst line 0----------------------------------------------------------------------File /home/ajoly/git/scikit-learn/doc/modules/feature_selectionrst line 203 in feature_selectionrstFailed example:    clffeature_importances_Expected:    array([ 012604616  007234783  038787583  041373018])Got:    array([ 012526045  007364783  038842821  041266352])  raise selffailureException(selfformat_failure(StringIOStringIO instance at 0x9d726acgetvalue())) ,,954,0.8165618448637316,0.1245136186770428,29925,398.1620718462824,36.157059314954054,108.03675856307434,2231,33,929,61,travis,arjoly,ogrisel,false,ogrisel,9,0.8888888888888888,14,19,439,true,true,true,false,9,196,9,163,219,2,60
1213985,scikit-learn/scikit-learn,python,1728,1362245910,,1362249815,65,,unknown,false,false,false,26,7,7,0,1,0,1,0,2,0,0,4,4,3,0,0,0,0,4,4,3,0,0,24,30,24,30,39.56580255973368,1.1105750793935472,50,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|doc/tutorial/statistical_inference/model_selection.rst|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py,34,0.016845329249617153,1,0,false,FIX : Another attempt to fix the balance of the sizes of the folds returned by KFold Augments the fix by @agramfort in pull request #1726,,953,0.8174186778593914,0.12021439509954059,30288,385.99445324881145,34.70021130480718,103.1431590068674,2231,33,928,63,travis,tjanez,agramfort,false,,4,0.75,3,3,144,false,true,false,false,0,2,0,0,0,0,65
1213820,scikit-learn/scikit-learn,python,1727,1362240358,1362241137,1362241137,12,12,github,false,false,false,28,1,1,0,1,0,1,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,14,20,14,20,13.549949182999336,0.38033413764791113,92,vlad@vene.ro,doc/whats_new.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,71,0.020705521472392636,0,0,false,FIX MinMaxScaler bug Closes #1723Main fix is to replace a / by * Also renamed the variables to be a bit more clear and added a test,,952,0.8172268907563025,0.11886503067484662,30287,385.8090930102024,34.70135701786245,103.04751213391884,2231,33,928,63,travis,amueller,larsmans,false,larsmans,164,0.8719512195121951,557,33,862,true,true,true,true,89,1036,90,259,402,7,1
1213438,scikit-learn/scikit-learn,python,1726,1362224637,1362314324,1362314324,1494,1494,github,false,false,false,100,14,2,3,12,1,16,0,4,0,0,2,5,2,0,0,0,0,5,5,3,0,0,4,20,39,59,17.343819018177932,0.4868230011354574,45,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,34,0.026053639846743294,0,7,false,MRG : fix kfold balance due to int rounding to avoid this: from sklearn import cross_validation list(cross_validationKFold(14 5 indicesTrue shuffleTruerandom_state32))[(array([13  2 12  9  1 10  4  3  8  6  5 11]) array([07])) (array([ 0 13 12  9  1 10  4  3  8  6  5  7])array([ 2 11])) (array([ 0  2 12  9  1 10  4  3  6  5 117]) array([13  8])) (array([ 0 13  2 12  1 10  4  3  8  511  7]) array([9 6])) (array([ 0 13  2  9  8  6 11  7])array([12  1 10  4  3  5]))],,951,0.8170347003154574,0.11800766283524904,30238,386.4342879820094,34.75758978768437,103.21449831338052,2231,33,928,63,travis,agramfort,ogrisel,false,ogrisel,29,0.8620689655172413,99,177,1186,true,true,true,true,8,48,14,18,14,6,87
1209723,scikit-learn/scikit-learn,python,1721,1362146460,1363281115,1363281115,18910,18910,merged_in_comments,false,false,false,20,3,1,15,2,1,18,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.633166477524736,0.13004817715974767,9,vlad@vene.ro,doc/developers/index.rst,9,0.006880733944954129,0,1,true,MRG add roll your own estimator docs This is a small step towards #1241 Better than nothing I think ),,950,0.8168421052631579,0.11697247706422019,30238,386.4342879820094,34.75758978768437,103.21449831338052,2231,34,927,71,travis,amueller,larsmans,false,larsmans,163,0.8711656441717791,554,33,861,true,true,true,true,93,1053,94,260,401,7,144
1206639,scikit-learn/scikit-learn,python,1719,1362083371,1362086298,1362086298,48,48,commits_in_master,false,false,false,44,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,5,0,4.075760181519358,0.11440102984615218,33,vlad@vene.ro,sklearn/cross_validation.py,33,0.025018953752843062,0,0,false,StratifiedKFold: remove pointless copy of labels cross_validationStratifiedKFold__iter__ currently makes a copy of the labels attribute then just argsorts it and never uses the copy There doesnt seem to be any reason to do that presumably this just accidentally survived a rewrite of the function,,949,0.8166491043203372,0.11448066717210008,30239,386.4215086477727,34.75644035847746,103.21108502265287,2230,34,926,62,travis,dougalsutherland,larsmans,false,larsmans,0,0,22,18,1556,false,true,false,false,0,0,0,0,0,0,-1
1205499,scikit-learn/scikit-learn,python,1718,1362064385,1362496938,1362496938,7209,7209,merged_in_comments,false,false,false,29,4,1,12,3,0,15,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.538230655864605,0.12738161132387163,10,noel.dawe@gmail.com,doc/modules/clustering.rst,10,0.007587253414264037,0,0,false,[WIP] Clustering docs: Mini-batch k-means Happy to take comments/suggestionsI need to run spell-check over this first (not working at home for some reason) but should be content complete,,948,0.8164556962025317,0.11305007587253414,30239,386.4215086477727,34.75644035847746,103.21108502265287,2230,34,926,67,travis,robertlayton,,false,,17,0.8235294117647058,7,10,650,true,true,false,false,2,10,6,2,6,0,37
1203620,scikit-learn/scikit-learn,python,1716,1362019264,1371044424,1371044424,150419,150419,github,false,false,false,46,19,4,10,27,2,39,0,4,2,0,6,13,6,0,0,2,0,11,13,8,0,0,770,292,888,396,139.34239297377553,3.9111406820599477,105,vlad@vene.ro,doc/modules/classes.rst|doc/modules/decomposition.rst|examples/document_clustering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_truncated_svd.py|sklearn/decomposition/truncated_svd.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/document_clustering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_truncated_svd.py|sklearn/decomposition/truncated_svd.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/document_clustering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_truncated_svd.py|sklearn/decomposition/truncated_svd.py|sklearn/tests/test_common.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/document_clustering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_truncated_svd.py|sklearn/decomposition/truncated_svd.py|sklearn/tests/test_common.py,65,0.006823351023502654,0,7,false,[MRG] add latent semantic analysis/sparse truncated SVD Reissue of #1519 with LatentSemanticAnalysis renamed TruncatedSVD I didnt touch RandomizedPCA and I dont intend to in this PRDocs and 20news clustering example make it very clear that this is LSA for NLP/IR folks looking for that transformation,,947,0.8162618796198522,0.11296436694465505,30239,386.4215086477727,34.75644035847746,103.21108502265287,2230,33,925,111,travis,larsmans,larsmans,true,larsmans,60,0.7333333333333333,97,31,955,true,true,false,false,34,139,29,57,116,5,983
1194746,scikit-learn/scikit-learn,python,1714,1361865095,,1405453858,726419,,unknown,false,false,false,17,6,3,47,6,0,53,0,5,4,1,6,14,7,0,0,6,2,8,16,10,0,0,1405,0,3175,0,56.154717106710955,1.576171757573989,22,vlad@vene.ro,examples/plot_elm_comparison.py|examples/simple_elm_example.py|examples/plot_classifier_comparison.py|examples/plot_elm_comparison.py|sklearn/elm.py|sklearn/random_hidden_layer.py|benchmarks/bench_covertype.py|examples/plot_classifier_comparison.py|examples/plot_elm_comparison.py|examples/simple_elm_example.py|sklearn/__init__.py|sklearn/elm.py|sklearn/random_hidden_layer.py,13,0.0,0,1,false,Random hidden layer and Extreme Learning Machine implementation still missing some docs/doctestsdemo IPython notebook in https://gistgithubcom/dclambert/b1081db34ddaa282f147,,946,0.8171247357293869,0.10879284649776454,30219,385.9161454713922,34.71325986961845,103.08084317813295,2229,34,924,206,travis,dclambert,arjoly,false,,0,0,16,2,6,false,false,false,true,0,0,0,0,0,0,630
1191966,scikit-learn/scikit-learn,python,1713,1361818723,1362239239,1362239239,7008,7008,github,false,false,false,192,10,10,2,13,0,15,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,484,8,484,8,52.89267008269027,1.4846113926384987,106,vlad@vene.ro,doc/whats_new.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/whats_new.rst|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|doc/whats_new.rst,72,0.019374068554396422,1,8,false,MRG CounterVectorizer using arrays instead of lists This is a new take on @ephess code in #1135 rebased on current master cleaned up and with one additional optimization for memory useCurrent TfidfVectorizer uses 447MB to load all of 20news:    Extracting features from the training dataset using a sparse vectorizer    done in 12139995s at 1817MB/s    Extracting features from the test dataset using the same vectorizer    done in 4654452s at 2965MB/sNew vectorizer uses 290MB:    Extracting features from the training dataset using a sparse vectorizer    done in 13596905s at 1622MB/s    Extracting features from the test dataset using the same vectorizer    done in 4770538s at 2893MB/sThats roughly the same memory usage as HashingVectorizer which however is a lot faster:    Extracting features from the training dataset using a sparse vectorizer    done in 6676072s at 3304MB/s    Extracting features from the test dataset using the same vectorizer    done in 4032882s at 3422MB/sSeems reasonable to me knowing that CountVectorizer currently cant handle very large sets at all Since I havent changed much and the innards of CountVectorizer have not significantly changed over the past half a year I assume the old measurements are still largely valid,,945,0.816931216931217,0.10879284649776454,30219,385.9161454713922,34.71325986961845,103.08084317813295,2229,34,923,66,travis,larsmans,larsmans,true,larsmans,59,0.7288135593220338,97,31,953,true,true,false,false,35,125,23,22,114,5,155
1191429,scikit-learn/scikit-learn,python,1712,1361811062,1362393319,1362393319,9704,9704,github,false,false,false,67,8,1,8,7,0,15,0,6,0,0,3,4,3,0,0,0,0,4,4,4,0,0,58,58,148,88,13.448010220183956,0.37746381776513616,75,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,46,0.0343027591349739,0,3,false,[MRG] Support float values for max_features This PR implements float values support for max_features in decision trees and ensembles For example one can now instantiate a random forest with max_features05*n_features with clf  RandomForestClassifier(max_features05) The goal is to alleviate the need to know the exact number of features in the dataset which can be cumbersome when one deals with feature engineering and the dimension of X varies,,944,0.8167372881355932,0.10887397464578673,30219,385.9161454713922,34.71325986961845,103.08084317813295,2229,34,923,67,travis,glouppe,glouppe,true,glouppe,30,1.0,93,22,837,true,true,false,false,17,155,21,79,135,0,5
1190103,scikit-learn/scikit-learn,python,1710,1361753031,1361762114,1361762114,151,151,commit_sha_in_comments,false,false,false,26,7,7,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,31.11589959594876,0.8733715391874114,8,vlad@vene.ro,doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst|doc/developers/index.rst,8,0.005956813104988831,0,0,false,Added Deprecation documentation for developer docs I have added guidelines on how to add deprecation warnings and test for deprecation for developers Suggestions are always welcome,,943,0.8165429480381761,0.10796723752792256,30216,385.95446121260255,34.71670638072544,103.09107757479481,2228,34,922,60,travis,hrishikeshio,,false,,4,0.75,5,12,311,false,true,false,false,5,70,4,0,5,0,-1
1183726,scikit-learn/scikit-learn,python,1706,1361560505,1361904061,1361904052,5725,5725,github,false,false,false,61,4,4,5,15,0,20,0,6,0,0,8,8,7,0,0,0,0,8,8,7,0,0,212,126,212,126,103.7612512878711,2.912405539015517,101,vlad@vene.ro,sklearn/linear_model/base.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/svm/classes.py|sklearn/svm/tests/test_sparse.py|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/svm/classes.py|sklearn/svm/tests/test_sparse.py|doc/whats_new.rst|sklearn/linear_model/base.py|sklearn/linear_model/logistic.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_logistic.py|sklearn/linear_model/tests/test_sgd.py|sklearn/svm/classes.py|sklearn/svm/tests/test_sparse.py,79,0.005196733481811433,0,0,true,MRG: scipysparse coef_ on linear models This PR adds sparsify and densify methods on linear models that support L1 regularizationBefore merge we should:* agree on the names of these methods (sparsify_coef is more explicit and maybe less ambiguous but also longer)* do the same trick on other estimators as appropriate (can someone tell me which ones)Subsumes #1702,,942,0.8163481953290871,0.10096510764662213,30216,385.95446121260255,34.71670638072544,103.09107757479481,2226,35,920,63,travis,larsmans,larsmans,true,larsmans,58,0.7241379310344828,96,31,950,true,true,false,false,35,122,23,24,114,5,9
1183075,scikit-learn/scikit-learn,python,1705,1361551890,1361573400,1361573400,358,358,github,false,false,false,29,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,0,14,0,8.772967285875776,0.2462425828510916,28,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,27,0.02001482579688658,0,0,true,[MRG]: FIX: use DOUBLE_t type This is just a small fix to _treepyx to avoid unnecessary back and forth type casting of wIll merge if Travis is happy,,941,0.8161530286928799,0.10007412898443291,30216,385.95446121260255,34.71670638072544,103.09107757479481,2226,35,920,60,travis,glouppe,glouppe,true,glouppe,29,1.0,92,22,834,true,true,false,false,17,155,18,79,134,0,-1
1182413,scikit-learn/scikit-learn,python,1702,1361538851,1361904060,1361904061,6086,6086,github,false,false,false,269,1,1,0,5,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.101661904887061,0.11512681952745855,7,peter.prettenhofer@gmail.com,sklearn/linear_model/base.py,7,0.0051813471502590676,0,0,true,WIP: LinearClassifierMixin support for sparse coef_ In the case of high-dimensional feature spaces and/or large number of classes it might be wise to store the coef_ array of a linear model as a scipy sparse matrix rather than a dense numpy arrayWe might even do this automagically eg if the sparsity of coef_ reaches a certain level however in the meanwhile Id suggest that the user is in charge of making the decision whether to convert coef_ to a sparse representationThis popped up on the mailing list recently: http://sourceforgenet/mailarchive/messagephpmsg_id30515870This PR is just a minor modification of LinearModelMixindecision_function that enables the use of sparse coef_NOTE: this PR is work in progress - no tests yetI was suspicious that adding dense_outputTrue might cause a performance regression however it seems that this is not the case::The following test is on the test set of RCV1 ccat shape: (23149 47152)OriginalDense---------In [7]: %timeit clfpredict(x)10000 loops best of 3: 516 us per loopIn [8]: %timeit clfpredict(X)100 loops best of 3: 655 ms per loopThis PRDense ---------In [16]: %timeit clfpredict(x)10000 loops best of 3: 54 us per loopIn [15]: %timeit clfpredict(X)100 loops best of 3: 683 ms per loopCSC coef_In [12]: %timeit clfpredict(x)1000 loops best of 3: 324 us per loopIn [13]: %timeit clfpredict(X)10 loops best of 3: 296 ms per loopCSR coef_In [18]: %timeit clfpredict(x)1000 loops best of 3: 566 us per loopIn [19]: %timeit clfpredict(X)10 loops best of 3: 303 ms per loop,,940,0.8159574468085107,0.0999259807549963,30216,385.95446121260255,34.71670638072544,103.09107757479481,2226,35,920,62,travis,pprett,larsmans,false,larsmans,32,0.875,93,28,1298,true,true,true,true,15,102,7,33,22,0,71
1181252,scikit-learn/scikit-learn,python,1700,1361506046,1361529866,1361529866,397,397,github,false,false,false,145,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,118,0,118,0,8.528491189806257,0.239381045951515,42,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/forest.py,42,0.031042128603104215,0,0,true,Speed regression in random forests for predicting  5k rows + clarify docs for n_jobs It seems that for predicting less than 5k rows most of the time is in the fork but for more rows most of the time is in the actual predict highly dependent on a bunch of parameters The slowdown for predicting millions of rows in parallel is worse than the slowdown for predicting a few rows so parallel should be the default imho To get the other behavior you can just set n_jobs1 before calling predictI tried to run the scikit-learn-speed project but it relies on some version of vbench that I cant seem to find Specifically the classes PythonBenchmark and LineProfilerBenchmarkMixin are missing I would like to add some more tests for random forests so we have tests on varying datasets and can track performance in the future,,939,0.8157614483493077,0.09903917220990392,30194,386.23567596211166,34.74200172219646,103.16619195866728,2225,35,920,59,travis,erg,glouppe,false,glouppe,7,0.7142857142857143,18,4,1603,true,true,false,false,17,56,8,0,17,0,396
1176566,scikit-learn/scikit-learn,python,1695,1361410021,1361410104,1361410104,1,1,github,false,false,false,6,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.159423124214316,0.11674835222287708,31,vlad@vene.ro,sklearn/cross_validation.py,31,0.022644265887509132,0,0,false,COSMIT: fix excessive indentation Fixes #1694,,938,0.8155650319829424,0.09861212563915267,30194,386.23567596211166,34.74200172219646,103.16619195866728,2224,35,918,58,travis,mrjbq7,amueller,false,amueller,8,0.875,64,78,1623,false,true,true,false,2,14,0,0,0,0,1
1171948,scikit-learn/scikit-learn,python,1693,1361317873,1361402310,1361402310,1407,1407,merged_in_comments,false,false,false,37,5,1,0,7,0,7,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,43,32,125,34,8.903694377014194,0.24991753440941072,42,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,40,0.028901734104046242,0,2,false,BUG: Build random forests the same way regardless of n_jobs and add a te st for this Dont predict in parallel since the cost of copying memory in joblib outweighs the speedups for random forests Fixes #1685,,937,0.8153681963713981,0.09393063583815028,30215,381.63163991395004,34.58547079265266,102.82972033758067,2222,35,917,60,travis,erg,amueller,false,amueller,6,0.6666666666666666,18,4,1600,true,true,false,false,16,45,7,0,14,0,17
1171364,scikit-learn/scikit-learn,python,1692,1361306851,,1420854190,992395,,unknown,false,false,false,55,10,3,0,28,0,28,0,4,0,0,7,38,7,0,0,0,0,38,38,36,0,0,327,50,798,279,56.321543440399275,1.5808880658147542,131,vlad@vene.ro,sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/spectral.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py,56,0.02023121387283237,0,5,false,Rename labels to classes fixes Issue #1591 Done for clusters/except for kmeans I need adviceThe _kmeanspy has lots of functions/attributs/variables with label in it Should those all be deprecatedPasses all the tests but I also I need to update the tests to make sure it is giving deprecation warningsfixes Issue #1591 ,,936,0.8162393162393162,0.09320809248554913,30215,381.63163991395004,34.58547079265266,102.82972033758067,2222,35,917,250,travis,hrishikeshio,amueller,false,,3,1.0,4,10,306,false,true,true,false,5,57,3,0,4,0,101
1164198,scikit-learn/scikit-learn,python,1689,1361156951,,1374503772,222447,,unknown,false,false,false,92,1,1,0,9,0,9,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,42,16,42,16,9.09513871227848,0.2556217913844475,47,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,43,0.030670470756062766,1,6,false,make min_density parameter for gradient boosting Im not sure why min_density is handled differently for gradient boosting compared to the random forest estimators This commit simply makes gradient boosting treat min_density the same as random forests I ran into this in a recent Kaggle competition I set max_depth  6 and training slowed to a crawl Setting min_density to 01 fixed that Im not sure if the tests are still necessary but I updated them regardlessI dont know the history of this but perhaps there is a good reason @pprett2b56ee1ba3428f572338934db06d8b9f3e07c481,,935,0.8171122994652407,0.0884450784593438,30205,380.3674888263532,34.53070683661645,102.53269326270485,2220,35,915,122,travis,jwkvam,glouppe,false,,1,1.0,1,2,1370,false,true,false,false,2,5,0,0,0,0,483
1162514,scikit-learn/scikit-learn,python,1688,1361100866,1361205379,1361205379,1741,1741,github,false,false,false,29,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,20,0,20,8.160346041260397,0.22935001647428274,56,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/tests/test_common.py,56,0.03991446899501069,0,0,false,Test for fit_transform(X) does the same as fit(X)transform(X) Issue #1687 Added test if fix(X)transform(X) is same as fir_transform(X)Fix for i  Issue #1687 The tests are passedSuggestions welcome,,934,0.816916488222698,0.08838203848895225,30182,380.6573454376781,34.49075607978265,102.21323967927904,2219,35,915,60,travis,hrishikeshio,larsmans,false,larsmans,2,1.0,4,9,304,false,true,false,false,2,44,2,0,4,0,79
1161636,scikit-learn/scikit-learn,python,1686,1361064181,1361403867,1361403867,5661,5661,github,false,false,false,29,16,14,5,4,0,9,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,428,4,805,61.30878883616739,1.7231097380548719,120,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/random_projection.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/random_projection.py|sklearn/tests/test_common.py,63,0.04490377761938703,2,0,false,MRG Pickle tests I picked up #1669 from @FedericoV and just made some small changes to adjust for what I changed in masterShould be good nowPing @GaelVaroquaux,,933,0.8167202572347267,0.08838203848895225,30182,380.6573454376781,34.49075607978265,102.21323967927904,2219,35,914,60,travis,amueller,amueller,true,amueller,162,0.8703703703703703,537,33,848,true,true,false,false,109,1187,98,304,408,11,624
1157767,scikit-learn/scikit-learn,python,1682,1360958300,1361054380,1361054380,1601,1601,merged_in_comments,false,false,false,65,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,44,0,44,0,4.136452438674983,0.11625784904457752,24,tomnyberg@gmail.com,sklearn/cluster/k_means_.py,24,0.016937191249117856,0,2,true,MiniBatchKMeans fails to reassign centers in some situations Sometimes to_reassign has only False values if thats the case the functionfails This patch makes it so no reassignment is done in that caseThe patch seems to work here but maybe to_reassign shouldnt ever be onlyFalse for some reason and theres a bug elsewhere I hope a more knowledgebleperson can look this over,,932,0.8165236051502146,0.08539167254763586,30180,380.284956925116,34.493041749502986,102.22001325381046,2219,35,913,59,travis,aflag,GaelVaroquaux,false,GaelVaroquaux,0,0,13,2,1561,false,false,false,false,0,0,0,0,0,0,8
1143709,scikit-learn/scikit-learn,python,1681,1360878408,1360922291,1360922291,731,731,merged_in_comments,false,false,false,22,3,2,1,3,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,11,0,17,8.956555629977682,0.25172956595954854,27,peter.prettenhofer@gmail.com,sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/tests/test_pairwise.py,27,0.018907563025210083,0,0,false,Added tests for sparse matrices Fixes Issue #1521 Very simple straightforward issueGives error when using chi2 and additice_chi2 with sparse matrices,,931,0.8163265306122449,0.08333333333333333,30180,380.21868787276344,34.493041749502986,102.1868787276342,2215,35,912,59,travis,hrishikeshio,GaelVaroquaux,false,GaelVaroquaux,1,1.0,2,9,301,false,true,false,false,1,26,1,0,4,0,91
1105263,scikit-learn/scikit-learn,python,1676,1360700584,1360959270,1360959270,4311,4311,merged_in_comments,false,false,false,13,2,2,0,21,0,21,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,6,28,6,14.417614638122458,0.4068499791735133,50,tomnyberg@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py,36,0.025157232704402517,0,4,false,Issue #1652 Renamed dicts to all uppercase letters according to pep8 in sklearn/metrics/pairwisepy,,930,0.8161290322580645,0.07826694619147449,30176,378.9103923647932,34.4313361611877,101.83589607635207,2214,36,910,63,travis,hrishikeshio,amueller,false,amueller,0,0,2,9,299,false,true,false,false,0,13,0,0,2,0,897
1105266,scikit-learn/scikit-learn,python,1675,1360688626,1360796506,1360796506,1798,1798,merged_in_comments,false,false,false,9,2,1,0,7,0,7,0,4,2,0,3,5,3,0,0,3,0,3,6,4,0,0,164,0,164,32,22.701329672217117,0.6406077382544996,22,vlad@vene.ro,benchmarks/bench_covertype.py|doc/datasets/covtype.rst|doc/datasets/index.rst|sklearn/datasets/__init__.py|sklearn/datasets/covtype.py,16,0.001397624039133473,0,0,false,MRG move covtype loading to sklearndatasets Fixes issue #1672,,929,0.8159311087190527,0.07686932215234102,30176,378.9103923647932,34.4313361611877,101.83589607635207,2214,36,910,62,travis,larsmans,amueller,false,amueller,57,0.7192982456140351,90,31,940,true,true,true,true,42,144,24,48,106,5,32
1105315,scikit-learn/scikit-learn,python,1670,1360516419,1360610046,1360610046,1560,1560,github,false,false,false,80,0,0,7,7,0,14,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,MRG Fix changing params test There was a stupid bug in the test for checking that estimators dont change their __init__ params: it was only done for classifiers and clustering algosThis is a fix for regressors and transformers I left out GaussianProcessRegressor since I couldnt figure out what was happening there (corr seemed to be a string then a callable then a number)These fixes are needed to make #1669 work Any ideas what I should put into whatsnew,,928,0.8157327586206896,0.07306889352818371,30175,378.95608947804476,34.43247721623861,101.83927091963547,2212,36,908,61,travis,amueller,amueller,true,amueller,161,0.8695652173913043,527,33,842,true,true,false,false,121,1230,101,324,398,11,24
1105270,scikit-learn/scikit-learn,python,1669,1360511603,1362842075,1362842075,38841,38841,merged_in_comments,false,true,false,36,18,15,2,19,0,21,0,2,0,0,71,78,70,0,0,0,7,71,78,72,0,3,547,558,671,693,370.2314982578571,10.358223182374614,527,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/tests/test_common.py|sklearn/tests/test_common.py|examples/exercises/plot_cv_diabetes.py|sklearn/cluster/dbscan_.py|sklearn/datasets/__init__.py|sklearn/ensemble/__init__.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/hmm.py|sklearn/preprocessing.py|sklearn/tree/tests/test_tree.py|sklearn/utils/testing.py|sklearn/decomposition/nmf.py|setup.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_spectral.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/datasets/mldata.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_lfw.py|sklearn/datasets/tests/test_mldata.py|sklearn/datasets/twenty_newsgroups.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/ensemble/forest.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/metrics/tests/test_metrics.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pipeline.py|sklearn/pls.py|sklearn/preprocessing.py|sklearn/svm/base.py|sklearn/tests/test_hmm.py|sklearn/tests/test_preprocessing.py|sklearn/utils/fixes.py|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/testing.py|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/validation.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/datasets/california_housing.py|sklearn/datasets/species_distributions.py|doc/modules/feature_extraction.rst|doc/modules/feature_extraction.rst|sklearn/ensemble/weight_boosting.py|examples/ensemble/plot_adaboost_multiclass.py|examples/ensemble/plot_adaboost_regression.py,93,0.006963788300835654,0,1,false,Pickle tests I added simple pickle tests for transformers classifiers and regressors to check to see whether after a particular model is trained it gives the same answer before and after it is pickled and unpickled,,927,0.8155339805825242,0.07311977715877438,30229,377.81600449899105,34.370968275497034,101.65734890337094,2212,36,908,74,travis,FedericoV,amueller,false,amueller,0,0,2,1,720,false,false,false,false,0,0,0,0,0,0,20
1105042,scikit-learn/scikit-learn,python,1668,1360424238,1360514286,1360514286,1500,1500,github,false,false,false,8,4,3,0,14,0,14,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,83,0,87,0,13.045978701422822,0.3650318779729206,93,noel.dawe@gmail.com,sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/ensemble/weight_boosting.py,93,0.06427090532135453,0,4,false,[MRG] Precompute X_argsorted in AdaBoost This fixes #1667,,926,0.8153347732181425,0.07256392536281962,30201,378.3980662891957,34.40283434323367,101.75159762921758,2210,38,907,61,travis,glouppe,glouppe,true,glouppe,28,1.0,91,22,821,true,true,false,false,17,147,11,81,132,0,266
976227,scikit-learn/scikit-learn,python,1660,1360203604,1360286974,1360286974,1389,1389,github,false,false,false,18,11,8,1,3,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,14,56,14,107,36.45044552141488,1.0199046353266172,25,robert.l.marchman@dartmouth.edu,sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/tests/test_text.py,24,0.016666666666666666,0,0,false,test cases for feature_extractiontext Extends test coverage in feature_extractiontext and fixes a few typos in the modules documentation,,925,0.8151351351351351,0.07291666666666667,30192,377.58346581876,34.34684684684685,101.48383677795444,2206,38,904,61,travis,rlmv,ogrisel,false,ogrisel,1,1.0,4,4,102,true,false,false,false,0,0,1,0,2,0,689
959951,scikit-learn/scikit-learn,python,1657,1360088881,1360278333,1360278333,3157,3157,github,false,false,false,59,7,1,5,5,0,10,0,3,0,0,7,11,7,0,0,0,0,11,11,10,0,0,160,18,263,18,30.678504116303326,0.8584023625917517,201,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,90,0.02975778546712803,0,1,false,[MRG] Remove compute_importances This basically solves #1447 Feature importances are now computed upon access to the feature_importances_ attribute As a result the compute_importances parameter is no longer required Importance scores can be computed on the fly (without rebuilding the entire model because one forgot to set compute_importancesTrue) Just like in the GBRT module it relies on a Python property ,,924,0.814935064935065,0.0726643598615917,30192,377.58346581876,34.34684684684685,101.48383677795444,2206,38,903,61,travis,glouppe,glouppe,true,glouppe,27,1.0,90,22,817,true,true,false,false,17,139,8,79,126,0,379
958651,scikit-learn/scikit-learn,python,1656,1360060021,1360060950,1360060950,15,15,github,false,false,false,82,2,2,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,4,2,4,8.769636781258761,0.2453794166718776,26,peter.prettenhofer@gmail.com,sklearn/feature_extraction/text.py|sklearn/feature_extraction/tests/test_text.py,26,0.018005540166204988,0,0,false,Fix AttributeError caused by an unlearned idf vector Just a quick fix for a silly use case If you call transform on a TfidfTransformer (with use_idfTrue) and have not computed an idf_vector with fit you get this rather unhelpful error:Traceback (most recent call last):  File htclspy line 30 in module    X  ttransform(X)  File /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/sklearn/feature_extraction/textpy line 918 in transform    expected_n_features  self_idf_diagshape[0]AttributeError: TfidfTransformer object has no attribute _idf_diagThis PR raises a more descriptive ValueError describing the error,,923,0.8147345612134345,0.07271468144044321,30190,377.5422325273269,34.34912222590262,101.45743623716461,2206,38,903,60,travis,rlmv,GaelVaroquaux,false,GaelVaroquaux,0,0,4,4,101,false,false,false,false,0,0,0,0,0,0,-1
956536,scikit-learn/scikit-learn,python,1653,1359994232,1370463912,1370463912,174494,174494,merged_in_comments,false,false,false,81,21,19,0,14,0,14,0,6,12,0,9,21,13,0,0,12,0,9,21,13,0,0,2248,205,2324,205,184.15004572891132,5.152621651701614,7,robertlayton@gmail.com,sklearn/mlp/profile.py|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/setup.py|sklearn/mlp/tests/test_sgd.py|sklearn/mlp/__init__.py|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/setup.py|sklearn/setup.py|sklearn/mlp/base.py|sklearn/mlp/classes.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/tests/__init__.py|sklearn/mlp/tests/test_sgd.py|sklearn/mlp/classes.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base.py|sklearn/mlp/classes.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base.py|sklearn/mlp/base_andreas.py|sklearn/mlp/classes_andreas.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/profile.py|sklearn/mlp/profile2.py|sklearn/mlp/tests/test_sgd.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base_andreas.py|sklearn/mlp/tests/test_sgd.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base_andreas.py|sklearn/mlp/classes.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base.py|sklearn/mlp/classes.py|sklearn/mlp/mlp_fast.c|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/tests/test_sgd.py|sklearn/mlp/mlp_fast.pyx|sklearn/mlp/base.py,7,0.0,2,7,false,Multi-Layer Perceptron This is a continuation of the GSoC project by David Marek and @amueller I tried accelerating the implementation using manual calls to CBLAS The performance increase (5%/7% on digits dataset for minibatch100/online) comes at the price of some readability Aside from multiplications the softmax/multinomial logistic loss computation takes a lot of time However running it in C using code from @larsmans mlperceptron branch does not increase efficiency significantly thoughId be happy for suggestions on how to proceed further,,922,0.8145336225596529,0.07157748436414177,30191,377.5628498559173,34.34798449869166,101.45407571792919,2206,38,902,111,travis,temporaer,temporaer,true,temporaer,1,1.0,4,2,1582,true,true,false,false,0,1,0,0,2,0,6
943391,scikit-learn/scikit-learn,python,1644,1359651203,1359988583,1359988583,5623,5623,github,false,false,false,99,1,1,0,5,0,5,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,140,0,140,0,4.126396429440028,0.11804013596509555,8,gabriel.synnaeve@gmail.com,doc/sphinxext/gen_rst.py,8,0.005718370264474625,0,1,false,Fix for Building docs without internet error Just a more graceful error message for when building the docs without internet accessas discussed in #1642Heres the error message now dumping search index donedumping object inventory donebuild succeeded 24 warningsEmbedding documentation hyperlinks in examplesWarning: Embedding the documentation hyperlinks requires internet accessPlease check your network connectionUnable to continue embedding due to a URL Error: (gaierror(-2 Name or service not known))[done]Build finished The HTML pages are in _build/htmlLet me know if you have better wording for the message :),,921,0.8143322475570033,0.0636168691922802,29315,382.602763090568,34.96503496503497,103.29183012109841,2203,39,898,66,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,42,0.9285714285714286,10,13,372,true,true,false,false,20,163,46,15,44,0,15
943143,scikit-learn/scikit-learn,python,1643,1359646221,,1398662202,650206,,unknown,false,false,false,39,29,3,51,32,0,83,0,6,0,0,1,4,1,0,0,0,0,4,4,3,0,0,123,0,390,137,12.775772805517109,0.365465118247754,108,vlad@vene.ro,sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/preprocessing.py,108,0.07725321888412018,0,8,false,MRG added classes parameter to LabelBinarizer Adds a classes parameter to LabelBinarizer I wanted that to add partial_fit to the naive Bayes modelsIf classes is specified there is no need to call fitThis PR still needs tests,,920,0.8152173913043478,0.06294706723891273,29315,382.602763090568,34.96503496503497,103.29183012109841,2203,39,898,188,travis,amueller,amueller,true,,160,0.875,523,33,832,true,true,false,false,137,1254,99,270,364,11,183
938505,scikit-learn/scikit-learn,python,1640,1359544125,1359563803,1359563803,327,327,merged_in_comments,false,false,false,38,2,1,0,8,0,8,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,9,0,9,2,4.565712356877766,0.13059251043555267,14,peter.prettenhofer@gmail.com,sklearn/metrics/cluster/supervised.py,14,0.009978617248752673,0,0,false,Bug fix in the adjusted rand score Ive fixed a bug in metricsadjusted_rand_score This is the first time I contribute to scikit-learn let me know if I didnt follow the processThe bug is addressing this issue: https://githubcom/scikit-learn/scikit-learn/issues/1639,,919,0.8150163220892275,0.06343549536707056,29315,382.5004264028654,34.96503496503497,103.22360566263006,2203,39,897,66,travis,dmollaaliod,dmollaaliod,true,dmollaaliod,0,0,0,0,0,false,false,false,false,0,0,0,0,0,0,2
937181,scikit-learn/scikit-learn,python,1638,1359509750,1359752172,1359752172,4040,4040,github,false,false,false,40,4,4,3,6,0,9,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,92,4,92,16.363089044846546,0.46803130745491206,28,robertlayton@gmail.com,sklearn/decomposition/tests/test_dict_learning.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/decomposition/tests/test_dict_learning.py,22,0.015680684248039915,0,2,false,MRG Fix dict learning random seeds and error message error Fixes #1621Both the actual error and the randomness of the testsI think I picked the right alpha but maybe someone who is familiar with the code can confirm,,918,0.8148148148148148,0.06200997861724875,29315,382.5004264028654,34.96503496503497,103.22360566263006,2199,40,896,66,travis,amueller,amueller,true,amueller,159,0.8742138364779874,520,33,830,true,true,false,false,140,1230,98,269,358,11,38
935895,scikit-learn/scikit-learn,python,1636,1359488285,1359499135,1359499135,180,180,github,false,false,false,26,1,1,0,2,0,2,0,2,0,0,6,6,6,0,0,0,0,6,6,6,0,0,82,0,82,0,26.045868021945626,0.7449887452154482,38,peter.prettenhofer@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py,21,0.007142857142857143,0,0,false,DOC: parameter desc ambiguity fixes issue  #1635Replaced with  **normalize** : boolean optional default False -       If True the regressors X will be normalized before regression,,917,0.8146128680479825,0.06142857142857143,29315,382.5004264028654,34.96503496503497,103.22360566263006,2199,40,896,64,travis,CamDavidsonPilon,GaelVaroquaux,false,GaelVaroquaux,0,0,433,44,580,false,true,false,false,0,0,0,0,0,0,30
933526,scikit-learn/scikit-learn,python,1631,1359400974,1359418530,1359418530,292,292,github,false,false,false,22,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.429184461602752,0.12668776547364083,10,vlad@vene.ro,sklearn/base.py,10,0.0071174377224199285,0,0,false,DOC: small fix in the regressions score method documentation Small nitpick on the documentation of the score function of the Regressor mixin,,916,0.8144104803493449,0.06192170818505338,29315,382.5004264028654,34.96503496503497,103.22360566263006,2198,40,895,64,travis,NelleV,amueller,false,amueller,15,0.8666666666666667,30,13,1106,true,true,false,true,4,10,3,3,4,0,9
927846,scikit-learn/scikit-learn,python,1628,1359226070,1360001411,1360001411,12922,12922,commits_in_master,false,false,false,119,66,32,12,38,0,50,0,6,4,0,18,192,16,0,2,15,2,186,203,187,0,7,2455,73,5467,193,276.0541582641451,7.913703435109499,537,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/nmf.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|doc/modules/neighbors.rst|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|doc/modules/neighbors.rst|sklearn/neighbors/base.py|doc/tutorial/statistical_inference/supervised_learning.rst|sklearn/cluster/tests/test_hierarchical.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py|doc/modules/neighbors.rst|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|doc/modules/neighbors.rst|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|doc/modules/neighbors.rst|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/nature.css_t|doc/sphinxext/gen_rst.py|doc/whats_new.rst|doc/sphinxext/gen_rst.py|doc/images/no_image.png|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp_util.c|sklearn/manifold/lpp_util.pyx|sklearn/setup.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|sklearn/manifold/__init__.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp_util.c|sklearn/manifold/lpp_util.pyx|sklearn/setup.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|sklearn/manifold/lpp.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py|sklearn/manifold/__init__.py,273,0.012125534950071327,0,8,false,Locality Preserving Projection module for manifold learning This pull request adds a new module Locality Preserving Projection (LPP) to the manifold learning package LPP is can be see as a linear approximation to the Laplacian Eigen Mapping Unlike other manifold learning algorithms LPP is a linear transformation and can be used like PCA Detail of LPP can be found in the following paperX He and P Niyogi Locality preserving projections Advances in Neural Information Processing Systems 16 (NIPS 2003) 2003 Vancouver CanadaCurrently I finished main LPP module and added examples The remained tasks are to write test codes and documents Can any one suggest me a guideline to write unit-tests  I dont understand its mannerThank you,,915,0.8142076502732241,0.06134094151212553,29081,385.54382586568545,35.211994085485365,103.74471304288024,2196,40,893,71,travis,lucidfrontier45,lucidfrontier45,true,lucidfrontier45,4,0.75,7,1,691,true,true,false,false,1,0,0,0,14,0,144
927514,scikit-learn/scikit-learn,python,1627,1359210244,1361227043,1361227043,33613,33613,github,false,false,false,35,9,2,14,16,0,30,0,6,0,0,2,3,2,0,0,1,0,3,4,3,0,0,17,0,28,81,9.349932760643844,0.2680310866015742,55,vlad@vene.ro,examples/ensemble/plot_gradient_boosting_regularization.py|sklearn/ensemble/gradient_boosting.py,53,0.03796561604584527,0,3,false,[MRG] Gbrt deviance fix The deviance computation in BinomialDeviance__call__ was wrong Reported in this issue: https://githubcom/scikit-learn/scikit-learn/issues/1625 Furthermore the example plot_gradient_boosting_regularizationpy used labels in {-1 1} and passed them directly to BinomialDeviance__call__ (which assumes {0 1}),,914,0.8140043763676149,0.06160458452722063,29057,385.65578001858415,35.27549299652407,104.07130811852566,2196,40,893,73,travis,pprett,pprett,true,pprett,31,0.8709677419354839,91,28,1271,true,true,false,false,20,119,5,22,65,0,64
928278,scikit-learn/scikit-learn,python,1623,1359079164,1359238955,1359238955,2663,2663,merged_in_comments,false,false,false,37,6,6,0,3,0,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,31,4,31,12.639002567813103,0.3623249824099877,7,kuantkid@gmail.com,sklearn/neighbors/classification.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py,7,0.004992867332382311,0,0,false,Test radius neighbors classifier outlier labeling extended To test for the solved Fix 1602 I have extended the test_radius_neighbors_classifier_outlier_labeling method Instead of only testing for outlier label -1 outlier labels -1 0 and 1 are now tested,,913,0.8138006571741512,0.06348074179743224,29081,385.54382586568545,35.211994085485365,103.74471304288024,2193,41,891,69,travis,basvandenberg,larsmans,false,larsmans,1,1.0,0,0,309,true,true,false,false,1,1,1,0,3,0,2520
841137,scikit-learn/scikit-learn,python,1620,1359034212,1359237152,1359237152,3382,3382,merged_in_comments,false,true,false,55,4,4,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,8.268569812413261,0.23703685006528022,2,gabriel.synnaeve@gmail.com,sklearn/neighbors/classification.py|sklearn/neighbors/classification.py,2,0.0014336917562724014,0,0,false,Fixed bug Issue #1602 Sorry I am a bit of a newbie here not sure if I followed the correct procedure Please indicate if I should have done something differently (Somehow the github help page on pull request is currently unavailable) The changes in the files changed tab seem to be okay though CheersBastiaan,,912,0.8135964912280702,0.06308243727598567,29081,385.54382586568545,35.211994085485365,103.74471304288024,2193,41,891,69,travis,basvandenberg,larsmans,false,larsmans,0,0,0,0,309,true,true,false,false,1,1,0,0,3,0,286
834090,scikit-learn/scikit-learn,python,1612,1358919696,,1358935388,261,,unknown,false,false,false,50,1,1,0,2,0,2,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.2135833946370935,0.12030477921528598,25,vlad@vene.ro,sklearn/cross_validation.py,25,0.018195050946142648,0,2,false,BF: explicitly mark train_test_split as not the one for nosetesting otherwise nose might consider running it resulting in  File /tmp/buildd/scikit-learn-013/debian/python-sklearn/usr/lib/python27/dist-packages/sklearn/cross_validationpy line 1349 in train_test_split    raise ValueError(At least one array required as input)ValueError: At least one array required as inputas was also reported on the mailing listMessage-ID: 50F6B2D31010801@sinacom,,911,0.814489571899012,0.06404657933042213,29311,382.518508409812,35.140390979495756,103.95414690730442,2190,41,890,66,travis,yarikoptic,GaelVaroquaux,false,,8,0.875,51,7,1504,false,true,false,false,6,0,0,0,1,0,254
833917,scikit-learn/scikit-learn,python,1611,1358914163,,1380504709,359842,,unknown,false,false,false,96,26,4,21,27,0,48,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,35,0,354,40,12.69002289604161,0.36378782839447443,23,vlad@vene.ro,sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py|sklearn/gaussian_process/gaussian_process.py,23,0.016751638747268753,0,2,false,[MRG] Gaussian process for arbitrary-dimensional output spaces I have fixed the Gaussian Process submodule so that Gaussian Processes can be trained on data which is vector like in y Now when training a GP each point in both X and Y can be of arbitrary dimension Previously the values of Y were required to be scalars which is a requirement which is not necessary for GPsThe changes are fully backwards compatible and the old tests work fine ~~I have not written new tests to test the new functionality but I can if it is necessary~~,,910,0.8153846153846154,0.06409322651128915,29081,385.54382586568545,35.211994085485365,103.74471304288024,2190,41,890,133,travis,JohnFNovak,JohntheBear,false,,0,0,6,4,478,false,true,false,false,0,0,0,0,0,0,340
832956,scikit-learn/scikit-learn,python,1610,1358880386,1358893874,1358893874,224,224,commit_sha_in_comments,false,false,false,12,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.249853169087202,0.12183561866692433,8,robertlayton@gmail.com,sklearn/covariance/robust_covariance.py,8,0.00583941605839416,0,0,false,FIX: Cast floats to int before slicing in robust_covariancepy Fixes #1 608,,909,0.8151815181518152,0.06350364963503649,29214,383.78859450948175,35.05168754706647,103.27240364208942,2189,40,889,66,travis,erg,larsmans,false,larsmans,5,0.6,17,4,1572,true,true,false,false,21,54,7,2,24,0,3
832984,scikit-learn/scikit-learn,python,1609,1358880381,1358894481,1358894481,235,235,github,false,false,false,49,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.537140035242166,0.1390674844677277,0,,doc/conf.py,0,0.0,0,0,false,update banner for version 12 as older version Activate old-version banner for 012 documentationThe main reason for the PR is just to get the go-ahead for pushingthe rebuilt-with-banner version onto the version 12 doc on sourcefourceLet me know if theres any reason to wait first :),,908,0.8149779735682819,0.06350364963503649,26662,329.4576550896407,31.20546095566724,89.67819368389468,2189,40,889,65,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,41,0.926829268292683,10,13,363,true,true,false,false,24,152,43,10,36,0,49
831837,scikit-learn/scikit-learn,python,1607,1358871724,1359399048,1359399048,8788,8788,github,false,false,false,30,3,1,6,7,0,13,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,8,11,15,8.763590480766975,0.2512363193472732,18,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,17,0.012435991221653255,0,0,false,MRG : add reconstruction_err_ for NMF with sparse input I hacked something to get the frobenius norm of a sparse matrix Let me know if there is a cleaner way,,907,0.814773980154355,0.06364301389904901,29214,383.78859450948175,35.05168754706647,103.27240364208942,2189,40,889,67,travis,agramfort,agramfort,true,agramfort,28,0.8571428571428571,95,177,1147,true,true,false,false,20,65,13,17,2,10,31
821555,scikit-learn/scikit-learn,python,1606,1358867941,,1362264163,56603,,unknown,false,false,false,103,136,40,49,59,0,108,0,6,0,0,7,14,3,0,0,2,0,14,16,9,0,0,604,1264,2987,2989,358.6378789143321,10.2815005875382,243,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/multiclass.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/multiclass.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/multiclass.rst|doc/whats_new.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/multiclass.rst|doc/whats_new.rst,119,0.04338235294117647,0,8,false,[MRG] Multi-label metrics: accuracy hamming loss and zero-one loss This pull request intents to bring 3 new features:- a tested and generalized  unique_labels function- multi-labels support for accuracy_score and zero_one_loss functions-  the hamming loss metrics (hamming_loss ) with multi-label supportBefore merging I would like to suggest to add a new module where multi-labels utilities such as unique_labels and  _is_label_indicator_matrix are collectedFurthermore I have to re-organise (cosmit) some of the function in a multi-label categories in metricspy But I will wait that reviews are doneThis pull request also tackles issue #558 Reviews and comments are welcome :-),,906,0.8156732891832229,0.06397058823529411,29214,383.78859450948175,35.05168754706647,103.27240364208942,2189,40,889,75,travis,arjoly,larsmans,false,,8,1.0,14,18,399,true,true,true,true,9,174,11,124,221,2,140
831511,scikit-learn/scikit-learn,python,1605,1358862599,1359230921,1359230921,6138,6138,merged_in_comments,false,false,false,28,8,2,14,9,0,23,0,4,0,0,3,8,2,0,0,0,0,8,8,6,0,0,10,0,110,55,18.01445352338804,0.5164418634350004,2,gabriel.synnaeve@gmail.com,doc/modules/neighbors.rst|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|doc/modules/neighbors.rst,2,0.0007407407407407407,0,1,false,Fix for change default of neighbors distance warning issue See issue #1599Removes the warning adds warnings to the regressor/classifier docstrings as well as to the narative docs,,905,0.8154696132596685,0.06444444444444444,29214,383.78859450948175,35.05168754706647,103.27240364208942,2189,40,889,70,travis,jaquesgrobler,amueller,false,amueller,40,0.925,10,13,363,true,true,true,true,23,149,40,10,32,0,72
830677,scikit-learn/scikit-learn,python,1604,1358832341,1358842283,1358842283,165,165,github,false,false,false,5,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.375203791070253,0.12542919549383216,16,vlad@vene.ro,doc/modules/linear_model.rst,16,0.011895910780669145,0,0,false,DOC typo: Pereptron - Perceptron ,,904,0.8152654867256637,0.06468401486988848,29214,383.78859450948175,35.05168754706647,103.27240364208942,2189,40,889,62,travis,darkrho,mblondel,false,mblondel,0,0,53,129,1581,false,true,false,false,0,0,0,0,1,0,165
829518,scikit-learn/scikit-learn,python,1603,1358804620,1358821007,1358821007,273,273,merged_in_comments,false,false,false,28,4,1,0,28,0,28,0,5,0,0,4,4,1,1,2,0,0,4,4,1,1,2,2,0,2,0,18.52447247237166,0.5289049150124254,14,vanderplas@astro.washington.edu,doc/conf.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,10,0.006756756756756757,0,8,false,Add SURVEY banner Creates a fixed banner with a link to the new survey as discussed in MLCan be switched off using confpyOnline build [here](http://jaquesgroblergithubcom/online-sklearn-build/indexhtml)J,,903,0.8150609080841639,0.060810810810810814,29313,382.49240951113836,35.137993381776006,103.94705420803056,2188,40,888,62,travis,jaquesgrobler,amueller,false,amueller,39,0.9230769230769231,10,13,362,true,true,true,true,24,141,40,10,25,0,4
826503,scikit-learn/scikit-learn,python,1600,1358705983,1358718965,1358718965,216,216,merged_in_comments,false,false,false,32,8,7,0,10,0,10,0,3,0,0,3,4,3,0,0,0,0,4,4,4,0,0,54,32,57,36,33.13908307380691,0.947428024808007,12,peter.prettenhofer@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py|sklearn/linear_model/ridge.py,10,0.0038022813688212928,0,5,false,Ridge sample weights: fixes #1489 This fixes #1489 (wrong behavior in ridge for sample weights) by fixing the formulas of the ridge when sample weights are passed and by shifting the intercept,,902,0.8148558758314856,0.06007604562737642,29192,382.7761030419293,35.2151274321732,104.27514387503426,2187,40,887,65,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,27,0.6296296296296297,301,3,1063,true,true,false,false,104,412,14,70,52,2,32
826434,scikit-learn/scikit-learn,python,1598,1358703049,1358705739,1358705739,44,44,github,false,false,false,40,1,1,0,2,0,2,0,2,0,0,3,3,2,0,0,0,0,3,3,2,0,0,28,2,28,2,13.741214128650643,0.3954710195187826,116,vlad@vene.ro,doc/whats_new.rst|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,94,0.01524390243902439,0,2,false,MRG undo renaming of class_prior to class_weight in naive bayes This is my current thoughts on #1511I am not sure that the rename was a good idea so Id like to stay with the old name for the release,,901,0.8146503884572697,0.06021341463414634,29190,382.8023295649195,35.21754025351147,104.28228845495033,2187,40,887,66,travis,amueller,larsmans,false,larsmans,158,0.8734177215189873,503,33,821,true,true,true,true,164,1282,101,260,348,8,0
823423,scikit-learn/scikit-learn,python,1594,1358557293,,1358557716,7,,unknown,false,false,false,28,1,1,0,0,0,0,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,14,0,4.441705546783742,0.12782779451425832,8,tomnyberg@gmail.com,sklearn/externals/joblib/parallel.py,8,0.0061068702290076335,0,0,true,Makes joblibs iterable threadsafe If two threads call the iterable at the exact same time a ValueError is thrown so it uses a lock to make it threadsafe,,900,0.8155555555555556,0.06030534351145038,29001,383.12471983724697,35.3780904106755,103.96193234716044,2187,40,885,68,travis,RONNCC,RONNCC,true,,0,0,1,5,376,false,false,false,false,2,5,0,0,0,0,-1
822001,scikit-learn/scikit-learn,python,1592,1358522440,1358527705,1358527705,87,87,github,false,false,false,6,1,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.6908836831957,0.13499866269134472,3,gabriel.synnaeve@gmail.com,examples/linear_model/plot_sparse_recovery.py,3,0.0022900763358778627,0,0,true,Remove warnings from example Fixes #1590 ,,899,0.8153503893214683,0.06030534351145038,28997,383.1775700934579,35.38297065213643,103.97627340759388,2187,40,885,69,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,38,0.9210526315789473,10,13,359,true,true,false,false,23,137,37,10,23,0,43
817325,scikit-learn/scikit-learn,python,1589,1358408595,1358760476,1358760476,5864,5864,merged_in_comments,false,false,false,20,4,3,5,0,0,5,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,13.998507075000866,0.402863474017803,16,tadej.janez@tadej.hicsalta.si,doc/modules/clustering.rst|doc/modules/clustering.rst|doc/modules/clustering.rst,16,0.012470771628994544,0,2,false,MRG: Affinity Propagation documentation update Extends the affinity propagation documents outlining how the algorithm works and its strengths and weaknesses,,898,0.8151447661469933,0.06157443491816056,28991,382.6704839432927,35.35580007588562,103.82532510089338,2184,40,884,71,travis,robertlayton,GaelVaroquaux,false,GaelVaroquaux,16,0.8125,6,10,608,true,true,true,false,6,12,6,0,5,0,38
818616,scikit-learn/scikit-learn,python,1588,1358381543,1358452092,1358452092,1175,1175,merged_in_comments,false,false,false,22,1,1,0,8,0,8,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,6,11,6,11,9.007876595083141,0.25924191338255237,57,tomnyberg@gmail.com,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py,42,0.03309692671394799,0,5,false,MRG rename cosine_kernel to cosine_similarity  Also make the test actually do somethingFixes #732Did a git grep Tests seem to pass,,897,0.8149386845039019,0.062253743104806934,28984,382.7629036709909,35.36433894562517,103.85040022081148,2184,40,883,71,travis,amueller,larsmans,false,larsmans,157,0.8726114649681529,499,33,817,true,true,true,true,170,1292,97,259,331,7,789
816335,scikit-learn/scikit-learn,python,1587,1358378298,,1358449032,1178,,unknown,false,false,false,11,4,1,2,1,3,6,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,4,33,4,41,8.965951499621934,0.25803533135943246,24,tadej.janez@tadej.hicsalta.si,sklearn/svm/base.py|sklearn/svm/tests/test_sparse.py,19,0.01499605367008682,0,1,false,MRG FIX sort indices in CSR matrix for SVM Closes #1476,,896,0.8158482142857143,0.06314127861089187,28984,382.7629036709909,35.36433894562517,103.85040022081148,2184,40,883,71,travis,amueller,larsmans,false,,156,0.8782051282051282,499,33,817,true,true,true,true,170,1291,96,259,329,7,847
816341,scikit-learn/scikit-learn,python,1586,1358361498,,1425386169,1117017,,unknown,false,true,false,14,20,6,14,25,0,39,0,8,0,0,4,5,4,0,0,0,0,5,5,5,0,0,280,50,327,71,52.13988547613574,1.4886816203615498,90,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/base.py|sklearn/svm/src/libsvm/libsvm_sparse_helper.c,63,0.023809523809523808,0,7,false,MRG decision_function for sparse SVC  This resolves issue #73 comes with new passing test,,895,0.8167597765363128,0.0626984126984127,29313,382.49240951113836,35.137993381776006,103.94705420803056,2184,40,883,279,travis,zaxtax,ogrisel,false,,5,1.0,75,13,1726,false,true,false,false,2,39,3,4,0,0,337
816528,scikit-learn/scikit-learn,python,1584,1358304466,1363732696,1363732696,90470,90470,merged_in_comments,false,false,false,146,2,2,2,12,0,14,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,448,0,448,0,17.8087616470498,0.5084701645057694,43,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py,31,0.02476038338658147,0,2,false,[MRG] DOC FIX: multi-target linear model attribute shapes I have two concerns-  The ElasticNetCV and derivated classes only select one point in hyperparameter space  Its not documented and honestly I dont know at the moment how this point is selected when y is multi-target  I would expect it to be the overall best one over all targets- The least_angle based classes do not document the alphas_ active_ and coef_path_ parameters and as they are they cant be easily documented Even the lars_path function misdocuments them as being shaped after max_features which is not true in the case of early stopping or especially in the case of lasso where features can be removedI might not have time to do this the following few days but if nobody else can spare the time and its holding back the release please ping me to do it,,894,0.8165548098434005,0.06309904153354633,29313,382.49240951113836,35.137993381776006,103.94705420803056,2182,40,882,86,travis,vene,vene,true,vene,34,0.9117647058823529,42,28,1010,true,true,false,false,16,61,5,37,36,1,731
819346,scikit-learn/scikit-learn,python,1583,1358302995,1359299381,1359299381,16606,16606,commit_sha_in_comments,false,false,false,106,6,1,7,46,0,53,0,6,0,0,2,3,1,0,1,1,0,3,4,1,0,2,299,0,1014,0,8.805548508585375,0.25141325306387363,7,peter.prettenhofer@gmail.com,doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/nature.css_t,7,0.005595523581135092,0,24,false,MRG: doc hyperlinks fixed size thumbnails This PR enhances the examples in the documentation by- Embedding hyperlinks to the example code on the website- Use fixed size thumbnails to improve alignmentThis is still WIP the documentation links for matplotlib are currently broken (due to recent changes on the matplotlib website) Also the thumbnail generation code requires PIL let me know if this dependency is a problem I could try to do the same using matplotlibThis PR should work well together with #1581 where it was first discussed The hyperlink code is from mne-tools/mne-python Have a look here to see the result:http://martinosorg/mne/auto_examples/indexhtml,,893,0.8163493840985442,0.06235011990407674,29313,382.49240951113836,35.137993381776006,103.94705420803056,2182,40,882,74,travis,mluessi,amueller,false,amueller,0,0,14,7,685,false,false,false,false,1,3,0,0,2,0,2106
919099,scikit-learn/scikit-learn,python,1582,1358289680,1358296783,1358296783,118,118,github,false,false,false,61,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.165060461998508,0.11941346271926978,7,peter.prettenhofer@gmail.com,sklearn/externals/joblib/parallel.py,7,0.005595523581135092,0,0,false,Changed minus sign to plus Sorry this may seem confusing and like like a duplicate but the other pull requests for the same issue did not correct the parallelpy file because it was external But now Gael has fixed the documentation in that file on the joblib repository so now it probably makes sense to change the file here for correctness/consistency,,892,0.8161434977578476,0.060751398880895285,29438,375.77281065289765,34.751002106121334,102.14688497859909,2182,40,882,69,travis,ApproximateIdentity,amueller,false,amueller,2,0.5,3,1,164,true,true,false,false,2,6,2,0,4,0,118
821172,scikit-learn/scikit-learn,python,1581,1358277633,1366291265,1366291265,133560,133560,github,false,true,false,487,62,12,1,36,5,42,0,8,2,1,15,53,15,0,3,4,3,48,55,44,0,4,154,3,2166,83,120.3389659591029,3.435880328479537,67,vlad@vene.ro,examples/cluster/plot_color_quantization.py|examples/cluster/plot_mini_batch_kmeans.py|examples/cluster/plot_mini_batch_kmeans.py|examples/grid_search_digits.py|examples/grid_search_text_feature_extraction.py|examples/plot_lda_qda.py|doc/images/blank_image.png|doc/images/no_image.png|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/static/nature.css_t|examples/applications/plot_hmm_stock_analysis.py|examples/applications/topics_extraction_with_nmf.py|examples/hashing_vs_dict_vectorizer.py|examples/manifold/plot_manifold_sphere.py|examples/plot_rfe_digits.py|examples/plot_roc.py|examples/cluster/plot_color_quantization.py|examples/svm/plot_svm_scale_c.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|examples/cluster/plot_mini_batch_kmeans.py|examples/plot_lda_qda.py|examples/grid_search_text_feature_extraction.py|doc/images/blank_image.png|doc/themes/scikit-learn/static/nature.css_t|examples/applications/plot_hmm_stock_analysis.py|examples/applications/topics_extraction_with_nmf.py|examples/hashing_vs_dict_vectorizer.py|examples/plot_rfe_digits.py|examples/plot_roc.py|examples/cluster/plot_color_quantization.py|examples/cluster/plot_mini_batch_kmeans.py|examples/svm/plot_svm_scale_c.py,20,0.004796163069544364,1,15,false,MRG Example gallery cleanup This is a bit of a cleanup for the examples gallery (#1284)There are a few commits that are outstanding for just this moment as I need some feedback on this before I push them - Ill get to that in a momentMost of the changes here are small ones like typos/titles too long or missing/whitespace/links related Two add-ons for the gallery include:animated thumbnails[1](https://fcloudgithubcom/assets/1378870/68138/54c7e5c2-5f2a-11e2-8ddb-1fc300179378jpeg)------------------------------------[2](https://fcloudgithubcom/assets/1378870/68131/2f0d6816-5f2a-11e2-9fd4-bfdad1d249fejpeg)------------------------------------[3](https://fcloudgithubcom/assets/1378870/68133/310c03ac-5f2a-11e2-988a-2eb2bba6b0b5jpeg)------------------------------------and icons for examples without plots (moved to @mluessi s PR #1583[icon](https://fcloudgithubcom/assets/1378870/68147/a598d36c-5f2a-11e2-9854-f8b2177298fejpeg)The pending commits address the following:1) [Color](http://scikit-learnorg/dev/auto_examples/cluster/plot_color_quantizationhtml) and [Greyscale](http://scikit-learnorg/dev/auto_examples/cluster/plot_lena_compresshtml) Vector Quantization ExamplesIve had a look through both and they are very similiar - I had initially removed the latter and all that links to it - but no Im not sure if we should Its quite nice for a beginner to see how its done on the greyscale 2d example and then see the colour 3d one However if I was to go though with removing the greyscale example - I would then add the histogram to the color example - which a quick rendering thereof looked like this:[figure_1](https://fcloudgithubcom/assets/1378870/68157/85d949fc-5f2b-11e2-800a-7c298c190620png)**What are your thoughts on this**2) The [OLS](http://scikit-learnorg/dev/auto_examples/linear_model/plot_olshtml) and [OLS with SGD](http://scikit-learnorg/dev/auto_examples/linear_model/plot_sgd_olshtml) examples both produce a similiar plot so I initially just dropped one - but then thought again both are nice for somebody new So I chose to combine them into one example making a normal OLS first using the diabetes dataset and then OLS with SGD on the same data The problem with this is that I had to add about 5000 iterations to SGD with a tiny tolerance for it to get kind of close to the normal OLS plots result - but still being out-scored Thought the plots are fine then doesnt it look a bit unimpressive in the example galleryLet me know what you think here3)  The [Digit Dataset](http://scikit-learnorg/dev/auto_examples/datasets/plot_digits_last_imagehtml) and [Iris Dataset](http://scikit-learnorg/dev/auto_examples/datasets/plot_iris_datasethtml) examples I removed and just had their references in tutorials point to [plots from other examples](http://scikit-learnorg/dev/auto_examples/plot_digits_classificationhtml) that display it just as well But now the Datasets section of the tutorial is rather lonely Is that fine or should I leave them[foreveralone](https://fcloudgithubcom/assets/1378870/68220/bcaad516-5f2d-11e2-9ab2-b37b64081269png)4) There are a few K-means examples and Im still torn here * [Kmeans clustering](http://scikit-learnorg/dev/auto_examples/cluster/plot_cluster_irishtml) * [Clustering text documents using k-means](http://scikit-learnorg/dev/auto_examples/document_clusteringhtml) * [A demo of K-Means clustering on the handwritten digits data](http://scikit-learnorg/dev/auto_examples/cluster/plot_kmeans_digitshtml) * [A demo of the K Means clustering algorithm](http://scikit-learnorg/dev/auto_examples/cluster/plot_mini_batch_kmeanshtml)The first is the most straight forward The next too are applications which are useful in my opinion even though the one is without plot The last one basically shows a comparison between kmeans and minibatch-k-means My fix to this was to remove the first simple Kmeans example and then use the comparison(last image in list) to serve that purpose tooI welcome all feedback on this Ive tried to be super-thorough on this but may have missed something here or there Cheers,,891,0.8159371492704826,0.060751398880895285,29313,382.49240951113836,35.137993381776006,103.94705420803056,2182,40,882,95,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,37,0.918918918918919,10,13,356,true,true,false,false,25,120,36,8,24,0,6
917865,scikit-learn/scikit-learn,python,1579,1358255369,1358335863,1358335863,1341,1341,merged_in_comments,false,false,false,48,2,1,0,6,0,6,0,5,0,4,1,5,5,0,0,0,4,1,5,5,0,0,779,0,782,0,4.582371859598652,0.13137847684603707,6,vanderplas@astro.washington.edu,doc/conf.py|doc/sphinxext/numpy_ext_old/__init__.py|doc/sphinxext/numpy_ext_old/docscrape.py|doc/sphinxext/numpy_ext_old/docscrape_sphinx.py|doc/sphinxext/numpy_ext_old/numpydoc.py,6,0.0,1,1,false,Get rid of numpy_ext_old I think we can assume that people use sphinx  10 now (maybe Im assuming wrong) hence we dont need numpy_ext_old anymore @gaelvaroquaux youre the one who implemented the hack to make sphinx  10 build the doc What do you thinkCheersN,,890,0.8157303370786517,0.06104417670682731,29438,375.77281065289765,34.751002106121334,102.14688497859909,2182,40,882,70,travis,NelleV,amueller,false,amueller,14,0.8571428571428571,30,13,1093,true,true,false,true,4,8,2,3,2,0,0
917530,scikit-learn/scikit-learn,python,1578,1358238058,1358290417,1358290417,872,872,github,false,false,false,12,2,0,1,3,0,4,0,4,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Elastic documentation My attempt at #1118 a more practical explanation of elastic-nets,,889,0.8155230596175478,0.061142397425583264,29438,375.77281065289765,34.751002106121334,102.14688497859909,2182,40,882,70,travis,zaxtax,agramfort,false,agramfort,4,1.0,75,13,1725,false,true,false,false,2,35,2,4,0,0,475
917447,scikit-learn/scikit-learn,python,1577,1358233752,1358243058,1358243058,155,155,github,false,false,false,67,1,1,0,0,0,0,0,1,0,0,3,3,3,0,0,0,0,3,3,3,0,0,12,0,12,0,13.54222199440131,0.3882611648386344,63,robertlayton@gmail.com,sklearn/cluster/k_means_.py|sklearn/manifold/mds.py|sklearn/metrics/pairwise.py,40,0.013676588897827836,0,0,false,Changed minus sign to plus sign in the docs of n_jobs in some files Searched the files in sklearn with string Parallel and checked the documentation There were only three places with the mistake I left the original Parallel file (in externals/joblib) UNCHANGED even though the same error is present there As mentioned in the comments on my other pull request that should maybe be handled differently,,888,0.8153153153153153,0.061142397425583264,29438,375.77281065289765,34.751002106121334,102.14688497859909,2182,40,882,67,travis,ApproximateIdentity,agramfort,false,agramfort,1,0.0,3,1,164,true,true,false,false,2,6,1,0,4,0,-1
818463,scikit-learn/scikit-learn,python,1576,1358208121,1359241531,1359241531,17223,17223,github,false,false,false,10,1,1,0,13,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,3,4,3,7.753809181988554,0.22138432241666808,13,peter.prettenhofer@gmail.com,sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,10,0.008032128514056224,0,5,false,FIX issue #1457 KNeighbors should test that n_samples  0 ,,887,0.8151071025930101,0.06104417670682731,29313,382.49240951113836,35.137993381776006,103.94705420803056,2181,40,881,74,travis,mrorii,larsmans,false,larsmans,0,0,7,11,1215,false,false,false,false,0,0,0,0,1,0,2072
816789,scikit-learn/scikit-learn,python,1571,1358097157,1358629097,1358629097,8865,8865,github,false,false,false,40,14,4,2,20,0,22,0,3,0,0,5,11,5,0,0,0,0,11,11,9,0,0,43,16,107,48,21.69410551244996,0.6243342376203626,108,tadej.janez@tadej.hicsalta.si,sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/perceptron.py,44,0.02618657937806874,0,9,false,MRG Backward compatibility Im running the test-suite of 0121 and check for backward compatibilityThese are some necessary fixes There are some more working on itId really love to get feedback on this soon as it is release critical,,885,0.8158192090395481,0.058919803600654665,29001,383.12471983724697,35.3780904106755,103.96193234716044,2180,40,880,70,travis,amueller,amueller,true,amueller,155,0.8774193548387097,497,33,814,true,true,false,false,173,1263,94,263,315,8,7
819720,scikit-learn/scikit-learn,python,1570,1358093223,1359832056,1359832056,28980,28980,merged_in_comments,false,false,false,13,14,3,14,11,0,25,0,5,0,0,4,5,4,0,0,0,0,5,5,5,0,0,39,0,76,6,40.32728173111276,1.1514118714305053,38,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/base.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c,31,0.009083402146985962,0,4,false,MRG Return index of support vectors in sparse SVM This fixes issue #7 ,,884,0.8156108597285068,0.058629232039636665,29313,382.49240951113836,35.137993381776006,103.94705420803056,2180,40,880,72,travis,zaxtax,amueller,false,amueller,3,1.0,75,13,1723,false,true,false,false,2,32,1,3,0,0,342
816795,scikit-learn/scikit-learn,python,1567,1358028859,1358451358,1358451358,7041,7041,merged_in_comments,false,false,false,16,2,2,1,12,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,19,11,19,9.185779709133714,0.26336441876234595,5,peter.prettenhofer@gmail.com,sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/dpgmm.py,5,0.004149377593360996,0,2,false,MRG Vbgmm dpgmm weights fix This is an update on #1294 adding testsThis closes #393,,883,0.8154020385050963,0.05892116182572614,29400,376.25850340136054,34.79591836734694,102.27891156462584,2179,40,879,72,travis,amueller,GaelVaroquaux,false,GaelVaroquaux,154,0.8766233766233766,497,33,813,true,true,true,false,173,1261,92,263,309,8,2353
816927,scikit-learn/scikit-learn,python,1566,1358028817,1358535038,1358535038,8437,8437,merged_in_comments,false,false,false,37,2,1,0,20,0,20,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,8,0,4.270264161666945,0.12243224576579277,43,peter.prettenhofer@gmail.com,sklearn/ensemble/forest.py,43,0.03568464730290456,0,1,false,P3K: division should produce integer The division produces a float under Python 3x it leads to DeprecationWarnings under Python 3x because these float n_features makes starts array float and values from starts are used as slice indices,,882,0.8151927437641724,0.05892116182572614,29400,376.25850340136054,34.79591836734694,102.27891156462584,2179,40,879,70,travis,kmike,larsmans,false,larsmans,1,1.0,317,0,1269,false,true,false,true,0,0,1,0,1,0,2328
910535,scikit-learn/scikit-learn,python,1564,1357921824,1357922185,1357922185,6,6,github,false,false,false,8,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.706243600712976,0.13493144259416737,10,peter.prettenhofer@gmail.com,doc/developers/index.rst,10,0.008285004142502071,0,0,true,added links to astropy and scipy workflow guids ,,881,0.8149829738933031,0.06048053024026512,29400,375.78231292517006,34.761904761904766,102.27891156462584,2176,41,878,62,travis,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,36,0.9166666666666666,10,13,352,true,true,false,false,25,114,33,8,20,0,0
794734,scikit-learn/scikit-learn,python,1563,1357887752,1357969918,1357969918,1369,1369,github,false,false,false,38,4,2,3,2,0,5,0,3,0,0,3,3,2,0,1,0,0,3,3,2,0,1,4,58,4,142,13.73646347131879,0.3938341797537528,9,peter.prettenhofer@gmail.com,.mailmap|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,8,0.004149377593360996,1,2,true,KNN graph fails for sparse inputs In IRC @fmailhot reports LabelSpreading fails for sparse inputs The reason seems to be in buidling knn  graph npasarray converts sparse matrix to matrix of objects and should be replaced by safe_asarray,,880,0.8147727272727273,0.06307053941908713,29390,375.70602245661786,34.77373256209595,102.21163661109222,2176,41,878,70,travis,kuantkid,kuantkid,true,kuantkid,11,0.8181818181818182,13,12,136,true,false,false,false,21,61,8,15,35,0,267
793981,scikit-learn/scikit-learn,python,1562,1357869869,1357872417,1357872417,42,42,github,false,false,false,13,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.130147066903609,0.11841425940073984,10,peter.prettenhofer@gmail.com,sklearn/utils/__init__.py,10,0.008305647840531562,0,0,false,Python 3x fix for sklearnutils (it breaks eg import sklearnmetrics under Python 33),,879,0.8145620022753128,0.06312292358803986,29390,375.70602245661786,34.77373256209595,102.21163661109222,2176,41,877,70,travis,kmike,ogrisel,false,ogrisel,0,0,317,0,1267,false,true,false,false,0,0,0,0,0,0,-1
793620,scikit-learn/scikit-learn,python,1561,1357864886,1357903605,1357903605,645,645,github,false,false,false,6,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,6,13,6,8.897653921641322,0.2551020780780916,45,robertlayton@gmail.com,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,40,0.03325020781379884,0,0,false,Added MinMaxScaler inverse_transform for issue #1552 ,,878,0.8143507972665148,0.06317539484621779,29390,375.70602245661786,34.77373256209595,102.21163661109222,2176,41,877,70,travis,kyleabeauchamp,mblondel,false,mblondel,6,0.3333333333333333,12,0,197,true,false,false,false,4,16,3,1,4,0,5
791426,scikit-learn/scikit-learn,python,1558,1357831091,1357917059,1357917059,1432,1432,github,false,false,false,8,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.537140035242166,0.13778511261032517,0,,doc/conf.py,0,0.0,0,0,false,warning for old version(011)  add old version banner,,876,0.815068493150685,0.06438127090301003,25109,313.6723883866342,30.586642239834323,89.05173443785097,2175,41,877,71,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,35,0.9142857142857143,10,12,351,true,true,false,false,25,97,20,8,15,0,23
791411,scikit-learn/scikit-learn,python,1557,1357830599,,1357830647,0,,unknown,false,false,false,11,6,6,0,0,0,0,0,0,0,0,10,10,5,1,2,0,0,10,10,5,1,2,9,0,9,0,57.96263793558826,1.6618804955440931,115,vanderplas@astro.washington.edu,doc/install.rst|doc/whats_new.rst|examples/ensemble/plot_forest_importances_faces.py|examples/exercises/plot_cv_diabetes.py|examples/exercises/plot_cv_digits.py|examples/plot_digits_pipe.py|doc/conf.py|doc/themes/scikit-learn/theme.conf|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,92,0.008361204013377926,0,0,false,warning for old version(011) adds a warning label for older version,,875,0.816,0.06438127090301003,29390,375.70602245661786,34.77373256209595,102.21163661109222,2175,41,877,69,travis,jaquesgrobler,jaquesgrobler,true,,34,0.9411764705882353,10,12,351,true,true,false,false,25,97,18,8,15,0,-1
792552,scikit-learn/scikit-learn,python,1556,1357823531,1357848192,1357848192,411,411,github,false,false,false,116,1,1,0,0,0,0,0,0,0,0,44,44,44,0,0,0,0,44,44,44,0,0,393,0,393,0,54.59465033629064,1.5653149647149576,92,tadej.janez@tadej.hicsalta.si,sklearn/_hmmc.c|sklearn/_hmmc.pyx|sklearn/cluster/_hierarchical.c|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/_hashing.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/metrics/cluster/expected_mutual_info_fast.pyx|sklearn/metrics/pairwise_fast.c|sklearn/metrics/pairwise_fast.pyx|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/utils/arraybuilder.c|sklearn/utils/arraybuilder.pyx|sklearn/utils/arrayfuncs.c|sklearn/utils/arrayfuncs.pyx|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pyx|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pyx,23,0.002510460251046025,0,0,false,MRG clean up Cython and C code Most importantly I added npimport_array() where it was missing per [Cython for Numpy users](http://wikicythonorg/tutorials/numpy) $(make in 2&1 | wc -l) is down from 706 to 517 for a fresh build because not calling import_array causes an unused function warning for each fileI took the liberty of modernizing some of the Cython code while I was at itAFAIC this can be merged the only thing I cant seem to solve is a similar warning related to _import_umath Im not sure if that can be solved with current Cython and Numpy If you know how to get rid of it feel free to answer [my SO question](http://stackoverflowcom/q/14201192/166749) about this,,874,0.8157894736842105,0.06443514644351464,29390,375.70602245661786,34.77373256209595,102.21163661109222,2175,41,877,69,travis,larsmans,larsmans,true,larsmans,56,0.7142857142857143,82,30,907,true,true,false,false,44,138,13,41,44,8,-1
791314,scikit-learn/scikit-learn,python,1554,1357821884,1357836811,1357836811,248,248,commit_sha_in_comments,false,false,false,191,2,1,1,5,0,6,0,5,0,0,2,2,1,0,0,0,0,2,2,1,0,0,136,0,148,0,9.997172861717948,0.2866347561326389,73,tadej.janez@tadej.hicsalta.si,doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py,51,0.042677824267782424,0,1,false,[MRG] FIX new doctest examples arent crossplatform I try to solve Build failed in Jenkins: python-27-numpy-162-scipy-0101 #1692Three errors are givenFAIL: Doctest: model_evaluationrst----------------------------------------------------------------------Traceback (most recent call last):  File /sp/lib/python/cpython-272/lib/python27/doctestpy line 2166 in runTest    raise selffailureException(selfformat_failure(newgetvalue()))AssertionError: Failed doctest test for model_evaluationrst  File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-27-numpy-162-scipy-0101/ws/doc/modules/model_evaluationrst line 0----------------------------------------------------------------------File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-27-numpy-162-scipy-0101/ws/doc/modules/model_evaluationrst line 314 in model_evaluationrstFailed example:    metricsprecision_recall_fscore_support(y_true y_pred beta05)Expected:    (array([ 066666667  1        ]) array([ 1   05]) array([ 071428571  083333333]) array([2 2] dtypeint64))Got:    (array([ 066666667  1        ]) array([ 1   05]) array([ 071428571  083333333]) array([2 2]))----------------------------------------------------------------------File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-27-numpy-162-scipy-0101/ws/doc/modules/model_evaluationrst line 472 in model_evaluationrstFailed example:    metricsprecision_recall_fscore_support(y_true y_pred beta05)Expected:    (array([ 066666667  0          0        ]) array([ 1  0  0]) array([ 071428571  0          0        ]) array([2 2 2] dtypeint64))Got:    (array([ 066666667  0          0        ]) array([ 1  0  0]) array([ 071428571  0          0        ]) array([2 2 2]))----------------------------------------------------------------------File https://jenkinsshiningpanda-cicom/scikit-learn/job/python-27-numpy-162-scipy-0101/ws/doc/modules/model_evaluationrst line 506 in model_evaluationrstFailed example:    hinge_loss([-1 1 1] pred_decision)Expected:    030303676038544258Got:    030303676038544253  raise selffailureException(selfformat_failure(StringIOStringIO instance at 0x41c9320getvalue()))Ive solved the  floating precision with # doctest: +ELLIPSIS However I havent succeeded to solve the dtypeint64 error properly Any advices is appreciated  ,,873,0.8155784650630011,0.06443514644351464,29390,375.70602245661786,34.77373256209595,102.21163661109222,2175,41,877,69,travis,arjoly,arjoly,true,arjoly,7,1.0,14,18,387,true,true,false,false,9,135,9,74,200,1,23
789626,scikit-learn/scikit-learn,python,1551,1357780929,1358092370,1358092370,5190,5190,merged_in_comments,false,false,false,62,6,1,28,11,1,40,0,3,0,0,1,5,1,0,0,0,0,5,5,4,0,0,221,0,658,0,4.47482006735015,0.12830109317909696,54,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py,54,0.04526404023470243,0,0,false,[MRG] DOC: more consistency in metrics docstrings This set off some of my quirks when looking at the new metrics documentation (I guess before it was well hidden in the class reference) Its a small PR but Id rather not merge straight away because some things might be rather written differently For example F_1 instead of F1 or F_beta instead of F-beta,,872,0.8153669724770642,0.06705783738474434,29390,375.70602245661786,34.77373256209595,102.21163661109222,2174,40,876,70,travis,vene,amueller,false,amueller,32,0.9375,42,28,1004,true,true,true,true,14,46,3,24,26,0,547
788829,scikit-learn/scikit-learn,python,1550,1357768624,1357811123,1357811123,708,708,github,false,false,false,80,2,1,0,3,0,3,0,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.501916593796008,0.2724369400181372,94,tadej.janez@tadej.hicsalta.si,AUTHORS.rst|doc/whats_new.rst,91,0.07653490328006729,0,0,false,Update AUTHORS file Following a private discussion among the project owners we have decided to update the AUTHORSrst file so that it reflects better the core contributors of the project (or past core contributors)Since this is a sensitive subject instead of completely deleting names Ive moved them to a new earlier versions section in the whats new file The contributors that I moved were confirmed by David Cournapeau to not have made substantial contributionsAny suggestion / comment welcome,,871,0.8151549942594719,0.06812447434819176,29390,375.70602245661786,34.77373256209595,102.21163661109222,2174,40,876,67,travis,mblondel,mblondel,true,mblondel,21,0.8571428571428571,206,29,1016,true,true,false,false,59,255,16,51,37,9,174
785593,scikit-learn/scikit-learn,python,1548,1357706543,1440538468,1440538468,1380472,1380472,commits_in_master,false,true,false,17,3,3,3,13,0,16,0,5,0,0,4,4,3,0,0,0,0,4,4,3,0,0,40,9,40,9,22.712696308513657,0.6511527341932368,207,vlad@vene.ro,sklearn/preprocessing.py|doc/whats_new.rst|sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/preprocessing.py,143,0.05419136325148179,0,8,false,[WIP] One vs one decision function: Addresses issue #1523 One vs one decision function: Addresses issue #1523,,870,0.8149425287356322,0.06858594411515664,29101,378.9216865399814,33.400914057936156,98.41586199786948,2173,41,876,358,travis,kyleabeauchamp,amueller,false,amueller,5,0.2,12,0,196,true,false,false,false,4,9,2,1,3,0,450
784191,scikit-learn/scikit-learn,python,1546,1357682820,,1358727037,17403,,unknown,false,false,false,24,3,1,2,4,0,6,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,89,0,133,0,4.531026595623667,0.12989234788904064,18,robertlayton@gmail.com,sklearn/multiclass.py,18,0.015332197614991482,0,0,false,Added n_jobs to multiclasspy Used Parallel/delayed to make the code in sklearn/multiclass parallel This is my first attempt at contributing so hopefully its okay,,869,0.8158803222094362,0.07495741056218058,29060,379.31865106675843,33.275980729525116,98.21059876118375,2173,41,875,78,travis,ApproximateIdentity,ApproximateIdentity,true,,0,0,3,1,157,false,true,false,false,2,5,0,0,1,0,17216
782442,scikit-learn/scikit-learn,python,1544,1357668134,,1358725530,17623,,unknown,false,false,false,58,15,8,12,25,0,37,0,5,2,1,4,9,4,0,0,2,1,6,9,4,0,0,245,10,356,63,44.68786019531068,1.2810807794667418,40,vlad@vene.ro,sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|examples/decomposition/plot_dict_learning.py|examples/cluster/plot_dict_face_patches.py|examples/decomposition/plot_dict_learning.py|doc/modules/decomposition.rst|examples/cluster/plot_dict_face_patches.py|doc/modules/decomposition.rst,25,0.011035653650254669,0,2,false,MRG: Mini bach kmeans Improve the convergence of the minibatch kmeans by randomly reassigning clusters that have too little countsI also add a more complex example using the minibatch kmeans I was working on a similar problem as in the example (just larger scale) when I found the limitations of our implementation and introduced the random reassignement,,868,0.8168202764976958,0.07724957555178268,29060,379.31865106675843,33.275980729525116,98.21059876118375,2173,41,875,78,travis,GaelVaroquaux,GaelVaroquaux,true,,26,0.6538461538461539,297,3,1051,true,true,false,false,107,410,15,63,47,2,0
782816,scikit-learn/scikit-learn,python,1543,1357662494,1357917030,1357917030,4242,4242,github,false,false,false,12,2,1,0,0,0,0,0,1,0,0,3,4,0,1,2,0,0,4,4,1,1,2,0,0,2,0,13.360928485269035,0.4610111149199933,0,,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,0,0.0,0,0,false,warnings for old version(010) add warning labels for older documentationfor #1533 ,,867,0.8166089965397924,0.07587382779198636,22127,289.9172956116961,28.83355176933159,82.70438830388213,2173,41,875,73,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,33,0.9393939393939394,10,12,349,true,true,false,false,27,95,17,8,14,0,-1
782701,scikit-learn/scikit-learn,python,1542,1357659953,1357916989,1357916989,4283,4283,github,false,false,false,13,2,1,0,0,0,0,0,1,0,0,3,4,0,1,2,0,0,4,4,1,1,2,0,0,2,0,13.45029137577879,0.7067686561933239,0,,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,0,0.0,0,0,false,warnings for old version(06) add warning label for older versionfor issue #1533,,866,0.8163972286374134,0.07587382779198636,10562,286.40409013444423,26.983525847377393,78.58360159060784,2173,41,875,73,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,32,0.9375,10,12,349,true,true,false,false,27,94,16,8,14,0,-1
782670,scikit-learn/scikit-learn,python,1541,1357659435,1357916960,1357916960,4292,4292,github,false,false,false,11,2,1,0,1,0,1,0,1,0,0,3,4,0,1,2,0,0,4,4,1,1,2,0,0,2,0,13.599366441756153,0.7058672241666117,0,,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,0,0.0,0,0,false,warnings for old version(07) add a warning label for older version,,865,0.8161849710982659,0.07587382779198636,11037,288.21237655159916,25.459816979251606,75.38280329799764,2173,41,875,73,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,31,0.9354838709677419,10,12,349,true,true,false,false,27,93,15,8,14,0,0
782640,scikit-learn/scikit-learn,python,1540,1357658929,1357916897,1357916897,4299,4299,github,false,false,false,11,2,1,0,35,0,35,0,5,0,0,3,4,0,1,2,0,0,4,4,1,1,2,0,0,2,0,13.402133496114445,0.6273970616226854,0,,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,0,0.0,0,8,false,warning for old version(08) Add a warning banner for older version,,864,0.8159722222222222,0.075809199318569,12605,307.8936929789766,27.8460928203094,82.66560888536296,2173,41,875,73,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,30,0.9333333333333333,10,12,349,true,true,false,false,27,92,14,8,15,0,0
782615,scikit-learn/scikit-learn,python,1539,1357658402,1357917014,1357917014,4310,4310,github,false,false,false,10,2,1,0,1,0,1,0,1,0,0,3,4,0,1,2,0,0,4,4,1,1,2,0,0,2,0,13.500227271970868,0.5122931399297521,0,,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/theme.conf,0,0.0,0,0,false,warning for old version(09) add warning banner for old version,,863,0.8157589803012746,0.07568027210884354,18468,297.7582846003899,30.918345245830626,87.50270738574832,2173,41,875,70,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,29,0.9310344827586207,10,12,349,true,true,false,false,27,90,13,8,17,0,0
781992,scikit-learn/scikit-learn,python,1538,1357646577,1357730910,1357730910,1405,1405,github,false,false,false,40,1,1,0,8,0,8,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,3,2,3,2,12.21304157828914,0.35011513627114016,92,tadej.janez@tadej.hicsalta.si,sklearn/base.py|sklearn/linear_model/stochastic_gradient.py|sklearn/tests/test_common.py,60,0.03154305200341006,0,9,false,MRG Fix SGD classifier grid search + common test This should fix the SGD grid-search problemI also added a common test that set_params returns selfIt didnt do that before when no parameters were given was that on purpose,,862,0.8155452436194895,0.07757885763000852,29063,379.2794962667309,33.272545848673566,98.20046106733648,2173,41,875,69,travis,amueller,amueller,true,amueller,153,0.8758169934640523,493,33,809,true,true,false,false,186,1254,96,249,301,8,0
781886,scikit-learn/scikit-learn,python,1537,1357643605,1362453574,1362453574,80166,80166,commits_in_master,false,false,false,6,29,4,3,14,0,17,0,3,2,0,60,115,61,0,0,5,7,106,118,100,0,3,500,435,1287,766,288.03851062381113,8.257291334999202,560,vlad@vene.ro,doc/modules/feature_extraction.rst|sklearn/feature_extraction/text.py|sklearn/tests/test_feature_extraction.py|sklearn/feature_extraction/tests/test_count_vectorizer.py|setup.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/test_spectral.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/datasets/mldata.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_lfw.py|sklearn/datasets/tests/test_mldata.py|sklearn/datasets/twenty_newsgroups.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/ensemble/forest.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/metrics/tests/test_metrics.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pipeline.py|sklearn/pls.py|sklearn/preprocessing.py|sklearn/svm/base.py|sklearn/tests/test_hmm.py|sklearn/tests/test_preprocessing.py|sklearn/utils/fixes.py|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/testing.py|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/validation.py|doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_count_vectorizer.py|sklearn/feature_extraction/text.py,60,0.011955593509820665,0,5,false,Support for token processor Fixes #1156 ,,861,0.8153310104529616,0.07771135781383433,29063,379.2794962667309,33.272545848673566,98.20046106733648,2173,41,875,80,travis,wrichert,wrichert,true,wrichert,0,0,9,2,517,true,true,false,false,1,4,0,0,1,0,3
781773,scikit-learn/scikit-learn,python,1536,1357640425,1357904047,1357904047,4393,4393,github,false,false,false,12,4,1,0,6,0,6,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,11,0,14,4,4.564278625622647,0.13084562291428667,40,robertlayton@gmail.com,sklearn/preprocessing.py,40,0.03410059676044331,0,0,false,issue #1403: binarize always converts to CSR format Should fix issue #1403,,860,0.8151162790697675,0.07757885763000852,29063,379.2794962667309,33.272545848673566,98.20046106733648,2173,41,875,74,travis,kyleabeauchamp,mblondel,false,mblondel,4,0.0,12,0,195,true,false,false,false,2,4,1,0,1,0,1548
780414,scikit-learn/scikit-learn,python,1534,1357620323,,1357694283,1232,,unknown,false,false,false,18,5,3,6,22,0,28,0,7,0,0,1,3,1,0,0,0,0,3,3,2,0,0,23,0,46,8,13.089122554657564,0.3752300274683543,35,tadej.janez@tadej.hicsalta.si,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,35,0.029888983774551667,0,12,false,Issue 1527: normalize option for zero-one loss I think this is the easiest way to address this issue,,859,0.8160651920838184,0.07771135781383433,29063,379.2794962667309,33.272545848673566,98.20046106733648,2173,39,875,69,travis,kyleabeauchamp,amueller,false,,3,0.0,12,0,195,true,false,false,false,1,0,0,0,1,0,349
778891,scikit-learn/scikit-learn,python,1530,1357565971,1357567915,1357567915,32,32,github,false,false,false,4,2,2,0,3,0,3,0,2,0,0,2,2,1,0,0,0,0,2,2,1,0,0,4,0,4,0,9.412717220950805,0.26983939335922813,11,peter.prettenhofer@gmail.com,doc/modules/linear_model.rst|examples/linear_model/plot_lasso_and_elasticnet.py,10,0.008417508417508417,0,0,false,Doc lasso + ARD ,,858,0.8158508158508159,0.08501683501683502,29063,379.2794962667309,33.272545848673566,98.20046106733648,2172,38,874,59,travis,agramfort,mblondel,false,mblondel,27,0.8518518518518519,93,177,1132,true,true,true,true,25,92,10,31,2,11,1
805147,scikit-learn/scikit-learn,python,1529,1357541741,1358703708,1358703708,19366,19366,github,false,false,false,167,6,1,7,13,0,20,0,4,4,0,3,8,7,0,0,7,0,4,11,10,0,0,1761,0,3512,0,27.820650496476773,0.797566364861325,15,peter.prettenhofer@gmail.com,sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/cluster/expected_mutual_info_fast.pyx|sklearn/utils/lgamma.c|sklearn/utils/lgamma.pxd|sklearn/utils/lgamma.pyx|sklearn/utils/setup.py|sklearn/utils/src/e_lgamma_r.h,9,0.0,1,4,false,[MRG] Added lgamma Cython interface to utils for MSVC **Without this we cannot build WIndows release binaries***Current status*:Works correctly on TravisWorks correctly on my MacOS X Mountain Lion with Apple clang 40Works correctly on my Windows 8 VM with MSVC 1500 (from Visual C++ Express 2008 as usual)*The backstory*:lgamma the logarithm of the gamma function is not available in mathh under MSVC which makes the current master not build under WindowsIn another PR (#1526) I tried bundling more and more of fdlibm until I gave up seeing how it gave wrong results on windows Now instead of bundling a whole library delI got everything thats needed in a single C file and wrote a Cython wrapper for it/del I used John D Cooks LogGamma function as suggested by @larsmansIt builds and returns accurate results on both my MacOS X as wellThe best thing is that it doesnt depend on endianness anymore so this should be good to merge,,857,0.8156359393232205,0.0851602023608769,29058,379.3447587583454,33.27827104411866,98.21735838667493,2170,38,874,73,travis,vene,amueller,false,amueller,31,0.9354838709677419,41,19,1002,true,true,false,true,12,44,2,16,24,0,3
948596,scikit-learn/scikit-learn,python,1526,1357507243,1360862019,1360862019,55912,55912,commit_sha_in_comments,false,false,false,155,6,2,1,9,0,10,0,4,13,0,4,23,16,0,0,16,0,10,26,19,0,0,1844,0,2092,0,78.5014776540836,2.2504915252337105,20,vlad@vene.ro,sklearn/setup.py|sklearn/src/fdlibm/README.txt|sklearn/src/fdlibm/e_lgamma_r.c|sklearn/src/fdlibm/e_log.c|sklearn/src/fdlibm/fdlibm.h|sklearn/src/fdlibm/k_cos.c|sklearn/src/fdlibm/k_sin.c|sklearn/src/fdlibm/k_standard.c|sklearn/src/fdlibm/s_copysign.c|sklearn/src/fdlibm/s_finite.c|sklearn/src/fdlibm/s_lib_version.c|sklearn/src/fdlibm/s_rint.c|sklearn/src/fdlibm/s_signgam.c|sklearn/src/fdlibm/w_lgamma.c|sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/cluster/expected_mutual_info_fast.pyx|sklearn/metrics/cluster/setup.py,12,0.0,0,2,false,[WIP] Bundle lgamma and deps from fdlibm for MSVC Under MSVC expected_mutual_info_fast fails to compile because lgamma is not part of that distribution of mathh  Here is my attempt at bundling it This now correctly builds on both windows and on my MacOS X  The scores seem always nan howeverdelThere are no test failures related to expected_mutual_info However the following need to be checked:1 Are the expected_mutual_info tests good enough that their passing means lgamma works properly Should we explicitly test the bundled lgamma/del2 Does this break anything on other platforms (for this I need your help)3 A (maybe) worrisome fact is that fdlibmh includes mathh and I only bundled the symbols that were undefined on Windows with MSVC  It needs the two implementations to work together  I dont see why they wouldnt these things are standardised but input from people who know about this sort of thing is very welcome,,856,0.8154205607476636,0.08573853989813243,29058,379.3447587583454,33.27827104411866,98.21735838667493,2170,39,873,72,travis,vene,larsmans,false,larsmans,30,0.9333333333333333,41,19,1001,true,true,false,false,12,38,1,15,22,0,10
1105288,scikit-learn/scikit-learn,python,1525,1357501665,,1361403707,65034,,unknown,false,false,false,64,6,2,0,15,4,19,0,2,0,0,2,4,2,0,0,0,0,4,4,4,0,0,126,18,192,24,18.310852380427036,0.522805701296978,33,vlad@vene.ro,sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py,25,0.021258503401360544,0,1,false,WIP simplify naive Bayes parametrization After moving class_prior to class_weight as in __init__ parameter in #1499 the parametrization was a bit weird I now got rid of fit_prior and let class_weight be auto None(uniform) or an array This is a bit more consistent with the other estimatorsIll try to make it pass the common tests now and I guess Ill also allow dictionaries,,855,0.816374269005848,0.0858843537414966,29313,382.49240951113836,35.137993381776006,103.94705420803056,2170,39,873,72,travis,amueller,amueller,true,,152,0.881578947368421,493,33,807,true,true,false,false,189,1233,105,247,309,7,2
779036,scikit-learn/scikit-learn,python,1524,1357497145,1360862132,1360862133,56083,56083,github,false,true,false,25,3,2,0,9,0,9,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,28,0,28,33,8.858985348774867,0.2539704874114263,23,robertlayton@gmail.com,sklearn/multiclass.py|sklearn/multiclass.py,23,0.0195578231292517,0,2,false,MRG break ties in OvO using scores Fixes #1398I was not able to produce a regression test yet :( any help would be appreciated,,854,0.8161592505854801,0.0858843537414966,29052,379.38868236266006,33.28514387993942,98.23764284730828,2169,39,873,72,travis,amueller,larsmans,false,larsmans,151,0.8807947019867549,493,33,807,true,true,true,true,188,1231,104,247,309,7,1178
780263,scikit-learn/scikit-learn,python,1519,1357481747,,1363547661,101098,,unknown,false,false,false,113,4,1,10,51,0,61,0,8,2,0,5,9,5,0,0,2,0,7,9,5,0,0,133,48,216,86,32.58818608560822,0.9342391279489868,97,vlad@vene.ro,doc/modules/classes.rst|doc/modules/decomposition.rst|examples/document_clustering.py|sklearn/decomposition/__init__.py|sklearn/decomposition/lsa.py|sklearn/decomposition/pca.py|sklearn/decomposition/tests/test_lsa.py,65,0.010221465076660987,0,16,false,MRG Dimensionality reduction using LSA/LSI Following the recent discussion on the ML I decided to implement latent semantic analysis (LSA) for dimensionality reduction The implementation is slightly different from what I described in my email because I figured it could be done simpler The code works (try the clustering example with --lsa100) the tests pass and theres documentation but* the docs need some more love and especially some reference* theres no doctest* theres a potential opportunity for optimization in fit_transform see the comment* maybe this should have a more general name like delReducedSVD/del TruncatedSVD its only LSI when applied to term count matricesLet me know what you think,,853,0.8171160609613131,0.08603066439522998,29047,379.247426584501,33.29087341205633,98.15127207629016,2167,39,873,77,travis,larsmans,larsmans,true,,55,0.7272727272727273,80,30,903,true,true,false,false,47,128,18,38,51,8,4
919349,scikit-learn/scikit-learn,python,1518,1357408009,1358296553,1358296553,14809,14809,github,false,false,false,94,0,0,0,28,0,28,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,10,false,MRG add common test to check if estimators overwrite their init params Addressing #1108 (checking that random_state is not altered) and even a bit moreCurrently the test is pretty strict: it checks that the init parameter is not changed AT ALL during fitFor default values that are set to None and instantiated later this is a bit stupidBut on the other hand sometimes None means a parameter will be set from the data - so the none should stay there after fitWhat do you think would be a good policy,,852,0.8169014084507042,0.08828522920203735,29047,378.90315695252525,33.29087341205633,98.11684511309257,2167,40,872,76,travis,amueller,amueller,true,amueller,150,0.88,492,33,806,true,true,false,false,188,1209,103,246,308,7,0
1105330,scikit-learn/scikit-learn,python,1517,1357405282,1357408460,1357408460,52,52,github,false,false,false,155,1,1,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,89,0,89,0,8.905003810646928,0.25527636049554514,12,robertlayton@gmail.com,sklearn/lda.py|sklearn/qda.py,9,0.007640067911714771,0,0,false,MRG document attributes in QDA and LDA rename to adhere to sklearn scheme Some cleanups in LDA and QDAThe additional s in scalings in LDA is on purpose to be consistent with QDAI feel LDA and QDA are still not up to speedThere is a tol parameter to fit in both that seems a bit weird First it should be a __init__ parameter Then it doesnt seem to do anything in QDA but raise a warning That seems a bit odd to meIn LDA it seems to implement some robustness to colinear features but it is also not very clear to meAlso from my understanding of LDA the implementation is suboptimal As the decision function is linear it should be possible to just store the linear decision function for predict and not do so many computations thereMaybe I should open an issue for that This PR closes #1480,,851,0.8166862514688602,0.08828522920203735,29029,380.79162217093256,33.345964380447136,98.59106410830549,2165,40,872,57,travis,amueller,ogrisel,false,ogrisel,149,0.8791946308724832,492,33,806,true,true,true,false,187,1207,102,246,308,7,0
1105246,scikit-learn/scikit-learn,python,1516,1357337800,1357416995,1357416995,1319,1319,merged_in_comments,false,false,false,11,7,5,0,6,0,6,0,4,1,1,1,3,1,0,1,1,1,1,3,1,0,1,0,151,0,193,18.7808845041316,0.5384083016472953,6,peter.prettenhofer@gmail.com,sklearn/datasets/tests/test_base.py|sklearn/datasets/tests/test_base.py|sklearn/datasets/#base.py#|sklearn/datasets/tests/test_base.py|sklearn/datasets/#base.py#,6,0.005067567567567568,0,1,true,TST more coverage of datasetsbase a few more tests for datasetsbase,,850,0.8164705882352942,0.09206081081081081,29047,378.90315695252525,33.29087341205633,98.11684511309257,2165,40,871,57,travis,bcajes,,false,,1,1.0,3,28,769,true,true,false,false,1,2,1,0,2,0,65
782885,scikit-learn/scikit-learn,python,1512,1357244906,1357764629,1357764629,8662,8662,github,false,true,false,108,52,26,39,50,0,89,0,8,0,0,4,15,1,0,0,0,0,15,15,12,0,0,46,0,1281,327,186.8531162355009,5.356618110235313,150,tadej.janez@tadej.hicsalta.si,doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/model_evaluation.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|sklearn/metrics/metrics.py|doc/modules/model_evaluation.rst|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst,94,0.03361344537815126,0,22,false,[MRG] Metric documentation (mainly in classification) I would like to partly tackle the issue in #1508 In this pr I intend to add definitions to classification metrics and remaining regression metricsBefore going into the MRG state I still have read two or three times the added documentation to find mistakes Furthermore I am not sure about the documentation on explained_variance_score It is new to meQuestions:1 After this pr I would like to add some multilabels metrics some thoughts about this2 Why is there a zero_one and zero_one_score instead of  zero_one_loss and accuracy_score3 Why ClassifierMixin uses a homebrew accuracy metrics in the score member,,849,0.8162544169611308,0.08991596638655462,29063,379.2794962667309,33.272545848673566,98.20046106733648,2164,41,870,71,travis,arjoly,arjoly,true,arjoly,6,1.0,14,18,380,true,true,false,false,6,95,6,51,171,0,5
778600,scikit-learn/scikit-learn,python,1507,1357153226,1357168508,1357168508,254,254,merged_in_comments,false,false,false,54,7,7,0,3,0,3,0,3,0,0,6,6,3,0,0,0,0,6,6,3,0,0,43,66,43,66,62.9241837717844,1.8061539479007522,128,tadej.janez@tadej.hicsalta.si,doc/modules/model_evaluation.rst|doc/modules/classes.rst|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/tests/test_metrics.py|doc/modules/classes.rst|doc/modules/model_evaluation.rst|doc/whats_new.rst|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,78,0.026724137931034484,0,0,false,[MRG]  mean absolute error and multioutput regression metrics This pull request implement two new features : * add the metrics mean_absolute_error * add multioutput support for r2_score  mean_absolute_error and mean_square_errorI have been unable to find a narrative documentation corresponding to those regression metrics So I wrote it Reviews and comments are welcome :-),,848,0.8160377358490566,0.09224137931034483,28756,376.9648073445542,33.28001112811239,98.69244679371262,2161,41,869,61,travis,arjoly,amueller,false,amueller,5,1.0,14,18,379,true,true,true,false,4,92,5,51,144,0,1
775487,scikit-learn/scikit-learn,python,1505,1357039182,1357218217,1357218217,2983,2983,github,false,false,false,41,3,2,7,0,0,7,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,0,32,0,9.282105511274423,0.2664303374172193,10,peter.prettenhofer@gmail.com,examples/manifold/plot_mds.py|sklearn/manifold/mds.py,10,0.008598452278589854,0,0,false,Fixes on the MDS The example showed two MDS version Ive improved the plot and added the NMDSThe MDS now uses the object IsotonicRegression instead of the isotonic_regression method to have some extra features (sorting of the keys)CheersN,,847,0.8158205430932703,0.09630266552020636,28756,376.9648073445542,33.28001112811239,98.69244679371262,2160,41,868,61,travis,NelleV,agramfort,false,agramfort,13,0.8461538461538461,28,13,1079,false,true,true,true,5,6,1,1,0,0,411
907643,scikit-learn/scikit-learn,python,1504,1357038834,1357057409,1357057410,309,309,github,false,false,false,34,2,1,0,5,0,5,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,5,0,5,4,4.455032934972893,0.12787605539691263,6,robertlayton@gmail.com,sklearn/isotonic.py,6,0.005159071367153913,0,1,false,ENH isotonic regression is now slighty more robust to noise By using sorting the similarities using lexsort on the pair similarities distance the isotonic regression is now slightly more robust to noiseCheersN,,846,0.8156028368794326,0.09630266552020636,28757,376.88215043293803,33.27885384428139,98.65424070661057,2160,41,868,62,travis,NelleV,agramfort,false,agramfort,12,0.8333333333333334,28,13,1079,false,true,true,true,5,6,0,1,0,0,40
778372,scikit-learn/scikit-learn,python,1503,1357007558,,1433809380,1279970,,unknown,false,false,false,49,8,2,4,26,0,30,0,3,3,0,2,7,4,0,0,3,0,4,7,4,0,0,453,121,633,190,22.050138504888068,0.6329211869177392,20,vanderplas@astro.washington.edu,examples/applications/face_recognition_pca_2d.py|sklearn/decomposition/pca_2d.py|sklearn/decomposition/tests/test_pca_2d.py|examples/applications/face_recognition_pca_2d.py|sklearn/decomposition/__init__.py,20,0.0,0,9,false,Add 2D2PCA ( Two-directional two-dimensional PCA) I add the Two-directional two-dimensional  PCA  in sklearn/decomposition/pca_2dpyTests are added in sklearn/decomposition/tests/test_pca_2dpyand examples are in examples/applications/face_recognition_pca_2dpyReference : Daoqiang Zhang Zhi-Hua Zhou :    Two-directional two-dimensional PCA for efficient face representation and    recognition Neurocomputing Volume 69 Issues 1-3 December 2005    Pages 224-231,,845,0.8165680473372781,0.09707903780068729,28757,376.88215043293803,33.27885384428139,98.65424070661057,2160,41,867,316,travis,yedtoss,amueller,false,,0,0,0,0,308,true,false,false,false,0,0,0,0,2,0,2175
907270,scikit-learn/scikit-learn,python,1502,1356999258,1357154719,1357154719,2591,2591,merged_in_comments,false,false,false,55,3,3,0,0,0,0,0,1,0,0,10,10,1,0,0,0,0,10,10,1,0,0,19,0,19,0,58.2863878925891,1.6730366477266037,89,tadej.janez@tadej.hicsalta.si,CONTRIBUTING.md|doc/tutorial/basic/tutorial.rst|doc/tutorial/common_includes/info.txt|doc/tutorial/statistical_inference/finding_help.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/whats_new.rst|doc/tutorial/basic/tutorial.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst,78,0.0025817555938037868,0,0,false,Typeos Reopening of the following PR while responded to feedback: https://githubcom/scikit-learn/scikit-learn/pull/1448For some reason github was displaying a bunch of commits and changes I did not make that are part of the original branch even though I properly updated my branch and such This PR now contains exactly the changes I made and no others,,844,0.816350710900474,0.0972461273666093,28757,376.88215043293803,33.27885384428139,98.65424070661057,2160,40,867,61,travis,MarkyV,amueller,false,amueller,1,1.0,3,2,30,true,false,false,false,1,1,4,0,2,0,-1
773251,scikit-learn/scikit-learn,python,1499,1356887579,1357226473,1357226473,5648,5648,commit_sha_in_comments,false,false,false,29,9,1,2,13,0,15,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,59,0,4.5140900294623,0.12958283412703608,9,robertlayton@gmail.com,sklearn/naive_bayes.py,9,0.007792207792207792,0,5,false,moved class_prior in NB to __init__ This pull request is trying to fix issue #1486Please let me know if this is the way you expected it to be ,,843,0.8161328588374852,0.10043290043290043,28757,376.56918315540565,33.27885384428139,98.65424070661057,2158,37,866,60,travis,mrshu,amueller,false,amueller,0,0,24,4,791,false,false,false,false,0,0,0,0,0,0,6
772601,scikit-learn/scikit-learn,python,1498,1356850723,1356965110,1356965110,1906,1906,merged_in_comments,false,false,false,40,2,1,2,6,0,8,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,86,0,104,4.944387264648379,0.1419350767470358,4,peter.prettenhofer@gmail.com,sklearn/datasets/tests/test_base.py,4,0.0034453057708871662,0,1,false,TST more code coverage for datasets Improving code coverage for datasets module Moved dataset imports inside test_data_home because it is preferable for import errors to only affect the tests that require those imported methods  My first commit to scikit -bcajes,,842,0.815914489311164,0.10335917312661498,28757,376.56918315540565,33.27885384428139,98.65424070661057,2158,37,866,59,travis,bcajes,amueller,false,amueller,0,0,3,28,764,false,true,false,false,0,0,0,0,0,0,513
770555,scikit-learn/scikit-learn,python,1497,1356771732,1357496587,1357496587,12080,12080,merged_in_comments,false,false,false,51,3,1,1,22,1,24,0,4,0,0,1,3,1,0,0,0,0,3,3,2,0,0,43,0,61,5,4.3138021832719184,0.12383331061705836,26,peter.prettenhofer@gmail.com,sklearn/cross_validation.py,26,0.022317596566523604,0,2,false,train_test_split: test_size default is None If test_size is None and train_size is not None test_sizeis the complement of train_size If test_size and train_sizeare None test_size  025Docstring is updatedExisting tests didnt seem to cover much of existing behaviourand no updates were added New behaviour was tested,,841,0.8156956004756243,0.10643776824034334,28757,376.56918315540565,33.27885384428139,98.65424070661057,2154,37,865,61,travis,sdicker8,,false,,0,0,0,0,554,false,false,false,false,1,3,0,0,0,0,270
763762,scikit-learn/scikit-learn,python,1496,1356580899,1356619668,1356619669,646,646,github,false,false,false,35,3,1,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,25,4,37,8.717560942043695,0.25024825096431047,41,robertlayton@gmail.com,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,38,0.032423208191126277,0,0,false,[MRG] MinMaxScaler on zero variance features A simple bugfix + non-regression testThere is no whats new entry so I did not document the bugfix When was that class introduced Is it new in 013,,840,0.8154761904761905,0.1083617747440273,28755,376.52582159624416,33.21161537123978,98.90453834115806,2151,37,863,58,travis,ogrisel,amueller,false,amueller,33,0.7878787878787878,650,120,1310,true,true,false,true,86,433,41,125,41,1,391
771495,scikit-learn/scikit-learn,python,1491,1356380330,1357228613,1357228613,14138,14138,github,false,false,false,109,61,30,5,12,0,17,0,4,0,0,16,18,13,0,0,0,0,18,18,14,0,0,594,149,630,435,238.49523785208947,6.846763520842572,123,tadej.janez@tadej.hicsalta.si,sklearn/linear_model/stochastic_gradient.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/tests/test_common.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/sparse/classes.py|sklearn/tests/test_common.py|doc/modules/metrics.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|sklearn/utils/__init__.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/utils/__init__.py|sklearn/tests/test_common.py|sklearn/svm/classes.py|sklearn/svm/sparse/classes.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/tests/test_common.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/sparse/classes.py|sklearn/tests/test_common.py|doc/modules/metrics.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|sklearn/utils/__init__.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/utils/__init__.py|sklearn/tests/test_common.py|sklearn/svm/classes.py|sklearn/svm/sparse/classes.py|sklearn/tests/test_common.py|sklearn/svm/classes.py|sklearn/svm/sparse/classes.py,55,0.008673026886383347,0,3,false,MRG LibLinear class weights fix Fixes #1411It seems I broke that in 5427f81c66838c92202c609e27cd92786474aa17 - or rather that I didnt fix there when I fixed it for libsvmThe current tests for class weights seem suboptimalThis branch contains tests for all estimators supporting class_weight~~I will also try to improve the tests in the svm moduleIf everything works Ill add another comment to linearcpp~~~~I want to check if the corrected behavior is consistent with the libsvm binary (see comments in #1411) but I feel being consistent inside sklearn is more important any way~~This PR is on top of #1485 because they touch the same stuff,,839,0.8152562574493445,0.1196877710320902,28776,376.1815401723659,33.18737837086461,98.79760911871004,2142,37,860,59,travis,amueller,amueller,true,amueller,148,0.8783783783783784,475,33,794,true,true,false,false,185,1193,104,257,290,6,2820
777182,scikit-learn/scikit-learn,python,1488,1356277383,1357695123,1357695123,23629,23629,merged_in_comments,false,false,false,24,38,4,21,62,0,83,0,6,0,0,11,13,10,0,0,0,0,13,13,12,0,0,804,102,1498,231,62.51193226031079,1.7945837871115269,174,tadej.janez@tadej.hicsalta.si,doc/whats_new.rst|doc/whats_new.rst|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/whats_new.rst|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/forest.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pxd|sklearn/tree/tree.py|sklearn/ensemble/forest.py,94,0.01201716738197425,0,61,false,MRG: Add sample weighting to decision trees This PR is the first part of #522 It  adds sample weighting to decision trees and forests,,838,0.815035799522673,0.12532188841201716,28754,375.9476942338457,33.177992627112744,98.69931140015302,2140,36,859,76,travis,glouppe,amueller,false,amueller,26,1.0,86,21,773,true,true,true,true,21,100,15,58,82,0,32
902892,scikit-learn/scikit-learn,python,1487,1356270924,1356274500,1356274500,59,59,merged_in_comments,false,false,false,54,3,2,0,1,0,1,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,17,0,49,0,8.773641712130772,0.2518730165418688,53,amueller@ais.uni-bonn.de,sklearn/random_projection.py|sklearn/random_projection.py,53,0.045768566493955096,0,0,false,[MRG] Random projection documentation inconsistencies I found some minor inconsistencies in the doc:  - Density was not set at 1 / sqrt(n_features) as said in the doc for every n_features  - found some copy & paste mistakes  - be more consistent with range in the docSo here a small pr to fix those,,837,0.8148148148148148,0.12435233160621761,28753,375.9607693110284,33.17914652384099,98.70274406148924,2139,36,859,58,travis,arjoly,amueller,false,amueller,4,1.0,14,18,369,true,true,true,false,3,83,4,43,137,0,10
775774,scikit-learn/scikit-learn,python,1485,1356202880,1357226940,1357226940,17067,17067,github,false,false,false,31,27,24,5,3,0,8,0,4,0,0,16,17,13,0,0,1,0,17,18,14,0,0,528,56,634,56,198.04253368649714,5.685389488180779,110,tadej.janez@tadej.hicsalta.si,sklearn/linear_model/stochastic_gradient.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/tests/test_common.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/sparse/classes.py|sklearn/tests/test_common.py|doc/modules/metrics.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|sklearn/utils/__init__.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/utils/__init__.py|sklearn/utils/__init__.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py|sklearn/svm/tests/test_svm.py|sklearn/svm/base.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/utils/__init__.py|doc/modules/metrics.rst|doc/modules/svm.rst|doc/tutorial/basic/tutorial.rst|sklearn/tests/test_common.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/sparse/classes.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/tests/test_common.py|sklearn/linear_model/stochastic_gradient.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/utils/__init__.py,53,0.007772020725388601,4,0,false,MRG Class weight refactor New PR as I messed up #1464@ogrisel gave his +1 there@erg your comments would still be welcomeAlso more comments by @larsmans and @mblondel ),,836,0.8145933014354066,0.12435233160621761,28753,375.9607693110284,33.17914652384099,98.70274406148924,2138,36,858,61,travis,amueller,amueller,true,amueller,147,0.8775510204081632,474,33,792,true,true,false,false,186,1171,104,247,278,6,2687
743182,scikit-learn/scikit-learn,python,1484,1356188727,1356202160,1356202160,223,223,github,false,false,false,31,0,0,6,7,0,13,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,2,false,MRG COSMIT pep8 Some indentation and missing __all__ definitionsI tried to be sensible and violated what the pep8 tool in a few places where it really seemed to hinder readibility,,835,0.8143712574850299,0.1227154046997389,28753,375.8564323722742,33.17914652384099,98.6679650819045,2137,36,858,57,travis,amueller,amueller,true,amueller,146,0.8767123287671232,474,33,792,true,true,false,false,186,1159,101,217,272,6,62
901548,scikit-learn/scikit-learn,python,1483,1356137998,1356175434,1356175434,623,623,github,false,false,false,32,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.501790619065366,0.12922007410437863,18,peter.prettenhofer@gmail.com,doc/index.rst,18,0.015611448395490026,0,0,true,Fix link to plot_lda_qda example This PR fix the following issue:When clicking in the plot_lda_vs_qda link http://scikit-learnorg/stable/auto_examples/plot_lda_vs_qdahtml (one of the examples at the top of scikit-learn) I get a 404 error,,834,0.8141486810551559,0.12575888985255854,28756,375.60856864654335,33.17568507441925,98.62289609125052,2137,36,857,57,travis,aweinstein,agramfort,false,agramfort,0,0,3,13,1204,false,false,false,false,0,0,0,0,0,0,-1
743180,scikit-learn/scikit-learn,python,1482,1356124582,1357168590,1357168590,17400,17400,merged_in_comments,false,false,false,21,7,2,1,8,0,9,0,4,0,0,3,3,2,0,0,0,0,3,3,2,0,0,272,222,348,276,27.344962048056495,0.7850153366924292,94,tadej.janez@tadej.hicsalta.si,doc/whats_new.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py|doc/whats_new.rst|sklearn/dummy.py|sklearn/tests/test_dummy.py,85,0.008628127696289905,0,4,true,[MRG] add multioutput support for dummy estimators I would like to add multioutput support to  dummy estimatorsWhat do you think,,833,0.8139255702280912,0.12510785159620363,28754,375.9476942338457,33.177992627112744,98.69931140015302,2135,36,857,63,travis,arjoly,amueller,false,amueller,3,1.0,14,18,367,true,true,true,false,3,79,3,39,132,0,1141
760885,scikit-learn/scikit-learn,python,1479,1356073340,1356348580,1356348580,4587,4587,merged_in_comments,false,false,false,51,4,3,3,5,0,8,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,8.958615175926885,0.2602368814060116,15,tadej.janez@tadej.hicsalta.si,doc/modules/clustering.rst|doc/modules/clustering.rst,15,0.012975778546712802,0,0,true,MRG: Clusting documentation update - DBSCAN Update to description of DBSCANMerge should be ready to go but Im happy to take ideas for other updatesThe focus is on the documentation in modules/ rather than examples/ etc however if you have good ideas Im happy to update those as well,,832,0.8137019230769231,0.1245674740484429,28229,374.08338942222537,33.29908958872082,97.66552127245032,2135,36,857,63,travis,robertlayton,amueller,false,amueller,15,0.8,5,10,581,true,true,true,true,5,9,5,0,0,0,337
755721,scikit-learn/scikit-learn,python,1478,1355709265,1355748408,1355748408,652,652,github,false,false,false,23,1,1,0,1,0,1,0,1,0,0,28,28,28,0,0,0,0,28,28,28,0,0,573,34,573,34,117.45550271600605,3.41186257353598,149,virgile.fritsch@gmail.com,sklearn/__init__.py|sklearn/base.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/tests/common.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/cluster/tests/test_dbscan.py|sklearn/cluster/tests/test_spectral.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/hmm.py|sklearn/isotonic.py|sklearn/lda.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pipeline.py|sklearn/pls.py|sklearn/preprocessing.py|sklearn/setup.py,40,0.005235602094240838,0,0,false,WIP cosmit pep8 This PR is to address error as reported by the somewhat stricter new pep8This is mostly about visual indentation,,831,0.8134777376654633,0.16841186736474695,28229,374.08338942222537,33.29908958872082,97.66552127245032,2120,35,852,62,travis,amueller,robertlayton,false,robertlayton,145,0.8758620689655172,466,32,786,true,true,true,true,201,1226,111,222,273,6,652
689429,scikit-learn/scikit-learn,python,1475,1355542109,,1357402543,31007,,unknown,false,false,false,9,16,2,10,24,6,40,0,7,0,0,2,3,2,0,0,0,0,3,3,3,0,0,39,0,135,163,9.176119613708044,0.2665496433109183,29,peter.prettenhofer@gmail.com,sklearn/metrics/pairwise.py|sklearn/decomposition/kernel_pca.py,27,0.024816176470588234,0,9,true,Add cos kernel computation in metrics/pairwisepy for Kernel PCA ,,830,0.8144578313253013,0.19209558823529413,28234,373.1671034922434,33.18693773464617,97.435715803641,2114,35,850,67,travis,aymas,,false,,2,1.0,0,0,833,true,false,false,false,0,0,2,1,4,0,310
692968,scikit-learn/scikit-learn,python,1471,1355458476,1358694549,1358694549,53934,53934,merged_in_comments,false,false,false,89,51,6,30,38,2,70,0,8,0,0,4,8,4,0,0,0,0,8,8,5,0,0,1658,546,2140,594,70.15574032147398,2.0378987562810083,43,peter.prettenhofer@gmail.com,examples/hashing_vs_dict_vectorizer.py|sklearn/feature_extraction/text.py|examples/document_classification_20newsgroups.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|examples/hashing_vs_dict_vectorizer.py|sklearn/feature_extraction/text.py|examples/document_classification_20newsgroups.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|examples/hashing_vs_dict_vectorizer.py|sklearn/feature_extraction/text.py|examples/document_classification_20newsgroups.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,29,0.012879484820607176,0,9,true,[MRG] Hashing text vectorizer New PR to wrap the FeatureHasher with the configurable text tokenization features to be able to extract hashed features directly from a collection of raw text files / stringsTODO:- delUpdate narrative documentation/del- delUpdate reference documentation/del- delUpdate classification example/del- delUpdate clustering example/del- delUpdate doc/whatnewrst/del- delAdd a new example for online text classification eg sentiment analysis from the twitter feed using the streaming API/del (twitter is a bad idea because of the oauth requirement Ill probably write another example later),,829,0.8142340168878166,0.20055197792088317,28233,373.1449013565686,33.15269365635958,97.43916693231324,2113,35,850,80,travis,ogrisel,amueller,false,amueller,32,0.78125,636,120,1297,true,true,false,true,92,428,35,126,12,0,881
746933,scikit-learn/scikit-learn,python,1470,1355386460,1355932652,1355932652,9103,9103,github,false,false,false,63,2,1,0,15,0,15,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,14,15,33,8.03195769260269,0.233313657707987,35,tadej.janez@tadej.hicsalta.si,examples/manifold/plot_manifold_sphere.py|sklearn/cluster/tests/test_spectral.py,26,0.02323503127792672,3,10,false,FIX spectral clustering test error in OSX and ADD spectral embedding to sphere examples Following @satras suggestion in #1461 to use a more proper affinity matrix to avoid possible test failureAdd spectral embedding to manifold example included in #1385@ogrisel @vene Would you please try whether this fixes the issue on Mac OSX 108 I do not have a mac handy :(,,828,0.8140096618357487,0.20464700625558535,28233,373.1449013565686,33.15269365635958,97.43916693231324,2111,37,849,62,travis,kuantkid,ogrisel,false,ogrisel,10,0.8,12,11,107,true,false,true,false,24,104,20,32,81,0,357
745204,scikit-learn/scikit-learn,python,1468,1355344935,1355687077,1355687077,5702,5702,github,false,false,false,9,2,2,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,5,0,5,8.862728802713015,0.2574438508746013,44,peter.prettenhofer@gmail.com,sklearn/tests/test_common.py|sklearn/tests/test_common.py,44,0.039532794249775384,0,0,false,Fix two random failures in test_common without setting nprandomseed() ,,827,0.8137847642079806,0.20754716981132076,28233,372.75528636701733,33.117274111854925,97.36832784330393,2108,37,848,62,travis,erg,ogrisel,false,ogrisel,4,0.5,17,4,1531,true,true,false,false,18,56,6,2,23,0,0
745082,scikit-learn/scikit-learn,python,1467,1355342760,1355343142,1355343142,6,6,merged_in_comments,false,false,false,9,2,2,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,8.653478483654624,0.2513655640247723,44,peter.prettenhofer@gmail.com,sklearn/tests/test_common.py|sklearn/tests/test_common.py,44,0.039568345323741004,0,0,false,Fix two bugs in test_commonpy by setting the seed ,,826,0.8135593220338984,0.2077338129496403,28233,372.75528636701733,33.117274111854925,97.36832784330393,2107,37,848,59,travis,erg,erg,true,erg,3,0.3333333333333333,17,4,1531,true,true,false,false,17,54,4,2,23,0,1
771262,scikit-learn/scikit-learn,python,1465,1355277657,1356097360,1356097360,13661,13661,github,false,false,false,69,5,1,2,13,0,15,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,4,37,33,4.67208096264909,0.13571424122264703,19,peter.prettenhofer@gmail.com,sklearn/tests/test_cross_validation.py,19,0.017163504968383016,0,0,false,MRG More Liberal StratifiedKFold Input Validation Since the last comment in the discussion of Issue #1017 is from over 4 months ago and this fix is so simple I went ahead and had a look myself I also made the indentation on cross_validationpy pep8I removed the test for the error I wasnt sure if its possible to test for a warning so currently this behavior is not checked ,,825,0.8133333333333334,0.21138211382113822,28233,372.75528636701733,33.117274111854925,97.36832784330393,2104,37,847,63,travis,AWinterman,ogrisel,false,ogrisel,1,1.0,11,10,727,true,false,false,false,3,7,1,3,11,0,51
743496,scikit-learn/scikit-learn,python,1458,1355163835,1355309617,1355309617,2429,2429,github,false,false,false,173,1,1,0,16,0,16,0,6,0,0,4,4,4,0,0,0,0,4,4,4,0,0,261,0,261,0,13.8171522290796,0.4013564537923211,14,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pyx,14,0.01262135922330097,0,9,false,FIX: Do not rely on strides for contiguous arrays When an array is contiguous in memory but has shape[dim]  1 thenits strides[dim] is not really used so it can be considered arbitraryeven if the array is contiguous This is generally true for 0-sized arraysIf there is no chance of 0-sized (or 1-sized and 1-dimensioal) arrays thenthis code is perfectly correct in current numpy But its a dangerousdesign choice and it would be nice to be able to change numpy at some point----This fixes gh-1406 (without a numpy change which I am sure will come soon but there is no reason to change it here anyway) Using itemsize is much clearer anyway so the only reason to not do it is to do some premature optimization because Cython doesnt compile it away(Recreated the c files with cython 0171)Btw all over this file int casts seem to be used for memory pointers Is this really correct Since it should be npy_intp from a numpy perspective,,823,0.8128797083839611,0.22815533980582525,28189,371.8826492603498,33.09801695696903,97.37841001809217,2102,37,846,61,travis,seberg,glouppe,false,glouppe,0,0,5,2,1371,false,true,false,false,0,1,0,0,0,0,11
625401,scikit-learn/scikit-learn,python,1452,1354993952,1357559535,1357559535,42759,42759,merged_in_comments,false,false,false,137,10,2,0,25,0,25,0,5,0,0,1,6,1,0,0,0,0,6,6,6,0,0,94,0,225,8,8.882837517959887,0.2580150349802173,13,peter.prettenhofer@gmail.com,sklearn/manifold/mds.py|sklearn/manifold/mds.py,13,0.012596899224806201,1,5,false,MRG Make MDS compute proximity from input This introduces a new parameter to MDS and changes the default behaviour as I did it before in the precomputed PRWhat are other proximities that can be used I only saw examples with euclidean distancesI used the name proximity as it was used in the docstring beforeI dont like the name proximity for two reasons:1) Its not used anywhere else2) it is misleading imho A distance is passed so a high value means far apart  A high proximity means close together rightShould I rename it into distance do we have any other algorithms that take dissimilarties@NelleV could you please have a look and voice your opinion ThanksI didnt adjust the tests yet as I first wanted to get some feedbackCloses #1390,,821,0.8136419001218027,0.22674418604651161,28108,372.9187419951615,33.19339689768037,97.6590294578056,2099,37,844,69,travis,amueller,,false,,143,0.8741258741258742,457,32,778,true,true,false,false,205,1268,117,238,170,6,1
625402,scikit-learn/scikit-learn,python,1451,1354925494,1355749276,1355749277,13729,13729,github,false,false,false,43,4,2,0,5,0,5,0,3,0,0,4,5,4,0,0,0,0,5,5,4,0,0,280,4,280,4,33.84297214930182,0.9830187285629984,16,peter.prettenhofer@gmail.com,examples/decomposition/plot_faces_decomposition.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py|examples/decomposition/plot_faces_decomposition.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py,10,0.009746588693957114,0,1,true,MRG renaming chunk_size to batch_size in MiniBatchDictionaryLearning This is the remainder of the inconsistent parameter renaming as far as I can tellId like to merge before the release if possible so that all the deprecations happen at the same timeCloses #1217,,820,0.8134146341463414,0.22807017543859648,28108,372.9187419951615,33.19339689768037,97.6590294578056,2098,37,843,65,travis,amueller,robertlayton,false,robertlayton,142,0.8732394366197183,455,32,777,true,true,true,true,205,1267,116,238,170,6,3732
625406,scikit-learn/scikit-learn,python,1448,1354768260,1356998913,1356998913,37177,37177,commits_in_master,false,false,false,50,49,10,5,6,4,15,0,3,0,0,12,201,3,0,0,1,0,201,202,189,0,1,65,39,2378,1111,91.69321015529508,2.663363495244199,122,tadej.janez@tadej.hicsalta.si,CONTRIBUTING.md|doc/tutorial/basic/tutorial.rst|doc/tutorial/common_includes/info.txt|doc/tutorial/statistical_inference/finding_help.rst|doc/tutorial/statistical_inference/model_selection.rst|doc/tutorial/statistical_inference/putting_together.rst|doc/tutorial/statistical_inference/settings.rst|doc/tutorial/statistical_inference/supervised_learning.rst|doc/tutorial/statistical_inference/unsupervised_learning.rst|doc/whats_new.rst|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/tests/test_extmath.py|sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,91,0.006958250497017893,0,3,false,Type-os in documentation and added some more learning links HelloI am hoping to maybe help contribute more in the future to this project and am starting off by reading through the docs and code  Hope my suggested links and type-os do help clarify things and are welcomeThank you,,819,0.8131868131868132,0.23359840954274355,28108,372.9187419951615,33.19339689768037,97.6590294578056,2092,37,842,67,travis,MarkyV,MarkyV,true,MarkyV,0,0,2,2,5,false,false,false,false,0,0,0,0,0,0,404
625430,scikit-learn/scikit-learn,python,1445,1354638953,1354703581,1354703581,1077,1077,github,false,false,false,125,10,1,2,10,0,12,0,3,0,0,2,7,2,0,0,0,0,7,7,5,0,0,52,20,317,53,8.62609459185979,0.2505507822929989,13,peter.prettenhofer@gmail.com,sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,9,0.009109311740890687,0,6,false,MRG: Simplify the shape of (n_)classes_ for single output trees/forests This is a early pull request The goal is to simplify the shape of the n_classes_ and classes_ attributes in trees and forests when considering  single output problemsFor single output problems - n_classes_ should be a scalar corresponding to the number of classes-  classes_ should be the list of class labels For multi output problems - n_classes_ should be a list of scalars such that n_classes_[i] is the number of classes in the i-th output - classes_ should be a list of lists of class labels such that classes_[i] is the list of class labels for the i-th outputTODO:- ~~Update treepy~~- ~~Add a regression test~~- ~~Update forestpy~~- ~~PEP8~~ ,,818,0.812958435207824,0.23785425101214575,28066,372.76419867455286,33.17180930663436,97.52013111950403,2087,37,840,58,travis,glouppe,glouppe,true,glouppe,25,1.0,83,21,754,true,true,false,false,16,52,13,9,8,0,130
625411,scikit-learn/scikit-learn,python,1443,1354582984,1356093272,1356093272,25171,25171,github,false,false,false,35,5,2,4,18,0,22,0,4,0,0,4,5,4,0,0,0,0,5,5,5,0,0,77,55,167,56,22.166702237748808,0.6438623326935436,46,peter.prettenhofer@gmail.com,sklearn/cross_validation.py|sklearn/grid_search.py|sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_grid_search.py,28,0.020202020202020204,0,5,false,MRG GridSearchCV with lists Addressing #1137Adds a allow_lists parameter to check_arrays that is exclusive with dtype and check_ccontiguous and really just checks the shapesThat makes allowing lists in GridSearchCV and cross_val_score pretty simple,,817,0.8127294981640147,0.23737373737373738,28066,372.76419867455286,33.17180930663436,97.52013111950403,2086,37,839,64,travis,amueller,amueller,true,amueller,141,0.8723404255319149,453,32,773,true,true,false,false,210,1308,119,235,182,6,28
625412,scikit-learn/scikit-learn,python,1442,1354557147,,1364657835,168344,,unknown,false,false,false,61,1,1,2,1,0,3,0,2,0,0,3,3,4,0,0,0,0,3,3,4,0,0,145,0,145,0,9.328003270229548,0.2709370742048843,21,vlad@vene.ro,sklearn/utils/arrayfuncs.c|sklearn/utils/arrayfuncs.pyx|sklearn/utils/setup.py|sklearn/utils/src/cholesky_delete.h,12,0.012133468149646108,0,0,false,COSMIT refactor sklearnutilsarrayfuncs * use fused types and memoryviews* turn boundschecking off* make cholesky_deletec a header file its included not compiled (+ static inline declaration for safety)* stylistic stuff (4 spaces for indent not 3)I dont usually do refactoring on Cython code because it bloats the repo but new contributors are taking over our bad habits (#1412),,816,0.8137254901960784,0.23761375126390294,28063,372.8040480347789,33.17535545023697,97.530556248441,2085,37,839,84,travis,larsmans,larsmans,true,,54,0.7407407407407407,78,30,869,true,true,false,false,63,184,42,52,95,7,1808
625431,scikit-learn/scikit-learn,python,1441,1354550095,1354666308,1354666308,1936,1936,github,false,false,false,45,5,2,4,9,0,13,0,4,0,0,2,3,2,0,0,0,0,3,3,3,0,0,0,38,0,85,13.019958227478728,0.3781730510776767,47,tadej.janez@tadej.hicsalta.si,sklearn/manifold/tests/test_spectral_embedding.py|sklearn/cluster/tests/test_spectral.py|sklearn/manifold/tests/test_spectral_embedding.py,28,0.028368794326241134,0,5,false,Fix spectral embedding related tests The fixes are:1 for discretization lower the bound from 09 to 08 to avoid random errors2 for spectral embedding rather than check the vector difference all less than tol check the mean square error which is more robust,,815,0.8134969325153374,0.23809523809523808,28063,372.8040480347789,33.17535545023697,97.530556248441,2085,37,839,59,travis,kuantkid,amueller,false,amueller,9,0.7777777777777778,12,9,97,true,false,true,false,21,96,19,32,77,0,9
625432,scikit-learn/scikit-learn,python,1440,1354542897,1354552009,1354552009,151,151,github,false,false,false,40,1,0,0,4,0,4,0,4,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,MRG add matplotlib version requirement rephrase As per discussion in #1439The nose version we require is actually even [lower](http://packagesubuntucom/searchkeywordsnose&searchonnames&suitelucid&sectionall) than Lucid but I think the phrasing basically says above lucid is fine and I wanted to keep it simple,,814,0.8132678132678133,0.24004085801838612,28063,372.8040480347789,33.17535545023697,97.530556248441,2084,37,839,56,travis,amueller,ogrisel,false,ogrisel,140,0.8714285714285714,453,32,773,true,true,true,false,208,1316,120,237,178,6,4
625413,scikit-learn/scikit-learn,python,1438,1354540846,1356107727,1356107727,26114,26114,commit_sha_in_comments,false,false,false,46,180,74,42,98,0,140,0,7,3,0,7,32,7,0,0,12,2,27,41,24,0,0,2410,1290,8084,3585,552.2111487416192,16.03932757003679,48,satra@mit.edu,sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/manifold/plot_lle_digits.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/document_classification_20newsgroups.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/utils/testing.py|examples/document_classification_20newsgroups.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/plot_johnson_lindenstraus_bound.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/manifold/plot_lle_digits.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/document_classification_20newsgroups.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/utils/testing.py|examples/document_classification_20newsgroups.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/random_projection.py|examples/plot_johnson_lindenstraus_bound.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|examples/plot_johnson_lindenstraus_bound.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/random_projection.py|sklearn/tests/test_random_projection.py|sklearn/tests/test_random_projection.py,21,0.010277492291880781,0,26,false,[MRG] Random Projections  Early pull requests for random projection I want to finish pr #372 My goal is to have a workable implementation of  Gaussian Bernoulli  and sparse randomprojection matrixNote that the materializeFalse and random_dot feature  wont probably be implemented in this pull request,,813,0.8130381303813038,0.24152106885919836,28063,372.8040480347789,33.17535545023697,97.530556248441,2084,37,839,61,travis,arjoly,amueller,false,amueller,2,1.0,14,17,349,true,true,true,false,3,26,2,1,16,0,2
625421,scikit-learn/scikit-learn,python,1437,1354528341,1357643126,1357643126,51913,51913,github,false,false,false,19,2,2,1,10,0,11,0,3,0,0,2,2,1,0,1,0,0,2,2,1,0,1,2,0,2,0,9.557993021713829,0.27761804761954384,10,peter.prettenhofer@gmail.com,setup.cfg|Makefile,9,0.009230769230769232,0,7,false,Test tweaking makefile for travis Try to force travis to see the doctest fixtures to avoid the mldata problem,,812,0.812807881773399,0.24102564102564103,28063,372.8040480347789,33.17535545023697,97.530556248441,2083,37,839,67,travis,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,25,0.64,286,3,1015,true,true,false,false,128,531,24,144,143,5,98
625433,scikit-learn/scikit-learn,python,1436,1354489943,1354490320,1354490320,6,6,commit_sha_in_comments,false,false,false,9,26,26,0,2,6,8,6,1,1,1,6,8,6,0,0,1,1,6,8,6,0,0,81,756,81,756,90.01668820065865,2.614573716183598,70,tadej.janez@tadej.hicsalta.si,examples/document_classification_20newsgroups.py|setup.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|examples/cluster/plot_lena_segmentation.py|sklearn/cluster/spectral.py|examples/cluster/plot_lena_segmentation_spectral_comparison.py|examples/cluster/plot_lena_segmentation_spectral_comparison.py|sklearn/cluster/spectral.py|examples/cluster/plot_lena_segmentation.py|examples/cluster/plot_lena_segmentation_spectral_comparison.py|examples/cluster/plot_lena_segmentation.py|sklearn/cluster/tests/test_spectral.py,44,0.012182741116751269,0,0,false,Enh spectral clustering Small adjustment to make test_discretize reproducible,,811,0.812577065351418,0.23857868020304568,28066,371.44587757428917,33.06491840661298,97.02130691940427,2083,37,838,53,travis,briancheung,amueller,false,amueller,1,1.0,1,0,375,true,true,false,false,4,12,3,1,1,0,2
625424,scikit-learn/scikit-learn,python,1432,1354444770,1355315980,1355315980,14520,14520,merged_in_comments,false,false,false,48,7,4,2,7,0,9,0,3,0,0,2,3,2,0,0,0,0,3,3,2,0,0,81,54,85,54,26.320286373871074,0.769711811315314,31,tadej.janez@tadej.hicsalta.si,sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/metrics.py,28,0.028,0,3,false,Fix for roc_curve when one class is present Minor fix for issue #1257 when only one class is in y_true:When one classes detected a warning is issued1 no postive classes true positive rate (TPRTP/(TP+FN)) is meaningless2 no negative classes false positive rate (FPRFP/(FP+TN)) is meaningless,,810,0.8123456790123457,0.236,27737,370.4798644409994,32.95237408515701,96.2973645311317,2083,37,838,66,travis,kuantkid,amueller,false,amueller,8,0.75,12,8,96,true,false,true,false,21,97,19,32,80,0,9219
618357,scikit-learn/scikit-learn,python,1430,1354411575,1354421418,1354421418,164,164,github,false,false,false,17,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,2,4.721645974770213,0.13807996975205997,6,mathieu@mblondel.org,sklearn/cluster/tests/test_k_means.py,6,0.006024096385542169,0,0,false,BUG: Dont test test_k_means_plus_plus_init_2_jobs on Mac OSX  107 be cause its broken See #636 Closes #1407,,809,0.8121137206427689,0.23694779116465864,27737,370.1553881097451,32.916321159462086,96.26131160543679,2082,37,837,57,travis,erg,ogrisel,false,ogrisel,1,0.0,17,4,1520,true,true,false,false,13,44,1,2,11,0,-1
616699,scikit-learn/scikit-learn,python,1429,1354396561,1354432242,1354432242,594,594,github,false,false,false,27,1,1,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,14,8,14,8.936690135779191,0.26134485944638763,25,tiagosbnunes@gmail.com,sklearn/pipeline.py|sklearn/tests/test_pipeline.py,18,0.018036072144288578,0,0,false,Fix pipeline fails if final estimator doesnt implement fit_transform Added test case and fallback to using fit followed by transform if final estimator does not implement fit_transform,,808,0.8118811881188119,0.23647294589178355,27737,370.1553881097451,32.916321159462086,96.26131160543679,2082,37,837,56,travis,tnunes,ogrisel,false,ogrisel,1,1.0,23,48,1091,false,true,false,false,2,8,1,1,0,0,30
606658,scikit-learn/scikit-learn,python,1428,1354299117,1354395405,1354395405,1604,1604,github,false,false,false,111,5,1,5,29,0,34,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,34,0,53,19,4.300743067090306,0.1257860239429629,15,vlad@vene.ro,sklearn/pipeline.py,15,0.014436958614051972,0,1,true,Add fit_transform to FeatureUnion Implements #1426The current implementation requires that all transformers implement fit_transform I believe all transformers in the scikit implement it either directly or through the TransformerMixin Is it ok to require its presence or should we check for its existence and call fit followed by transform if its not presentFeatureUniontransform and FeatureUnionfit_transform and also _transform_one and _fit_transform_one share similar logic which led to some code duplicationLet me know if you prefer to factor out common parts at the expense of extra function callsI believe this addition is already covered by the test test_pipeline#test_feature_union If you think additional test cases are needed tell me which,,807,0.8116480793060719,0.2271414821944177,27714,369.95742224146636,32.94363859421231,96.19686800894854,2079,37,836,56,travis,tnunes,amueller,false,amueller,0,0,23,47,1090,false,true,false,false,0,0,0,0,0,0,4
625366,scikit-learn/scikit-learn,python,1425,1354129149,1354133129,1354133129,66,66,github,false,false,false,44,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,49,0,49,0,4.536068043055197,0.1326704342953534,5,vlad@vene.ro,benchmarks/bench_covertype.py,5,0.004775549188156638,0,0,false,MRG Add random-seed arg to  bench_covertype I add a random-seed argument to bench_covertype to allow more reproducible benchmarksSee also pull request #1388Can somebody else able to check that he has the same accuracy between two runs PS: I have 32 bit machine0,,806,0.8114143920595533,0.22540592168099333,27677,368.9344943454854,33.02381038407342,96.25320663366695,2069,37,834,58,travis,arjoly,ogrisel,false,ogrisel,1,1.0,14,17,344,true,true,true,false,3,19,1,1,13,0,8
625367,scikit-learn/scikit-learn,python,1424,1354121437,1354207676,1354207676,1437,1437,github,false,false,false,33,2,1,0,4,0,4,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,20,0,4.344405896337811,0.12706472027130875,4,jaquesgrobler@gmail.com,examples/linear_model/plot_omp.py,4,0.0038204393505253103,0,2,false,Broken plots in OMP example fix Fixes the plot bug in #1422Feel free to suggest a better name for the changed variable This fixes the problem thoughIt was previously called n_atoms,,805,0.8111801242236025,0.22540592168099333,27677,368.9344943454854,33.02381038407342,96.25320663366695,2069,37,834,57,travis,jaquesgrobler,ogrisel,false,ogrisel,28,0.9285714285714286,9,12,308,true,true,true,true,26,93,29,10,40,0,7
623545,scikit-learn/scikit-learn,python,1421,1354104315,,1354457045,5878,,unknown,false,false,false,127,2,1,0,9,0,9,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,46,0,50,8.678079906684918,0.25381555548585016,37,satra@mit.edu,sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py,31,0.02966507177033493,2,7,false,FIX spectral embedding test fail on master The error in issue #1420 seems to result from that when the matrices has many small values (which is quite common for rbf kernel) or rank deficient then the will have a array must not contain infs or NaNs error from some observationsI have change it to use nearest_neighbors which will have a sparse adjacency matrices and the test passes on my box And when using amg solver an warning will be issued if the laplacian matrix is not sparse as it works better for sparse matricesI am not quite sure why this will happen when using rbf affinities and provide just some observations for your reference Would you please have a look at this @satra @GaelVaroquaux :),,804,0.8121890547263682,0.22583732057416267,27677,368.9344943454854,33.02381038407342,96.25320663366695,2067,37,834,57,travis,kuantkid,amueller,false,,7,0.8571428571428571,12,7,92,true,false,true,false,20,91,18,32,77,0,5
625370,scikit-learn/scikit-learn,python,1419,1354041882,1354201880,1354201880,2666,2666,merged_in_comments,false,false,false,197,6,4,3,9,0,12,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,91,0,123,49,17.86199174199548,0.5237820212280697,10,vlad@vene.ro,sklearn/feature_extraction/image.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/image.py,10,0.009541984732824428,0,0,false,Faster patch extraction This is a very small contribution to the patch extraction method implemented in sklearnfeature_extractionimage:Using stride tricks a numpy array of dimension n is augmented to a patch indexer array of dimension 2n where the first n indices indicate the location of the patch and the last n dimensions index the patch itselfUp until there nothing changes in memory (except for shape and strides)If one then calls a reshape on this array ravelling the first n dimensions into 1 dimension numpy proceeds to copy the data effectively extracting patches and placing them in a list Since this happens inside numpy it is a lot faster than extraction using a for loopA benchmark on the patch extraction for the MiniBatchDictionaryLearning example shows a speed up of factor 16-17:100x extract_patches_2d (old) ~ 20 seconds100x extract_patches_2d (new) ~ 13 secondsMy only concern/question is: Will numpy always handle reshaping of strided arrays by copying If not this whole thing may be slightly unstableOtherwise the extractor extends to any dimension permits easy views on arrays and seems to me to have a rather general use actuallyThanks in advance for your time,,803,0.8119551681195517,0.22614503816793893,27666,361.6713655750741,32.892358852020536,95.9661678594665,2057,37,833,61,travis,eickenberg,agramfort,false,agramfort,2,0.5,4,2,327,true,true,false,false,0,3,1,0,17,0,8
625372,scikit-learn/scikit-learn,python,1418,1354013487,1354020058,1354020058,109,109,github,false,false,false,28,1,1,0,1,0,1,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.772792640389254,0.13995605207694,6,vlad@vene.ro,doc/install.rst,6,0.005649717514124294,0,0,false,Add Install guide for Archlinux This PR add the doc for installing scikit-learn on Archlinux which is another linux distribution Both the stable and git version are provided ,,802,0.8117206982543641,0.22504708097928436,27666,361.6713655750741,32.892358852020536,95.9661678594665,2057,37,833,60,travis,kuantkid,ogrisel,false,ogrisel,6,0.8333333333333334,12,7,91,true,false,true,false,19,90,17,32,77,0,99
587741,scikit-learn/scikit-learn,python,1416,1353977992,1354451578,1354451578,7893,7893,merged_in_comments,false,false,false,57,13,1,16,14,0,30,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,45,0,112,169,4.531252905189002,0.13287317479216162,8,vlad@vene.ro,sklearn/multiclass.py,8,0.007561436672967864,0,6,false,implemented predict_proba for OneVsRestClassifier This also involved writing function predict_proba_ovr to mimic themethodology of existing codeNote that in the multilabel case the marginal probability of the sample having the given label is returned These probabilities do not sum to unity since the set of such probabilities over all labels do not partition the sample space,,801,0.8114856429463171,0.22589792060491493,27666,361.6713655750741,32.892358852020536,95.9661678594665,2057,37,832,61,travis,AWinterman,amueller,false,amueller,0,0,11,10,712,true,false,false,false,0,1,0,0,1,0,1
625373,scikit-learn/scikit-learn,python,1415,1353976834,1354019602,1354019602,712,712,github,false,false,false,12,2,2,0,2,0,2,0,3,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.457523758385402,0.13071116496834506,5,olivier.grisel@ensta.org,.travis.yml,5,0.004734848484848485,0,1,false,Travis (2) Quick pull-request with simplified travis config using:virtualenv:     system_site_packages: true,,800,0.81125,0.22632575757575757,27666,361.63522012578613,32.892358852020536,95.93002241017857,2057,37,832,60,travis,SnippyHolloW,ogrisel,false,ogrisel,1,1.0,57,24,1371,true,true,false,false,1,6,1,0,1,0,79
625359,scikit-learn/scikit-learn,python,1410,1353887519,1355153331,1355153331,21096,21096,github,false,false,false,59,8,1,0,26,0,26,0,4,0,0,2,6,2,0,0,0,0,6,6,4,0,0,99,0,354,24,9.287763017090779,0.27235172801430785,8,vlad@vene.ro,examples/plot_kernel_approximation.py|sklearn/kernel_approximation.py,8,0.007692307692307693,0,3,false,MRG Nystroem kernel approxmation Implements the Nystroem method for kernel approximationThis method is pretty simple and basically works for all kernelsI wanted to do this for some time now There is a new NIPS paper that shows why this method is awesomeMissing: Docs testsAlso documentation in the exampleHere is the plot:[nystrom method comparison](http://iimgurcom/8TnwOpng),,798,0.8120300751879699,0.2298076923076923,27666,361.6713655750741,32.892358852020536,95.9661678594665,2056,36,831,71,travis,amueller,amueller,true,amueller,139,0.8705035971223022,448,32,765,true,true,false,false,232,1306,163,257,212,6,16
625374,scikit-learn/scikit-learn,python,1409,1353882750,1353885962,1353885962,53,53,github,false,false,false,51,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.141410293106611,0.12212102309590782,28,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py,28,0.02681992337164751,0,1,false,Fix staged_predict_proba() in gradient_boosting This function must be broken because of illegal calling of _score_to_proba():In predict_proba we have line:    return self_score_to_proba(score)But in staged_predict_proba there is:    yield self_score_to_proba(X)So last line throws exception somewhere saying that two sizes are incompatible (yes one of them is X[1:] times bigger than another),,797,0.8117942283563363,0.2289272030651341,27578,360.5047501631736,32.92479512655015,95.40213213430995,2056,36,831,61,travis,enizhibitsky,pprett,false,pprett,0,0,3,1,858,false,false,false,false,0,0,0,0,0,0,53
625375,scikit-learn/scikit-learn,python,1408,1353879300,1354047551,1354047551,2804,2804,github,false,false,false,33,74,63,1,29,0,30,0,5,4,2,16,22,15,0,0,4,2,16,22,15,0,0,144,4951,169,4993,475.1600555457249,13.933513008822922,140,vlad@vene.ro,doc/whats_new.rst|examples/manifold/plot_compare_methods.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|examples/manifold/plot_lle_digits.py|sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/cluster/__init__.py|sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/cluster/tests/test_spectral.py|doc/whats_new.rst|examples/cluster/plot_cluster_comparison.py|examples/cluster/plot_segmentation_toy.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/cluster/plot_cluster_comparison.py|examples/cluster/plot_segmentation_toy.py|sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|doc/modules/manifold.rst|sklearn/manifold/spectral_embedding.py|doc/modules/classes.rst|doc/modules/manifold.rst|sklearn/manifold/__init__.py|doc/modules/manifold.rst|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/__init__.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectra_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectra_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/decomposition/__init__.py|sklearn/manifold/__init__.py|sklearn/manifold/spectra_embedding.py|sklearn/manifold/tests/test_spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/__init__.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/__init__.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/decomposition/__init__.py|sklearn/manifold/__init__.py|sklearn/manifold/spectra_embedding.py|sklearn/manifold/tests/test_spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/__init__.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py,77,0.0028735632183908046,2,23,false,MRG rebase spectral embedding for merge (#1396) @kuantkid @amuellerhere is a rebased version of #1396 but i couldnt send the pr to weis branch doesnt seem to show up under base repo,,796,0.8115577889447236,0.2289272030651341,27666,361.6713655750741,32.892358852020536,95.9661678594665,2056,36,831,64,travis,satra,ogrisel,false,ogrisel,8,0.875,60,2,1043,false,true,false,true,6,29,0,21,0,0,117
625378,scikit-learn/scikit-learn,python,1405,1353798383,1353975411,1353975411,2950,2950,github,false,false,false,66,16,1,0,22,0,22,0,4,2,0,1,12,3,0,0,4,0,10,14,6,0,0,172,0,1375,83,13.949608869003974,0.411343700674101,22,vlad@vene.ro,sklearn/metrics/pairwise.py|sklearn/metrics/pairwise_fast.pyx|sklearn/metrics/setup.py,21,0.003734827264239029,0,3,false,MRG add chi2 and exponentiated chi2 kernel Basically a cython version of some of #1387Includes an exponentiated and non-exponentiated versionThis is a slightly different version of the chi2 kernel than in #1387 but I think they are equivalent My version is based on [sminchisescu et al](http://sminchisescuinsuni-bonnde/code/libsvm-chi2html) and [fulkerson et al (pdf)](http://citeseerxistpsuedu/viewdoc/downloaddoi10111504613&reprep1&typepdf)I added a reference to a paper evaluating kernels in the vision setup,,795,0.8113207547169812,0.2250233426704015,27617,360.86468479559693,32.9145091791288,95.55708440453344,2056,37,830,62,travis,amueller,amueller,true,amueller,138,0.8695652173913043,448,32,764,true,true,false,false,229,1291,163,257,211,6,7
625382,scikit-learn/scikit-learn,python,1402,1353735116,1353945831,1353945831,3511,3511,merged_in_comments,false,false,false,63,1,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.265960664227344,0.1257926536867308,7,vlad@vene.ro,sklearn/naive_bayes.py,7,0.0065298507462686565,0,0,false,BernoulliNB: Fix the denominator of P(feature | label) For the Bernoulli Naïve Bayes when calculating feature_log_prob_ I believe the denominator is incorrect N_c should be the number of data of class c The probability P(feature | label) should be the number of times that feature is seen in data with that label divided by the number of data with that labelCheersShaun,,794,0.8110831234256927,0.2248134328358209,27559,360.7532929351573,32.9474944664175,95.50419100838202,2055,37,830,61,travis,sjackman,larsmans,false,larsmans,0,0,76,8,909,false,false,false,false,0,0,0,0,1,0,5
625365,scikit-learn/scikit-learn,python,1401,1353713499,,1354073097,5993,,unknown,false,false,false,306,3,3,0,1,0,1,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,113,0,113,0,17.998476270223144,0.530730653785598,119,vlad@vene.ro,sklearn/feature_extraction/text.py|sklearn/feature_extraction/text.py|sklearn/externals/joblib/parallel.py|sklearn/feature_extraction/text.py,110,0.10261194029850747,1,1,true,Idea: speed up the parallelization of CountVectorizer by adding a batch mechanism to joblibParallel A couple of weeks ago I submitted this experimental idea to the joblib mailing list but it didnt receive much attention:---As I was studying the implementation of the sklearn CountVectorizer my attention was drawn to a comment in the code saying that its main loop could not be efficiently parallelized with joblib:https://githubcom/scikit-learn/scikit-learn/blob/33a5911b55617e919fcfabc283c24784deaed686/sklearn/feature_extraction/textpy#L469I dont know much about the internals of multiprocessing but I imagined that there might be a tradeoff between the size of individual jobs and the number of times that a process in the pool is dispatched a new job For instance if the vectorizer is passed a very long list of very short documents then it would seem possible that the dispatching overhead makes it very suboptimal Perhaps a smaller number of longer jobs would work better in that caseTo explore this hypothesis I had the simple idea of chaining together jobs as batches (to be executed by a single process) and dispatch those instead of individual jobs The user can then experiment with different batch sizes trying to find the sweet spot---Today I have decided to try my idea within a more realistic setting by implementing a minimal version of it for CountVectorizer (its not a full solution see caveats in the code comments) Using a 1M-line text corpus and a 24-core Linux machine parallelizing without batches almost doubles the required time (which I gotta admit is worryingly far from the +20% figure mentioned by @larsmans my implementation is probably not optimal in that regard)  whereas I have obtained a 35X speedup with my batch mechanism:https://gistgithubcom/4137131I dont know if the idea makes much sense but I thought I would submit it here anyway just to see what people thinks,,793,0.8121059268600253,0.2248134328358209,27559,360.7532929351573,32.9474944664175,95.50419100838202,2055,37,829,61,travis,cjauvin,cjauvin,true,,1,1.0,3,1,735,true,false,false,false,2,6,3,0,1,0,5993
625384,scikit-learn/scikit-learn,python,1399,1353700028,1354058456,1354058456,5973,5973,merged_in_comments,false,false,false,95,1,1,0,2,0,2,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.199094245596154,0.12398483545113118,6,vlad@vene.ro,sklearn/neighbors/base.py,6,0.005612722170252572,0,0,true,DOC: clarify documentation for radius_neighbors This follows a  a hrefhttp://sourceforgenet/mailarchive/forumphpthread_nameCAFvE7K6-bgP7coaDgw2%3Dz59nGsT08aTgUczs0PWks3nTsEx5dQ%40mailgmailcom&forum_namescikit-learn-general discussion/a on the mailing listIve updated the documentation to better reflect that multiple points can be queried at the same time and to better describe why arrays of dtype object are returnedThere remains an inconsistency in the radius_neighbors function: when there are the same number of neighbors for every queried point (rare except when a single point is queried) the brute and kd_tree methods modify the type of the returned array the ball_tree method does not  This is being discussed in Issue #1400,,792,0.8118686868686869,0.225444340505145,27559,360.7532929351573,32.9474944664175,95.50419100838202,2055,36,829,62,unknown,jakevdp,amueller,false,amueller,29,0.896551724137931,757,0,562,true,true,false,true,14,25,1,2,1,0,168
625385,scikit-learn/scikit-learn,python,1397,1353681173,1353700083,1353700083,315,315,github,false,false,false,165,11,6,2,21,0,23,0,8,3,1,3,9,0,0,2,3,2,4,9,0,0,2,0,0,0,0,34.24541086398936,1.0111476307438665,9,vlad@vene.ro,travis.yml|travis.yml|.travis.yml|README.rst|requirements.txt|travis.yml|travis.yml|requirements.txt|travis.yml,9,0.0,0,16,true,Travis Travis is a continuous integration service that builds the project and check if tests pass https://travis-ciorg/This pull request contains:* the travis config (travisyml) file that tells what to install of the VM (before_install:) how to install (install:) the (python) requirementstxt and sklearn and how to make tests (script:)* the pip-formated requirements of sklearn (requirementstxt)* the link to the build status image (READMErst) of course you need to change this link to https://securetravis-ciorg/scikit-learn/scikit-learnpng once you will have mergedTo setup travis you need one more thing than pulling this as described here http://abouttravis-ciorg/docs/user/getting-started/#Step-one%3A-Sign-in  go there https://travis-ciorg/ and sign-in with the scikit-learn github account (OAuth) give read _and_ write permissions (for travis hooks on push) go to your profile https://travis-ciorg/profile and activate scikit-learn Now you should have a continuously updated build status and a glimpse of it in the readme :)The full report is there https://travis-ciorg/SnippyHolloW/scikit-learn and so once merged there https://travis-ciorg/scikit-learn/scikit-learnCheersKeep up the good scikit-learn coming :),,791,0.8116308470290771,0.225444340505145,27559,360.7532929351573,32.9474944664175,95.50419100838202,2054,36,829,57,unknown,SnippyHolloW,ogrisel,false,ogrisel,0,0,57,24,1368,true,true,false,false,0,0,0,0,1,0,11
625386,scikit-learn/scikit-learn,python,1396,1353646369,1354058164,1354058164,6863,6863,merged_in_comments,false,true,false,58,64,57,2,2,0,4,0,2,4,0,14,20,13,0,0,4,0,16,20,15,0,0,77,4052,143,4232,403.24975431227364,11.906559839224022,141,vlad@vene.ro,sklearn/decomposition/__init__.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/__init__.py|sklearn/manifold/__init__.py|sklearn/manifold/spectra_embedding.py|sklearn/manifold/tests/test_spectra_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/__init__.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/__init__.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/__init__.py|sklearn/manifold/__init__.py|sklearn/manifold/spectra_embedding.py|sklearn/manifold/tests/test_spectra_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/__init__.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/manifold/plot_compare_methods.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|doc/modules/manifold.rst|doc/modules/classes.rst|doc/modules/manifold.rst|sklearn/manifold/__init__.py|doc/modules/manifold.rst|sklearn/manifold/spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|examples/cluster/plot_cluster_comparison.py|examples/cluster/plot_segmentation_toy.py|sklearn/cluster/spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/manifold/spectral_embedding.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/manifold/spectral_embedding.py|doc/whats_new.rst|examples/cluster/plot_cluster_comparison.py|examples/cluster/plot_segmentation_toy.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/manifold/spectral_embedding.py|sklearn/manifold/tests/test_spectral_embedding.py|sklearn/cluster/tests/test_spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py,75,0.0,0,0,true,MRG: spectral embedding estimator (was PR #1170) Detailed comments and description are in original pull request  PR #1170 Open the new one to have a clean diff  after rebaseIn summary this changes:1 New estimator spectral embedding out of original manifoldspectral_embedding2 Examples/Doc changes3 Add more testsThanks you all for previous review and help :),,790,0.8113924050632911,0.22693032015065914,27559,360.7532929351573,32.9474944664175,95.50419100838202,2054,36,829,61,unknown,kuantkid,amueller,false,amueller,5,0.8,12,7,87,true,false,true,false,17,82,16,31,72,0,372
605829,scikit-learn/scikit-learn,python,1395,1353635223,,1392492857,647567,,unknown,false,false,false,176,17,2,17,60,1,78,0,13,3,0,1,6,4,0,0,7,0,3,10,8,0,0,620,0,1890,0,35.88079246298945,1.0521581797291475,22,vlad@vene.ro,sklearn/neural_networks/__init__.py|sklearn/neural_networks/backprop_sgd.pyx|sklearn/neural_networks/mlperceptron.py|sklearn/setup.py|sklearn/neural_networks/__init__.py|sklearn/neural_networks/backprop_sgd.pyx|sklearn/neural_networks/mlperceptron.py|sklearn/setup.py,19,0.002824858757062147,2,23,false,WIP multilayer perceptron classifier This is a very early PR just to let you know Im working on this (and that I beat @amueller to it ) and to keep a TODO list for myselfThis PR contains a working implementation of a multilayer perceptron classifier trained with batch gradient descent Features:* one hidden layer with logistic activation function* minibatch SGD with momentum method (rolling average of gradients)* multiclass cross-entropy loss* [preliminary] 0878 F1-score on small version of 20news (n_hidden400 alpha0 max_iter100 random_state42 tol1e-4 learning_rate02)* L2 penalty doesnt seem to work very well though so turned off by default* tanh hidden layer activationTODO:* reuse parts of SGDClassifier* hinge loss* multitarget/multilabel output* optimize further* [adaptive learning rates](http://arxivorg/abs/12061106)* tests* demos* documentationNot TODO at least not in this PR:* regression* conjugate gradient optimization* multiple hidden layers* L1 penalty* Students t penalty* reuse @ppretts Cython code for datasets and weight vectors (took too much refactoring)* [dropout](http://arxivorg/pdf/12070580pdf) regularization,,789,0.8124207858048162,0.22693032015065914,27666,361.6713655750741,32.892358852020536,95.9661678594665,2054,36,828,169,travis,larsmans,larsmans,true,,53,0.7547169811320755,77,30,858,true,true,false,false,62,177,42,51,91,7,3
625393,scikit-learn/scikit-learn,python,1392,1353555021,,1353627510,1208,,unknown,false,true,false,84,1,1,0,3,0,3,0,4,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.667872240591264,0.13840115601594602,4,vlad@vene.ro,doc/install.rst,4,0.0037914691943127963,0,0,false,WIP recommend OpenBLAS for Ubuntu (and Debian) I just spent way too much time optimizing matrix multiplication on an Ubuntu/AMD box and found out that installing OpenBLAS instead of ATLAS can give a major speedup to good old npdot on two 2-d arrays of dtypenpfloat64 It takes just two apt-get commands to replace ATLAS so I added those to the installation instructionsThe main thing left to do for this PR is test whether the same trick works on my Debian/Intel box at work,,788,0.8134517766497462,0.22843601895734597,27502,359.21023925532694,32.76125372700167,95.0476328994255,2053,36,827,56,unknown,larsmans,larsmans,true,,52,0.7692307692307693,77,30,857,true,true,false,false,62,175,41,51,91,7,596
511353,scikit-learn/scikit-learn,python,1388,1353501047,1355226379,1355226379,28755,28755,github,false,false,false,69,36,2,8,73,0,81,0,7,0,0,3,10,3,0,0,0,0,10,10,9,0,0,54,0,1050,282,17.173866936910763,0.5092020975005586,62,vlad@vene.ro,sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py,59,0.009569377990430622,0,35,false,MRG: Issue #1047 base gradient boosting uses decision tree instead of tree It is an early pull request to treat issue  #1047 TODO list:-DONE: Use sklearntreeDecisionTreeRegressor instead of sklearntree_tree in gradient tree boosting-DONE: Set a benchmark to measure the impact of 1-REPLACE: Implement a _fit method in sklearntreeDecisionTreeRegressro as suggested in issue #1047BY using the check_input argsPS: This is my first pull request,,787,0.8132147395171537,0.23062200956937798,27479,358.78307070854106,32.60671785727283,94.79966519887914,2053,35,827,69,unknown,arjoly,glouppe,false,glouppe,0,0,14,17,337,false,true,true,true,2,4,0,0,0,0,24
507884,scikit-learn/scikit-learn,python,1387,1353487077,,1353933747,7444,,unknown,false,true,false,70,7,5,1,10,0,11,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,152,51,164,51,36.83813510302275,1.092244148122085,21,vlad@vene.ro,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/metrics/pairwise.py,20,0.019138755980861243,1,5,false,WIP chi square kernel and histogram intersection kernel This adds the chi square kernel and histogram intersection kernels (issue #1172) and raise an TypeError for Manhattan distances for sparse inputs (issue #1384)Currently they both supports dense matrix only Not sure how to make it effectively support dense/sparse input and sparse/sparse inputsSome discussion for adapting @jakevdp https://githubcom/jakevdp/pyDistances/ for possible solutions for sparse inputs can be found in (issue #1384),,786,0.8142493638676844,0.23062200956937798,27479,358.78307070854106,32.60671785727283,94.79966519887914,2053,35,827,65,unknown,kuantkid,,false,,4,1.0,12,7,85,true,false,false,false,14,80,15,23,69,0,167
506846,scikit-learn/scikit-learn,python,1385,1353434562,1354451848,1354451848,16954,16954,merged_in_comments,false,false,false,121,7,1,15,21,0,36,0,5,1,0,1,4,2,0,0,1,0,3,4,2,0,0,131,0,192,0,9.440795976964537,0.2799162629491384,0,,examples/manifold/plot_compare_methods.py|examples/manifold/plot_manifold_sphere.py,0,0.0,1,9,false,New Manifold Learning Example using a Sphere Hi allThis is another manifold learning comparison example this time using a spherical datasetI sort of merged @jakevdp s [S-curve](http://scikit-learnorg/stable/auto_examples/manifold/plot_compare_methodshtml) example into mine when I added LLE and MDSHere is the plot the example generates:[Spherical Dataset Manifold Learning](http://i46tinypiccom/1076xvrpng)The end idea with this which would be pretty cool to have in the example gallery is to use something like the [Basemap Matplotlib](http://matplotliborg/basemap/users/exampleshtml) toolkit to have an actual Earth Globe Something like (without the contours of course):[Basemap globe](http://matplotliborg/basemap/_images/contour1png)and then cut it open as with the example and apply the manifold learning techniques to itIt could be a very useful tool for developing intuition when it comes to manifold-learning,,784,0.8150510204081632,0.23466407010710807,27479,358.78307070854106,32.60671785727283,94.79966519887914,2051,35,826,61,unknown,jaquesgrobler,amueller,false,amueller,27,0.9259259259259259,9,12,300,true,true,false,true,27,87,32,8,38,0,41
496936,scikit-learn/scikit-learn,python,1382,1353329263,1353533086,1353533086,3397,3397,github,false,false,false,81,5,1,4,13,0,17,0,6,0,0,2,3,2,0,0,0,0,3,3,2,0,0,46,26,64,28,9.268076250511573,0.2751702329787453,36,vlad@vene.ro,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,34,0.03343166175024582,0,6,false,MRG: Add dummy feature utility function This PR adds a simple utility function to the preprocessing module that adds a dummy feature (a column with all 1) to a dataset It supports ndarrays COO matrices and CSC matrices efficiently For CSR matrices it converts to COO firstIts not used in the scikit yet but since its not trivial to implement in the sparse case it think it is nice to have a well-tested function for this purpose in the scikit,,783,0.8148148148148148,0.24090462143559488,27444,354.831657192829,32.28392362629354,94.33756012243114,2049,35,825,57,unknown,mblondel,mblondel,true,mblondel,20,0.85,186,29,965,true,true,false,false,52,190,17,53,46,5,55
494545,scikit-learn/scikit-learn,python,1381,1353272929,1359918970,1359918970,110767,110767,github,false,true,false,178,153,6,78,93,3,174,0,7,1,0,2,19,3,0,0,3,0,18,21,13,0,0,718,0,3500,1558,77.12061611792974,2.2897668021387343,33,vlad@vene.ro,sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py|sklearn/grid_search.py|sklearn/metrics/__init__.py|sklearn/metrics/score_objects.py,27,0.010794896957801767,0,23,false,MRG adding SomeScore objects for better () grid search interface This is hopefully my last shot at adding more advanced score functions as discussed in the other PRs and the MLCompare to #1198 and #1014This is sort of a draft (again) and it would be great if I could get some feedback before going furtherThis PR basically implements a new interface in GridSearchCV using callable score objects with an (estimator X y)signature To create these objects I introduced an internal factory-style functionThe idea would be to either pass a string or a callable object to GridSearchCV~~At the moment I complicated things a bit by keeping score_func as the parameter name in GridSearchCV I think it might be better to deprecate it and add a new score parameter~~~~I am not sure if I am entirely happy with this The factory seems a bit overly complicated but maybe that is a matter of taste Alternative suggestions welcome It feels a bit as if decorators would be a more natural~~I am happy now,,782,0.8145780051150895,0.2423945044160942,27444,354.831657192829,32.28392362629354,94.33756012243114,2049,35,824,75,unknown,amueller,amueller,true,amueller,137,0.8686131386861314,446,32,758,true,true,false,false,230,1285,174,224,224,2,33
486642,scikit-learn/scikit-learn,python,1376,1353018254,1353068412,1353068412,835,835,github,false,false,false,10,1,1,0,3,0,3,0,3,0,0,8,8,6,0,0,0,0,8,8,6,0,0,34,10,34,10,36.0650907670792,1.0797740864352199,91,vlad@vene.ro,doc/modules/classes.rst|doc/modules/ensemble.rst|examples/ensemble/plot_random_forest_embedding.py|examples/manifold/plot_lle_digits.py|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tests/test_common.py,49,0.01644100580270793,0,0,false,MRG rename RandomForestEmbedding to RandomTreesEmbedding Renaming as discussed in #1288,,781,0.8143405889884763,0.23597678916827852,27304,354.70993261060653,32.22970993261061,94.05215353061823,2040,35,821,55,unknown,amueller,amueller,true,amueller,136,0.8676470588235294,442,32,755,true,true,false,false,236,1293,189,205,227,2,712
486303,scikit-learn/scikit-learn,python,1375,1353011215,,1353012480,21,,unknown,false,false,false,65,62,62,0,5,0,5,0,4,1,1,53,55,42,1,6,1,1,53,55,42,1,6,826,344,826,344,433.99979833011514,12.993776690636677,439,vlad@vene.ro,doc/whats_new.rst|doc/conf.py|doc/themes/scikit-learn/layout.html|setup.py|sklearn/__init__.py|doc/whats_new.rst|doc/support.rst|sklearn/covariance/robust_covariance.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_utils.py|sklearn/utils/extmath.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_utils.py|sklearn/utils/tests/test_utils.py|examples/linear_model/plot_sparse_recovery.py|sklearn/linear_model/bayes.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/tests/test_common.py|doc/whats_new.rst|doc/conf.py|doc/whats_new.rst|sklearn/ensemble/forest.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/rfe.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|doc/logos/scikit-learn-logo-notext.png|doc/logos/scikit-learn-logo-small.png|doc/logos/scikit-learn-logo-thumb.png|doc/logos/scikit-learn-logo.bmp|doc/logos/scikit-learn-logo.png|doc/logos/scikit-learn-logo.svg|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|examples/document_classification_20newsgroups.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/tests/test_rfe.py|AUTHORS.rst|examples/linear_model/plot_logistic_l1_l2_sparsity.py|doc/modules/feature_selection.rst|sklearn/feature_selection/rfe.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|doc/modules/feature_extraction.rst|sklearn/manifold/mds.py|sklearn/cluster/k_means_.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/linear_model/coordinate_descent.py|sklearn/utils/fixes.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/datasets/base.py|sklearn/datasets/samples_generator.py|sklearn/feature_extraction/image.py|sklearn/metrics/cluster/tests/test_supervised.py|sklearn/preprocessing.py|sklearn/utils/arpack.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|doc/whats_new.rst|README.rst|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|doc/whats_new.rst|sklearn/__init__.py,75,0.008737864077669903,0,2,false,imbalanced data set / stratify train_set undersample one class Dear SciKittersgiven a data set with with 2 y_labels (80 % 0 and 20 % 1) I would like to create train set which shows an equal weightingCurrently train_test_split only allows for a test_training split which gives the same weightingFrom the mailing list I got the recommendation to submit a PRCheersPaul,,780,0.8153846153846154,0.23689320388349513,27304,354.70993261060653,32.22970993261061,94.05215353061823,2040,35,821,55,unknown,pzc,ogrisel,false,,0,0,0,0,0,false,true,false,false,0,0,0,0,0,0,4
486284,scikit-learn/scikit-learn,python,1374,1353010975,1353180742,1353180742,2829,2829,github,false,false,false,47,1,1,0,0,0,0,0,0,0,0,3,3,1,1,1,0,0,3,3,1,1,1,3,0,3,0,13.734845136122779,0.41121565324670545,13,peter.prettenhofer@gmail.com,doc/conf.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/theme.conf,13,0.0029182879377431907,0,0,false,DOC: add google analytics theme option This adds an option in confpy to turn off Google Analytics in the generated html - addresses Issue #1205  By default this changes nothing but by setting the google_analytics option to False the documentation can be generated without the GA javascript,,779,0.8151476251604621,0.23735408560311283,27304,354.70993261060653,32.22970993261061,94.05215353061823,2040,35,821,54,unknown,jakevdp,larsmans,false,larsmans,28,0.8928571428571429,748,0,554,true,true,false,false,15,43,5,2,7,0,-1
470980,scikit-learn/scikit-learn,python,1373,1353002405,1353588301,1353588301,9764,9764,github,false,false,false,79,34,8,33,64,2,99,0,9,3,0,9,18,9,0,0,4,0,15,19,11,0,0,121,43,299,161,54.61954496160482,1.637600740575423,98,vlad@vene.ro,doc/modules/classes.rst|doc/modules/isotonic.rst|doc/modules/linear_model.rst|doc/supervised_learning.rst|examples/plot_isotonic_regression.py|sklearn/__init__.py|sklearn/isotonic.py|sklearn/linear_model/__init__.py|sklearn/manifold/mds.py|sklearn/tests/test_common.py|sklearn/tests/test_isotonic.py|sklearn/random_classifier.py|sklearn/tests/test_random_classifier.py|sklearn/tests/test_random_classifier.py|sklearn/random_classifier.py,51,0.0019474196689386564,0,12,false,MRG: Dummy estimators This PR is gonna win the prize of the most silly classifier ever added to scikit-learnI think its a good sanity check when doing classification to compare with random classification This estimator supports three strategies for generating random predictions: stratified most_frequent and uniform The first two strategies can give surprisingly good accuracy if youre doing binary classification or if your dataset is unbalancedIf people think its useful Im gonna add it to the documentation,,778,0.8149100257069408,0.23758519961051608,27303,354.7595502325752,32.2308903783467,94.05559828590265,2039,35,821,60,unknown,mblondel,mblondel,true,mblondel,19,0.8421052631578947,184,29,961,true,true,false,false,54,186,16,56,33,5,3
469793,scikit-learn/scikit-learn,python,1372,1352994390,1353946609,1353946609,15870,15870,github,false,false,false,28,3,1,5,5,5,15,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.480437905782039,0.1343322877876148,7,vlad@vene.ro,doc/developers/index.rst,7,0.006862745098039216,0,0,false,doc fix - trailing underscore and init param update Suggested fix for #1351Please let me know if you can recommend better wording anywhere or have suggestionsJ,,777,0.8146718146718147,0.23921568627450981,27303,354.7595502325752,32.2308903783467,94.05559828590265,2038,35,821,64,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,26,0.9230769230769231,9,12,295,true,true,false,false,26,82,32,5,36,0,24
484645,scikit-learn/scikit-learn,python,1371,1352988845,1352993076,1352993076,70,70,commit_sha_in_comments,false,false,false,28,3,3,0,4,0,4,0,5,0,0,2,2,1,0,0,0,0,2,2,1,0,0,4,0,4,0,14.830252762429074,0.44464016803875833,17,peter.prettenhofer@gmail.com,doc/modules/decomposition.rst|sklearn/decomposition/nmf.py|doc/modules/decomposition.rst,10,0.00980392156862745,0,3,false,Change the links again so that everyone can access them Change the links again so that everyone can access them though the commits seems a little bit redundant ,,776,0.8144329896907216,0.23921568627450981,27303,354.7595502325752,32.2308903783467,94.05559828590265,2037,35,821,53,unknown,fannix,larsmans,false,larsmans,6,0.8333333333333334,10,1,673,false,true,false,false,2,3,1,0,1,0,0
484337,scikit-learn/scikit-learn,python,1369,1352981326,1352983771,1352983771,40,40,github,false,false,false,41,2,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.599329185907138,0.13789723324941655,1,vlad@vene.ro,doc/Makefile,1,0.0009746588693957114,0,2,false,BUG: Fix path in doc cleaning In the documentation Makefile all paths but one are set relativelyto the doc folder This fixes it for the path to the generatedfolder which could cause trouble when rebuilding documentationafter API change,,775,0.8141935483870968,0.2378167641325536,27303,354.7595502325752,32.2308903783467,94.05559828590265,2037,35,821,54,unknown,AlexandreAbraham,jaquesgrobler,false,jaquesgrobler,1,1.0,13,2,213,false,true,false,false,2,2,0,0,0,0,1
484173,scikit-learn/scikit-learn,python,1367,1352975762,1352983143,1352983143,123,123,github,false,false,false,22,1,1,0,1,0,1,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.959224843377551,0.14868763624464404,7,peter.prettenhofer@gmail.com,sklearn/decomposition/nmf.py,7,0.00682261208576998,0,1,false,Fix broken links in the doc I think we should link to a more stable site People are moving all the time,,774,0.813953488372093,0.2378167641325536,27303,354.7595502325752,32.2308903783467,94.05559828590265,2037,35,821,54,unknown,fannix,jaquesgrobler,false,jaquesgrobler,5,0.8,10,1,673,false,true,false,false,0,1,0,0,1,0,122
481707,scikit-learn/scikit-learn,python,1366,1352937778,1353008341,1353008341,1176,1176,github,false,false,false,11,2,1,0,7,0,7,0,4,0,0,4,7,6,0,0,1,0,7,8,7,0,0,10,5,13,5,17.58371426181755,0.5271954776323992,70,vlad@vene.ro,sklearn/__init__.py|sklearn/isotonic.py|sklearn/linear_model/__init__.py|sklearn/manifold/mds.py|sklearn/tests/test_common.py|sklearn/tests/test_isotonic.py,47,0.012670565302144249,0,0,false,API : move isotonic regression out of linear_model here we go,,773,0.8137128072445019,0.2378167641325536,27303,354.7595502325752,32.2308903783467,94.05559828590265,2036,35,820,54,unknown,agramfort,larsmans,false,larsmans,26,0.8461538461538461,89,177,1078,true,true,true,false,45,210,32,100,102,2,4
479662,scikit-learn/scikit-learn,python,1364,1352912264,1353262992,1353262992,5845,5845,merged_in_comments,false,false,false,29,1,1,0,3,0,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,14,0,14,0,4.54031325950301,0.1361284650328901,7,vlad@vene.ro,setup.py,7,0.006796116504854369,0,0,false,P3K: Fix build for py3k + pip Installing scikit-learn with pip fails on python 3This is the same fix as used in numpy For more info seehttp://projectsscipyorg/numpy/ticket/1857,,772,0.8134715025906736,0.23689320388349513,27292,354.90253554155066,32.24388099076653,94.09350725487323,2036,36,820,53,unknown,AStaric,GaelVaroquaux,false,GaelVaroquaux,1,1.0,4,0,656,false,true,false,false,1,4,1,0,0,0,6
473917,scikit-learn/scikit-learn,python,1361,1352806538,1352901774,1352901774,1587,1587,github,false,false,false,21,4,0,0,13,0,13,0,4,0,0,0,5,0,0,0,0,0,5,5,5,0,0,0,0,3,26,0,0.0,0,,,0,0.0,0,5,false,FIX Python 3 compatibility Fixed old-style imports a bug that prevented scikit-learn from importing in python 32 and some failing tests,,771,0.8132295719844358,0.23828125,27126,355.0099535500995,32.2200103222001,93.93202093932021,2035,36,819,56,unknown,AStaric,ogrisel,false,ogrisel,0,0,4,0,655,false,true,false,false,0,0,0,0,0,0,63
464249,scikit-learn/scikit-learn,python,1359,1352756086,,1379332039,442932,,unknown,false,false,false,95,3,2,1,18,0,19,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,7,5,62,5,13.216502849563257,0.39685776129124734,34,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/decomposition/nmf.py,31,0.0302734375,0,7,false,Fixed bug with transform() method of ProjectedGradientNMF The slice X[j :]  is converted to an array (in scipyoptimizennls) which results in an empty tuple for the size of the slice when X is sparse scipyoptimizennls then raises a ValueError claiming a vector is required The problem only occurs when one invokes fit followed by a transform which is what happens when ProjectedGradientNMF is used in a Pipeline which is passed on to cross_validationcross_val_scoreIve added a statement  demonstrating the problem to the unit test Without the fix I am submitting the unit test fails        ,,770,0.8142857142857143,0.23828125,27126,355.0099535500995,32.2200103222001,93.93202093932021,2035,36,818,128,unknown,mbatchkarov,larsmans,false,,3,0.3333333333333333,6,14,476,false,false,false,false,3,3,3,0,0,0,102
470519,scikit-learn/scikit-learn,python,1357,1352753377,1352806550,1352806550,886,886,github,false,false,false,33,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,3.597270254547459,0.10801682080570016,36,vlad@vene.ro,sklearn/cross_validation.py,36,0.0351219512195122,0,0,false,fixed the __repr__ method of cross_validationBootstrap which failed if selfrandom_state is None with the following error:File /scikit-learn/sklearn/cross_validationpy line 700 in __repr__    selfrandom_stateTypeError: %d format: a number is required not NoneType,,769,0.8140442132639792,0.23804878048780487,27126,355.0099535500995,32.2200103222001,93.93202093932021,2035,36,818,56,unknown,mbatchkarov,GaelVaroquaux,false,GaelVaroquaux,2,0.0,6,14,476,false,false,false,false,3,3,2,0,0,0,-1
470312,scikit-learn/scikit-learn,python,1356,1352750228,,1352753027,46,,unknown,false,false,false,34,2,2,0,1,0,1,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,148,0,148,0,7.562097353704964,0.22707043312573333,41,vlad@vene.ro,sklearn/decomposition/nmf.py|sklearn/cross_validation.py,36,0.03515625,0,0,false,Crossvalidation __repr__ fix The  __repr__ method of cross_validationBootstrap failed if selfrandom_state is NoneNote: please ignore the a25c3a9 commit it is a part of another pull request and was accidentally left of this branch,,768,0.8151041666666666,0.23828125,27126,355.0099535500995,32.2200103222001,93.93202093932021,2035,36,818,55,unknown,mbatchkarov,mbatchkarov,true,,1,0.0,6,14,476,false,false,false,false,1,3,1,0,0,0,4
460875,scikit-learn/scikit-learn,python,1355,1352740432,,1352753038,210,,unknown,false,false,false,31,2,1,2,2,0,4,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,144,0,238,0,4.172907304632556,0.12530172844074963,7,peter.prettenhofer@gmail.com,sklearn/decomposition/nmf.py,7,0.006842619745845552,0,0,false,fixed bug with nnmftransform() where if X is sparser so is the slice X [j :] and thus in scipyoptimizennls the call to chkfinite results in an empty tuple for bshape,,767,0.8161668839634941,0.23851417399804498,27126,355.0099535500995,32.2200103222001,93.93202093932021,2035,36,818,54,unknown,mbatchkarov,mbatchkarov,true,,0,0,6,14,476,false,false,false,false,1,3,0,0,0,0,161
465985,scikit-learn/scikit-learn,python,1353,1352584040,1352587662,1352587662,60,60,merged_in_comments,false,false,false,51,1,1,0,1,0,1,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,28,15,28,15,8.652569916914043,0.25981095690693057,7,vlad@vene.ro,sklearn/decomposition/fastica_.py|sklearn/decomposition/tests/test_fastica.py,6,0.0058823529411764705,0,2,false,FIX + ENH: catch custom function argument errors and inform user - added expception in case a user passes a non-tuple returning function and does not supply the derrivatives via fun_prime- added an example on the custom function type expected to the docs- added regression tests and positive tests,,766,0.8159268929503917,0.23921568627450981,27117,354.79588450049783,32.23070398642918,93.81568757605929,2032,36,816,56,unknown,dengemann,GaelVaroquaux,false,GaelVaroquaux,1,1.0,14,15,133,true,true,false,false,1,17,1,0,3,0,26
463727,scikit-learn/scikit-learn,python,1350,1352499667,1352583963,1352583963,1404,1404,commits_in_master,false,false,false,44,8,3,0,29,0,29,0,4,0,1,3,7,1,0,1,0,1,6,7,4,0,1,5,0,799,13,13.434016481208143,0.40277538076900393,6,vlad@vene.ro,sklearn/svm/src/liblinear/LIBLINEAR_CHANGES|doc/presentations.rst|doc/tutorial/basic/tutorial.rst|sklearn/decomposition/fastica_.py,5,0.001998001998001998,0,12,true,FIX: in fastica_ the fun_prime a is string by default but is being called If fun is a callable the default sting fun_prime is called and throws an exception I edited it in a way I thought it was originally intendedWdytBestDenis,,765,0.8156862745098039,0.24275724275724275,27112,352.09501327825313,32.19976394216583,93.83298908232517,2031,36,815,57,unknown,dengemann,dengemann,true,dengemann,0,0,14,15,132,true,true,false,false,0,0,0,0,1,0,4
458828,scikit-learn/scikit-learn,python,1347,1352489802,,1352902715,6881,,unknown,false,false,false,29,5,1,12,10,0,22,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,64,34,136,34,9.087672779251502,0.272464522108025,35,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,33,0.033066132264529056,0,3,true,MRG: metricspy: bugfix in precision_recall_curve and added tests Ok I hope I got the pull request right this time -- this should solve the problem mentioned in issue 1342,,763,0.817824377457405,0.24348697394789579,27112,352.09501327825313,32.19976394216583,93.83298908232517,2031,35,815,57,unknown,conradlee,amueller,false,,5,1.0,4,1,959,true,true,false,false,8,20,6,0,1,0,12
459189,scikit-learn/scikit-learn,python,1345,1352410390,1352487480,1352487481,1284,1284,github,false,false,false,54,0,0,0,4,0,4,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,7,false,metricspy: bug fix on for precision_recall_curve This should fix the bug noted in issue #1342Please review this bugfix  Note that the latest version of precision_recall_curve uses an auxillary funciton called pairwise which Ive added at the end of metricspy - this auxillary function should probably go somewhere else but Im not sure where,,762,0.8175853018372703,0.24424424424424424,27112,352.09501327825313,32.19976394216583,93.83298908232517,2031,34,814,57,unknown,conradlee,conradlee,true,conradlee,4,1.0,4,1,958,false,true,false,false,6,16,3,0,0,0,1003
459069,scikit-learn/scikit-learn,python,1344,1352408996,1352466197,1352466197,953,953,github,false,false,false,40,2,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,5,0,7,0,4.3552751386786115,0.13058024925310618,38,vlad@vene.ro,sklearn/feature_extraction/text.py,38,0.03803803803803804,0,0,false,CountVectorizer decode open file pointer fix Fixed an issue where CountVectorizerdecode leaves file pointers open after reading the contents of the file This produces unpredictable behaviour as closing the file pointer is left to the implementation of the python interpreter,,761,0.8173455978975033,0.24424424424424424,27118,351.8327310273619,32.155763699387855,93.7753521646139,2031,34,814,56,unknown,mattilyra,ogrisel,false,ogrisel,0,0,5,1,419,false,true,false,false,0,0,0,0,0,0,-1
477849,scikit-learn/scikit-learn,python,1340,1352302905,1352314896,1352314896,199,199,commits_in_master,false,false,false,31,11,11,0,1,0,1,0,1,0,0,6,6,3,0,0,0,0,6,6,3,0,0,288,80,288,80,125.37372431361926,3.775015880620146,118,vlad@vene.ro,doc/modules/classes.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,67,0.01582591493570722,0,1,false,OneHotEncoder: minor stuff Merged with master to make the merge button green + some minor improvementsDo you know of any way to link to the narrative docs from the docstring,,760,0.8171052631578948,0.24826904055390703,27064,352.1652379544783,32.18297369198936,93.88856044930536,2028,34,813,64,unknown,larsmans,larsmans,true,larsmans,51,0.7647058823529411,76,30,843,true,true,false,false,52,120,34,25,81,7,0
459167,scikit-learn/scikit-learn,python,1339,1352244882,,1352658348,6891,,unknown,false,false,false,33,1,1,0,8,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,127,0,127,0,4.380375624860308,0.1318960427497344,3,vlad@vene.ro,sklearn/datasets/base.py,3,0.002952755905511811,0,6,false,added recursive_load_files  Added it as separate function as i am not sure you guys like it It recursively load datasetfirst sub-folder is always Categories  after that data can contain subcategories recursively ,,759,0.8181818181818182,0.2470472440944882,27090,351.4581026208933,32.07825765965301,93.65079365079364,2027,34,812,65,unknown,v3ss0n,larsmans,false,,1,0.0,10,10,770,false,true,false,false,0,0,1,0,0,0,144
467463,scikit-learn/scikit-learn,python,1338,1352244360,,1352658575,6903,,unknown,false,false,false,24,2,1,0,3,0,3,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,4,0,131,0,4.5295304428689445,0.13638719417993242,37,vlad@vene.ro,sklearn/feature_extraction/text.py,37,0.03641732283464567,0,2,false,Added term_counts_per_doc as CountVectorizers property  So that  when we want to find out top 1020 etc terms we can retrieve from inherited instances ,,758,0.8192612137203166,0.2470472440944882,27090,351.4581026208933,32.07825765965301,93.65079365079364,2027,34,812,64,unknown,v3ss0n,larsmans,false,,0,0,10,10,770,false,true,false,false,0,0,0,0,0,0,4260
475215,scikit-learn/scikit-learn,python,1335,1352189022,1352822305,1352822305,10554,10554,github,false,false,false,42,2,1,0,8,0,8,0,5,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.486856234885516,0.13510229356582776,8,vlad@vene.ro,doc/modules/clustering.rst,8,0.007905138339920948,0,2,false,WIP: Updating clustering documentation Updating clustering documentation to provide more information and clearer information to non-expertsSignificantly a work in progress and Im happy to take directions/patches/pull requests/ideas/examples/etc*TODO*- More algorithms to go (going to update all algorithms)- More examples,,757,0.8190224570673712,0.24802371541501977,27090,351.4211886304909,32.07825765965301,93.65079365079364,2025,34,812,65,unknown,robertlayton,larsmans,false,larsmans,14,0.7857142857142857,5,10,536,false,true,true,false,2,6,0,0,0,0,0
479151,scikit-learn/scikit-learn,python,1334,1352166467,1352319377,1352319377,2548,2548,commit_sha_in_comments,false,false,false,114,2,1,0,6,0,6,0,7,3,0,1,5,4,0,0,3,0,2,5,4,0,0,147,0,152,0,13.853793288174757,0.417147140411339,19,vlad@vene.ro,sklearn/metrics/cluster/expected_mutual_info_fast.c|sklearn/metrics/cluster/expected_mutual_info_fast.pyx|sklearn/metrics/cluster/setup.py|sklearn/metrics/cluster/supervised.py,19,0.0,0,11,false,MRG: cythonized expected_mutual_information I read http://commentsgmaneorg/gmanecomppythonscikit-learn/3997 for guidance Here is a small script that shows decent speedup pythonfrom sklearnmetricsclustersupervised import contingency_matrixfrom sklearnmetricsclustersupervised import expected_mutual_informationimport numpy as npfrom expected_mutual_info_fast import expected_mutual_information as fasttrue  nparray([nprandomrandint(100) for i in range(1000)])pred  nparray([nprandomrandint(100) for i in range(1000)])contingency  contingency_matrix(true pred)# original expected_mutual_information from supervisedpy%timeit expected_mutual_information(contingency len(true))1 loops best of 3: 253 s per loop# cython version%timeit fast(contingency len(true))10 loops best of 3: 758 ms per loopThis is my first time integrating a piece of Cython into a larger project so I am eager to hear any ways I could improve it Thanks,,756,0.8187830687830688,0.24802371541501977,27090,351.4211886304909,32.07825765965301,93.65079365079364,2025,34,811,63,unknown,coreylynch,GaelVaroquaux,false,GaelVaroquaux,1,1.0,23,17,721,true,false,false,false,1,1,1,0,6,0,684
624818,scikit-learn/scikit-learn,python,1333,1352145389,,1370343093,303295,,unknown,false,false,false,366,19,3,0,8,0,8,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,81,2,765,311,17.55506180534399,0.5285937735396002,50,vlad@vene.ro,sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,41,0.04043392504930966,0,0,false,WIP: Multi target ridge with individual penalties Hi everybodyhere is an augmented version of linear_modelridgeridge_regression It adds the solvers svd and eigen Using one decomposition of the design matrix these solvers handle multiple targets y and individual penalties for each targetPenalties are passed using the alpha-argument using an array or an iterable with length corresponding to the number of targetsThis is a first attempt at rendering this idea sklearn compatible and I am not sure it is yet so any feedback is very welcomeAs of now what is still missing is a certain number of decisions in the beginning of the function ie if multiple penalties for multiple targets are passed then the right solver should be chosen automatically I guessBoth SVD and eigen are provided because I have been having to deal with ill-conditioned matrices latelely which provoke a SVD did not converge from scipy due to some infinitesimally negative eigenvalues of the usually positive semidefinite matrix npdot(X XT) Eigen seems to be better adapted hereI added these solvers to the test_ridgetest_ridge() and they pass but this does not test for multiple penalties yetSo TODO:- boiler plate decision code at the beginning of ridge_regression- tests for the new functionalityAs a matter of fact I also plan to add a general fold-wise cross-validation scheme to RidgeCV which should benefit from the fact that one can test several penalties at once given a decomposition of the design matrix For this reason I have rendered the functionality of what I added to ridge_regression more general:If passing individual penalties to ridge_regression using an array alpha then we impose yshape[1]  alphashape[-1]All other potential dimensions of alpha can be used to try several different penalties per targetI know this feature smells strongly of YAGNI but I have been using it for an efficient grid search procedure in cross-validationOne last question: Any suggestions on how to introduce a caching option for the decomposition using joblibMemory Options would be 1) Pass your own SVD having decorated it with Memory or 2) pass your own Memory which defaults to the transparent mem  Memory(cachedirNone)Thanks for your timeMichael,,755,0.8198675496688742,0.2485207100591716,27090,351.38427464008856,32.07825765965301,93.65079365079364,2025,34,811,125,unknown,eickenberg,eickenberg,true,,1,1.0,4,2,305,true,true,false,false,0,0,0,0,3,0,760
624836,scikit-learn/scikit-learn,python,1331,1352143756,1352143774,1352143774,0,0,github,false,false,false,18,2,2,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.147184049001707,0.12487425236818635,13,vlad@vene.ro,sklearn/pipeline.py,13,0.012845849802371542,0,0,false,remove steps from Attributes of docstring remove steps from Attributes of docstring of pipelinepy as discussed in #1328,,754,0.8196286472148541,0.2490118577075099,27090,351.38427464008856,32.07825765965301,93.65079365079364,2025,34,811,60,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,25,0.92,9,12,285,true,true,false,false,24,66,25,5,37,0,-1
478880,scikit-learn/scikit-learn,python,1330,1352143500,1352311593,1352311593,2801,2801,merged_in_comments,false,false,false,17,3,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.771660885438236,0.1436776324810205,2,peter.prettenhofer@gmail.com,doc/modules/tree.rst,2,0.001976284584980237,0,0,false,DOC: Improved the code that shows how to export a decision tree to Graphviz Fixes issue #1329,,753,0.8193891102257637,0.2490118577075099,27090,351.38427464008856,32.07825765965301,93.65079365079364,2025,34,811,65,unknown,tjanez,GaelVaroquaux,false,GaelVaroquaux,2,1.0,3,3,27,true,true,false,false,2,5,2,1,3,0,0
624837,scikit-learn/scikit-learn,python,1323,1351970734,1352033466,1352033466,1045,1045,github,false,false,false,14,1,1,0,2,0,2,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,19,0,19,0,4.46749694625736,0.1350441795083452,33,vlad@vene.ro,sklearn/feature_extraction/text.py,33,0.03260869565217391,0,0,false,Add docs for vocabulary_ and stop_words_ attributes of Countvectorizer (Also rename max_df_stop_words_ to stop_words_),,752,0.8191489361702128,0.2509881422924901,26961,345.86996031304477,31.45283928637662,93.28289010051556,2024,33,809,62,unknown,dnouri,mblondel,false,mblondel,2,0.5,159,42,1352,false,true,false,false,4,3,2,0,0,0,951
458605,scikit-learn/scikit-learn,python,1322,1351968600,1356545147,1356545147,76275,76275,commit_sha_in_comments,false,false,false,60,3,3,6,2,0,8,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,0,98,0,98,13.464297631090556,0.4070008436760182,91,vlad@vene.ro,sklearn/tests/test_common.py|sklearn/utils/tests/test_testing.py|sklearn/utils/testing.py,78,0.024703557312252964,0,1,false,WIP Refactoring test common This is an attempt to re-factor test_common by avoiding:* code duplication* making the tests simpler (testing less functionality in one test)* concentrating the special properties of some estimators / models in one place* etcDISCLAIMER: I havent been able with a clear strategy this is just try and error at the moment,,751,0.8189081225033289,0.2509881422924901,26961,345.86996031304477,31.45283928637662,93.28289010051556,2024,33,809,79,unknown,ibayer,ibayer,true,ibayer,16,0.625,2,0,245,true,true,false,false,8,37,42,30,24,0,374
624838,scikit-learn/scikit-learn,python,1321,1351954820,,1352034143,1322,,unknown,false,false,false,15,4,4,0,4,0,4,0,2,0,0,13,13,11,0,0,0,0,13,13,11,0,0,251,92,251,92,76.40152764125406,2.309488167195105,83,vlad@vene.ro,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|doc/modules/decomposition.rst|doc/whats_new.rst|examples/decomposition/plot_faces_decomposition.py|examples/decomposition/plot_image_denoising.py|examples/decomposition/plot_sparse_coding.py|examples/linear_model/plot_omp.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/tests/test_sparse_pca.py|doc/whats_new.rst|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|sklearn/semi_supervised/label_propagation.py|doc/whats_new.rst,62,0.0009891196834817012,0,0,false,MRG parameter renaming #1217* n_atoms to n_components everywhere* max_iters to max_iter in LabelPropagation,,750,0.82,0.2512363996043521,26961,345.86996031304477,31.45283928637662,93.28289010051556,2024,33,809,62,unknown,ibayer,amueller,false,,15,0.6666666666666666,2,0,245,true,true,false,false,8,43,41,30,23,0,55
488688,scikit-learn/scikit-learn,python,1320,1351951110,1352033894,1352033894,1379,1379,github,false,false,false,87,13,8,0,3,0,3,0,4,0,0,5,12,5,0,0,0,0,12,12,12,0,0,36,63,77,251,35.70518010014756,1.0793068344923396,32,vlad@vene.ro,sklearn/hmm.py|sklearn/hmm.py|sklearn/tests/test_hmm.py|sklearn/tests/test_hmm.py|sklearn/gaussian_process/tests/test_gaussian_process.py|sklearn/qda.py|sklearn/tests/test_qda.py|sklearn/tests/test_qda.py,18,0.006923837784371909,0,2,false,Add a few tests to improve code coverage Related to #1230Before:sklearnhmm                                         466     47    90%   268-271 300-301 321-322 338-339 421 423 428 463-465 548 551 584-588 734 743-750 776 788 839-840 969 1033 1037 1041 1046 1055 1140 1197 1204 1225 1236-1238sklearnqda                                          75      6    92%   88 92 109 113 121-124After:sklearnhmm                                         461     37    92%   300-301 423 458-460 543 546 579-583 729 738-745 771 783 834-835 964 1028 1032 1036 1041 1050 1135 1192 1199 1220 1231-1233sklearnqda                                          73      0   100%   ,,749,0.8197596795727636,0.2512363996043521,26961,345.86996031304477,31.45283928637662,93.28289010051556,2024,33,809,63,unknown,dnouri,mblondel,false,mblondel,1,0.0,159,42,1352,false,true,false,false,2,2,1,0,0,0,93
624839,scikit-learn/scikit-learn,python,1319,1351885045,,1351951199,1102,,unknown,false,true,false,87,2,2,0,1,1,2,1,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,199,0,199,0,4.176279905465912,0.12624163757415668,35,vlad@vene.ro,sklearn/feature_extraction/text.py,35,0.0344149459193707,0,0,true,Add a few tests to improve code coverage Related to #1230Before:sklearnhmm                                         466     47    90%   268-271 300-301 321-322 338-339 421 423 428 463-465 548 551 584-588 734 743-750 776 788 839-840 969 1033 1037 1041 1046 1055 1140 1197 1204 1225 1236-1238sklearnqda                                          75      6    92%   88 92 109 113 121-124After:sklearnhmm                                         461     37    92%   300-301 423 458-460 543 546 579-583 729 738-745 771 783 834-835 964 1028 1032 1036 1041 1050 1135 1192 1199 1220 1231-1233sklearnqda                                          73      0   100%   ,,748,0.820855614973262,0.25172074729596855,26950,346.01113172541744,31.465677179962896,93.32096474953617,2024,33,808,62,unknown,dnouri,dnouri,true,,0,0,159,42,1351,false,true,false,false,1,2,0,0,0,0,8
497772,scikit-learn/scikit-learn,python,1318,1351884644,1356368748,1356368748,74735,74735,merged_in_comments,false,false,false,90,11,2,10,23,0,33,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,26,11,48,52,14.166745198054498,0.42823593087175993,36,vlad@vene.ro,sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_extmath.py,30,0.029498525073746312,0,7,true,[MRG] SVD / PCA sign flipping to solve ambiguity As discussed in Issue #1260 and previously on the mailing listthis PR adds a function that adjusts the output of an SVD such thatthere is no sign ambiguity like it usually happensOpen questions:✓ Is this way OK or should it be a parameter in randomized_svd or a  wrapper that calls the svd itself✓ Should the adjustment be made in-place  This doesnt work if full_matricesTrue but AFAIK we never use that   Should I make it work,,747,0.820615796519411,0.25172074729596855,26950,346.01113172541744,31.465677179962896,93.32096474953617,2024,33,808,78,unknown,vene,amueller,false,amueller,29,0.9310344827586207,41,19,936,true,true,false,true,22,96,48,24,64,2,226
624840,scikit-learn/scikit-learn,python,1317,1351883798,1351941711,1351941711,965,965,merged_in_comments,false,false,false,9,1,1,0,2,0,2,0,2,0,0,8,8,6,0,0,0,0,8,8,6,0,0,106,10,106,10,34.7855410677473,1.0515060694442582,97,vlad@vene.ro,doc/modules/ensemble.rst|doc/whats_new.rst|examples/ensemble/plot_gradient_boosting_quantile.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/ensemble/plot_gradient_boosting_regularization.py|sklearn/cluster/k_means_.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py,61,0.007866273352999017,0,1,true,WIP parameter renaming #1217renaming parameter learn_rate to learning_rate,,746,0.8203753351206434,0.25172074729596855,26950,346.01113172541744,31.465677179962896,93.32096474953617,2024,33,808,61,unknown,ibayer,amueller,false,amueller,14,0.6428571428571429,2,0,244,true,true,false,false,6,47,42,30,23,0,3
624841,scikit-learn/scikit-learn,python,1316,1351872981,,1351961590,1476,,unknown,false,false,false,23,2,2,0,2,0,2,0,2,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,9.821809677544607,0.2968951757596455,9,peter.prettenhofer@gmail.com,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/layout.html,9,0.008849557522123894,0,1,true,hompage: link headers in side panel Add links to the headers in the side panel* News* Presentations* Participate* ,,745,0.8214765100671141,0.25172074729596855,26950,345.56586270871986,31.42857142857143,93.28385899814472,2024,33,808,62,unknown,ibayer,amueller,false,,13,0.6923076923076923,2,0,244,true,true,false,false,6,46,44,32,23,0,30
458393,scikit-learn/scikit-learn,python,1315,1351871906,1352312725,1352312725,7346,7346,merged_in_comments,false,false,false,8,1,1,0,7,0,7,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,56,11,56,11,8.50761333162156,0.2571694461929686,6,peter.prettenhofer@gmail.com,sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py,5,0.004916420845624385,0,1,true,MRG hotfix for NMF sparsity problem Closes #1160,,744,0.821236559139785,0.25172074729596855,26950,345.56586270871986,31.42857142857143,93.28385899814472,2024,33,808,66,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,135,0.8666666666666667,436,31,742,true,true,false,false,217,1175,208,199,254,3,52
480175,scikit-learn/scikit-learn,python,1314,1351867202,1353934816,1353934816,34460,34460,github,false,false,false,54,4,1,0,6,0,6,0,3,0,0,9,9,8,0,0,0,0,9,9,8,0,0,41,44,43,48,38.52476910681043,1.1568002364934395,227,vlad@vene.ro,doc/whats_new.rst|sklearn/cluster/dbscan_.py|sklearn/cluster/spectral.py|sklearn/decomposition/fastica_.py|sklearn/ensemble/gradient_boosting.py|sklearn/gaussian_process/gaussian_process.py|sklearn/hmm.py|sklearn/tests/test_common.py|sklearn/tests/test_hmm.py,90,0.0275049115913556,0,3,true,MRG Check what __init__ does in test_common Used inspection magic to make sure that parameters passed to __init__ do the same as if set using fit_paramsThis doesnt test for side-effects for __init__ yet and doesnt test what fit doesAlso I renamed GMMHMMhmm_ into GMMHMMhmms_ to avoid a conflict with the init option,,743,0.8209959623149394,0.25245579567779963,27126,355.0099535500995,32.2200103222001,93.93202093932021,2024,33,808,75,unknown,amueller,amueller,true,amueller,134,0.8656716417910447,436,31,742,true,true,false,false,218,1175,207,199,254,3,7540
624819,scikit-learn/scikit-learn,python,1313,1351864637,,1379357736,458218,,unknown,false,false,false,14,1,1,0,2,0,2,0,3,12,0,0,12,12,0,0,12,0,0,12,12,0,0,3880,0,3880,0,57.427572331208765,1.7359295018416983,0,,sklearn/src/cblas/ATL_drefgemm.c|sklearn/src/cblas/ATL_drefgemmNN.c|sklearn/src/cblas/ATL_drefgemmNT.c|sklearn/src/cblas/ATL_drefgemmTN.c|sklearn/src/cblas/ATL_drefgemmTT.c|sklearn/src/cblas/ATL_drefsyrk.c|sklearn/src/cblas/ATL_drefsyrkLN.c|sklearn/src/cblas/ATL_drefsyrkLT.c|sklearn/src/cblas/ATL_drefsyrkUN.c|sklearn/src/cblas/ATL_drefsyrkUT.c|sklearn/src/cblas/atlas_reflevel3.h|sklearn/src/cblas/atlas_reflvl3.h,0,0.0,0,0,true,Add dgemm and dsyrk cblas functions This will maybe solve the segfaults in #1006,,742,0.8221024258760108,0.2527040314650934,26950,345.56586270871986,31.42857142857143,93.28385899814472,2024,33,808,129,unknown,fabianp,fabianp,true,,29,0.7241379310344828,119,22,902,true,true,false,false,21,41,16,7,9,0,17
624843,scikit-learn/scikit-learn,python,1308,1351807492,1351883793,1351883793,1271,1271,github,false,false,false,13,1,1,0,3,0,3,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,24,12,24,12,8.999023863689573,0.2720248308742706,44,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py,33,0.032448377581120944,0,0,false,MRG cross_val_score now honors _pairwise This was simply not implemented beforeFixes #1222,,741,0.8218623481781376,0.2527040314650934,26950,345.6771799628943,31.42857142857143,93.28385899814472,2023,33,807,59,unknown,amueller,amueller,true,amueller,133,0.8646616541353384,435,31,741,true,true,false,false,215,1158,206,205,253,3,55
459478,scikit-learn/scikit-learn,python,1305,1351797922,1352574211,1352574211,12938,12938,github,false,false,false,73,5,1,13,19,0,32,0,4,0,0,6,15,6,0,0,0,0,15,15,15,0,0,25,55,79,144,25.92721280682225,0.7785048407305037,78,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/lda.py|sklearn/qda.py|sklearn/tests/test_common.py|sklearn/tree/tree.py|sklearn/utils/validation.py,43,0.018700787401574805,0,7,false,MRG check in all classifiers in fit and predict that input is finite  This addressed the rest of #1027I added a test into array2d and check_arrays For the ensembles I added a parameter in the trees fit not to test again I didnt add it into the predict_proba of the tree yet Do you think I shouldIll continue to add tests for the other estimators (probably but just extending this one),,740,0.8216216216216217,0.25196850393700787,27115,352.09293748847506,32.19620136455836,93.82260741287111,2023,33,807,65,unknown,amueller,amueller,true,amueller,132,0.8636363636363636,435,31,741,true,true,false,false,215,1150,205,205,251,3,81
505544,scikit-learn/scikit-learn,python,1304,1351735681,,1354765245,50492,,unknown,false,false,false,163,11,11,0,36,0,36,0,8,0,0,8,8,8,0,0,0,0,8,8,8,0,0,278,394,278,394,80.95643127682897,2.447278705858612,94,vlad@vene.ro,sklearn/base.py|sklearn/lda.py|sklearn/base.py|sklearn/naive_bayes.py|sklearn/tests/test_naive_bayes.py|sklearn/preprocessing.py|sklearn/base.py|sklearn/preprocessing.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/base.py|sklearn/preprocessing.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/base.py|sklearn/tree/tests/test_tree.py,38,0.01376597836774828,0,13,false,WIP: Classes support for all classifiers Im taking a stab at decoupling the classes in y from the classes that the estimators can predict I think this cleanup would allow for easier design of an ensemble class that could use arbitrary classifiers and combine their results through voting averaging stacking etcSee https://githubcom/scikit-learn/scikit-learn/issues/1183So far Ive done naive_bayes lda trees and forests I tried to do qda and almost got it right but there are issues with nan and inf LabelBinarizer needs some testing as wellIve gone with the convention of class variables set in __init__ having no trailing underscore and any other private variables using one So selfclasses gets set in __init__ and then selfclasses_ gets set in fit Maybe there needs to be a parameter method that resets selfclasses_ to None so that classifiers are not in half-working states if the user sets classes after the factAny help with the remaining classifiers code cleanups or suggestions would be appreciated,,739,0.8227334235453315,0.2527040314650934,26950,345.0834879406308,31.3543599257885,93.17254174397031,2023,33,806,76,unknown,erg,erg,true,,0,0,17,4,1489,true,true,false,false,5,26,0,2,11,0,318
456758,scikit-learn/scikit-learn,python,1300,1351686962,1352378840,1352378840,11531,11531,merged_in_comments,false,false,false,142,11,6,0,12,0,12,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,102,71,129,126,42.45337237680217,1.283356628737712,30,vlad@vene.ro,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,26,0.02584493041749503,4,6,false,MRG: Lars drop for good For ill-conditioned design this implements the strategy suggested by @fabianp and used in R to drop heavily correlated regressorsThis is a continuation of PR #1238 with a rebase and an addition of the new strategyHowever there are two issues one is that the X matrix is not full rank (has degenerate regressors) and that the lars cannot make good choices The other one is that at the end of the path numerical errors are bigger than the max residual and thus no good path can be computed This is seen because alpha than increases in the path It tells us that it is time to stop the pathThis PR deals with both issues and it solves the issue reported on the mailing list by Alejandro Weinstein and @NelleVhttp://sourceforgenet/mailarchive/messagephpmsg_id29927302@fabianp @agramfort please review :),,738,0.8224932249322493,0.2554671968190855,26864,346.2254318046456,31.45473496128648,93.54526503871352,2023,33,806,63,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,24,0.625,275,3,982,true,true,false,false,121,460,31,144,158,4,70
456768,scikit-learn/scikit-learn,python,1298,1351682516,,1352378931,11606,,unknown,false,false,false,53,7,7,0,0,0,0,0,1,0,0,4,4,3,0,0,0,0,4,4,3,0,0,183,58,183,58,39.26066231885907,1.186843096678103,47,vlad@vene.ro,doc/modules/linear_model.rst|sklearn/utils/arrayfuncs.pyx|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py,26,0.02390438247011952,2,0,false,WIP: Early stopping in Lars degenerate cases This is a rebased version of #1238It also addresses the test failure reported by @amueller What remains to be seen is if this strategy is better than the one suggested by @fabianp : dropping for good regressors that are too co-linear with the active set,,737,0.8236092265943012,0.2559760956175299,26864,346.2254318046456,31.45473496128648,93.54526503871352,2023,33,806,63,unknown,GaelVaroquaux,GaelVaroquaux,true,,23,0.6521739130434783,275,3,982,true,true,false,false,119,459,30,144,156,4,-1
456437,scikit-learn/scikit-learn,python,1297,1351637568,1352374198,1352374198,12277,12277,merged_in_comments,false,false,false,24,8,1,0,12,0,12,0,6,0,0,1,2,1,0,0,0,0,2,2,1,0,0,2,0,19,0,4.689249145474564,0.141756103202898,0,,examples/exercises/plot_cv_digits.py,0,0.0,0,14,false,first pull request to familiarize myself Changed example svc kernel to be linear however the error curve ends up flat under the new kernel,,736,0.8233695652173914,0.25553319919517103,26860,346.27699180938197,31.45941921072226,93.55919583023082,2023,33,805,66,unknown,coreylynch,GaelVaroquaux,false,GaelVaroquaux,0,0,23,17,715,true,false,false,false,0,0,0,0,1,0,78
624844,scikit-learn/scikit-learn,python,1296,1351632603,1352159556,1352159556,8782,8782,commit_sha_in_comments,false,false,false,26,3,1,5,1,0,6,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,6,132,4.060888761584796,0.12276075519217164,26,vlad@vene.ro,sklearn/metrics/metrics.py,26,0.026236125126135216,0,0,false,FIX : add condition pos_labelNone for multiclass purpose in metricspr add a condition (pos_labelNone) to enable multiclass evaluation even with only 2 class labels in metricsprecision_recall_fscore_support,,735,0.8231292517006803,0.25630676084762866,26860,346.27699180938197,31.45941921072226,93.55919583023082,2023,31,805,65,unknown,aymas,agramfort,false,agramfort,1,1.0,0,0,788,true,false,false,false,0,0,1,0,2,0,79
479034,scikit-learn/scikit-learn,python,1294,1351617660,,1360798695,153017,,unknown,false,true,false,30,1,1,0,7,0,7,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,11,0,11,0,4.421552911744091,0.1336636402608626,10,vlad@vene.ro,sklearn/mixture/dpgmm.py,10,0.010101010101010102,0,8,false,BF: dpgmm: setting the weights to something reasonable This is about issue https://githubcom/scikit-learn/scikit-learn/issues/393  It seems like the weights are important so this PR sets them to a reasonable value,,734,0.8242506811989101,0.25656565656565655,26860,346.27699180938197,31.45941921072226,93.55919583023082,2023,31,805,80,unknown,alextp,amueller,false,,3,1.0,41,3,1670,false,false,false,true,0,1,0,0,0,0,146
624845,scikit-learn/scikit-learn,python,1292,1351604150,1351606014,1351606014,31,31,github,false,true,false,22,1,1,0,2,0,2,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,3,0,3,0,4.067233644032921,0.122952509267216,13,vlad@vene.ro,sklearn/mixture/gmm.py,13,0.013091641490433032,0,1,false,FIX : pass random_state to kmeans in gmmfit the random_state of gmm was not passed to K-Means in the init furing fit,,733,0.8240109140518418,0.25478348439073517,26860,346.27699180938197,31.45941921072226,93.55919583023082,2023,31,805,53,unknown,aymas,ogrisel,false,ogrisel,0,0,0,0,788,true,false,false,false,0,0,0,0,1,0,5
624846,scikit-learn/scikit-learn,python,1290,1351519294,1351645039,1351645039,2095,2095,github,false,false,false,33,3,3,0,0,0,0,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,30,0,30,0,14.162243188417468,0.42812466564723545,6,peter.prettenhofer@gmail.com,examples/exercises/plot_cv_diabetes.py|doc/tutorial/statistical_inference/model_selection.rst|examples/exercises/plot_cv_diabetes.py,6,0.006085192697768763,0,0,false,DOC: further improvements to the model selection exercise  Ive improved the answer to the bonus question so that it also prints out the scores of different alphas and gives a better textual explanation,,732,0.8237704918032787,0.25760649087221094,26861,346.2641003685641,31.458248017571947,93.5557127433826,2022,30,804,56,unknown,tjanez,GaelVaroquaux,false,GaelVaroquaux,1,1.0,3,3,20,true,true,false,false,1,3,1,1,2,0,-1
624847,scikit-learn/scikit-learn,python,1289,1351473051,,1351589442,1939,,unknown,false,false,false,24,1,1,0,3,2,5,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,2,0,2,0,4.529857249205141,0.13693756411937485,0,,sklearn/_hmmc.c|sklearn/_hmmc.pyx,0,0.0,2,1,false,BUG: attempt to fix hmm in 32bits @amueller @agramfort  can you test this to tell me if it fixes the scikit on 32bit,,731,0.8248974008207934,0.25737538148524924,26861,346.2641003685641,31.458248017571947,93.5557127433826,2022,29,803,53,unknown,GaelVaroquaux,GaelVaroquaux,true,,22,0.6818181818181818,274,3,979,true,true,false,false,116,456,28,145,159,4,963
459875,scikit-learn/scikit-learn,python,1288,1351468016,1352896879,1352896879,23814,23814,github,false,false,false,135,18,12,4,54,0,58,0,9,0,0,6,6,3,0,0,0,0,6,6,3,0,0,220,63,220,63,97.60536293249577,2.9506050009287614,154,vlad@vene.ro,doc/modules/classes.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_common.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/preprocessing.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,84,0.03160040774719674,0,25,false,MRG Random forests hashing This PR implements a simple but awesome method: random forests hashingIt is a way to do an unsupervised embedding of a dense feature space to a high-dimensional space with a very sparse binary representationI named it RandomForestHasher though it actually inherits from ExtraTreesClassifier Not sure if it was a good ideaThe example looks like this:[plot](http://iimgurcom/Y6uWfpng)With more estimators in the forest the top right looks better separated but the other two plots are way more smooth which I think doesnt convey the nature of the decision function so wellThis method was proposed [here](http://wwwinfethzch/personal/vezhneva/Pubs/VezhnevetsCVPR2012bpdf) where it was called Extremely Randomized Hashing Forests ( I have to add the reference still)This is rather new but the idea seems nice and simple and it wasnt much to implement,,730,0.8246575342465754,0.2579001019367992,26861,346.2641003685641,31.458248017571947,93.5557127433826,2022,29,803,62,unknown,amueller,amueller,true,amueller,131,0.8625954198473282,433,31,737,true,true,false,false,205,1129,204,206,245,7,18
459633,scikit-learn/scikit-learn,python,1287,1351460801,1354475413,1354475413,50243,50243,github,false,false,false,164,95,26,19,34,0,53,0,6,2,0,6,18,7,0,0,4,0,16,20,15,0,0,1152,0,2563,484,107.61557878835394,3.2314204569947402,58,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/utils/extmath.py|examples/ensemble/plot_partial_dependency.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_partial_dependency.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_partial_dependency.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/datasets/__init__.py|sklearn/datasets/cal_housing.py|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_partial_dependency.py|sklearn/ensemble/gradient_boosting.py,35,0.0040858018386108275,0,21,false,MRG Gbrt partial dependence plots This PR provides functionality to interpret GBRT models via partial dependence plots [1] Partial dependence plots summarize the marginal effect of certain features on the target responseBelow are some example PDPs for the california housing dataset (response is median house price) the example is taken from ELSII Section 10141:[1-way pdp](https://lh3googleusercontentcom/-NuzNLaYWB2U/UI11gKCQKzI/AAAAAAAACBk/j99CnUPa_iU/s1105/gbrt-pdp-1png)[2-way pdp](https://lh5googleusercontentcom/-GAp9lOMe0uA/UI11iTczkQI/AAAAAAAACBs/xs4qO4L3gMo/s1105/gbrt-pdp-2png)The functionality is exposed via the function gradient_boostingpartial_dependence it is designed for both convenience and flexibility Eg one can either pass a grid of data points on which the partial dependence function will be evaluated or one can simply pass the training data and the function will automatically generate a reasonable data grid Evaluating the partial dependence function on a single tree is implemented in cython - this routine can be used to generate similar plots for single decision trees or Random ForestThe PR includes:  - partial_dependence functionality  - dataset loader for california housing dataset (from StatLib)  - Full example taken from ELSII 10141,,729,0.8244170096021948,0.2574055158324821,27126,355.0099535500995,32.2200103222001,93.93202093932021,2022,29,803,65,unknown,pprett,pprett,true,pprett,30,0.8666666666666667,82,27,1181,true,true,false,false,33,77,21,17,76,0,699
456465,scikit-learn/scikit-learn,python,1286,1351372599,1352375594,1352375594,16716,16716,merged_in_comments,false,false,false,66,5,2,1,9,0,10,0,4,0,0,3,4,3,0,0,0,0,4,4,4,0,0,14,10,32,23,12.368282730147596,0.37389329703964036,12,vlad@vene.ro,sklearn/ensemble/forest.py|sklearn/ensemble/base.py|sklearn/ensemble/tests/test_base.py,12,0.0010309278350515464,0,5,false,MRG: Change dynamic defaults in ensembles The classes BaseEnsemble BaseForest ForestClassifier and ForestRegressor have a parameter estimator_params with default value [] I switched these to None and initialize to [] if default is usedAll current actual classifiers initialize their super with a tuple which means the default isnt being used However leaving the list as default could cause problems for future or 3rd party subclasses,,728,0.8241758241758241,0.25876288659793817,26860,346.27699180938197,31.45941921072226,93.55919583023082,2020,29,802,67,unknown,guyrt,GaelVaroquaux,false,GaelVaroquaux,0,0,10,2,353,true,true,false,false,1,1,0,0,2,0,892
624848,scikit-learn/scikit-learn,python,1283,1351350876,1351425962,1351425962,1251,1251,github,false,false,false,95,6,1,0,23,0,23,0,4,1,1,0,5,2,0,0,1,1,3,5,3,0,0,154,0,243,0,4.878986354225587,0.14748464718158233,0,,examples/plot_classifier_comparison.py|examples/plot_iris_classifiers.py,0,0.0,0,4,false,MRG more extensive classifier comparision on synthetic datasets Overview of classification algorithms on 2d data in the spirit of the [clustering example](http://scikit-learnorg/dev/auto_examples/cluster/plot_cluster_comparisonhtml)Looks like this:[big plot](http://iimgurcom/tXmLspng)I extended an somewhat rewrote a previous exampleIm not entirely happy with the color scheme suggestions welcomeI was wondering if I should add label noiseI think I listed most classifiers (only one of the standard linear ones though)I removed GRBT as I couldnt really tell the difference to RF (maybe because I dont know GRBT well enought) and it seemed crowded already ^^Wdyt,,727,0.8239339752407153,0.2601036269430052,26833,346.62542391830954,31.49107442328476,93.65333730853798,2019,28,802,54,unknown,amueller,amueller,true,amueller,130,0.8615384615384616,433,31,736,true,true,false,false,203,1119,201,218,239,7,1
456662,scikit-learn/scikit-learn,python,1282,1351278191,,1352377640,18324,,unknown,false,false,false,57,5,1,0,12,0,12,0,4,0,0,2,3,2,0,0,0,0,3,3,2,0,0,38,22,82,43,8.272939134609366,0.2500573279780092,30,vlad@vene.ro,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,26,0.027055150884495317,1,4,true,MRG: LassoLars path ending contained junk Addresses mailing list discussionhttp://sourceforgenet/mailarchive/forumphpthread_name20120918124544GF27767%40pharenormalesuporg&forum_namescikit-learn-generalThis also addresses a nasty bug that one of my users in separate code that was relying on alpha in the path being decreasing encounteredPlease could I get a quick review as Id like this merged quickly :)@agramfort  this is for the RandomizedWardLasso,,726,0.8250688705234159,0.26326742976066597,26815,341.82360619056493,31.3257505127727,93.4178631363043,2018,28,801,67,unknown,GaelVaroquaux,GaelVaroquaux,true,,21,0.7142857142857143,274,3,977,true,true,false,false,113,423,29,137,155,4,869
624849,scikit-learn/scikit-learn,python,1280,1351254300,1351357157,1351357157,1714,1714,commits_in_master,false,true,false,11,1,1,0,1,0,1,0,4,0,0,2,2,1,0,0,0,0,2,2,1,0,0,18,0,18,0,9.455403151873984,0.2857984095665775,5,peter.prettenhofer@gmail.com,doc/tutorial/statistical_inference/model_selection.rst|examples/exercises/plot_cv_diabetes.py,5,0.005165289256198347,0,3,true,DOC: further improvements to the model selection exercise See Issue #1278,,725,0.8248275862068966,0.2665289256198347,26815,341.82360619056493,31.3257505127727,93.4178631363043,2016,28,801,53,unknown,tjanez,GaelVaroquaux,false,GaelVaroquaux,0,0,3,3,17,false,true,false,false,0,1,0,0,0,0,454
459321,scikit-learn/scikit-learn,python,1279,1351203110,1352887832,1352887832,28078,28078,github,false,false,false,76,23,2,1,61,0,62,0,7,0,0,5,6,2,0,0,0,0,6,6,3,0,0,236,72,531,162,46.21020562330754,1.3854580722233798,104,vlad@vene.ro,doc/modules/classes.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|doc/modules/classes.rst|doc/modules/preprocessing.rst|doc/whats_new.rst|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py,80,0.016614745586708203,1,38,false,MRG added OneHotEncoder New one-hot encoder with tests + docsThis is less general then what @larsmans did in #242 but I think together with DictVectorizer we can cover many use-casesIn particular this estimator assumes input to be coded in integers from 0-n_valuesI think this is a sensible assumption and gets rid of a lot of hassle )I am not sure if this should sit in preprocessing or should go into feature_extractionWdyt,,724,0.824585635359116,0.27102803738317754,27112,352.09501327825313,32.19976394216583,93.83298908232517,2014,28,800,63,unknown,amueller,amueller,true,amueller,129,0.8604651162790697,433,31,734,true,true,false,false,207,1116,202,218,229,8,4
624850,scikit-learn/scikit-learn,python,1275,1351106532,,1351643587,8950,,unknown,false,true,false,21,1,1,0,9,1,10,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.5253623484115115,0.13679192445633584,4,vlad@vene.ro,sklearn/mixture/dpgmm.py,4,0.0041025641025641026,0,7,false,MRG warn about dpgmm and vbgmm In light of #393 I consider them broken and we should let the users know,,723,0.8257261410788381,0.27589743589743587,26807,339.76200246204354,31.33509904129518,91.58055731711866,2013,29,799,59,unknown,amueller,amueller,true,,128,0.8671875,432,31,733,true,true,false,false,204,1106,201,217,234,8,181
624851,scikit-learn/scikit-learn,python,1274,1351104329,1351125179,1351125179,347,347,github,false,false,false,16,1,1,0,2,0,2,0,2,0,0,9,9,9,0,0,0,0,9,9,9,0,0,59,0,59,0,39.845680471539225,1.2044488138916307,100,vlad@vene.ro,sklearn/covariance/robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/mlcomp.py|sklearn/datasets/mldata.py|sklearn/datasets/samples_generator.py|sklearn/decomposition/nmf.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/text.py|sklearn/utils/validation.py,41,0.00819672131147541,0,0,false,MRG use the numbers module introduced in Python 26 to check for number types Fixes #1255,,722,0.8254847645429363,0.2766393442622951,26807,339.76200246204354,31.33509904129518,91.58055731711866,2013,29,799,50,unknown,amueller,amueller,true,amueller,127,0.8661417322834646,432,31,733,true,true,false,false,203,1101,200,213,234,8,128
624852,scikit-learn/scikit-learn,python,1273,1351099394,1351099683,1351099683,4,4,github,false,false,false,5,1,1,0,2,0,2,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.5664567359766,0.13803409399040875,3,peter.prettenhofer@gmail.com,AUTHORS.rst,3,0.0030643513789581204,0,0,false,COSMIT: fix typo in AUTHORSrst ,,721,0.8252427184466019,0.278855975485189,26807,339.76200246204354,31.33509904129518,91.58055731711866,2013,29,799,49,unknown,mrjbq7,amueller,false,amueller,7,0.8571428571428571,55,64,1504,false,true,false,false,5,12,6,0,0,0,2
473123,scikit-learn/scikit-learn,python,1271,1351029960,1352313221,1352313221,21387,21387,merged_in_comments,false,false,false,112,7,1,0,20,0,20,0,3,0,0,2,7,2,0,0,0,0,7,7,6,0,0,102,0,363,0,9.220157041328587,0.27870537207156865,17,vlad@vene.ro,examples/covariance/plot_covariance_estimation.py|sklearn/grid_search.py,17,0.017241379310344827,1,6,false,BF: GridSearchCV + unsupervised covariance shrinkage selection (example) Following @GaelVaroquauxs suggestion I revisited the shrunk covariance example by adding an unsupervised GridSearchCV estimator selectionThis lead me to adapt grid_searchpy since it broke when yNone (as far as I understood a call to clffit was made with yNone but the latter argument may be unexpected) I decided to completely separate the case yNone and the complementary case The tests did not break subsequently to my change so I guess it could be kept like that ThoughtsI took this opportunity to magnify the example Im quite proud of the result but I am afraid it breaks with some old versions of matplotlib,,720,0.825,0.2870182555780933,26807,339.76200246204354,31.33509904129518,91.58055731711866,2012,29,798,70,unknown,VirgileFritsch,GaelVaroquaux,false,GaelVaroquaux,7,0.7142857142857143,7,0,904,true,true,false,false,8,17,2,0,15,0,46
624853,scikit-learn/scikit-learn,python,1270,1351026344,,1351099303,1215,,unknown,false,false,false,8,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.677036636907464,0.14078874358736024,3,peter.prettenhofer@gmail.com,AUTHORS.rst,3,0.00303951367781155,0,0,false,COSMIT: fix typo and add myself to authors ,,719,0.8261474269819193,0.2887537993920973,26832,340.2280858676208,31.380441264162194,91.68157423971378,2012,29,798,50,unknown,mrjbq7,mrjbq7,true,,6,1.0,55,64,1503,false,true,false,false,3,11,5,0,0,0,-1
624854,scikit-learn/scikit-learn,python,1269,1351025972,1351028449,1351028449,41,41,github,false,false,false,10,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,8,0,4.318946494651589,0.13000946920208217,11,vlad@vene.ro,sklearn/ensemble/forest.py,11,0.011144883485309016,0,0,false,DOC: fix Controls typo in sklearnensembleforest Nothing fancy just s/Controlls/Controls/g,,718,0.8259052924791086,0.2887537993920973,26832,340.2280858676208,31.380441264162194,91.68157423971378,2012,29,798,49,unknown,mrjbq7,glouppe,false,glouppe,5,1.0,55,64,1503,false,true,false,false,3,11,4,0,0,0,-1
624821,scikit-learn/scikit-learn,python,1268,1351012715,1404667963,1404667963,894194,894194,commit_sha_in_comments,false,false,false,48,1,1,0,19,0,19,0,6,0,0,39,39,39,0,0,0,0,39,39,39,0,0,420,0,420,0,51.37346931562563,1.5464509884595343,297,vlad@vene.ro,sklearn/_hmmc.c|sklearn/_hmmc.pyx|sklearn/cluster/_hierarchical.c|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/metrics/_euclidean_fast.c|sklearn/metrics/_euclidean_fast.pyx|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/libsvm.c|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/tree/_tree.c|sklearn/utils/arraybuilder.c|sklearn/utils/arraybuilder.pyx|sklearn/utils/arrayfuncs.c|sklearn/utils/arrayfuncs.pyx|sklearn/utils/graph_shortest_path.c|sklearn/utils/graph_shortest_path.pyx|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pyx|sklearn/utils/seq_dataset.c|sklearn/utils/seq_dataset.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pyx,134,0.01606425702811245,0,8,false,BUG: fix segfault at import of the scikit Hot fix to solve the segfault on the buildbotIn general to use the C API of numpy (cimport numpy in Cython) youshould always insert npimport_array() after the import If not youcan have race conditions during the imports,,717,0.8256624825662483,0.29518072289156627,26833,340.215406402564,31.379271792196178,91.67815749263966,2012,29,798,204,unknown,GaelVaroquaux,larsmans,false,larsmans,20,0.7,273,3,974,true,true,false,true,88,422,32,137,186,6,4
624855,scikit-learn/scikit-learn,python,1266,1350990694,1350991427,1350991427,12,12,github,false,false,false,4,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.428266346310833,0.1338579096694675,7,vlad@vene.ro,README.rst,7,0.007028112449799197,0,0,false,Fix typo in README ,,716,0.8254189944134078,0.3002008032128514,26792,336.5556882651538,31.389967154374443,91.03463720513587,2011,29,798,50,unknown,cdeil,ogrisel,false,ogrisel,0,0,11,1,496,false,true,false,false,0,0,0,0,0,0,-1
624856,scikit-learn/scikit-learn,python,1263,1350917650,1350999781,1350999781,1368,1368,github,false,false,false,14,2,1,0,28,0,28,0,5,0,0,2,3,2,0,0,0,0,3,3,3,0,0,4,0,36,27,3.917123386811362,0.11840704859877665,10,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx,10,0.010050251256281407,1,12,false,FIX: prevent early stopping in tree construction This fixes issue #1254 raised by @amueller ,,715,0.8251748251748252,0.30251256281407035,26792,336.5556882651538,31.389967154374443,91.03463720513587,2010,29,797,50,unknown,glouppe,glouppe,true,glouppe,24,1.0,80,21,711,true,true,false,false,18,60,8,14,7,1,5
625893,scikit-learn/scikit-learn,python,1262,1350907774,,1350919249,191,,unknown,false,false,false,14,17,17,0,8,0,8,0,2,0,0,3,3,2,0,1,0,0,3,3,2,0,1,10,0,10,0,25.386702906808374,0.7673908294464258,35,vlad@vene.ro,MANIFEST.in|MANIFEST.in|sklearn/hmm.py|sklearn/hmm.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py,25,0.010040160642570281,0,0,false,Bug in AUC metric - error message More explicit error message for issue #1257,,714,0.8263305322128851,0.3032128514056225,26792,336.5556882651538,31.389967154374443,91.03463720513587,2010,29,797,50,unknown,jaquesgrobler,,false,,24,0.9583333333333334,9,12,271,true,true,false,false,21,66,30,7,42,3,0
624858,scikit-learn/scikit-learn,python,1259,1350819443,1352135280,1352135280,21930,21930,github,false,false,false,36,23,1,37,92,0,129,0,6,0,0,1,12,1,0,0,2,0,12,14,10,0,0,24,0,577,18,4.6746668853078415,0.1413061204391398,0,,sklearn/linear_model/sgd_fast.pyx,0,0.0,0,43,false,WIP: Adding Passive Aggressive learning rates I have added Passive Aggressive learning rates as described inhttp://jmlrcsailmitedu/papers/volume7/crammer06a/crammer06apdfI have tried to make as few changes as possible so these rates should integrate well with everything else,,713,0.8260869565217391,0.3081570996978852,26792,336.5556882651538,31.389967154374443,91.03463720513587,2010,29,796,67,unknown,zaxtax,mblondel,false,mblondel,2,1.0,65,11,1639,false,true,false,false,3,3,3,0,0,0,41
624859,scikit-learn/scikit-learn,python,1246,1350425703,1350427199,1350427199,24,24,github,false,false,false,37,1,1,0,6,0,6,0,4,0,0,6,6,6,0,0,0,0,6,6,6,0,0,29,28,29,28,22.68425824195715,0.6837916753774841,35,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/tests/test_svm.py,29,0.00298804780876494,0,0,false,MRG fix seed liblinear using srand Fixes issue #919 There was a random test-failureThis fixes itThe test annoyed me as I was trying to figure out the random SVC failure introduced in the last merge,,712,0.8258426966292135,0.3207171314741036,26768,335.9982068141064,31.418111177525404,91.0789001793186,2003,30,791,50,unknown,amueller,amueller,true,amueller,126,0.8650793650793651,426,31,725,true,true,false,false,198,1027,201,219,242,10,11
624860,scikit-learn/scikit-learn,python,1244,1350400057,1350581001,1350581001,3015,3015,merged_in_comments,false,false,false,83,3,2,9,9,0,18,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,15,19,15,28,13.56671126009044,0.40896520533721326,21,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,21,0.020874751491053677,0,5,false,Fix issue #1239 confusion matrix fails when y_predy_true contains unexpected labels(not-exist in labels) The issue #1239 is from when y_pred or y_true contains unexpected labels not present in labels The problem comes occurs labels is explicitly given rather than calculated from y_pred and y_true As the docstring says the returned matrix should be [n_classes n_classes] where n_classes is equal to shape of labels so the fix is just eliminate the labels not exist in labels A test is added to address this issue,,711,0.8255977496483825,0.3210735586481113,26751,336.0248215019999,31.438077081230606,91.09939815334006,2003,31,791,51,unknown,kuantkid,amueller,false,amueller,3,1.0,12,7,49,true,false,false,false,12,61,14,18,51,0,46
624861,scikit-learn/scikit-learn,python,1238,1350215597,1351682815,1351682815,24453,24453,commit_sha_in_comments,false,true,false,19,3,1,3,28,5,36,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,8,31,25,59,8.7014341516667,0.2623038749437277,28,vlad@vene.ro,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,25,0.025176233635448138,1,9,false,ENH: early stopping LARS for degenerate active set As per discussion on mailing list by  Alejandro Weinstein and @NelleV,,710,0.8253521126760563,0.32628398791540786,26720,335.47904191616766,31.437125748502993,91.09281437125749,2000,31,789,66,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,19,0.6842105263157895,269,3,965,true,true,false,false,91,411,38,133,199,6,76
624862,scikit-learn/scikit-learn,python,1237,1350080637,1350118544,1350118544,631,631,github,false,false,false,6,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.4803727353716365,0.13543701750198045,8,vlad@vene.ro,doc/modules/ensemble.rst,8,0.00808080808080808,0,0,true,Fixed inconsistency between docs and code ,,709,0.8251057827926658,0.32727272727272727,26656,335.38415366146455,31.475090036014404,91.08643457382954,1999,31,787,56,unknown,kalaidin,mblondel,false,mblondel,0,0,4,11,96,false,true,false,false,0,0,0,0,0,0,-1
624863,scikit-learn/scikit-learn,python,1234,1349986892,1350155941,1350155940,2817,2817,github,false,false,false,43,3,1,2,1,0,3,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,26,0,36,0,9.406302376342015,0.28434292897011654,12,vlad@vene.ro,sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/utils/fixes.py,7,0.005045408678102927,2,1,false,MRG fix bincount mess I made in kmeans This should clean up the stuff I pushed earliercc @ogrisel @gaelvaroquaux Could you have a brief look What I pushed earlier is buggy but I didnt dare push again after so many failed fixes,,708,0.8248587570621468,0.32593340060544906,26656,335.38415366146455,31.475090036014404,91.08643457382954,1999,31,786,56,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,125,0.864,422,31,720,true,true,true,false,192,994,199,216,243,9,705
624864,scikit-learn/scikit-learn,python,1232,1349820786,1349917490,1349917490,1611,1611,github,false,false,false,37,0,0,2,6,0,8,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,MRG Speed up K-Means m-step Rewrite loop over means as loop over samplesIn case of 2 means (which I think is most adversary) with 1000 samples and 784 featuresI get a 50% speedupCloses #724,,707,0.8246110325318247,0.32507739938080493,26656,334.93397358943577,31.475090036014404,90.89885954381754,1996,30,784,59,unknown,amueller,amueller,true,amueller,124,0.8629032258064516,420,30,718,true,true,false,false,188,984,193,217,235,8,84
624865,scikit-learn/scikit-learn,python,1231,1349808269,1351381245,1351381245,26216,26216,merged_in_comments,false,false,false,21,5,1,8,19,0,27,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,8,0,77,101,3.97722046790899,0.12023026399750733,10,vlad@vene.ro,sklearn/hmm.py,10,0.010330578512396695,0,5,false,fixed bug of initialization in hmmpy This is a bug fix for Issue #871It fixed some initialization problem of MultinominalHMM,,706,0.8243626062322946,0.32541322314049587,26708,334.2818631121761,31.413808596675157,90.7218810843193,1996,30,784,67,unknown,lucidfrontier45,GaelVaroquaux,false,GaelVaroquaux,3,0.6666666666666666,7,1,582,true,true,false,false,0,1,0,0,1,0,2
624866,scikit-learn/scikit-learn,python,1227,1349792950,1350397236,1350397236,10071,10071,merged_in_comments,false,false,false,45,3,1,5,14,0,19,0,6,0,0,2,3,2,0,0,0,0,3,3,3,0,0,15,7,51,18,9.112492215158293,0.27546809475204137,22,mathieu@mblondel.org,sklearn/linear_model/isotonic_regression_.py|sklearn/linear_model/tests/test_isotonic_regression.py,20,0.020682523267838676,0,2,false,ENH: Consider order in X for IsotonicRegression Before IsotonicRegression assumed X was an increasing sequence Thisallows to shuffle both X y (see the test) and obtain the sameresult This makes IsotonicRegression behave more like a standardRegression (which is what I would expect),,705,0.8241134751773049,0.32574974146845914,26708,334.2818631121761,31.413808596675157,90.7218810843193,1996,29,784,59,unknown,fabianp,fabianp,true,fabianp,28,0.7142857142857143,116,22,878,true,true,false,false,18,27,13,7,9,1,159
624867,scikit-learn/scikit-learn,python,1226,1349774822,1351001747,1351001747,20448,20448,commits_in_master,false,false,false,68,21,3,4,11,1,16,0,5,0,0,1,6,1,0,0,0,2,4,6,6,0,0,54,0,177,163,13.367240039258693,0.40293642260641244,18,vlad@vene.ro,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py,18,0.018556701030927835,0,7,false,ENH: add verbose output for gradient boosting algorithms I wanted to have verbose output for the work I was doing with gradient tree boosting I didnt see the option so I decided to add one in I liked it maybe you guys would too It just prints out the following nowbuilding tree 95 of 200building tree 96 of 200building tree 97 of 200,,704,0.8238636363636364,0.3247422680412371,26770,336.6828539409787,31.378408666417634,91.07209562943594,1995,29,784,58,unknown,jwkvam,amueller,false,amueller,0,0,0,2,1239,true,true,false,false,0,0,0,0,2,0,48
624868,scikit-learn/scikit-learn,python,1223,1349716152,1349730478,1349730478,238,238,github,false,false,false,75,2,0,0,5,0,5,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,24,0,0,0.0,0,,,0,0.0,0,0,false,add dual_gap_ and eps_ to Enet and Lasso docstring  I think dual_gap_ can be interesting for the user and should thereforebe mentioned in the docstringIm not so sure about the usefulness of eps_:    eps_ : float        eps_ : tol * norm(y 2)^2Why is eps_ not just a local variable It appear to me that its only used once to check if the solver converged with required tol (L234)Best Immanuel,,703,0.8236130867709816,0.3224101479915433,26702,334.09482435772605,31.420867350760247,90.66736574039398,1994,28,783,57,unknown,ibayer,agramfort,false,agramfort,12,0.6666666666666666,2,0,219,true,true,false,false,5,54,44,34,19,0,101
624871,scikit-learn/scikit-learn,python,1221,1349656539,1351357940,1351357940,28356,28356,merged_in_comments,false,false,false,36,9,3,15,17,1,33,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,145,0,225,21.499686314469127,0.650846214104793,29,vlad@vene.ro,sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py,19,0.02040816326530612,0,14,false,MRG: Spectral fix Fixes to numerical stability in spectral embeddingThe key to the numerical stability is to avoid arpack when it fails and switch to lobpcg or symeig depending on the size of the problem,,702,0.8233618233618234,0.32116004296455425,26701,333.80772255720757,31.384592337365646,90.63330961387214,1993,28,782,65,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,18,0.6666666666666666,268,3,958,true,true,false,false,94,398,39,127,181,6,392
624875,scikit-learn/scikit-learn,python,1215,1349641704,1349698138,1349698138,940,940,github,false,false,false,11,1,0,0,1,0,1,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,9,15,0,0.0,0,,,0,0.0,0,0,false,MRG make multi-class more robust in discovering scoring functions Closes #1019,,701,0.8231098430813124,0.3232432432432432,26707,333.99483281536675,31.527314936159062,90.9499382184446,1993,28,782,56,unknown,amueller,mblondel,false,mblondel,123,0.8617886178861789,418,30,716,true,true,true,false,186,945,191,214,231,8,937
624877,scikit-learn/scikit-learn,python,1214,1349639081,1350216656,1350216656,9626,9626,commits_in_master,false,false,false,22,5,0,0,10,1,11,0,5,0,0,0,8,0,0,0,0,0,8,8,6,0,0,0,0,280,58,0,0.0,0,,,0,0.0,0,3,false,MRG rename rho to l1_ratio Renaming rho in ElasticNet and SGD to l1_ratio to get them to be the sameCloses #1139,,700,0.8228571428571428,0.3232432432432432,26656,335.38415366146455,31.475090036014404,91.08643457382954,1993,28,782,61,unknown,amueller,larsmans,false,larsmans,122,0.860655737704918,418,30,716,true,true,false,false,186,945,187,214,231,8,1304
624879,scikit-learn/scikit-learn,python,1211,1349564624,1349624199,1349624199,992,992,github,false,false,false,22,1,0,0,3,0,3,0,5,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Mark Cython outputs as binary so their changes dont clutter diffs Any new Cython files would need to be added to gitattributes,,699,0.8226037195994278,0.31978021978021975,26703,333.5954761637269,31.49458862300116,90.81376624349325,1993,27,781,55,unknown,kcarnold,larsmans,false,larsmans,2,0.5,16,0,1510,false,true,false,false,0,0,0,0,0,0,552
624881,scikit-learn/scikit-learn,python,1210,1349559795,,1350254731,11582,,unknown,false,false,false,37,2,1,2,5,0,7,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,21,17,26,17,13.452311031671005,0.40551789127620347,28,vlad@vene.ro,sklearn/cluster/k_means_.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py,19,0.01098901098901099,0,2,false,MRG remove unreachable code from grid_search test unsupervised setting This significantly increases test-coverageMotivated by comments on my randomized search PRWe should try to get 100% test coverage here This is really the core of sklearn,,698,0.8237822349570201,0.31978021978021975,26743,335.1905171446733,31.41008862132147,91.0144710765434,1993,27,781,59,unknown,amueller,amueller,true,,121,0.8677685950413223,416,30,715,true,true,false,false,185,931,182,213,224,8,719
624883,scikit-learn/scikit-learn,python,1208,1349524606,1349624054,1349624054,1657,1657,github,false,false,false,33,2,0,0,2,0,2,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,10,19,0,0.0,0,,,0,0.0,1,0,false,MRG better error messages in CountVectorizer for empty vocabulary As suggested by @ianozsvald in #1207 but only at fit time and with suggestions for the user as to what might cause the problem,,697,0.8235294117647058,0.31903190319031904,26703,333.5954761637269,31.49458862300116,90.81376624349325,1992,27,781,56,unknown,larsmans,larsmans,true,larsmans,50,0.76,75,30,811,true,true,false,false,45,86,29,25,72,4,1222
624885,scikit-learn/scikit-learn,python,1204,1349350672,1349976298,1349976298,10427,10427,github,false,false,false,71,3,2,0,7,0,7,0,4,0,0,2,4,2,0,0,0,0,4,4,2,0,0,36,24,36,24,13.229207425028424,0.4010179883869821,24,vlad@vene.ro,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py,24,0.026905829596412557,1,4,false,MRG Use scores not p-values for feature selection This solves the numerical stability issue that @amueller encountered some time ago: p-values at least those returned by chi2 may contain ties even when the scores dontThis also paves the way for score functions that are not based on significance tests such as mutual information and infogain (whence the name of this branch but I wont be pushing those in this PR),,696,0.8232758620689655,0.312780269058296,26690,333.4582240539528,31.47246159610341,90.82053203446984,1985,26,779,60,unknown,larsmans,larsmans,true,larsmans,49,0.7551020408163265,75,30,809,true,true,false,false,45,81,27,24,63,4,45
624886,scikit-learn/scikit-learn,python,1201,1349277544,1349285091,1349285091,125,125,github,false,false,false,24,4,1,0,5,0,5,0,3,0,0,2,3,2,0,0,0,0,3,3,3,0,0,6,0,35,17,8.872293155597585,0.2689503094988646,5,vlad@vene.ro,sklearn/neighbors/base.py|sklearn/neighbors/classification.py,5,0.0057405281285878304,0,1,false,MRG arbitraty labels for Neighbors classifiers Fixes problem from ml Uses the selfclasses y  unique(y) idiomMakes the code shorter and more robust,,695,0.823021582733813,0.3145809414466131,26690,333.1584863244661,31.43499437991757,90.97040089921319,1983,26,778,55,unknown,amueller,amueller,true,amueller,120,0.8666666666666667,413,30,712,true,true,false,false,182,909,176,202,218,8,0
453604,scikit-learn/scikit-learn,python,1200,1349224433,1368116603,1368116603,314869,314869,commit_sha_in_comments,false,true,false,58,66,0,111,135,11,257,0,14,0,0,0,15,0,0,0,8,2,13,23,9,0,1,0,0,1050,164,0,0.0,0,,,0,0.0,0,67,false,Added Restricted Boltzmann machines RBMs are a state-of-the-art generative model Theyve been used to win the Netflix challenge [1] and in record breaking systems for speech recognition at Google [2] and Microsoft This pull request adds a class for Restricted Boltzmann Machines (RBMs) to scikits-learn The code is both easy to read and efficient[1] http://techblognetflixcom/2012/04/netflix-recommendations-beyond-5-starshtml[2] http://researchgooglecom/pubs/archive/38130pdf,,694,0.8227665706051873,0.3142857142857143,26690,333.1584863244661,31.43499437991757,90.97040089921319,1982,26,777,114,unknown,ynd,vene,false,vene,0,0,36,1,1678,false,false,false,false,0,0,0,0,1,0,432
453133,scikit-learn/scikit-learn,python,1199,1349213000,1349324357,1349324357,1855,1855,commit_sha_in_comments,false,false,false,42,2,0,0,17,0,17,0,8,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,36,0,0,0.0,0,,,0,0.0,0,8,false,This is a minor issue but there was a TODO comment so I fixed it Previously the ℓ₂ norm was computed (which uses a sqrt) then squared Also ℓ₂ norm is computed using npdotThere is no timing difference for simple lasso,,693,0.8225108225108225,0.3127147766323024,26690,332.6714125140502,31.397527163731738,90.85799925065567,1982,26,777,54,unknown,luispedro,jakevdp,false,jakevdp,0,0,185,39,1252,false,false,false,false,0,0,0,0,0,0,93
453109,scikit-learn/scikit-learn,python,1198,1349212544,1359919005,1359919005,178441,178441,merged_in_comments,false,true,false,89,2,0,5,35,0,40,0,4,0,0,0,9,0,0,0,0,0,9,9,9,0,0,0,0,74,20,0,0.0,0,,,0,0.0,0,16,false,WIP GridSearch with advanced score functions Follow up on #1014This PR does the handling of score functions in the regressor and classifier mixins with minimal intrusion into the codeThis enables estimator to overwrite the score function and do whatever they see fit - though that shouldnt be necessary atmTo complete this PR I will probably also need to do some minor adjustments to the other CV classesIm a bit tempted to add a test to the common tests to see if cross_valdation_score with AUC works,,692,0.8222543352601156,0.3127147766323024,26690,332.6714125140502,31.397527163731738,90.85799925065567,1982,26,777,82,unknown,amueller,amueller,true,amueller,119,0.865546218487395,410,30,711,true,true,false,false,181,896,174,193,217,8,9
451865,scikit-learn/scikit-learn,python,1197,1349179213,1349179400,1349179400,3,3,github,false,false,false,21,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,15,0,0,0.0,0,,,0,0.0,0,0,false,DOC: return values of make_moons and make_circles Just a small PR The documentation of make_moons and make_circles was not fully complete ,,691,0.8219971056439942,0.3076049943246311,26690,332.6714125140502,31.397527163731738,90.85799925065567,1980,25,777,51,unknown,glouppe,glouppe,true,glouppe,23,1.0,76,20,691,true,true,false,false,29,132,51,28,47,1,2
450781,scikit-learn/scikit-learn,python,1196,1349133757,1349900357,1349900357,12776,12776,github,false,false,false,45,8,0,0,44,0,44,0,7,0,0,0,14,0,0,0,0,0,14,14,12,0,0,0,0,357,16,0,0.0,0,,,0,0.0,0,15,false,MRG Kfold parameter rename Renames k to n_folds in KFold and StratifiedKFold cross validationAlso renames n_bootstraps to n_iterations to go conform with other randomized samplersId like to have better names for p and n too if there are any suggestionsCloses issue #1161,,690,0.8217391304347826,0.30514285714285716,26681,332.7836287995203,31.408118136501628,90.88864735204827,1980,25,776,63,unknown,amueller,amueller,true,amueller,118,0.864406779661017,410,30,710,true,true,false,false,181,883,173,179,210,8,1
448327,scikit-learn/scikit-learn,python,1194,1349043990,,1362351661,221794,,unknown,false,true,false,143,27,1,62,54,0,116,0,7,0,0,2,11,2,0,0,1,0,11,12,7,0,0,191,13,1218,45,9.063267404968999,0.27473600149601873,71,vlad@vene.ro,sklearn/grid_search.py|sklearn/tests/test_grid_search.py,57,0.06529209621993128,0,10,false,MRG Random sampling hyper parameters This is another shot at implementing randomized hyper parameter search in sklearnIn contrast to #455 I use scipystats distributions for each parameterIf I understand James Paper correctly one of the important points is not to use a gridImagine having one totally irrelevant parameter Using a grid basically makes a large part of the samples redundant while using a continuous distribution is not affected by the presence of the irrelevant parameter at allFor discrete parameters the user can also specify a list of names ie kernel[rbf linear poly] which is sampled from using a uniform distributionI dont like passing objects as parameters but this seems to be the best way to specify a distributionA user can put in any object that supports rvs() and scipystatsdistributions already has a solid amount of predefined ones,,689,0.8229317851959361,0.3035509736540664,26679,332.8085760335845,31.41047265639642,90.89546084935718,1979,25,775,86,unknown,amueller,amueller,true,,117,0.8717948717948718,410,30,709,true,true,false,false,182,879,172,179,210,8,8108
447019,scikit-learn/scikit-learn,python,1193,1348960048,1349009705,1349009705,827,827,github,false,false,false,64,2,0,0,2,0,2,0,4,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,30,2,0,0.0,0,,,0,0.0,0,3,false,MRG build CSR instead of COO matrix in DictVectorizer and use arrayarray Somewhat related to #1135 which also tries to leverage arrayarray and which is still on my TODO list for reviewing and mergingThis one should be pretty uncontroversial except for the fact that we havent ever used arrayarray before and it isnt really NumPy-friendly (little control over the itemsize of an array),,688,0.8226744186046512,0.29965156794425085,26601,331.67926017818877,31.389797376038494,90.9740235329499,1978,25,774,54,unknown,larsmans,larsmans,true,larsmans,48,0.75,75,30,804,true,true,false,false,42,78,23,16,58,4,817
446238,scikit-learn/scikit-learn,python,1189,1348914938,1349016004,1349016004,1684,1684,github,false,false,false,20,1,0,0,2,0,2,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,33,14,0,0.0,0,,,0,0.0,0,0,false,Add support for query_id in dump_svmlight_file Add support now for saving a file with query_id Added a test and docstring,,687,0.8224163027656477,0.29466357308584684,26593,331.7790395968864,31.39924040160945,91.00139134358666,1977,25,774,53,unknown,fabianp,ogrisel,false,ogrisel,27,0.7037037037037037,115,22,868,true,true,true,true,18,26,12,7,4,1,1610
444975,scikit-learn/scikit-learn,python,1187,1348859701,1349017792,1349017792,2634,2634,github,false,false,false,22,2,0,11,7,0,18,0,6,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,2,93,0,0.0,0,,,0,0.0,0,5,true,MRG: FIX: wrong probabilities for OvR LogisticRegression delI wonder if the LinearSVC code has the same issue: need to check before merging/del,,686,0.8221574344023324,0.29350348027842227,26548,331.7764050022601,31.414795841494655,90.96730450504747,1975,25,773,56,unknown,ogrisel,ogrisel,true,ogrisel,31,0.7741935483870968,589,119,1220,true,true,false,false,101,362,41,94,11,4,6
439754,scikit-learn/scikit-learn,python,1184,1348694596,,1350406030,28523,,unknown,false,false,false,101,8,1,1,21,1,23,0,6,1,0,14,18,15,0,0,1,1,16,18,17,0,0,218,14,411,54,68.76845747934124,2.107540424458721,38,vlad@vene.ro,sklearn/svm/__init__.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/exceptions.py|sklearn/svm/libsvm.pxd|sklearn/svm/libsvm.pyx|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/sparse/base.py|sklearn/svm/sparse/classes.py|sklearn/svm/src/libsvm/libsvm_helper.c|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|sklearn/svm/src/libsvm/svm.cpp|sklearn/svm/src/libsvm/svm.h|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py,27,0.0011668611435239206,0,9,false,MRG: adding iter_limit to libsvm This commit adds support for a new libsvm parameter which permits setting ahard limit on the while loop in the svmcpp file in the Solver::Solvefunction Without this functionality it has been observed that libsvm hangswithout converging in a reasonable time -- it can take days instead ofseconds and perhaps it is in fact an infinite loop I dont know  Thiscommit allows the user to set a hard limit on the number of libsvm iterationsand modifies the libsvm bindings to raise a new exception of SolverTimeoutwhen this limit is reached,,684,0.8245614035087719,0.2928821470245041,26548,331.7764050022601,31.414795841494655,90.96730450504747,1970,25,771,67,unknown,jaberg,amueller,false,,3,0.6666666666666666,96,24,1008,false,true,false,false,1,0,0,0,0,0,6
438488,scikit-learn/scikit-learn,python,1182,1348662434,1348911079,1348911079,4144,4144,github,false,false,false,66,5,0,16,5,0,21,0,5,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,112,34,0,0.0,0,,,0,0.0,0,0,false,Add support for preference contraints in svmlight format The preference constrains (qid in svmlight parlance) addsthe ability to control which pairs are considered when dealing withpairwise loss functions (eg for SVMrank) It is explained inhttp://svmlightjoachimsorg/In order to preserve API compatibility I return this information (ifpresent) as a second column to the y vectorAdded a test and paragraph to the docstring,,683,0.8243045387994143,0.28554641598119856,26548,331.7764050022601,31.414795841494655,90.96730450504747,1968,25,771,55,unknown,fabianp,fabianp,true,fabianp,26,0.6923076923076923,115,22,865,true,true,false,false,17,24,6,4,4,1,27
431621,scikit-learn/scikit-learn,python,1176,1348423957,1424472433,1424472433,1267414,1267414,merged_in_comments,false,true,false,51,474,51,102,140,2,244,0,13,4,1,7,30,8,0,0,14,2,24,40,24,0,0,4129,180,22876,2956,390.95246779090684,11.981733781172514,281,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/linear_model/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/metrics/metrics.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/isotonic.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/metrics/metrics.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/isotonic.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/metrics/metrics.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/metrics/metrics.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|examples/plot_calibration.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|examples/plot_calibration.py|sklearn/isotonic_regression.py|sklearn/metrics/metrics.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py|examples/plot_calibration.py|sklearn/calibration.py|sklearn/tests/test_calibration.py,228,0.029232643118148598,2,45,false,WIP : Isotonic calibration working version of calibration module containing Isotonic calibrationId like to have feedback on APIA few issues : brier_score is not a score (cc @mblondel)I am not sure of how to handle OOB samples@mblondel : any idea of how to handle the multiclass case,,682,0.8240469208211144,0.2838002436053593,26521,331.699408016289,31.409072056106485,90.9090909090909,1961,25,768,274,unknown,agramfort,ogrisel,false,ogrisel,25,0.84,85,177,1026,true,true,true,true,58,210,43,175,106,0,7
428739,scikit-learn/scikit-learn,python,1175,1348255063,1348859832,1348859832,10079,10079,commits_in_master,false,false,false,72,21,3,2,17,0,19,0,6,0,0,2,23,2,0,0,0,0,23,23,16,0,0,134,95,491,168,22.950348386495588,0.703369653538292,19,vlad@vene.ro,sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py|sklearn/tests/test_preprocessing.py|sklearn/preprocessing.py,16,0.02,0,1,true,MRG: New solver for Ridge Ive added the lsqr solver I mentioned on the mailing-list On my box its 8 times faster than the recently merged improvements on the news20 exampleFor sample_weight the solution I gave on the mailing-list (multiply both X and y by sqrt(sample_weight)) should work but involves some work especially in the sparse case (need to implement inplace_csr_row_scale) So I suggest this is addressed in a separate PR,,681,0.8237885462555066,0.28625,26521,331.699408016289,31.409072056106485,90.9090909090909,1960,25,766,56,unknown,mblondel,mblondel,true,mblondel,18,0.8333333333333334,172,28,906,true,true,false,false,39,107,0,37,15,7,421
413733,scikit-learn/scikit-learn,python,1173,1348166623,1349010670,1349010670,14067,14067,github,false,false,false,113,19,2,16,32,3,51,0,7,0,0,2,10,2,0,0,2,1,9,12,7,0,0,27,27,342,117,13.333940876538465,0.40635366769456527,6,vlad@vene.ro,sklearn/linear_model/randomized_l1.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py,5,0.006321112515802781,0,7,false,MRG Feature stacker This estimator provides a Y piece for the pipelineI used it to combine word ngrams and char ngrams into a single transformerBasically it just concatenates the output of several transformers into one large featureIf you think this is helpful Ill add some docs and an exampleWith this together with Pipeline one can build arbitrary complex graphs (with one source and one sink) of estimators in sklearn :)TODO--------- ~~tests~~- ~~narrative documentation~~- ~~example~~Thanks to the awesome implementation of the BaseEstimator grid search simply works - though with complicated graphs you get parameter names like feature_stacker__first_feature__feature_selection__percentile (more or less from my code ^^),,680,0.8235294117647058,0.2831858407079646,26597,330.3380080460202,31.281723502650674,90.57412490130466,1958,26,765,60,unknown,amueller,amueller,true,amueller,116,0.8706896551724138,403,30,699,true,true,true,false,175,836,166,136,191,8,12
408383,scikit-learn/scikit-learn,python,1170,1348072930,1353645455,1353645455,92875,92875,commits_in_master,false,true,false,232,63,19,103,64,0,167,0,5,2,0,3,23,3,0,0,4,0,21,25,16,1,1,2,1551,100,4052,92.06594230251747,2.8097711702876236,10,vlad@vene.ro,sklearn/decomposition/__init__.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py|sklearn/decomposition/tests/test_spectra_embedding.py,10,0.0,2,47,false,MRG: spectral embedding estimator This is related to issue #793 the code is not thoroughly checked yet1 Move the spectra_embedding into the decomposition module2 Create an estimator object around it3 Write documentation4 Add to the References1 and 2 are almost finished and I am testing the packages will update the docs and refs laterThe PR includes a test request and I have tried with the Pipeline to combine SpectralEmbedding and Kmeans to SpectralClustering but it is only tested under a very-very small dataset In issue 793 @amueller suggest to implement spectral clustering as pipeline so maybe this PR is related to PR #1165The affinity(similarity matrix) can be setted as predefined affinity(currently only nearest_neighbors and rbf) precomputed or callable I am not sure whether I have done the callable part right I also write a simple test caseIn addition to the previous 4 tasks I think after we have settled down whether to use pipeline for spectral clustering there will be a  **5** Refactoring SpectralClustering @satra Would you please help to have a look whether the current structure is consistent with your proposal in PR #734 I have not factorized out the laplacian calculation yet as I am not sure where to put it in the graphpy or still in this estimatorPS: my first time to write an estimator subject to silly bugs :),,678,0.8244837758112095,0.27319587628865977,26597,329.62364176410875,31.244125277286912,90.53652667594089,1958,26,764,89,unknown,kuantkid,kuantkid,true,kuantkid,2,1.0,10,5,22,true,false,false,false,6,21,3,3,32,0,10
423977,scikit-learn/scikit-learn,python,1169,1348063891,1348174887,1348174887,1849,1849,github,false,false,false,39,4,1,0,4,0,4,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,109,0,141,0,4.497592358204374,0.13726254277900354,27,vlad@vene.ro,sklearn/linear_model/ridge.py,27,0.03492884864165589,1,3,false,MRG Ridge CG performance improvements Replaces @fabianps old PR #418 I havent looked at the speed of the thing much and only tested on SciPy 072 but it solves the horrendous memory usage of RidgeClassifier in the 20newsgroups example,,677,0.8242245199409158,0.2729624838292367,26597,329.62364176410875,31.244125277286912,90.53652667594089,1958,26,764,55,unknown,larsmans,larsmans,true,larsmans,47,0.7446808510638298,75,30,794,true,true,false,false,37,74,19,18,57,4,122
408409,scikit-learn/scikit-learn,python,1168,1348053665,,1410819487,1046037,,unknown,false,false,false,67,18,3,7,12,0,19,0,6,2,0,3,7,5,0,0,2,0,5,7,5,0,0,344,178,718,184,17.72756355579594,0.5410307626937778,121,vlad@vene.ro,sklearn/linear_model/tests/test_strong_rules.py|benchmarks/bench_strong_rules.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py,114,0.02099737532808399,0,11,false,WIP: enet strong rules (2) continuing work from PR #992This PR aims to extend the current coordinate_descentenet_path to uses an active set of features and strong rule filtering ([0] p 20)[0] Tibshirani R J Bien J Friedman T Hastie N Simon J Taylor and RJ Tibshirani “Strong Rules for Discarding Predictors in Lasso-type Problems” Journal of the Royal Statistical Society: Series B (Statistical Methodology) (2011),,676,0.8254437869822485,0.27165354330708663,26605,329.5245254651381,31.234730313850783,90.50930276263861,1958,26,764,228,unknown,ibayer,agramfort,false,,11,0.7272727272727273,2,0,200,true,true,false,false,4,71,47,36,10,0,366
408162,scikit-learn/scikit-learn,python,1167,1348048394,1348127253,1348127253,1314,1314,merged_in_comments,false,false,false,4,5,0,4,5,0,9,0,4,0,0,0,1,0,0,0,1,0,1,2,2,0,0,0,0,262,0,0,0.0,0,,,0,0.0,0,0,false,Contributing guide for #1164 ,,675,0.8251851851851851,0.27380952380952384,26605,329.5245254651381,31.234730313850783,90.50930276263861,1958,26,764,54,unknown,zaxtax,zaxtax,true,zaxtax,1,1.0,62,9,1607,false,true,false,false,0,0,0,0,0,0,348
407763,scikit-learn/scikit-learn,python,1165,1348000940,1351377954,1351377954,56283,56283,merged_in_comments,false,false,false,38,22,9,39,22,6,67,6,5,0,0,3,6,3,0,0,1,1,5,7,5,0,0,10,240,52,754,26.661525454292367,0.8136879840094712,34,vlad@vene.ro,examples/document_classification_20newsgroups.py|setup.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py|sklearn/cluster/spectral.py,18,0.017241379310344827,0,7,false,Enh spectral clustering Added an alternative to kmeans [1] to handle the embedding space of spectral clusteringAlso added a eigendecomposition tolerance option to decrease eigsh calculation time[1] Multiclass spectral clustering 2003     Stella X Yu Jianbo Shi,,674,0.8249258160237388,0.27320954907161804,26605,329.5245254651381,31.234730313850783,90.50930276263861,1957,26,763,70,unknown,briancheung,GaelVaroquaux,false,GaelVaroquaux,0,0,1,0,300,true,true,false,false,2,5,0,0,1,0,829
419690,scikit-learn/scikit-learn,python,1162,1347901900,1349031680,1349031680,18829,18829,merged_in_comments,false,false,false,95,26,8,0,68,0,68,0,6,0,0,1,10,0,0,1,9,2,8,19,1,1,10,0,0,1,0,8.880188221003888,0.27101856703552185,8,vlad@vene.ro,MANIFEST.in|MANIFEST.in,8,0.011049723756906077,1,16,false,Collapse toc trees for User-Guide This is a working version of the collapsible TOC-tree basically chucked what I was doing on this and rather  used @AlexandreAbraham s jQuery suggestion  script as its more stable and has a prettier- **swooshy** effectIt works well on my system on Chrome/Chromium Ive put up an online build of it to check whether it works on FireFox and IEand on different systemsFurthermore the little + and - buttons are just prototypes for now Need feedback on thisThe online build is herehttp://jaquesgroblergithubcom/online-sklearn-build/user_guidehtmlLooking forward to feedback,,673,0.8246656760772659,0.2569060773480663,26692,328.11329237224635,31.09545931365203,90.25176082721414,1956,26,762,64,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,23,0.9565217391304348,8,11,236,true,true,true,false,22,61,21,8,21,3,15
397329,scikit-learn/scikit-learn,python,1159,1347833299,1347916142,1347916142,1380,1380,commits_in_master,false,false,false,42,2,0,3,8,0,11,0,6,0,0,0,6,0,0,0,0,0,6,6,6,0,0,0,0,186,9,0,0.0,0,,,0,0.0,0,3,false,WIP Refactor linear classifiers Factored out the common code to LinearSVC LogisticRegression Perceptron and SGDClassifier (the prediction code) into a LinearClassifierMixin Improved the docstringsTODO: check whether RidgeClassifier and the naive Bayes estimators for discrete data can be given the same treatment,,672,0.8244047619047619,0.24393723252496433,26692,328.11329237224635,31.09545931365203,90.25176082721414,1954,25,761,52,unknown,larsmans,agramfort,false,agramfort,46,0.7391304347826086,75,30,791,true,true,false,true,34,71,18,16,40,4,11
415594,scikit-learn/scikit-learn,python,1152,1347723742,1348075031,1348075032,5854,5854,github,false,false,false,44,3,0,0,19,0,19,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,65,0,0,0.0,0,,,0,0.0,0,11,false,metricspy: modified precision_recall_curve to lower computational compl See [issue 1150](https://githubcom/scikit-learn/scikit-learn/issues/1150)I know this wont pass pep8 yet  For now I just want someone to review it to determine whether it is correct  Once thats sorted out I can polish the code to look nice,,671,0.8241430700447094,0.22727272727272727,26692,328.11329237224635,31.09545931365203,90.25176082721414,1952,25,760,61,unknown,conradlee,GaelVaroquaux,false,GaelVaroquaux,3,1.0,4,1,904,false,true,false,false,3,6,0,0,0,0,1
415423,scikit-learn/scikit-learn,python,1151,1347723202,1347731038,1347731038,130,130,github,false,false,false,24,0,0,0,3,0,3,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,MRG Removed deprecated for 013 Removes some of the things that are scheduled for removal in 013I tried to avoid conflicts with #1132,,670,0.8238805970149253,0.22727272727272727,26674,328.82207392966933,31.15393266851616,90.3126640173952,1952,25,760,54,unknown,amueller,amueller,true,amueller,115,0.8695652173913043,395,29,694,true,true,false,false,164,734,157,133,164,8,81
408113,scikit-learn/scikit-learn,python,1148,1347637272,,1349757620,35339,,unknown,false,false,false,77,1,0,0,4,0,4,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,2,true,BF: lars -- assure quiting the loop in a degenerate case before crashing if data is entirely degenerate (eg all Xs are 0s) then loop would eventually crashdue to:TypeError: unsupported operand type(s) for *: long and numpyfloat64when i63  swapping product order in (2 ** i) * eps would allow to loop until 1023but most probably with the same success  Thus it should be as sensible to quitloop before crashing with a TypeError,,669,0.8251121076233184,0.22238586156111928,26663,329.14525747290253,31.166785432997038,90.19990248659191,1948,25,759,67,unknown,yarikoptic,yarikoptic,true,,7,1.0,49,7,1373,true,true,false,false,4,2,2,0,1,0,3
380443,scikit-learn/scikit-learn,python,1147,1347610425,1349367101,1349367101,29277,29277,merged_in_comments,false,false,false,250,20,1,15,72,0,87,0,5,0,0,1,3,1,0,0,0,0,3,3,3,0,0,27,0,295,64,4.678920935715928,0.14363134619212428,6,vlad@vene.ro,sklearn/metrics/cluster/supervised.py,6,0.008902077151335312,1,34,true,FIX for issue #884 and improve contingency matrix built efficiency I think(I am not so sure) that the log operation breaks for log(a/b) as the division/multiplication will have possible loss of precision and log(a/b) should be written as log(a) - log(b) This causes the issue #884For example in issue #776 @ogrisel:1normalized_mutual_info_score([1 1 1 1 1 1 1] [0 1 2 3 4 5 6]) Will be zero2 for i in nplogspace(1 4 4):    print normalized_mutual_info_score(npones(i dtypenpint) nparange(i dtypenpint))Will be all zeroAnother is to use coo_matrix to accelerate the built of the contingency matrix A quick test is shown belowUsing coo_matrixcoo_matrix is similar to matlabs accumarray and I think it is more efficient to calculate :-)%timeit contingency  contingency_matrix(random_integers(110000(11000000))[0]                                                                          random_integers(110000(11000000))[0])1 loops best of 3: 604 ms per loopUsing original python dict%timeit contingency  contingency_matrix(random_integers(110000(11000000))[0]                                                                          random_integers(110000(11000000))[0])1 loops best of 3: 184 s per loopFurther issue is that when using dict contingency  contingency_matrix(random_integers(110(1100))random_integers(110(1100))) such code will fail as the ndarray not hashable a check for the input string should be madeOne thing remaining is that the contigency_matrix is usually (if not all the time) sparse so use sparse operation should further accelearte expecially calculate the x * log(x) with dealing the 0log0 case as we can get out the non-zero items with no cost than dense matrix But currently I put a todense() for the contingency matrix I will further fix it if decided to use coo_matrix :-),,668,0.8248502994011976,0.22106824925816024,26663,328.7702059033117,31.166785432997038,90.12489217267375,1946,25,759,72,unknown,kuantkid,amueller,false,amueller,1,1.0,8,4,17,true,false,false,false,4,5,1,0,7,0,103
402119,scikit-learn/scikit-learn,python,1146,1347565114,1347731885,1347731885,2779,2779,github,false,false,false,47,0,0,0,14,0,14,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Example: added a pretty PCA 3D plot of iris  as this data set is used in so many examplesUnfortunately the mpl bug about 3d rotations and colors makes it not as nice as it could beNot sure if anyone thinks this over complicates the example,,667,0.8245877061469266,0.22503725782414308,26663,328.7702059033117,31.166785432997038,90.12489217267375,1945,25,758,54,unknown,amueller,amueller,true,amueller,114,0.868421052631579,395,29,692,true,true,false,false,165,705,161,124,162,8,69
374190,scikit-learn/scikit-learn,python,1144,1347532871,1347957359,1347957359,7074,7074,github,false,false,false,26,5,0,6,12,0,18,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,892,0,0,0.0,0,,,0,0.0,0,5,false,MRG: check for allocation errors in _treepyx This PR follows from the discussion in #1086 It checks for  allocation errors (on malloc/calloc/realloc) in the tree module ,,666,0.8243243243243243,0.2204968944099379,26660,328.20705176294075,31.13278319579895,90.0225056264066,1944,24,758,58,unknown,glouppe,glouppe,true,glouppe,22,1.0,74,20,672,true,true,false,false,29,139,51,27,52,1,171
373461,scikit-learn/scikit-learn,python,1143,1347496782,1347798502,1347798502,5028,5028,github,false,false,false,57,4,1,16,14,0,30,0,5,0,0,9,10,9,0,0,0,0,10,10,10,0,0,573,55,610,55,35.86396932771197,1.1009409394891712,36,vlad@vene.ro,sklearn/linear_model/logistic.py|sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pxd|sklearn/svm/liblinear.pyx|sklearn/svm/src/liblinear/liblinear_helper.c|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py,31,0.004665629860031105,0,2,false,MRG Refactor liblinear bindings Heres yet another refactoring of the liblinear bindings Ive replaced all the prediction code in LinearSVC and LogisticRegression with equivalent NumPy code Highlights:* no more copying of X at predict time* no more unchecked malloc at predict time* -4000 lines of C code* simpler handling of coef_ and intercept_,,665,0.8240601503759398,0.223950233281493,26660,328.20705176294075,31.13278319579895,90.0225056264066,1944,24,757,58,unknown,larsmans,larsmans,true,larsmans,45,0.7333333333333333,75,30,787,true,true,false,false,32,54,15,10,35,4,541
368635,scikit-learn/scikit-learn,python,1141,1347475419,1347635415,1347635415,2666,2666,github,false,false,false,71,6,0,6,7,0,13,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,66,13,0,0.0,0,,,0,0.0,0,5,false,MRG: Fix predict_proba w/ sparse matrix regression This PR fixes the SGDClassifierpredict_proba w/ sparse matrix regression The regression was introduced in #1030 and thus release 012 where I used len to get the number of rows of matrix X Even thought predict_proba was covered by a number of tests the test suite used a trick automatically test for dense and sparse inputs however predict_proba was not exposed to the sparse inputs,,664,0.8237951807228916,0.22135007849293564,26638,327.65222614310386,31.120955026653654,89.53374877993843,1943,24,757,57,unknown,pprett,larsmans,false,larsmans,29,0.8620689655172413,76,27,1135,true,true,true,true,41,126,24,20,61,0,3
370336,scikit-learn/scikit-learn,python,1138,1347394752,1347460121,1347460121,1089,1089,github,false,false,false,25,1,1,0,7,0,7,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.567881897287921,0.14022645442181575,6,vlad@vene.ro,doc/sphinxext/gen_rst.py,6,0.009584664536741214,0,2,false,Print running time as a floating-point number with two decimals Otherwise printing it as integer truncates the result and prints0 seconds on most example,,663,0.8235294117647058,0.2220447284345048,26638,327.65222614310386,31.120955026653654,89.53374877993843,1938,24,756,52,unknown,fabianp,fabianp,true,fabianp,25,0.68,111,22,850,true,true,false,false,12,13,3,3,0,1,33
365884,scikit-learn/scikit-learn,python,1135,1347307873,,1361818751,241847,,unknown,false,true,false,118,1,1,7,35,1,43,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,199,0,199,0,4.176279905465912,0.12820493444977998,76,vlad@vene.ro,sklearn/feature_extraction/text.py,76,0.12101910828025478,0,22,false,new CountVectorizer using arrays instead of lists This implementation doesnt change the interface of CountVectorizer and passes the tests Here are some benchmark figures:Time needed to turn 500K documents into count vectors: * old list based CountVectorizer: 2m31797s * new array based CountVectorizer: 2m43890sPeak memory needed turning 500K documents into count vectors: * old list based CountVectorizer: 63gb * new array based CountVectorizer: 920mbThe array based CountVectorizer is a little bit slower because it has to recreate the feature arrays after the terms are sorted and the dimensions are remapped It also introduces a new dependency on the array modulePrevious attempt to reduce the memory footprint of CountVectorizer by stacking together sparse matrices: #1122,,662,0.824773413897281,0.22770700636942676,26637,327.66452678604946,31.122123362240494,89.53711003491384,1936,24,755,87,unknown,ephes,larsmans,false,,3,0.0,4,6,653,false,true,false,false,3,10,4,0,0,0,4587
363240,scikit-learn/scikit-learn,python,1133,1347188285,1350398161,1350398161,53497,53497,merged_in_comments,false,false,false,32,3,2,0,4,3,7,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,21,27,27,27,9.15024909020088,0.28089943914947413,5,vlad@vene.ro,sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py,5,0.008012820512820512,0,2,false,WIP sparse matrix support in randomized logistic regression His adds sparse matrix support to randomized logistic regressionNeeds a bit more testing and I should probably also do the lasso for completeness,,661,0.8245083207261724,0.23076923076923078,26518,329.39889886115094,31.261784448299267,90.31601176559319,1932,24,754,71,unknown,amueller,larsmans,false,larsmans,113,0.8672566371681416,392,29,688,true,true,false,false,161,666,160,109,163,8,6161
362118,scikit-learn/scikit-learn,python,1132,1347110607,,1348177402,17779,,unknown,false,false,false,47,2,0,1,5,0,6,0,3,0,0,0,15,0,0,0,0,7,8,15,11,0,0,0,0,289,37,0,0.0,0,,,0,0.0,0,0,false,MRG rm svmsparse and linear_modelsparse The modules sklearn{svmlinear_model}sparse are scheduled for removal in the next release so I suggest we remove them now Tests pass on my box the main thing that Id like someone to check is whether I correctly removed test_sparse_svc_clone_with_callable_kernel from the SVM tests,,660,0.8257575757575758,0.23832528180354268,26512,329.473445986723,31.268859384429692,90.3364514182257,1930,24,753,62,unknown,larsmans,larsmans,true,,44,0.75,75,30,783,true,true,false,false,33,49,13,10,29,4,945
361251,scikit-learn/scikit-learn,python,1131,1347058479,1348427075,1348427075,22809,22809,github,false,false,false,32,6,0,8,12,0,20,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,134,95,0,0.0,0,,,0,0.0,0,3,true,MRG add MinMaxScaler #1111 This adds a MinMax Scaler as described in #1111Unfortunately I couldnt figure out how to get column-wise max and min in sparse matrices Any hintsThanksAndy,,659,0.8254931714719271,0.24237560192616373,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,752,66,unknown,amueller,amueller,true,amueller,112,0.8660714285714286,390,29,686,true,true,false,false,159,665,159,107,167,8,1822
360861,scikit-learn/scikit-learn,python,1128,1347047595,1347484547,1347484547,7282,7282,github,false,false,false,102,2,0,0,3,0,3,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,MRG Rfe estimator params This addresses issue #1028 and part of #1115:RFE no clones the estimator before changing it therefore being more strict in the fit doesnt change parameters sense Also other meta-estimators like GridSearch behave this wayThis is unfortunately a slight API breakage as now after fitting rfeestimator is an unfitted estimator and rfeestimator_ is a fitted oneThe rfeestimator was not documented thoughThe other commit makes it possible to use RFE and RFECV in a grid search As for example linear classifiers basically always need a grid search for C I think this is an important feature,,658,0.8252279635258358,0.24006359300476948,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,752,55,unknown,amueller,amueller,true,amueller,111,0.8648648648648649,390,29,686,true,true,false,false,156,659,159,107,173,8,3
358801,scikit-learn/scikit-learn,python,1126,1346976141,1350156904,1350156904,53012,53012,merged_in_comments,false,false,false,15,20,0,39,56,0,95,0,6,0,0,0,10,0,0,0,3,1,9,13,9,0,0,0,0,588,99,0,0.0,0,,,0,0.0,0,14,false,Factor Analysis -- implemented with EM + SVD Plain Factor Analysis with a simple test,,657,0.8249619482496194,0.23937007874015748,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,751,66,unknown,osdf,osdf,true,osdf,1,1.0,16,4,949,false,true,false,false,0,3,0,0,0,0,570
358779,scikit-learn/scikit-learn,python,1125,1346975498,,1374765551,463167,,unknown,false,true,false,12,4,0,3,15,1,19,0,7,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,41,8,0,0.0,0,,,0,0.0,0,4,false,added multiclass_log_loss metric Dont know whether this is helpful just practicing :),,656,0.8262195121951219,0.23937007874015748,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,751,132,unknown,ephes,arjoly,false,,2,0.0,4,6,649,false,true,false,false,1,1,3,0,0,0,1034
358731,scikit-learn/scikit-learn,python,1124,1346974494,,1346974628,2,,unknown,false,false,false,12,3,0,0,0,0,0,0,1,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,367,8,0,0.0,0,,,0,0.0,0,0,false,Multiclass log loss New error metric for multiclass classification described here:https://wwwkagglecom/wiki/MultiClassLogLoss,,655,0.8274809160305343,0.23817034700315456,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,751,47,unknown,ephes,ephes,true,,1,0.0,4,6,649,false,true,false,false,0,0,1,0,0,0,-1
358619,scikit-learn/scikit-learn,python,1122,1346971633,,1362489360,258628,,unknown,false,false,false,159,2,0,0,14,0,14,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,344,0,0,0.0,0,,,0,0.0,0,2,false,CountVectorizerBuffered Ran out of memory with CountVectorizer (having 34 million documents with 35Gb data and only 8Gb RAM)  The problem is that CountVectorizer stores term_counts_per_doc for all documents before creating the sparse coo_matrixSo i wrote a new class CountVectorizerBuffered which creates a partial sparse feature matrix for every 5K documents and merges the partial feature matrices in the end This fixes the memory issue but it also  alters the behavior of the class which makes it kind of incompatible with CountVectorizer For example its not possible to set max_df min_df or max_features and then simply call fit_transform because fit_transform doesnt have the required token statistics for that Therefore you now have to call fit before transform if you want to use max_df min_df or max_featuresI thought about replacing CountVectorizer with CountVectorizerBuffered and i have a branch (countvectorizer) in which i did but this breaks a lot of test-cases well im not sure what to do here,,654,0.8287461773700305,0.23817034700315456,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,751,85,unknown,ephes,larsmans,false,,0,0,4,6,649,false,true,false,false,0,0,0,0,0,0,1081
357126,scikit-learn/scikit-learn,python,1119,1346929258,1347266454,1347266454,5619,5619,github,false,false,false,83,8,0,5,16,0,21,0,6,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,282,40,0,0.0,0,,,0,0.0,0,2,false,MRG Sgd clone fix The SGD* object initialization does not work properly with set_params and thus clone This PR moves input argument parsing and creation of helper classes (eg loss_function) to the fit (actually partial_fit) methods This PR addresses #1114The following issues need to be resolved: - Factory for loss function objects (epsilon issue - see failing test) - Should we do input argument checks in __init__ or exclusively in fit - The sgd holds some opportunities for refactoring (code duplication etc),,653,0.8284839203675345,0.23817034700315456,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,751,53,unknown,pprett,pprett,true,pprett,28,0.8571428571428571,75,27,1129,true,true,false,false,41,119,19,17,52,0,0
356224,scikit-learn/scikit-learn,python,1117,1346886378,1347929418,1347929418,17384,17384,github,false,false,false,48,3,0,0,8,0,8,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,84,14,0,0.0,0,,,0,0.0,0,3,false,ENH in NMF only use svd initialization  n_components  n_features Since NMF can provide sparse solutions n_components  n_features is often used The current initialization breaks in this setting I think the default values should work robustly therefore the different initialization strategy in this caseCloses issue #584,,652,0.8282208588957055,0.24050632911392406,26512,329.473445986723,31.268859384429692,90.3364514182257,1929,24,750,57,unknown,amueller,amueller,true,amueller,110,0.8636363636363636,390,29,684,true,true,false,false,159,646,161,102,175,8,535
351201,scikit-learn/scikit-learn,python,1110,1346709450,1346750577,1346750577,685,685,github,false,false,false,20,3,0,0,5,0,5,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,12,0,0,0.0,0,,,0,0.0,0,7,false,Fixed a little documentation for SGDClassifier Fixed sparse support in SGDClassifier in doc And fixed return statement in SGDClassifier predict_proba,,651,0.8279569892473119,0.268370607028754,26138,333.3843446323361,31.52498278368659,91.24646109113169,1921,24,748,47,unknown,buma,ogrisel,false,ogrisel,1,1.0,3,0,353,false,true,false,false,3,21,1,0,0,0,112
351002,scikit-learn/scikit-learn,python,1109,1346701137,1346701811,1346701811,11,11,github,false,false,false,19,1,0,0,1,0,1,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,2,false,DOC Added SGDCLassifier support only binary prediction probabilites I updated documentation of SGDClassifier that it supports only binary predictive_probabilities,,650,0.8276923076923077,0.2688,26138,333.3843446323361,31.52498278368659,91.24646109113169,1921,24,748,47,unknown,buma,amueller,false,amueller,0,0,3,0,353,false,true,false,false,2,16,0,0,0,0,11
343644,scikit-learn/scikit-learn,python,1106,1346617360,1346627582,1346627582,170,170,merged_in_comments,false,false,false,64,6,0,4,10,0,14,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,117,0,0,0.0,0,,,0,0.0,0,1,false,MRG: Affinity propagation Ive done very little on this code:- change the example that used a deprecated API- change the default value of convit which was 3 times higher than the one mention in the paper As a results examples should run faster (its still insanely slow on my laptop)- renamed convit into convergence_iteration and deprecated the old interfacerefs #1072,,649,0.827426810477658,0.27887788778877887,26058,334.82999462736973,31.890398342159795,91.37309079745185,1921,23,747,56,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,11,0.8181818181818182,27,13,958,true,true,true,false,10,15,10,8,30,0,0
349186,scikit-learn/scikit-learn,python,1104,1346608527,1346619020,1346619020,174,174,github,false,false,false,13,1,0,0,3,0,3,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,3,9,0,0.0,0,,,0,0.0,0,0,false,MRG raise ValueError in r2_score when given only a single sample Closes #1054,,648,0.8271604938271605,0.2764227642276423,26047,334.89461358313815,31.86547395093485,91.37328675087342,1921,23,747,56,unknown,amueller,amueller,true,amueller,109,0.8623853211009175,390,29,681,true,true,false,false,167,612,171,88,164,9,23
340828,scikit-learn/scikit-learn,python,1103,1346606812,1346670417,1346670417,1060,1060,merged_in_comments,false,false,false,74,2,0,7,7,0,14,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,39,2,0,0.0,0,,,0,0.0,1,5,false,FIX: this fixes issues #746 ProbabilisticPCA minor things Fix the issues opened here: https://githubcom/scikit-learn/scikit-learn/issues/7461 It uses a non-stanard dim attribute : change every dim to n_features including in the doc strings2 The score function has an unnecessary loop : use matrix multiplication to calculate the log likelihood no loop any more3 Sign error fixed The test script(test_probabilistic_pca_1) also contains an error similar to this one fixed as well Passed test_probabilistic_pca_{1234}@amueller,,647,0.8268933539412674,0.2768729641693811,26047,334.89461358313815,31.86547395093485,91.37328675087342,1921,23,747,53,unknown,kuantkid,amueller,false,amueller,0,0,8,2,5,false,false,false,false,1,0,0,0,0,0,4
349063,scikit-learn/scikit-learn,python,1102,1346600038,1346624522,1346624522,408,408,github,false,false,false,79,11,4,0,10,0,10,0,4,0,0,7,8,7,0,0,0,0,8,8,8,0,0,45,73,401,109,54.49099396476839,1.6747385511197814,60,vanderplas@astro.washington.edu,sklearn/decomposition/fastica_.py|sklearn/tests/test_common.py|sklearn/decomposition/fastica_.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/manifold/mds.py|sklearn/neighbors/base.py|sklearn/pls.py|sklearn/tests/test_common.py|sklearn/manifold/mds.py|sklearn/pls.py|sklearn/tests/test_common.py,42,0.009787928221859706,1,0,false,WIP: Mixins everywhere PLS was kind of ignored by the common tests because of thisI wonder whether GMM should have ClusterMixinThere seem to be a couple of random failing general tests: with RandomizedLogisticRegression (that I shouldnt have touched) and with PLS (in case of bad luck numerically I think) but I could only reproduce them with nosetests on the small file not with make test so I dont have the seeds Based on initial work by @amueller,,646,0.826625386996904,0.27732463295269166,26047,334.89461358313815,31.86547395093485,91.37328675087342,1921,23,747,55,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,28,0.9285714285714286,41,19,875,true,true,true,false,22,84,52,17,77,0,8
349061,scikit-learn/scikit-learn,python,1101,1346599952,1346618877,1346618877,315,315,merged_in_comments,false,false,false,16,2,0,0,4,0,4,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,15,0,0,0.0,0,,,0,0.0,0,0,false,MRG: Deprecated sparse classes from the SVM module - refs #1093 Those classes are now deprecated,,645,0.8263565891472868,0.27732463295269166,26047,334.89461358313815,31.86547395093485,91.37328675087342,1921,23,747,57,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,10,0.8,27,13,958,true,true,true,false,10,15,9,8,28,0,6
349033,scikit-learn/scikit-learn,python,1100,1346597327,1346617740,1346617740,340,340,github,false,false,false,57,1,0,0,2,0,2,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,1,false,FIX in the makefile - we should delete pyc and so only from the source c ode and not from everything in the root folderSometimes people put there virtualenv sandbox in there and it deletes all the so from the sandbox hence forcing the developper to reinstall numpy and scipy each time they run a make,,644,0.8260869565217391,0.2777777777777778,26047,334.89461358313815,31.86547395093485,91.37328675087342,1921,23,747,56,unknown,NelleV,ogrisel,false,ogrisel,9,0.7777777777777778,27,13,958,true,true,false,false,10,15,8,8,28,0,15
412269,scikit-learn/scikit-learn,python,1096,1346526512,,1346600463,1232,,unknown,false,false,false,29,1,0,0,2,0,2,0,2,0,0,0,2,0,0,0,3,0,2,5,5,0,0,0,0,2322,0,0,0.0,0,,,0,0.0,0,0,false,WIP count vectorizer: cythonized term counting Just putting it into cython seems to double the speed of fit + transform for CountVectorizer on my boxCan any one confirm,,643,0.8273716951788491,0.28500823723228996,26078,334.49651046859424,31.82759414065496,91.26466753585397,1921,23,746,54,unknown,amueller,amueller,true,,108,0.8703703703703703,390,29,680,true,true,false,false,165,598,166,88,161,9,39
412250,scikit-learn/scikit-learn,python,1095,1346518292,1346529985,1346529985,194,194,merged_in_comments,false,false,false,7,13,0,0,10,0,10,0,4,0,0,0,35,0,0,0,0,0,35,35,34,0,0,0,0,287,4,0,0.0,0,,,0,0.0,0,3,false,MRG: remove deprecated objects scheduled for 012 ,,642,0.8271028037383178,0.2922297297297297,26065,331.13370420103587,31.728371379244198,91.00326107807405,1921,23,746,55,unknown,vene,NelleV,false,NelleV,27,0.9259259259259259,41,19,874,true,true,false,false,20,77,44,17,60,0,0
412234,scikit-learn/scikit-learn,python,1094,1346513748,1346513929,1346513929,3,3,github,false,false,false,137,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,Fix warning in Example: Sample pipeline for text feature extraction and evaluation Avoid warning by adding underscore to: grid_searchbest_estimator_ and grid_searchbest_score_Loading 20 newsgroups dataset for categories:[altatheism talkreligionmisc]857 documents2 categoriesPerforming grid searchpipeline: [vect tfidf clf]parameters:{clf__alpha: (1e-05 1e-06) clf__penalty: (l2 elasticnet) vect__max_df: (05 075 10) vect__max_n: (1 2)}[Parallel(n_jobs-1)]: Done   1 jobs       | elapsed:   350s[Parallel(n_jobs-1)]: Done  50 jobs       | elapsed:  23min[Parallel(n_jobs-1)]: Done  66 out of  72 | elapsed:  29min remaining:   159s[Parallel(n_jobs-1)]: Done  72 out of  72 | elapsed:  30min finisheddone in 188127sBest score: 0930Best parameters set:clf__alpha: 1e-05clf__penalty: l2vect__max_df: 10vect__max_n: 1I noticed also that the doc string and terminal output dont fit together since they estimate different parameters I dont understand why the number of samples is differenthttp://scikit-learnorg/dev/auto_examples/grid_search_text_feature_extractionhtml,,641,0.8268330733229329,0.29322033898305083,26064,331.14640883977904,31.72958870472683,91.00675260896256,1921,23,746,55,unknown,ibayer,ogrisel,false,ogrisel,10,0.7,2,0,182,true,true,false,false,10,80,53,36,31,0,2
412112,scikit-learn/scikit-learn,python,1092,1346459752,1346510116,1346510116,839,839,github,false,false,false,25,4,1,0,3,0,3,0,3,0,0,3,9,2,0,0,0,0,9,9,8,0,0,70,40,104,40,14.18239054200385,0.4358592898095611,40,peter.welinder@gmail.com,doc/modules/feature_extraction.rst|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py,26,0.03350083752093802,0,1,true,MRG Robust int test Replaces isinstance(x int) with isinstance(x (int npinteger)) - and same for float in two placesThis makes it invariant wrt int32/int64,,640,0.8265625,0.2981574539363484,26063,330.46848022100295,31.692437555154815,90.70329586003146,1919,24,745,57,unknown,amueller,amueller,true,amueller,107,0.8691588785046729,390,29,679,true,true,false,false,160,583,155,84,155,9,53
317352,scikit-learn/scikit-learn,python,1088,1346357384,1346747685,1346747685,6505,6505,commit_sha_in_comments,false,false,false,44,14,7,5,8,0,13,0,4,0,0,3,4,3,0,0,0,0,4,4,3,0,0,679,51,1022,102,35.57329248675912,1.0932558553410647,37,peter.prettenhofer@gmail.com,sklearn/ensemble/__init__.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py,27,0.03225806451612903,0,3,false,GBRT API consistency This is a cerry-pick of #1036 that addresses #1085 and some other API issues of the GBRT moduleSummary:  * Better docstrings  * GradientBoostingClassifier now has staged_decision_function staged_predict_proba and staged_predictThese methods are important for efficient model selection for GBRT models,,639,0.8262910798122066,0.3089983022071307,26053,330.4801750278279,31.704602157141217,90.66134418301155,1917,24,744,57,unknown,pprett,pprett,true,pprett,27,0.8518518518518519,75,27,1122,true,true,false,false,38,108,15,14,34,0,3
329706,scikit-learn/scikit-learn,python,1087,1346352079,1347536810,1347536810,19745,19745,merged_in_comments,false,false,false,163,4,0,1,23,4,28,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,43,21,0,0.0,0,,,0,0.0,0,6,false,Mechanism to propagate optional estimatorfit arguments when using CV Instead of opening a feature request for this issue:http://stackoverflowcom/questions/12131472/additional-fitting-parameters-for-cross-validationI decided to propose a possible solution In a nutshell what it allows to do:python import numpy as np from sklearncross_validation import cross_val_score from sklearnnaive_bayes import MultinomialNB n_samples n_classes  100 5 X  nprandomrandint(10 size(n_samples n_samples)) y  nparange(n_classes) clf  MultinomialNB() print cross_val_score(clf X y cv5                         class_prior[02] * n_classes # those two params will be                           sample_weight[001] * n_samples) # propagated to clffit Ill admit that as a sklearn newbie I may not be aware of all the possible effects this addition might have on other types of estimator (I only tested it in the context of a MultinomialNB) Also its quite possible that it could be generalized in a better way as similar problems likely show up elsewhere in the codebase Nevertheless I thought it could be an easy and useful modification,,638,0.8260188087774295,0.30952380952380953,26053,330.4801750278279,31.704602157141217,90.66134418301155,1917,24,744,68,unknown,cjauvin,agramfort,false,agramfort,0,0,2,1,650,false,false,false,false,0,0,0,0,0,0,267
316547,scikit-learn/scikit-learn,python,1086,1346350459,,1357918644,192803,,unknown,false,false,false,156,2,0,5,27,0,32,0,7,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,311,0,0,0.0,0,,,0,0.0,3,8,false,WIP: cleanup of the Cython code for sklearntree I noticed that the sklearntree_tree code uses a lot of unchecked malloc calls which IMHO is a recipe for disaster Instead of adding checks everywhere I decided to try and replace them all (well except for one) with NumPy memory allocation ie npempty As far as I can tell this is not significantly slower than using malloc directlyBefore I proceed to replace the value member as well whose allocation rules I currently dont understand I would like some feedback (@pprett @bdholt1 @glouppe) I only ran the benchmark to check the speedThe unit tests currently crash because I broke the pickling code but if you disable the pickling test the rest passesIf this works well it might be a good idea to always use NumPy memory allocation in Cython code in the future so we get proper exceptions garbage collection and less chance of memory leaks,,637,0.8273155416012559,0.3100511073253833,26053,330.4801750278279,31.704602157141217,90.66134418301155,1917,24,744,90,unknown,larsmans,larsmans,true,,43,0.7674418604651163,74,30,774,true,true,false,false,31,43,13,8,25,5,11
314216,scikit-learn/scikit-learn,python,1084,1346319244,1346668640,1346668640,5823,5823,github,false,false,false,25,9,0,4,7,0,11,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,60,13,0,0.0,0,,,0,0.0,0,12,false,MRG: SGD fix for yshape  (n_samples 1) This fixes #604 The PR also includes some cosmit in the SGD module (constants input validation),,636,0.8270440251572327,0.31871838111298484,26053,330.4801750278279,31.704602157141217,90.66134418301155,1917,25,744,56,unknown,pprett,pprett,true,pprett,26,0.8461538461538461,75,27,1122,true,true,false,false,36,103,14,11,30,0,5
313814,scikit-learn/scikit-learn,python,1082,1346274870,1346502871,1346502871,3800,3800,github,false,false,false,30,5,1,5,5,0,10,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,107,0,107,4.196782373302811,0.12897458742920198,36,peter.welinder@gmail.com,sklearn/tests/test_common.py,36,0.06196213425129088,0,1,false,MRG Qda lda decision function This changes the shape of the decision function of QDA and LDA to [n_samples 1] in the two-class case to match the rest of sklearn,,635,0.8267716535433071,0.3270223752151463,26040,329.95391705069125,31.720430107526884,90.55299539170507,1916,25,743,58,unknown,amueller,amueller,true,amueller,106,0.8679245283018868,390,29,677,true,true,false,false,162,561,154,80,148,9,741
333636,scikit-learn/scikit-learn,python,1079,1346098082,,1346525519,7123,,unknown,false,false,false,15,2,0,0,5,0,5,0,3,0,0,0,8,0,0,0,0,0,8,8,8,0,0,0,0,86,52,0,0.0,0,,,0,0.0,0,1,false,MRG add ClusteringMixin with fit_predict and some tests Introduces a clustering meta-class and some testing,,634,0.8280757097791798,0.33676975945017185,26044,328.59775764091535,31.677161726309322,90.50069113807403,1912,25,741,60,unknown,amueller,NelleV,false,,105,0.8761904761904762,387,29,675,true,true,true,false,161,546,150,78,143,9,1
333015,scikit-learn/scikit-learn,python,1078,1346083333,1346083396,1346083396,1,1,github,false,false,false,9,1,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,correcting typos in the doc attempt to pull something,,633,0.8278041074249605,0.3415492957746479,26042,328.6229936256816,31.679594501190387,90.50764150218878,1912,24,741,55,unknown,buguen,fabianp,false,fabianp,0,0,2,6,364,false,false,false,false,0,0,0,0,0,0,-1
332857,scikit-learn/scikit-learn,python,1077,1346080335,1346080812,1346080812,7,7,github,false,false,false,15,1,0,0,0,0,0,0,4,0,0,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,4,false,FIX removed ancient templates from manifest to make sklearn pip-installable A humble sprint contribution :-),,632,0.8275316455696202,0.34275618374558303,26040,328.6482334869432,31.682027649769587,90.51459293394777,1912,24,741,55,unknown,ludwigschwardt,jaquesgrobler,false,jaquesgrobler,0,0,5,0,719,false,false,false,false,0,0,0,0,0,0,-1
308975,scikit-learn/scikit-learn,python,1076,1346076920,1346670332,1346670332,9890,9890,github,false,false,false,46,18,0,20,7,0,27,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,MRG support custom kernels on sparse matrices I think I mentioned this on the ML We dont actually need sparse kernels we just need kernels on sparse dataThis fixes code that previously did random memory access maybe should put a guard up somewhereCloses #918,,631,0.8272583201267829,0.34275618374558303,26040,328.6482334869432,31.682027649769587,90.51459293394777,1911,24,741,66,unknown,amueller,amueller,true,amueller,104,0.875,387,29,675,true,true,false,false,158,535,145,73,135,9,3360
332247,scikit-learn/scikit-learn,python,1075,1346069273,1346069288,1346069288,0,0,github,false,false,false,26,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Just a tweak for the headers Long headers appear to fine with this font size adjustmentIf anyone spots one that looks bad let me know,,630,0.8269841269841269,0.34458259325044405,26040,328.6482334869432,31.682027649769587,90.51459293394777,1911,24,741,54,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,22,0.9545454545454546,8,11,215,true,true,false,false,23,53,17,4,16,3,-1
332216,scikit-learn/scikit-learn,python,1074,1346068605,1346097679,1346097679,484,484,merged_in_comments,false,false,false,15,4,0,0,4,0,4,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,26,0,0.0,0,,,0,0.0,0,0,false,MRG Fix for old numpy add regression test for isotropic data Non-regression tests ad #1071,,629,0.8267090620031796,0.34458259325044405,26040,328.6482334869432,31.682027649769587,90.51459293394777,1911,24,741,55,unknown,amueller,amueller,true,amueller,103,0.8737864077669902,387,29,675,true,true,false,false,157,532,139,73,134,9,17
328485,scikit-learn/scikit-learn,python,1070,1346020220,1346524414,1346524414,8403,8403,merged_in_comments,false,false,false,57,1,0,0,13,0,13,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,37,0,0.0,0,,,0,0.0,3,3,false,MRG Testing Regressors with int input fixed #716 This is a fix for #716 with a non-regression test for all estimators~~~Unfortunately the test causes a segfault in the trees (I think)You can reproduce by running nosetests sklearn/tests/test_commonpy -m regressors_int in this branch@glouppe @pprett @bdholt1 any one has an idea what could be causing this~~~,,628,0.8264331210191083,0.34581105169340465,25981,327.5855432816289,31.676994726915826,90.56618297986991,1910,24,740,61,unknown,amueller,NelleV,false,NelleV,102,0.8725490196078431,387,29,674,true,true,true,false,156,520,136,70,134,9,1
289045,scikit-learn/scikit-learn,python,1068,1346007491,1346256444,1346256444,4149,4149,github,false,false,false,7,3,0,5,9,0,14,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,64,0,0,0.0,0,,,0,0.0,0,2,false,MRG some cleanup in grid_search Closes #777,,627,0.8261562998405104,0.3464285714285714,25981,327.5855432816289,31.676994726915826,90.56618297986991,1910,24,740,58,unknown,amueller,amueller,true,amueller,101,0.8712871287128713,387,29,674,true,true,false,false,155,515,134,70,133,9,860
289175,scikit-learn/scikit-learn,python,1067,1346005822,1346524155,1346524155,8638,8638,merged_in_comments,false,false,false,19,3,0,5,2,0,7,0,3,0,0,0,6,0,0,0,0,0,6,6,6,0,0,0,0,34,59,0,0.0,0,,,0,0.0,0,0,false,MRG rudimentary testing of tranformer objects Unfortunately takes a bit long even with 10 data points Didnt benchmark yet,,626,0.8258785942492013,0.3464285714285714,25981,327.47007428505447,31.676994726915826,90.45071398329549,1910,24,740,63,unknown,amueller,NelleV,false,NelleV,100,0.87,387,29,674,true,true,true,false,153,514,133,70,133,9,975
322054,scikit-learn/scikit-learn,python,1066,1345992248,,1346517635,8756,,unknown,false,false,false,94,2,0,0,15,2,17,0,3,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,76,40,0,0.0,0,,,0,0.0,1,6,false,MRG added min_df keyword to CountVectorizer default2 This is a resurrection of #348As this was before @ogrisels rewrite I just wrote it from scratchI used the float/int convention that is now widely used throughout the scikit (for example in cv)I took the liberty of making the default for min_df two as I feel that is still very conservative and should always improve performance I corrected all the doctests for thatThis does change results compared to previous versions and if someone is -1 I can set the default back to one,,625,0.8272,0.3464285714285714,25979,326.2250279071558,31.640940759844494,90.45767735478657,1910,24,740,62,unknown,amueller,amueller,true,,99,0.8787878787878788,387,29,674,true,true,false,false,151,510,128,65,133,9,1230
267184,scikit-learn/scikit-learn,python,1065,1345910807,1346627976,1346627976,11952,11952,merged_in_comments,false,false,false,49,60,16,44,37,0,81,0,5,0,0,7,16,7,0,0,0,0,16,16,13,0,0,486,120,1620,356,95.2569733360515,2.9274849450534965,65,vlad@vene.ro,sklearn/decomposition/dict_learning.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py|sklearn/decomposition/dict_learning.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,37,0.01675977653631285,1,17,false,MRG : coordinate descent + dict learning code improvements contains:- LassoLars multiple targets - ElasticNet and Lasso multiple targets- better input checks using validationpy (with some additions) - use Lasso multitarget in dict learningcontains commits from PR #913 that I will close if @vene is ok,,624,0.8269230769230769,0.35940409683426444,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,24,739,70,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,24,0.8333333333333334,83,174,997,true,true,true,false,75,181,33,122,58,1,1
264965,scikit-learn/scikit-learn,python,1064,1345905321,,1347727400,30367,,unknown,false,false,false,81,28,9,31,17,5,53,0,8,2,0,3,10,5,0,0,3,0,8,11,7,0,0,201,27,691,56,22.476077216505363,0.6907460458884528,35,vlad@vene.ro,sklearn/linear_model/__init__.py|sklearn/linear_model/isotonic_regression_.py|sklearn/manifold/mds.py|sklearn/manifold/tests/test_mds.py|sklearn/linear_model/tests/test_isotonic_regression.py,29,0.0111731843575419,0,8,false,WIP Isotonic regression Here is a PR for a new isotonic regression replacing the one in the manifolds mds implementation It has now a proper sklearn interface with a fit methodThe isotonic regression code now implements weights and a couple of extra options (Fabian and Alex actually implemented this version of the code so it is necessarily perfect) The MDS code has been updated to use this new versionTheres still the documentation to work on and a small example,,623,0.8282504012841091,0.35940409683426444,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,24,739,79,unknown,NelleV,agramfort,false,,8,0.875,26,13,950,true,true,true,true,3,10,7,0,26,0,11
338505,scikit-learn/scikit-learn,python,1063,1345857839,1345858823,1345858823,16,16,github,false,false,false,44,2,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,true,make precision_recall_curve work on native python lists I changed metricsprecision_recall_curve to support native python lists Previously it assumed that the input arrays were ndarrays (and called the ravel instance method)Note that now precision_recall_curve is consistent with roc_curve which already supports native Python lists,,622,0.8279742765273312,0.367112810707457,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,23,738,51,unknown,welinder,ogrisel,false,ogrisel,0,0,3,2,1486,false,false,false,false,0,0,0,0,0,0,16
258134,scikit-learn/scikit-learn,python,1060,1345817587,,1346280297,7711,,unknown,false,false,false,45,13,1,15,23,1,39,0,8,0,0,1,2,1,0,0,0,0,2,2,2,0,0,40,0,90,47,4.279982596654582,0.13153456890705356,3,peter.prettenhofer@gmail.com,sklearn/cross_validation.py,3,0.005747126436781609,0,17,true,WIP Stratified Shuffle Split It definitely requires further testing Im just interested in knowing if you can find cases where it doesnt behave properly so that it can be fixed And I would like to be positively sure it works properly before discussing design considerations,,621,0.8293075684380032,0.367816091954023,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,23,738,61,unknown,schwarty,amueller,false,,1,0.0,4,0,802,false,true,false,false,1,0,0,0,0,0,151
337531,scikit-learn/scikit-learn,python,1058,1345764357,,1345764628,4,,unknown,false,true,false,4,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,110,0,0,0.0,0,,,0,0.0,0,2,false,Maybe fixes issue #1051 ,,620,0.8306451612903226,0.36923076923076925,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,23,737,50,unknown,kyleabeauchamp,kyleabeauchamp,true,,2,0.0,9,0,57,false,false,false,false,3,1,4,0,0,0,-1
410185,scikit-learn/scikit-learn,python,1057,1345763946,,1345764118,2,,unknown,false,false,true,5,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,110,0,0,0.0,0,,,0,0.0,0,0,false,Possible fix for issue #1051 ,,619,0.8319870759289176,0.36923076923076925,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,23,737,50,unknown,kyleabeauchamp,kyleabeauchamp,true,,1,0.0,9,0,57,false,false,false,false,1,1,2,0,0,0,-1
410184,scikit-learn/scikit-learn,python,1056,1345763609,,1345763635,0,,unknown,false,false,true,5,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,110,0,0,0.0,0,,,0,0.0,0,0,false,Possible fix for issue #1051 ,,618,0.8333333333333334,0.36923076923076925,25983,326.136319901474,31.63606973790555,90.44375168379325,1908,23,737,50,unknown,kyleabeauchamp,kyleabeauchamp,true,,0,0,9,0,57,false,false,false,false,0,1,0,0,0,0,-1
223087,scikit-learn/scikit-learn,python,1050,1345677833,1346785783,1346785783,18465,18465,merged_in_comments,false,false,false,24,5,0,14,20,3,37,0,4,0,0,0,24,0,0,0,0,0,24,24,24,0,0,0,0,285,0,0,0.0,0,,,0,0.0,0,4,false,MRG: Set __all__ in a consistent manner  I have set off to address issue #940 and to bring the discussion to a line-comment environment,,617,0.833063209076175,0.3716475095785441,25976,326.22420696027103,31.64459501077918,90.46812442254388,1907,23,736,64,unknown,vene,amueller,false,amueller,26,0.9230769230769231,41,19,864,true,true,false,true,18,61,43,7,51,0,3
222846,scikit-learn/scikit-learn,python,1049,1345673274,1345720044,1345720045,779,779,github,false,false,false,12,2,0,1,4,0,5,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,49,0,0,0.0,0,,,0,0.0,0,5,false,Fixed docstring for C param in BaseLibLinear/SVM subclasses And added deprecation warning,,616,0.8327922077922078,0.3716475095785441,25976,326.22420696027103,31.64459501077918,90.46812442254388,1907,23,736,52,unknown,fsav,ogrisel,false,ogrisel,1,0.0,1,0,1304,false,false,false,false,0,4,1,0,0,0,10
409786,scikit-learn/scikit-learn,python,1048,1345666743,,1345673935,119,,unknown,false,false,true,15,1,0,0,8,0,8,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,12,0,0,0.0,0,,,0,0.0,0,2,false,Fixed docstring for LogisticRegressions C param And added a deprecated warning if C is None,,615,0.8341463414634146,0.372848948374761,25976,326.22420696027103,31.64459501077918,90.46812442254388,1907,23,736,50,unknown,fsav,amueller,false,,0,0,1,0,1304,false,false,false,false,0,2,0,0,0,0,8
409533,scikit-learn/scikit-learn,python,1046,1345570351,1345581363,1345581363,183,183,github,false,false,true,41,1,0,0,38,0,38,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,5,0,0,0.0,0,,,0,0.0,0,11,false,Changing export_graphviz to enable it to operate on Tree objects export_graphviz currently operates on BaseDecisionTree derived classes I want to visualise trees that are internal to GradientBoostingClassifier This requires that export_graphviz can also accept a Tree object as its first argument,,614,0.8338762214983714,0.3754716981132076,25971,326.21000346540376,31.650687305071042,90.4470370798198,1906,23,735,50,unknown,TimSC,amueller,false,amueller,2,1.0,5,4,365,false,false,false,false,0,0,0,0,0,0,15
335732,scikit-learn/scikit-learn,python,1045,1345569097,1345574107,1345574107,83,83,github,false,false,false,46,1,0,0,3,0,3,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,5,0,0.0,0,,,0,0.0,1,0,false,FIX : make as_float_array keep fortran order on dense array when copy issue raised by @ibayer for coordinate descent Otherwise if X is fortran ordered 2 copies occurthis is a hot fix A cleaner version should make use of array2d in coordinate_descentpy (no time now),,613,0.833605220228385,0.3754716981132076,25971,326.21000346540376,31.650687305071042,90.4470370798198,1906,23,735,50,unknown,agramfort,ogrisel,false,ogrisel,23,0.8260869565217391,83,174,993,true,true,true,true,76,173,32,117,53,1,26
254798,scikit-learn/scikit-learn,python,1043,1345470454,1345474208,1345474208,62,62,github,false,false,false,27,3,0,0,4,0,4,0,2,0,0,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,1,0,false,Fixes broken links on Support page of website This fixes the issue raised by @amueller in #1022 in both the header and the sidebarAnywhere I missed ,,612,0.8333333333333334,0.3768939393939394,25958,326.37337237075275,31.666538254102782,90.49233376993605,1904,23,734,50,unknown,jaquesgrobler,ogrisel,false,ogrisel,21,0.9523809523809523,8,11,208,true,true,true,true,24,51,17,4,21,3,47
166334,scikit-learn/scikit-learn,python,1042,1345381672,1374764867,1374764867,489719,489719,commits_in_master,false,true,false,16,9,1,2,10,0,12,0,4,0,0,1,4,1,0,0,0,0,4,4,4,0,0,110,0,430,39,4.542400515430145,0.13943109774887547,26,vlad@vene.ro,sklearn/linear_model/omp.py,26,0.049429657794676805,0,5,false,WIP: OrthogonalMatchingPursuitCV As discussed in issue #1930 Todo:  - [ ] narratives  - [ ] coverage,,611,0.8330605564648118,0.3726235741444867,25948,326.4220749190689,31.678742099583783,90.45013103129337,1903,23,733,137,unknown,vene,vene,true,vene,25,0.92,40,19,861,true,true,false,false,18,59,37,4,47,0,57
166395,scikit-learn/scikit-learn,python,1040,1345321864,1347921729,1347921729,43331,43331,github,false,false,false,25,5,0,5,5,0,10,0,4,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,35,34,0,0.0,0,,,0,0.0,0,2,false,MRG fix percentile tiebreaking add warning Fixes the remainders of issue #994I admit to being lazy and not do the masks / indices refactoring,,610,0.8327868852459016,0.3754863813229572,25948,326.2679204562972,31.678742099583783,90.45013103129337,1903,23,732,66,unknown,amueller,amueller,true,amueller,98,0.8775510204081632,385,29,666,true,true,false,false,139,434,114,58,119,9,2576
252350,scikit-learn/scikit-learn,python,1039,1345306378,1345479614,1345479614,2887,2887,github,false,false,false,51,2,1,0,3,0,3,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,17,24,17,4.56850689035897,0.1402325084056068,5,olivier.grisel@ensta.org,sklearn/manifold/tests/test_locally_linear.py,5,0.009765625,0,1,false,MRG: LLE test fix This addresses the discussion in #1031 (replaces #1038)Ive changed the locally linear embedding test to a different random seed which allows the test to pass on my boxIve also added some  warnings about arpack in the LLE documentation and cleaned up the docs a bit,,609,0.8325123152709359,0.376953125,25948,326.2679204562972,31.678742099583783,90.45013103129337,1902,23,732,51,unknown,jakevdp,ogrisel,false,ogrisel,27,0.8888888888888888,657,0,465,true,true,false,false,11,45,13,11,12,0,148
252346,scikit-learn/scikit-learn,python,1038,1345306072,1345306131,1345306131,0,0,commits_in_master,false,false,false,50,10,9,0,0,0,0,0,1,0,0,6,7,6,0,0,0,0,7,7,7,0,0,196,29,220,29,67.50838132879423,2.072202117527109,13,virgile.fritsch@gmail.com,sklearn/covariance/robust_covariance.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/utils/extmath.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_utils.py|sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/manifold/tests/test_locally_linear.py,5,0.009765625,0,0,false,Locally Linear Embedding test fix This addresses the discussion in #1031Ive changed the locally linear embedding test to a different random seed which allows the test to pass on my boxIve also added some  warnings about arpack in the LLE documentation and cleaned up the docs a bit,,608,0.8322368421052632,0.376953125,25948,326.2679204562972,31.678742099583783,90.45013103129337,1902,23,732,49,unknown,jakevdp,jakevdp,true,jakevdp,26,0.8846153846153846,657,0,465,true,true,false,false,10,44,11,11,12,0,-1
90513,scikit-learn/scikit-learn,python,1030,1345138142,1346242042,1346242042,18398,18398,github,false,false,false,21,6,0,5,10,0,15,0,6,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,23,4,0,0.0,0,,,0,0.0,0,7,false,MRG: SGDClassifierpredict_proba shape Changes shape of SGDClassifierpredict_proba to 2d arrayThis PR adresses #1023In addition cosmit in the sgd documentation,,605,0.8347107438016529,0.39014373716632444,26014,324.3638041054817,31.55992926885523,90.02844622126548,1894,23,730,65,unknown,pprett,pprett,true,pprett,24,0.875,70,27,1108,true,true,false,false,30,86,10,9,16,0,5
246601,scikit-learn/scikit-learn,python,1029,1345071952,1345144007,1345144007,1200,1200,commit_sha_in_comments,false,false,false,61,7,5,0,4,0,4,0,3,0,0,4,7,3,0,0,0,0,7,7,5,0,0,124,180,138,207,39.91565929734319,1.2252081734964961,11,peter.prettenhofer@gmail.com,sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/utils/__init__.py|sklearn/feature_selection/tests/test_feature_select.py|sklearn/feature_selection/univariate_selection.py|sklearn/feature_selection/univariate_selection.py|doc/developers/utilities.rst|sklearn/utils/__init__.py|sklearn/feature_selection/univariate_selection.py,10,0.002079002079002079,0,1,false,MRG Sparse rfe This adds sparse matrix support to RFE and RFCEI started using a npwhere(boolean_mask)[0] pattern quite a lot for indexing into sparse matricesIs that ok Is it better to do nparange(len(mask))[mask] or something like thatThis PR depends on #1025 which should be merged first So this PR actually only adds a single commitCloses issue #1018,,604,0.8344370860927153,0.40124740124740127,26014,324.3638041054817,31.55992926885523,90.02844622126548,1894,23,729,51,unknown,amueller,amueller,true,amueller,96,0.8854166666666666,379,29,663,true,true,false,false,133,402,100,49,109,9,1046
69836,scikit-learn/scikit-learn,python,1025,1344990903,1345143348,1345143348,2540,2540,merged_in_comments,false,false,false,111,13,0,17,11,0,28,0,3,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,263,360,0,0.0,0,,,0,0.0,0,0,false,MRG sparse matrix support in univariate feature selection This PR adds sparse matrix support to f_classif and f_regression univariate feature selectionIm not very good with sparse stuff so any suggestions are more then welcomeI added a utility function that computes the element wise square of an array-like or sparse matrix and put it in utilsI hope this is not nonsense but seemed the simplest thing to do Is there a better way to to element-wise operations in sparse matricesAlso Im not sure if there might be a better place for the function in on of the submodules and whether it should be documented in the developers docs,,603,0.8341625207296849,0.4118895966029724,25974,324.7863247863248,31.60853160853161,90.16709016709017,1894,23,728,52,unknown,amueller,amueller,true,amueller,95,0.8842105263157894,378,29,662,true,true,false,false,134,389,94,48,100,9,606
123647,scikit-learn/scikit-learn,python,1024,1344902507,1345391044,1345391044,8142,8142,github,false,false,false,18,4,0,2,8,0,10,0,5,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,95,32,0,0.0,0,,,0,0.0,2,3,false,MRG renamed min_n and max_n parameters in CountVectorizer  To make grid-searching them together possiblecc @agramfort and @ogrisel,,602,0.8338870431893688,0.420824295010846,25974,324.7863247863248,31.60853160853161,90.16709016709017,1891,23,727,56,unknown,amueller,amueller,true,amueller,94,0.8829787234042553,378,29,661,true,true,false,false,130,367,89,47,96,9,847
70014,scikit-learn/scikit-learn,python,1015,1344771920,1347538306,1347538306,46106,46106,github,false,false,false,78,22,1,3,56,1,60,0,7,0,0,1,9,1,0,0,0,0,9,9,9,0,0,2,0,300,50,4.375285262007037,0.13429915254904828,7,virgile.fritsch@gmail.com,sklearn/covariance/robust_covariance.py,7,0.015250544662309368,0,19,false,MRG: Speedups in covariance With this simple change I get a 16 speedup on running MinCovDet for a 100x100 matrixI have also turned the pinv calls to a new symmetric_pinv function based on linalgeigh for a considerable speedup All tests passTimings for MinCovDet on a 100x100 symmetric euclidean distance matrix:Before: 124sWithout final inversion: 076s (16x faster)With LAPACK calls: 028 (44x faster) (but alas wrong)With symmetric_pinv: 037  (33x faster)With vectorized symmetric_pinv: 034,,601,0.8336106489184693,0.4226579520697168,25889,325.852678743868,31.712310247595507,90.46313105952335,1886,23,726,70,unknown,vene,VirgileFritsch,false,VirgileFritsch,24,0.9166666666666666,40,19,854,true,true,false,false,17,33,19,4,26,0,407
153268,scikit-learn/scikit-learn,python,1014,1344731838,1359918999,1359918999,253119,253119,commits_in_master,false,true,false,22,9,0,0,27,0,27,0,5,0,0,0,5,0,0,0,0,0,5,5,4,0,0,0,0,81,5,0,0.0,0,,,0,0.0,1,13,false,WIP Auc grid search This enables grid search with AUC scoreWhat do you think about doing it like thiscc @ogrisel,,600,0.8333333333333334,0.42637362637362636,25889,325.852678743868,31.712310247595507,90.46313105952335,1885,24,725,106,unknown,amueller,amueller,true,amueller,93,0.8817204301075269,376,29,659,true,true,false,false,127,358,87,47,91,9,29678
69424,scikit-learn/scikit-learn,python,1013,1344730427,1345026677,1345026677,4937,4937,github,false,false,false,22,4,0,2,4,0,6,0,3,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,86,5,0,0.0,0,,,0,0.0,1,0,false,MRG auc_score and average_precision_score Closes issue #158 These are just convenience wrappers but should come in handy Also @pprett wanted them ),,599,0.8330550918196995,0.42731277533039647,25889,325.852678743868,31.712310247595507,90.46313105952335,1885,24,725,50,unknown,amueller,amueller,true,amueller,92,0.8804347826086957,376,29,659,true,true,false,false,127,357,85,47,91,9,3869
236613,scikit-learn/scikit-learn,python,1009,1344616211,1346239822,1346239822,27060,27060,github,false,false,false,55,3,0,3,3,0,6,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,7,49,0,0.0,0,,,0,0.0,0,0,true,MRG bugfix in SVM: classifier behavior with only one class present This PR checks the behavior of all classifiers regarding training with only one class The classifier should either yield a constant predictor or raise a sensible error messageIt fixes a bug in the SVMs which yielded garbage results (and now raise a ValueError),,598,0.8327759197324415,0.4314159292035398,25888,325.86526576019776,31.713535228677376,90.46662546353524,1881,24,724,63,unknown,amueller,ogrisel,false,ogrisel,91,0.8791208791208791,375,29,658,true,true,true,false,124,361,84,48,90,9,2962
624888,scikit-learn/scikit-learn,python,1008,1344559306,1344593405,1344593405,568,568,github,false,false,false,5,4,0,1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,50,0,0,0.0,0,,,0,0.0,0,0,false,COSMIT: doc fixes to sklearnfeature_selectionunivariate_selection ,,597,0.8324958123953099,0.4314159292035398,25893,324.60510562700347,31.668790792878383,90.37191518943344,1881,24,723,46,unknown,mrjbq7,amueller,false,amueller,4,1.0,52,61,1428,true,true,false,false,12,17,4,10,1,0,567
16511,scikit-learn/scikit-learn,python,1006,1344538140,,1351007793,107827,,unknown,false,false,false,117,37,0,30,103,7,140,0,12,0,0,0,9,0,0,0,4,0,9,13,11,0,0,0,0,2433,50,0,0.0,0,,,0,0.0,0,50,false,MRG: Speed up euclidean_distances with Cython euclidean_distances usually pops up near the top as cumulative time in every profiler output that uses it (Im talking about our new toy in http://jenkins-scikit-learngithubcom/scikit-learn-speed/) so optimizing it sounds like a good ideaHere is my first go Im new at writing cython so feedback is highly appreciated I have a sinking feeling that Im copying around memory when I dont want thatHere is a hackish benchmark script: https://gistgithubcom/3305769I have intentionally not included the c file in this PR so the commits will be lighter until this is done**TODO**:- fast CSR dot product (turns out almost impossible)PS be gentle Im typing with a broken finger :),,596,0.8338926174496645,0.43237250554323725,25893,324.60510562700347,31.668790792878383,90.37191518943344,1881,24,723,83,unknown,vene,amueller,false,,23,0.9565217391304348,40,19,851,true,true,false,false,16,20,13,1,18,0,16
15211,scikit-learn/scikit-learn,python,1005,1344501102,1344505805,1344505805,78,78,github,false,false,false,13,4,2,0,3,0,3,0,2,1,0,3,4,4,0,0,1,0,3,4,4,0,0,200,10,208,10,17.3288461063182,0.5319040832188372,23,peter.prettenhofer@gmail.com,sklearn/manifold/mds.py|sklearn/manifold/tests/test_mds.py|sklearn/linear_model/__init__.py|sklearn/linear_model/isotonic_regression_.py,19,0.015317286652078774,0,0,false,MDs: the eps option was overwritten in the algorithm This is now fixed,,595,0.8336134453781513,0.424507658643326,25893,324.60510562700347,31.668790792878383,90.37191518943344,1881,24,723,44,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,7,0.8571428571428571,26,13,934,true,true,true,false,5,17,7,0,27,0,2
14484,scikit-learn/scikit-learn,python,1003,1344470820,1344601740,1344601740,2182,2182,merged_in_comments,false,false,false,7,2,0,9,9,0,18,0,6,0,0,0,13,0,0,0,0,0,13,13,12,0,0,0,0,94,42,0,0.0,0,,,0,0.0,0,5,false,ENH: understandable error message for X sparse ,,594,0.8333333333333334,0.424507658643326,25893,324.60510562700347,31.668790792878383,90.37191518943344,1880,24,722,45,unknown,GaelVaroquaux,amueller,false,amueller,16,0.6875,244,3,898,true,true,false,true,62,174,26,19,101,6,597
624891,scikit-learn/scikit-learn,python,1001,1344439895,1344440127,1344440127,3,3,github,false,false,false,4,1,0,0,2,0,2,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,6,false,Just fixing small typo ,,593,0.8330522765598651,0.42543859649122806,25893,324.60510562700347,31.668790792878383,90.37191518943344,1879,24,722,43,unknown,serch,amueller,false,amueller,0,0,1,12,999,false,true,false,false,0,0,0,0,0,0,3
12686,scikit-learn/scikit-learn,python,1000,1344423580,1345235304,1345235304,13528,13528,github,false,false,false,54,11,5,4,27,0,31,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,222,0,363,0,31.05500407453116,0.9532243620836061,18,peter.prettenhofer@gmail.com,sklearn/lda.py|sklearn/qda.py|sklearn/metrics/pairwise.py|sklearn/lda.py|sklearn/qda.py|sklearn/lda.py|sklearn/qda.py,12,0.017699115044247787,1,14,false,MRG Qda and LDA cleanup A little cleanup and support for arbitrary classes in LDA and QDAAs @mblondel suggested I reused LabelEncoder Im not sure that really helps here thoughPlease have a look at the second commit It seems to me using LabelEncoder makes the code longer and harder to read WDYT,,592,0.8327702702702703,0.4225663716814159,25888,325.47898640296665,31.713535228677376,90.42799752781211,1879,24,722,50,unknown,amueller,amueller,true,amueller,90,0.8777777777777778,375,29,656,true,true,false,false,126,350,78,41,85,9,40
11463,scikit-learn/scikit-learn,python,999,1344381008,1344381434,1344381434,7,7,commits_in_master,false,false,false,27,27,25,0,0,0,0,0,2,0,0,16,20,16,0,0,0,0,20,20,20,0,0,536,339,612,339,211.97323777470032,6.506457183893826,49,virgile.fritsch@gmail.com,examples/cluster/plot_cluster_comparison.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|examples/cluster/plot_affinity_propagation.py|examples/cluster/plot_cluster_comparison.py|sklearn/cluster/affinity_propagation_.py|sklearn/metrics/pairwise.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/manifold/isomap.py|sklearn/svm/base.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/svm/base.py|sklearn/decomposition/kernel_pca.py|sklearn/svm/base.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/grid_search.py|sklearn/manifold/isomap.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/tests/test_grid_search.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/cluster/spectral.py|sklearn/svm/base.py|sklearn/svm/base.py|sklearn/cluster/spectral.py|sklearn/base.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/grid_search.py|sklearn/cluster/spectral.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/metrics/pairwise.py|sklearn/pipeline.py|sklearn/decomposition/tests/test_kernel_pca.py,21,0.008908685968819599,0,0,false,Arbitrary class labels This PR adds support for arbitrary class labels to QDA LDA and NeighborsClassifiersIt also does some minor cleaning up in QDA and LDA,,591,0.8324873096446701,0.42538975501113585,25888,325.47898640296665,31.713535228677376,90.42799752781211,1875,24,721,42,unknown,amueller,amueller,true,amueller,89,0.8764044943820225,375,29,655,true,true,false,false,127,352,76,41,82,9,-1
9580,scikit-learn/scikit-learn,python,996,1343948816,,1344339093,6504,,unknown,false,false,false,66,15,7,6,20,2,28,2,5,1,0,4,6,4,0,0,1,0,5,6,5,0,0,71,55,397,103,34.234568636135904,1.0629454249552261,31,peter.prettenhofer@gmail.com,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_cd_fast.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/tests/test_cd_fast.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/tests/test_cd_fast.py,29,0.010121457489878543,1,3,false,WIP: Refactor enet_coordinate_descent This PR aims to simplify cd_fastenet_coordinate_descent by transferring most of the code to Python so that only the inner loop of the coordinate descent iterations remains in Cython This PR is closely connected to #992The same has been done  by @fabianp here:https://githubcom/scikit-learn/scikit-learn/pull/947/filesPleas dont bother with checking docstrings and similar yet I first want do some benchmarking before making things pretty,,590,0.8338983050847457,0.38461538461538464,25796,326.4847263141573,31.555279888354786,90.32408125290743,1863,24,716,44,unknown,ibayer,ibayer,true,,9,0.7777777777777778,2,0,152,true,true,false,false,7,55,22,6,25,0,15
65853,scikit-learn/scikit-learn,python,995,1343947293,1345227190,1345227190,21331,21331,github,false,false,false,113,9,0,1,22,1,24,0,6,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,48,32,0,0.0,0,,,0,0.0,0,12,false,feature_extractiontextCountVectorizer analyzer char_nospace A new in-built analyzer that creates character n-grams but only inside word boundaries egstring translation to 5-grams: [strin tring trans ransl ansla nslat slati latio ation]This produces substantially smaller feature sets (10MB vs 60MB in one of my corpora) while only slightly reducing the score with any of the default learnersProduced n-grams still provide resilience to misspellings/cases/affixes/derivations but they dont account for any word order (because they are bound to single words) Along with reduced dimensionality this may be desired in some casesI have no references to attach but the provided code seems to workIm very open to suggestions regarding the analyzer keyword (currently char_nospace),,589,0.833616298811545,0.38461538461538464,25796,326.4847263141573,31.555279888354786,90.32408125290743,1863,24,716,52,unknown,kernc,ogrisel,false,ogrisel,1,1.0,6,6,499,true,true,false,false,2,7,3,0,1,0,6
16806,scikit-learn/scikit-learn,python,992,1343866208,,1348053974,69796,,unknown,false,true,false,152,57,3,90,51,10,151,0,6,1,0,3,12,4,0,0,6,0,11,17,11,0,0,48,50,2067,589,17.103308270625565,0.5310385428720144,44,peter.prettenhofer@gmail.com,sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_cd_fast.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx,41,0.018108651911468814,0,16,false,WIP: enet strong rules This PR aims to extend the current coordinate_descentenet_path to uses an active set of features and strong rule filtering ([0] p 20)A prototype is available here:https://gistgithubcom/3188143I suggest to integrate the prototype the following way:* add two options to enet_coordinate_descent (done)* -- possibility to skip the dual gap calculation calc_dual_gap* -- do the iterations only over a provided subset of features iter_set* make these options accessible though enetfit* replace the loop over the range of alphas in coordinate_descent_enet_path (line 106 - 122) with theloop in enet_path from the gist* -- in the loop from the gist replace the shrinkage() calls by calls to enetfit[0] Tibshirani R J Bien J Friedman T Hastie N Simon J Taylor and RJ Tibshirani “Strong Rules for Discarding Predictors in Lasso-type Problems” Journal of the Royal Statistical Society: Series B (Statistical Methodology) (2011),,588,0.8350340136054422,0.3762575452716298,25833,326.0171098981922,31.510084001083882,90.19471218983472,1860,24,715,74,unknown,ibayer,ibayer,true,,8,0.875,2,0,151,true,true,false,false,6,50,17,2,25,0,541
253874,scikit-learn/scikit-learn,python,991,1343842057,1345418401,1345418401,26272,26272,github,false,false,false,44,1,0,0,4,0,4,0,3,0,0,0,2,0,0,0,0,1,1,2,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,DOC: add tutorial links Small update to the tutorial documentationThe main reason Im doing a PR rather than just merging the changes is to find out if people know of other tutorials out there that should be added to the list of links,,587,0.8347529812606473,0.37349397590361444,25833,326.0171098981922,31.510084001083882,90.19471218983472,1860,24,715,51,unknown,jakevdp,jakevdp,true,jakevdp,25,0.88,642,0,448,true,true,false,false,10,24,11,8,8,0,127
318252,scikit-learn/scikit-learn,python,986,1343405872,,1374497783,518198,,unknown,false,false,false,95,21,4,10,66,0,76,0,6,0,0,0,9,0,0,0,0,0,9,9,7,0,0,0,0,9039,14,0,0.0,0,,,0,0.0,0,24,true,[WIP] Lazy argsort for trees Following a discussion on the mailing list about the cost complexity of trees issue #964 was createdStrategy B proposed that we argsort features only when we need to in the node  This PR does just thatFurthermore it ties in with issue #936 in that the argsort matrix is no longer required but is computed when required halving memory usageTODO: before merge: remove the sort operation completely and rely on min / max random split instead so as to be as fast or faster than the current master,,585,0.8376068376068376,0.36767676767676766,25795,325.52820314014343,31.55650319829424,90.13374685016475,1852,24,710,157,unknown,bdholt1,glouppe,false,,8,1.0,6,15,368,false,true,true,false,7,21,0,0,0,0,0
624809,scikit-learn/scikit-learn,python,984,1343324796,,1445615143,1704779,,unknown,false,true,false,85,3,2,1,29,0,30,0,6,1,0,1,3,2,0,0,1,0,2,3,2,0,0,274,579,335,583,9.263914087882075,0.2880119126534882,16,vanderplas@astro.washington.edu,sklearn/utils/tests/test_arpack.py|sklearn/utils/arpack.py,16,0.03219315895372234,0,3,false,MRG: Update ARPACK Backport This updates scikit-learns arpack backport with bug fixes from scipy v011 and adds a copy of scipys arpack test suite to scikit-learnFor reference the main bugs fixed in this new version are reported here:http://projectsscipyorg/scipy/ticket/1515Note that in scipy 010 there were some minor updates to the ARPACK fortran source bundled with scipy so a few rare cases will fail under older scipy versions  Short of bundling ARPACK with scikit-learn I dont think we have any means of addressing this,,584,0.839041095890411,0.36619718309859156,25781,325.70497653310576,31.57363950195881,90.1826926806563,1850,24,709,351,unknown,jakevdp,jakevdp,true,,24,0.9166666666666666,640,0,442,true,true,false,false,9,20,9,8,8,0,2
624808,scikit-learn/scikit-learn,python,983,1343294624,1343724985,1343724985,7172,7172,github,false,false,false,84,16,7,6,7,0,13,0,4,0,0,5,6,4,0,0,0,0,6,6,4,0,0,836,31,844,65,58.56049141989677,1.8206252129548837,18,peter.prettenhofer@gmail.com,sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|doc/tutorial/statistical_inference/unsupervised_learning.rst|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/_hierarchical.c|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/hierarchical.py,10,0.01016260162601626,0,0,false,MRG: Faster ward The main contribution of the PR is to implement a variant of the Ward strategy that gives speed ups for a large number of clusters The speed up get really important when the number of clusters is on the order of magnitude of the sample size for very sparse matrices A typical application is light data decimation as in building super-voxels on image For brain images (n_samples  50000) for 10000 clusters the speed up is approximately a factor of 3,,583,0.8387650085763293,0.36585365853658536,25781,325.70497653310576,31.57363950195881,90.1826926806563,1849,24,709,43,unknown,GaelVaroquaux,agramfort,false,agramfort,15,0.6666666666666666,242,3,885,true,true,false,true,90,193,38,9,94,5,27
226422,scikit-learn/scikit-learn,python,982,1343221522,1346787625,1346787625,59435,59435,merged_in_comments,false,false,false,140,16,0,26,31,0,57,0,4,0,0,0,2,0,0,0,1,0,2,3,1,0,1,0,0,329,0,0,0.0,0,,,0,0.0,1,9,false,WIP: Added scale_c fiasco example A while back you may remember the great Scale_C Fiasco of 2012After much discussion it was decided that we drop the use of the scale_c parameterThe following example is based on the initial plots that @amueller didYou may recognize the plots from the mailing list discussions regarding the issueThe example will hopefully serve as a reference tool that can be used to explain to the nextperson that sends a SERIOUS BUG titled message to the mailing list how it worksI wouldnt say the example is entirely ready for mergingI would really first like some feedback on it from you all regarding readability making sensegetting the point across and mistakesThe plots are shown belowThanks in advanceJ------------------------------------------------------------------------------------------------------------------------[SVC L1 case ](http://oi45tinypiccom/2nar59hjpg)[SVC L2 case](http://oi47tinypiccom/mi1d29jpg)------------------------------------------------------------------------------------------------------------------------,,582,0.8384879725085911,0.3693877551020408,25761,325.9578432514266,31.598152245642634,90.25270758122744,1845,23,708,65,unknown,jaquesgrobler,amueller,false,amueller,20,0.95,8,11,182,true,true,false,true,17,42,14,1,19,2,11
225553,scikit-learn/scikit-learn,python,979,1343175828,,1343257598,1362,,unknown,false,false,false,105,4,0,0,23,0,23,0,4,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,35,0,0,0.0,0,,,0,0.0,2,10,false,MRG corrected warning for always present labels ugh This corrects the error in the warning that @mbondel pointed outIt also makes LabelBinarizer return npint I figured that was sensible Is this ok for everybodyShould this go into whatsnewrstThere is one small issue though that is a bit weirdECOC produces binary labels [-1 1] which means that the error message will always say label xxx always present and never label not xxx always present@mbondel I didnt understand why the ECOC sometimes produces [0 1] and sometimes [-1 1]In principal all estimators should be agnostic to what the labels actually are,,581,0.8399311531841652,0.36024844720496896,25757,325.34844896532985,31.564234965252165,90.26672360911597,1841,23,707,41,unknown,amueller,amueller,true,,88,0.8863636363636364,368,29,641,true,true,false,false,143,359,212,18,90,3,419
224265,scikit-learn/scikit-learn,python,977,1343137626,,1343208354,1178,,unknown,false,false,false,54,5,0,0,9,1,10,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,70,0,0.0,0,,,0,0.0,0,2,false,MRG: Added test coverage for Spectral Clustering Adds a test for the amg mode for Spectral ClusteringAll of the uncovered lines in spectralpy are either warnings or code branches if pyamg isnt availableI wasnt sure about MRG or WIP Functionally this works but I feel like the test can be better written,,580,0.8413793103448276,0.34182590233545646,25676,325.1285246923197,31.62486368593239,90.16201900607572,1839,23,707,40,unknown,robertlayton,GaelVaroquaux,false,,13,0.8461538461538461,5,10,431,false,true,true,false,5,9,3,1,4,0,589
222576,scikit-learn/scikit-learn,python,975,1343064845,1350637864,1350637864,126216,126216,github,false,false,false,128,1,0,0,18,0,18,0,7,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,10,false,GBClassifier MultinomialDeviance: added logsumexp and nan_to_num to avoid underflows and NaNs Following my recent (20120723) email on the mailing list here is the PR to address the issue of numerical underflows and NaNs when using GradientBoostingClassifier and MultinomialDeviance with a large number of classes and a large dataset The issue boils down to:1) Computing log(exp(array)sum()) which generates (well known) underflows when the array has values of different order of magnitude The solution uses sklearnutilsextmathlogsumexp() as suggested by Gael instead of numpylogaddexpreduce() 2) Divisions are protected with nan_to_num to avoid NaNs in the resultsAt the moment there is no toy example to test this issue easily Ill try to work on it Help is warmly welcome Anyway this changes solve the problem I observed on my dataset,,579,0.8411053540587219,0.3217391304347826,25672,324.78965409784985,31.551885322530385,90.02025553131817,1835,23,706,64,unknown,emanuele,pprett,false,pprett,0,0,26,0,1209,false,true,false,false,0,0,0,0,0,0,43
221996,scikit-learn/scikit-learn,python,974,1343044624,1343044772,1343044772,2,2,github,false,false,false,16,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Fixes broken links on Support page of website Just a few broken links fixed in supportrst,,578,0.8408304498269896,0.31497797356828194,25669,323.81471814250654,31.51661537262846,89.91390393081149,1834,23,706,43,unknown,jaquesgrobler,jaquesgrobler,true,jaquesgrobler,19,0.9473684210526315,8,11,180,true,true,false,false,14,39,11,1,17,0,-1
63451,scikit-learn/scikit-learn,python,973,1342890976,1343040417,1343040417,2490,2490,merged_in_comments,false,false,false,37,1,0,4,2,0,6,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,37,33,0,0.0,0,,,0,0.0,2,1,false,MRG allow user-specified comment in SVMlight dumper Since SVMlight files may contain comments I thought it would be useful if the user can pass one to dump_svmlight_file @ogrisel @mblondel did I handle all the Unicode issues correctly,,577,0.8405545927209706,0.31346578366445915,25656,323.0433426878703,31.493607733083877,89.72560024945432,1828,23,704,46,unknown,larsmans,GaelVaroquaux,false,GaelVaroquaux,42,0.7619047619047619,72,29,734,true,true,true,false,29,43,158,12,17,2,43
220058,scikit-learn/scikit-learn,python,972,1342887731,1342996775,1342996775,1817,1817,github,false,false,false,45,3,0,0,7,0,7,0,4,0,0,0,8,0,0,0,0,0,8,8,8,0,0,0,0,275,0,0,0.0,0,,,0,0.0,0,7,false,Tree uses float64 for y This PR changes the dtype of y in the tree and ensemble module Memory consumption of y should not be an issue numerical stability of gradient_boosting is enhancedNow float64 is used consistently in sklearns regressors (cd_fast sgd liblinear libsvm),,576,0.8402777777777778,0.31194690265486724,25656,323.0433426878703,31.493607733083877,89.72560024945432,1827,23,704,46,unknown,pprett,pprett,true,pprett,23,0.8695652173913043,68,26,1082,true,true,false,false,22,80,9,12,15,0,1268
219307,scikit-learn/scikit-learn,python,971,1342827235,1342828366,1342828366,18,18,github,false,false,false,28,1,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,6,0,0.0,0,,,0,0.0,0,1,true,Issue #339: minor optimization Profiling shows teststesttrain_hmm_and_keep_track_of_log_likelihood()takes 35s This is a quick win optimization and cuts the number in half by running the above method fewer times,,575,0.84,0.29977628635346754,25756,321.20670911632243,31.332505047367608,89.14427706165553,1825,23,703,45,unknown,acompa,ogrisel,false,ogrisel,0,0,13,9,802,false,true,false,false,2,3,0,0,0,0,10
216162,scikit-learn/scikit-learn,python,969,1342707892,1342708016,1342708016,2,2,github,false,false,false,9,2,0,0,0,0,0,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,5,0,0,0.0,0,,,0,0.0,0,2,false,Documentation fix - BaseLibSVM Docstring fixes for  Issue #968,,573,0.8411867364746946,0.29977628635346754,25755,321.1026985051446,31.333721607454866,89.10891089108911,1820,23,702,44,unknown,jaquesgrobler,agramfort,false,agramfort,18,0.9444444444444444,7,11,176,true,true,true,true,15,37,10,1,15,0,-1
213511,scikit-learn/scikit-learn,python,963,1342599647,1342602487,1342602487,47,47,github,false,false,false,23,1,0,0,5,0,5,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,1,false,COSMIT: Use nparrayfill for scalar values In the spirit of 999a749561fef6ba5d65f3d7f01c45f0461c35a6 I modified a few places that used scalar values to use ndarrayfill(value),,572,0.8409090909090909,0.30269058295964124,25778,323.3377298471565,31.266971836449684,89.68888199239662,1816,23,701,45,unknown,mrjbq7,GaelVaroquaux,false,GaelVaroquaux,3,1.0,51,61,1406,false,true,false,false,8,6,3,10,0,0,11
63856,scikit-learn/scikit-learn,python,962,1342578691,1343048625,1343048625,7832,7832,merged_in_comments,false,false,false,89,4,0,2,11,2,15,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,94,0,0.0,0,,,0,0.0,1,4,false,MRG: Silhouette Coefficient returns nan when a cluster has 1 sample (issue #960) The Silhouette Coefficient returns a nan value when any cluster (or class) has just 1 valueThis corresponds to issue #960I have written a test based on @aflags code to repeat this behaviourI fixed this bug by using npnan_to_num This sets the silhouette coefficient to zero in cases where there is 1 sample in a cluster (which is correct as the sample has a mean distance of zero to other points in the cluster),,571,0.840630472854641,0.29954954954954954,25752,323.6253494874184,31.298539919229576,89.77943460702082,1814,23,700,46,unknown,robertlayton,GaelVaroquaux,false,GaelVaroquaux,12,0.8333333333333334,5,10,424,false,true,true,false,4,4,2,0,0,0,303
213094,scikit-learn/scikit-learn,python,961,1342577113,,1342578443,22,,unknown,false,false,false,69,15,11,0,0,0,0,0,1,60,2,6,69,56,0,1,60,2,7,69,57,0,1,6385,1524,6428,1584,294.4576790919737,8.902089849929192,140,peter.prettenhofer@gmail.com,sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/setup.py|sklearn/datasets/svmlight_format.py|doc/modules/clustering.rst|benchmarks/bench_plot_parallel_pairwise.py|doc/data_transforms.rst|doc/developers/debugging.rst|doc/developers/utilities.rst|doc/logos/identity.pdf|doc/modules/ensemble.rst|doc/modules/kernel_approximation.rst|doc/modules/outlier_detection.rst|examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_stock_market.py|examples/applications/plot_tomography_l1_reconstruction.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/covariance/plot_outlier_detection.py|examples/covariance/plot_sparse_cov.py|examples/decomposition/plot_sparse_coding.py|examples/ensemble/README.txt|examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_forest_importances_faces.py|examples/ensemble/plot_forest_iris.py|examples/plot_kernel_approximation.py|examples/plot_multilabel.py|examples/plot_random_dataset.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_parameters_selection.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/species_distributions.py|sklearn/datasets/tests/data/svmlight_multilabel.txt|sklearn/ensemble/__init__.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/__init__.py|sklearn/ensemble/tests/test_base.py|sklearn/ensemble/tests/test_forest.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/tests/test_selector_mixin.py|sklearn/kernel_approximation.py|sklearn/preprocessing.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/tests/test_kernel_approximation.py|sklearn/tests/test_preprocessing.py|sklearn/utils/arraybuilder.c|sklearn/utils/arraybuilder.pyx|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/src/MurmurHash3.cpp|sklearn/utils/src/MurmurHash3.h|sklearn/utils/tests/test_logsumexp.py|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py,22,0.0022522522522522522,1,0,false,WIP: Silhouette Coefficient returns nan when a cluster has 1 sample (issue #960) The Silhouette Coefficient returns a nan value when any cluster (or class) has just 1 valueThis corresponds to issue #960I have written a test based on @aflags code to repeat this behaviour**TODO*** Rebase the branch to remove all the commits* Determine the best way to fix this code* Do that,,570,0.8421052631578947,0.29954954954954954,25752,323.6253494874184,31.298539919229576,89.77943460702082,1814,23,700,45,unknown,robertlayton,robertlayton,true,,11,0.9090909090909091,5,10,424,false,true,false,false,3,3,0,0,0,0,-1
210297,scikit-learn/scikit-learn,python,959,1342478549,1343345802,1343345803,14454,14454,github,false,false,false,41,1,0,0,22,0,22,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,10,false,MRG trying to fix linker issue in array_funcs There seems to be some trouble with detecting which / whether atlas is installedIm not so sure what the right thing to do is when NO_ATLAS_INFO is -1 or -3Closes #860,,569,0.8418277680140598,0.29705215419501135,25771,323.3867525513174,31.27546466958985,89.71324356835203,1810,23,699,52,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,87,0.8850574712643678,363,27,633,true,true,true,false,143,340,338,6,90,1,21
209887,scikit-learn/scikit-learn,python,958,1342467029,1342597575,1342597575,2175,2175,merged_in_comments,false,false,false,12,5,0,11,8,0,19,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,151,38,0,0.0,0,,,0,0.0,0,2,false,ENH: add store_loo_values attribute to _RidgeGCV see Issue #957 Ref Issue #957,,568,0.8415492957746479,0.2984054669703872,25755,323.58765288293534,31.294894195301882,89.76897689768977,1810,22,699,46,unknown,npinto,GaelVaroquaux,false,GaelVaroquaux,6,1.0,70,39,1306,false,true,false,false,3,5,1,0,0,0,11
209499,scikit-learn/scikit-learn,python,956,1342456206,1342456262,1342456262,0,0,github,false,false,false,14,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,[Docstring Typo] making there - making their Am I wrong  Thanks\-- matthias,,567,0.8412698412698413,0.2968036529680365,25755,323.58765288293534,31.294894195301882,89.76897689768977,1809,22,699,43,unknown,Carreau,glouppe,false,glouppe,0,0,59,8,729,false,true,false,false,0,0,0,0,0,0,-1
59345,scikit-learn/scikit-learn,python,955,1342378437,1343071310,1343071310,11547,11547,github,false,false,false,40,3,0,6,8,0,14,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,6,false,[WIP] Add memory_profiler tips to the developers guide As discussed here are some instructions on how to install and use memory_profiler and the new magic functionsPutting this out here so we can discuss what direction it should go in,,566,0.8409893992932862,0.29357798165137616,25755,323.58765288293534,31.294894195301882,89.76897689768977,1809,22,698,46,unknown,vene,ogrisel,false,ogrisel,22,0.9545454545454546,37,19,826,true,true,true,true,22,20,12,1,14,0,110
205050,scikit-learn/scikit-learn,python,953,1342196522,1342196871,1342196871,5,5,github,false,false,false,45,1,1,0,1,0,1,0,2,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,0,4.777182189764812,0.14442358351220028,0,,doc/themes/scikit-learn/static/nature.css_t,0,0.0,0,1,true,added alternating columns for tables in documentation and a tighter layout in pre Just some tweaks for the naturecss file that I got from the nisl tutorial* alternating column colours for the documentation* tighter layout in preBetters the readabilty especially with columns,,565,0.8407079646017699,0.29345372460496616,25755,323.58765288293534,31.294894195301882,89.76897689768977,1799,23,696,43,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,17,0.9411764705882353,6,9,170,true,true,true,false,12,30,11,5,15,0,5
201555,scikit-learn/scikit-learn,python,950,1342055110,1342076822,1342076822,361,361,github,false,false,false,19,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,DOC: fix value error text in Treecompute_feature_importances See issue #949 this fixes the ValueError text to match the code,,564,0.8404255319148937,0.30930232558139537,25755,323.58765288293534,31.294894195301882,89.76897689768977,1793,23,694,43,unknown,mrjbq7,glouppe,false,glouppe,2,1.0,49,61,1399,false,true,false,false,6,2,2,3,0,0,-1
7081,scikit-learn/scikit-learn,python,948,1342038665,1342039516,1342039516,14,14,github,false,false,false,11,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,DOC: fix typos in tree docstrings Minor fix to docstring typos,,563,0.8401420959147424,0.3114754098360656,25755,323.58765288293534,31.294894195301882,89.76897689768977,1793,23,694,43,unknown,mrjbq7,glouppe,false,glouppe,1,1.0,49,61,1399,false,true,false,false,4,2,1,3,0,0,-1
56380,scikit-learn/scikit-learn,python,947,1342015886,,1347536947,92017,,unknown,false,false,false,129,1,1,10,13,0,23,0,7,0,0,7,7,5,0,0,0,0,7,7,5,0,0,346,40,346,40,27.566183371207305,0.8333798308621154,71,peter.welinder@gmail.com,doc/modules/classes.rst|doc/modules/linear_model.rst|sklearn/linear_model/__init__.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,41,0.02107728337236534,0,8,false,WIP: Implementation of Group Lasso model Hi allthis is my implementation of the Group Lasso using Block Coordinate Descent Contrary to the Lasso implementation the Cython code is maintained to a minimum and only the coordinate descent innermost loop is compiled code The duality gap computation is maintained outside the loopIt should be reasonably fast for small group sizes For large groups some time could be won by translating into BLAS the innermost loop from the Cython code However recent developments [1] suggest that this innermost loop could be substituted with more efficient alternatives something I am currently looking intoI think the code as it is here might already be useful for othersFeedback is welcome [1] http://www-statstanfordedu/~nsimon/SGLpaperpdfTODO:- example- implement without precomputed Gram,,562,0.8416370106761566,0.3114754098360656,25755,323.58765288293534,31.294894195301882,89.76897689768977,1793,23,694,68,unknown,fabianp,fabianp,true,,24,0.7083333333333334,107,20,788,false,true,false,false,6,5,0,0,0,0,4
59331,scikit-learn/scikit-learn,python,946,1342015452,1342612972,1342612972,9958,9958,github,false,false,false,114,47,8,25,97,0,122,0,6,0,0,4,12,4,0,0,1,0,12,13,11,0,0,1633,180,4931,408,48.467524353011314,1.465268284308693,46,peter.prettenhofer@gmail.com,sklearn/tree/_tree.pyx|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py,29,0.04918032786885246,1,32,false,[MRG] Tree speedup Concerns issue #933---Hi guys This is a **very very** early pull request to improve the tree module in terms of training time This is so early that please dont look at the code for now it still needs A LOT of work The main contribution is basically to rewrite the Tree class from treepy into a Cython class in _treepyx In its current state this already achieves a ~2x speedup wrt master Not bad but I expect better in the changes to come @pprett Could you point me to the benchmark you used in #923 Thanks :)---Update 16/07: All tests now pass This is ready for merge,,561,0.8413547237076648,0.3114754098360656,25755,323.58765288293534,31.294894195301882,89.76897689768977,1793,23,694,47,unknown,glouppe,glouppe,true,glouppe,21,1.0,71,19,608,true,true,false,false,14,44,24,5,32,0,16
200016,scikit-learn/scikit-learn,python,945,1341971385,1341972185,1341972185,13,13,github,false,false,false,17,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,6,0,0,0.0,0,,,0,0.0,0,0,false,Typo in dense_cholesky doc delse_cholesky should be dense_choleskyThis typo only affects documentation so nothing too harmful,,560,0.8410714285714286,0.30985915492957744,25745,323.48028743445326,31.30704991260439,89.72616041949894,1791,23,693,43,unknown,cpa,ogrisel,false,ogrisel,0,0,6,1,709,false,true,false,false,1,0,0,0,0,0,-1
199843,scikit-learn/scikit-learn,python,944,1341965334,1341993094,1341993094,462,462,merged_in_comments,false,false,false,13,1,0,0,1,0,1,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,10,0,0.0,0,,,0,0.0,0,0,false,tree: check length of sample_mask and X_argsorted Pull request to fix issue #942,,559,0.8407871198568873,0.30985915492957744,25745,323.48028743445326,31.30704991260439,89.72616041949894,1791,23,693,42,unknown,mrjbq7,GaelVaroquaux,false,GaelVaroquaux,0,0,49,61,1398,false,true,false,false,2,1,0,0,0,0,460
194782,scikit-learn/scikit-learn,python,938,1341692136,1341907386,1341907386,3587,3587,github,false,false,false,14,4,0,0,9,0,9,0,4,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,16,39,0,0.0,0,,,0,0.0,0,0,false,MRG: preserve double precision values in svmlight serializer This is a fix for #937,,557,0.8420107719928187,0.32634032634032634,25737,323.1534366864825,31.316781287640364,89.59863231922913,1776,24,690,45,unknown,ogrisel,agramfort,false,agramfort,30,0.7666666666666667,538,116,1137,true,false,true,true,83,202,50,28,4,0,2
54470,scikit-learn/scikit-learn,python,932,1341499724,1341842412,1341842413,5711,5711,github,false,false,false,63,5,0,3,9,0,12,0,3,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,12,18,0,0.0,0,,,0,0.0,1,2,false,roc_curve fix Simple fix for problem mentioned in #922reversed the order of thresholds array Played around with it a bit and it appears to fix the bug as suggested by @staigercPlease let me know if youre fine with how I did this or if youd prefer not havingthe reversal happen in the return statement Then Ill change it quick:v:,,555,0.8432432432432433,0.3416289592760181,25573,323.34884448441716,31.243890040276856,89.97771086692995,1769,24,688,46,unknown,jaquesgrobler,agramfort,false,agramfort,16,0.9375,6,9,162,true,true,true,true,11,29,9,4,29,0,5
188128,scikit-learn/scikit-learn,python,928,1341384076,1341398082,1341398082,233,233,github,false,false,false,115,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,8,0,0.0,0,,,0,0.0,1,1,false,ENH do not fail the test reslying on numpy div 0 warnings if those are not spit out by numpy in general email will follow on the mailing list but the reason is simple$ python -c import numpy as np from platform import platform print np__version__ platform() 10 / nparray([0])162 Linux-310-1-amd64-x86_64-with-debian-wheezy-sid-c:1: RuntimeWarning: divide by zero encountered in divide$ python -c import numpy as np from platform import platform print np__version__ platform() 10 / nparray([0])141 Linux-2632-5-amd64-x86_64-with-debian-604(sid)yoh@abel:~$ python -c import numpy as np from platform import platform print np__version__ platform() 10 / nparray([0])162 Linux-2632-armv5tel-with-debian-wheezy-sidie apparently at least this warning is not reliably provided by numpy across releases/architectures,,554,0.8429602888086642,0.352549889135255,25573,323.11422203104837,31.243890040276856,89.97771086692995,1762,23,687,43,unknown,yarikoptic,ogrisel,false,ogrisel,6,1.0,44,7,1301,false,true,false,true,2,9,1,0,1,0,233
185154,scikit-learn/scikit-learn,python,927,1341247428,1341993159,1341993159,12428,12428,merged_in_comments,false,false,false,25,1,1,0,8,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,90,0,90,0,4.380912642760979,0.13303586137259535,7,duckworthd@gmail.com,sklearn/decomposition/fastica_.py,7,0.015625,0,2,false,[WIP] Decomposition module speedup Speedups and improvements in the decomposition moduleIncludes:- merge g and gprime in FastICA More details in the [working notes](https://githubcom/scikit-learn/scikit-learn-speed/wiki/Decomposition),,553,0.8426763110307414,0.36607142857142855,25569,323.12565997888066,31.248777816887635,89.99178692948492,1754,23,685,45,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,21,0.9523809523809523,37,19,813,true,true,true,false,17,13,11,1,14,0,20
184812,scikit-learn/scikit-learn,python,926,1341232351,1341312505,1341312505,1335,1335,github,false,false,false,20,1,0,1,7,0,8,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,10,14,0,0.0,0,,,0,0.0,0,2,false,FIX: fix grid search when X is list #925 feedback welcome especially on the X  as_float_array(X) added to GridSearchCVfit,,552,0.842391304347826,0.37471264367816093,25569,323.12565997888066,31.248777816887635,89.99178692948492,1753,23,685,44,unknown,agramfort,ogrisel,false,ogrisel,22,0.8181818181818182,77,169,943,true,true,true,true,87,151,189,28,45,2,55
52898,scikit-learn/scikit-learn,python,924,1341220163,1341854687,1341854687,10575,10575,github,false,false,false,51,38,25,4,17,0,21,0,4,1,0,7,11,6,0,0,1,1,9,11,7,0,0,367,33,557,98,94.30250567776632,2.863698501690355,15,peter.prettenhofer@gmail.com,sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_gradient_boosting_quantile.py|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_gradient_boosting_quantile.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/tests/test_gradient_boosting.py|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/__init__.py|doc/modules/ensemble.rst|doc/modules/ensemble.rst|examples/ensemble/plot_gradient_boosting_quantile.py,8,0.006928406466512702,0,3,false,MRG: GBRT loss-functions Two new regression loss functions for gradient boosting: * Huber loss (aka M-Loss) - robust loss function * Quantile loss - loss function for quantile regression can be used to create prediction intervals (see example)Some other gimmicks: * GBRT supports max_features for variance reduction (similar to RandomForest),,551,0.8421052631578947,0.37644341801385683,25583,323.2224524098034,31.231677285697536,89.94253996794745,1753,23,685,47,unknown,pprett,pprett,true,pprett,22,0.8636363636363636,64,26,1063,true,true,false,false,18,32,113,11,6,0,81
17251,scikit-learn/scikit-learn,python,923,1340978766,1341855540,1341855540,14612,14612,github,false,false,false,189,38,3,10,50,0,60,0,6,0,0,4,13,4,0,0,2,0,13,15,10,0,0,663,102,1238,352,22.859219051780414,0.6931290698752604,15,peter.prettenhofer@gmail.com,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,8,0.014736842105263158,1,38,true,MRG: Multi-output decision trees Hi folksJust to let you know I am currently working on a multi-output extension of our decision trees Basically this will make our implementation capable of handling classification or regression problems with several outputs As I was discussing with @pprett a very simple way to solve this kind of problems is to build n independent models ie one for each output However by doing that you lose the (likely) correlation between the outputs (classes or regression values) Hence an often better way is to build a single model to predict simultaneously all n outputs With regard to decision trees this amounts to store n output values in a leaf and to use splitting criteria that compute the average reduction among all outputsThis PR includes a working prototype of multi-output decision trees I tried as much as possible not to impair training time on single-output problems A lot of things still need to be done:1) ~~Write multi-output unit tests~~2) ~~Patch RandomForest* and ExtraTrees* to account for the API changes~~3) ~~Patch GradientBoosting* to account for the API changes~~4) ~~Update the documentation~~,,550,0.8418181818181818,0.3705263157894737,25587,323.36733497479185,31.4612889357877,89.73306757337711,1745,23,682,46,unknown,glouppe,glouppe,true,glouppe,20,1.0,71,19,596,true,true,false,false,5,9,37,0,6,0,1
51670,scikit-learn/scikit-learn,python,921,1340891642,1340898227,1340898227,109,109,github,false,false,false,34,3,1,2,5,0,7,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,16,1,24,4.731433116457692,0.1436803589166918,8,mathieu@mblondel.org,sklearn/tests/test_preprocessing.py,8,0.016877637130801686,0,3,false,Scaler bugfix Heres a new test that fails with the current version:Scaler(with_meanFalse copyFalse)fit(X) changes X if it is sparse - which shouldnt be happeningThe second commit removes the guilty linePlease review,,549,0.8415300546448088,0.37130801687763715,25570,322.6046147829488,31.169339069221742,90.02737583105201,1743,24,681,41,unknown,fhoeni,ogrisel,false,ogrisel,0,0,0,0,2,false,true,false,false,0,0,0,0,0,0,8
178636,scikit-learn/scikit-learn,python,920,1340838137,,1345997684,85992,,unknown,false,false,false,35,2,0,0,6,0,6,0,2,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,35,8,0,0.0,0,,,0,0.0,0,3,false,HLP trying to get custom kernels to work with sparse SVM This segfaults atm :-/Also needs more testingNot really on my todo list could be a basis for someone to work on this,,548,0.843065693430657,0.37209302325581395,25570,322.6046147829488,31.169339069221742,90.02737583105201,1739,24,680,65,unknown,amueller,amueller,true,,86,0.8953488372093024,344,27,614,true,true,false,false,148,322,546,14,96,1,19594
168993,scikit-learn/scikit-learn,python,915,1340303367,,1340451698,2472,,unknown,false,false,false,46,1,0,0,7,0,7,0,6,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,71,0,0,0.0,0,,,0,0.0,0,4,false,Method to make dataset from python dictionary (as alternative to txt-files) I added a method to make a dataset from a python dictionaryThis is handy if the training data is stored in some other format than the folder/category/filetxt structure load_files() assumes eg JSONPlease review,,547,0.8446069469835467,0.36721991701244816,25566,317.88312602675427,30.97864351091293,89.41563013377142,1721,25,674,42,unknown,tobigue,tobigue,true,,0,0,7,14,92,false,true,false,false,0,0,0,0,0,0,64
168527,scikit-learn/scikit-learn,python,914,1340285693,1340403020,1340403020,1955,1955,github,false,false,false,27,4,0,5,8,0,13,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,70,0,0,0.0,0,,,0,0.0,0,1,false,Chi2 kernel approxmation and zero elements As discussed with A Mueller I modified the code so that zero valued elements are mapped correctly (ie mapped to zero),,546,0.8443223443223443,0.36875,25566,317.88312602675427,30.97864351091293,89.41563013377142,1720,25,674,44,unknown,alexis-mignon,amueller,false,amueller,2,1.0,1,0,86,true,true,false,false,3,19,7,0,47,0,1554
48495,scikit-learn/scikit-learn,python,913,1340212962,1345931356,1345931356,95306,95306,merged_in_comments,false,true,false,99,8,1,1,6,0,7,0,3,0,0,1,4,1,0,0,0,0,4,4,4,0,0,37,0,121,54,4.067312828075031,0.12368554069686327,8,peter.prettenhofer@gmail.com,sklearn/linear_model/least_angle.py,8,0.016597510373443983,0,2,false,WIP: suport for multiple targets in linear models As part of the scikit-learn-speed project heres the current status of a trivial enhancementThis makes linear models accept 2d arrays for y intelligently by moving redundant computation out of the loop This does NOT mean multitask learning the correlation between the targets is not taken into account its the same as yshape[1] different copies of the estimatorMore details in the [working notes](https://githubcom/scikit-learn/scikit-learn-speed/wiki/Linear-models)TODO list:☑ LassoLars multiple targets ☑ ElasticNet and Lasso multiple targets ☐ Investigate bayes logistic sgd modules☐ Multiprocessing☐ Test coverage☐ Update the docs,,545,0.8440366972477065,0.3651452282157676,25556,318.0075129128189,30.990765377993426,89.4506182501174,1713,25,673,66,unknown,vene,agramfort,false,agramfort,20,0.95,37,19,801,true,true,true,true,15,15,6,1,4,0,4
166944,scikit-learn/scikit-learn,python,912,1340211372,1343051928,1343051928,47342,47342,merged_in_comments,false,false,false,36,1,0,0,13,0,13,0,3,0,0,0,30,0,0,0,0,24,6,30,27,0,1,0,0,91,0,0,0.0,0,,,0,0.0,0,1,false,MRG rm deprecated stuff including the scikitslearn package According to the deprecation warnings its time to get rid of scikitslearn This needs some more testing and Id like to get rid of some more old code,,544,0.84375,0.3651452282157676,25647,322.64982259133626,31.46566849923968,89.5231411081218,1713,25,673,52,unknown,larsmans,GaelVaroquaux,false,GaelVaroquaux,41,0.7560975609756098,72,29,703,true,true,true,false,30,49,410,20,15,2,2
49878,scikit-learn/scikit-learn,python,911,1339938167,,1341537720,26659,,unknown,false,false,false,64,37,0,7,38,0,45,0,4,0,0,0,3,0,0,0,1,0,3,4,3,0,0,0,0,1711,202,0,0.0,0,,,0,0.0,0,15,false,WIP: Covariance Updates This very much work in process for issue #910 The idea is to get first a python version of the covariant updates to work and turn it then step bystep into cython code Reference: [1] Friedman J T Hastie and R Tibshirani “Regularization Paths for Generalized Linear Models via Coordinate Descent” Journal of Statistical Software 33 no 1 (2010): 1,,543,0.8453038674033149,0.3518886679920477,25565,317.8955603363974,30.979855270878154,89.41912771367105,1707,25,670,57,unknown,ibayer,ogrisel,false,,7,1.0,2,0,106,true,true,false,false,11,25,14,1,39,0,11
49907,scikit-learn/scikit-learn,python,909,1339787923,1353243352,1353243352,224257,224257,github,false,false,false,151,22,1,53,94,0,147,0,7,5,0,4,17,8,0,1,6,0,12,18,9,0,1,189,56,2214,169,36.11768223805061,1.0967911379052528,84,vlad@vene.ro,.gitattributes|sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/_hashing.c|sklearn/feature_extraction/_hashing.pyx|sklearn/feature_extraction/hashing.py|sklearn/feature_extraction/setup.py|sklearn/feature_extraction/tests/test_feature_hasher.py|sklearn/tests/test_common.py|sklearn/utils/murmurhash.pxd,77,0.0,1,30,true,MRG feature hashing transformer This PR implements feature hasher for large-scale learning or memory-restricted predictionIt implements [Weinberger et al](http://alexsmolaorg/papers/2009/Weinbergeretal09pdf)s algorithm except that it cuts one corner: instead of running two hash functions on the input it runs a single one taking its lowest bit to determine the sign and the rest to determine the column index That should be ok with a 32-bit hash function as scipysparse matrices cannot hold more than 2**31 elements anywayTodo:* delSupport multiple hash functions in particular @ogrisels implementation of murmurhash/del* delDo we want to support the same style of input as DictVectorizer I have a gut feeling that thats going to slow things down If we dont we should explicitly document this/del* delNeeds optimization/del* delNeeds more tests/del* delMore documentation needed/del* delShould this thing be called HashingVectorizer instead That means more typing than FeatureHasher but it is a vectorizer/del,,542,0.8450184501845018,0.34824902723735407,25583,323.2224524098034,31.231677285697536,89.94253996794745,1704,25,668,90,unknown,larsmans,ogrisel,false,ogrisel,40,0.75,72,29,698,true,true,true,true,31,50,409,21,28,2,20
325374,scikit-learn/scikit-learn,python,908,1339631099,1339761580,1339761580,2174,2174,merged_in_comments,false,false,false,16,1,0,0,0,3,3,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,6,1,0,0.0,0,,,0,0.0,0,0,false,MRG Check that X is non_zero for MultinomialNB Fixes part of #735 as discussed in #893,,541,0.844731977818854,0.342911877394636,25568,317.81914893617017,30.97622027534418,89.36952440550688,1701,26,666,38,unknown,amueller,larsmans,false,larsmans,85,0.8941176470588236,331,25,600,true,true,true,false,171,373,544,58,89,1,-1
325315,scikit-learn/scikit-learn,python,907,1339625981,1339628836,1339628836,47,47,merged_in_comments,false,false,false,15,1,0,0,1,0,1,0,2,0,0,0,8,0,0,0,0,0,8,8,8,0,0,0,0,58,12,0,0.0,0,,,0,0.0,0,0,false,MRG add ClusterMixin that adds a fit_predict convenience method to all clustering classes Closes #901,,540,0.8444444444444444,0.34099616858237547,25568,317.78003754693367,30.97622027534418,89.33041301627034,1700,26,666,38,unknown,amueller,amueller,true,amueller,84,0.8928571428571429,331,25,600,true,true,false,false,168,365,542,58,88,1,43
157781,scikit-learn/scikit-learn,python,906,1339597436,,1379332233,662246,,unknown,false,false,false,127,2,1,0,1,0,1,0,1,1,0,0,1,1,0,0,5,0,0,5,5,0,0,96,0,169,167,4.407136344174521,0.1340177692716787,0,,sklearn/bandits/context_free_bandits.py,0,0.0,0,0,false,Add support for Multi-Armed Bandits As I advertised already on the mailing-list here is a contribution integrating support for Multi-Armed Bandits [1] to sklearnBasically Ive set up a basic API for bandits and implemented one of the simplest bandit algorithms This comes along with :- (almost) clean code : no pyflakes warning few pep8 warnings only 1 pyling W (871/10)- documentation (see sklearn/bandits/context_free_banditspy)- example (see examples/bandits/simple_banditpy)- unit tests (90% coverage see sklearn/tests/test_banditspy)- basic perf tests (see sklearn/tests/test_banditspy)Next steps would be (after this first commit is accepted):- add other bandits algorithms (I have 3 algos implemented but not submitted)- add more tests and documentationI can also be reached through IRC if needed[1] see http://enwikipediaorg/wiki/Multi-armed_bandit for an introduction,,539,0.8460111317254174,0.3390804597701149,25568,317.662703379224,30.97622027534418,89.33041301627034,1700,26,666,124,unknown,oddskool,larsmans,false,,0,0,0,1,387,false,true,true,false,0,0,0,0,0,0,41
323724,scikit-learn/scikit-learn,python,900,1339344535,1339365846,1339365846,355,355,github,false,false,false,37,3,0,0,6,0,6,0,4,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,69,23,0,0.0,0,,,0,0.0,0,0,false,KNeighborsClassifier predict_proba() method as established in the comments Im not sure class labels are required to be consecutive and starting with 0 (ie nparange(n_classes) format) so I made it a little more robust hope it works :-),,538,0.845724907063197,0.32761904761904764,25558,317.0435871351436,30.94921355348619,89.05235151420298,1693,25,663,38,unknown,kernc,agramfort,false,agramfort,0,0,6,5,446,false,true,false,false,1,2,0,0,0,0,11
10164,scikit-learn/scikit-learn,python,899,1339242865,1344354967,1344354967,85201,85201,merged_in_comments,false,false,false,30,13,1,0,42,0,42,0,8,0,0,5,9,5,0,0,7,0,9,16,12,0,0,339,15,1642,24,18.371597470884236,0.5704175003338531,47,robertlayton@gmail.com,sklearn/linear_model/__init__.py|sklearn/linear_model/cd_fast.c|sklearn/linear_model/cd_fast.pyx|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/tests/test_coordinate_descent.py,35,0.022857142857142857,0,10,false,MRG : Multi task Lasso + Multi task Elastic-Net this estimator is an extension to Lasso to impose the active features to be the same for multiple targets / tasks,,537,0.845437616387337,0.32571428571428573,25833,326.0171098981922,31.510084001083882,90.19471218983472,1688,25,662,55,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,21,0.8095238095238095,74,165,920,true,true,true,false,91,145,250,24,39,2,71
170078,scikit-learn/scikit-learn,python,894,1339112506,1340358250,1340358250,20762,20762,github,false,false,false,77,3,0,3,5,0,8,0,4,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,192,0,0,0.0,0,,,0,0.0,0,4,false,MRG Svm sparse/dense cleanup Inspired by #626 I dug around the SVM code and it seemed to me there was still some cleaning up to do after the implementation merge Hope this makes it a bit better Still need to add more testsAlso I noticed that having callable kernels in the sparse case seems not to be supported Is that correctNot that our callable kernel option makes much sense at the moment any way ^^,,536,0.8451492537313433,0.30943396226415093,25539,320.0595168174165,31.011394338071188,89.66678413406946,1686,25,660,45,unknown,amueller,amueller,true,amueller,83,0.891566265060241,330,25,594,true,true,false,false,162,349,536,58,82,1,10737
176165,scikit-learn/scikit-learn,python,893,1339093255,1340737543,1340737543,27404,27404,github,false,true,false,94,27,0,0,23,0,23,0,4,0,0,0,30,0,0,0,1,0,30,31,30,0,0,0,0,340,394,0,0.0,0,,,0,0.0,0,11,false,MRG factorize common tests This is a shot at factorizing common tests for all estimatorsAny suggestions on what to test are very welcome :)Closes #406I think I would like to merge the stuff I already did before going on with more tests At the moment everything passesThere are some (minor) issues for going further for example the fit_pairwise problem which prevents me from testing clusteringsI feel there is already quite a lot in this PR and the longer it gets the less motivation someone will have to review ),,535,0.8448598130841122,0.31285988483685223,25559,317.9701866270198,30.987127821902266,89.44011894049063,1685,25,660,43,unknown,amueller,amueller,true,amueller,82,0.8902439024390244,330,25,594,true,true,false,false,160,344,531,58,72,1,134
322530,scikit-learn/scikit-learn,python,892,1339089381,1339089482,1339089482,1,1,github,false,false,false,34,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,0,false,fix docstring example in mldata According to http://mldataorg/repository/data/viewslug/leukemia/7129 is the number of features and notsamples It should therefore be transpose_dataTrue confirmed by :http://wwwinfedacuk/teaching/courses/dme/html/datasets0405html7129 is the number of genes / features,,534,0.8445692883895131,0.31226053639846746,25534,320.1221900211483,31.01746690686927,89.68434244536697,1685,25,660,35,unknown,ibayer,agramfort,false,agramfort,6,1.0,2,0,96,true,true,false,false,9,25,14,1,25,0,-1
322080,scikit-learn/scikit-learn,python,891,1339005206,1339319671,1339319671,5241,5241,github,false,false,false,65,31,4,0,13,0,13,0,2,0,0,1,11,1,0,0,5,4,7,16,11,0,0,34,0,1324,921,8.820696671633367,0.267135792307945,12,peter.prettenhofer@gmail.com,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,12,0.022944550669216062,0,2,false,merge dense and sparse coordinate descent #861I have added support of sparse data to the dense ElasticNet class and applied the tests originally written for the sparse version to the former dense version Do we need additions tests Please let me know if this goes in the right direction I will then update the docstrings and put some more effort in reducing duplicated code,,533,0.8442776735459663,0.3135755258126195,25534,320.1221900211483,31.01746690686927,89.68434244536697,1683,25,659,38,unknown,ibayer,agramfort,false,agramfort,5,1.0,2,0,95,true,true,false,false,9,26,15,1,23,0,177
321653,scikit-learn/scikit-learn,python,889,1338918025,1338922345,1338922345,72,72,github,false,false,false,20,2,0,0,5,0,5,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,24,13,0,0.0,0,,,0,0.0,0,0,false,[MRG] ENH: make_regression supports multiple targets Request for review on adding this feature Will be used in linear models benchmarks,,532,0.8439849624060151,0.30798479087452474,25535,319.7180340708831,30.97709026825925,89.48502056001567,1676,25,658,35,unknown,vene,amueller,false,amueller,19,0.9473684210526315,36,19,786,true,true,false,true,16,15,5,3,2,0,4
321207,scikit-learn/scikit-learn,python,888,1338830454,1338849243,1338849243,313,313,github,false,false,false,17,3,0,0,10,0,10,0,2,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,20,0,0,0.0,0,,,0,0.0,0,1,false,Few fixes and addons for hmmpy regarding documentation also a *Whats New* update for the API change,,531,0.8436911487758946,0.304029304029304,25529,319.32312272317756,30.9451995769517,89.38853852481492,1673,25,657,38,unknown,jaquesgrobler,amueller,false,amueller,15,0.9333333333333333,6,9,131,true,true,false,false,19,73,22,4,62,0,212
321184,scikit-learn/scikit-learn,python,887,1338827051,1338827171,1338827171,2,2,github,false,false,false,11,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Fix some typos in the docs Just a few spelling errors,,530,0.8433962264150944,0.3010948905109489,25529,319.32312272317756,30.9451995769517,89.38853852481492,1673,25,657,37,unknown,danohuiginn,amueller,false,amueller,0,0,13,16,1516,false,true,false,false,0,0,0,0,0,0,-1
222257,scikit-learn/scikit-learn,python,886,1338769427,1343054910,1343054910,71424,71424,github,false,false,false,143,2,0,1,12,0,13,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,32,27,0,0.0,0,,,0,0.0,0,6,false,MRG: enh in OvR use constant predictor if one class always present or never Closes #559This solution seemed easier than working with the matrix produced by LabelBinarizerAnother possibility would be to modify LabelBinarizer to take care of the issueBoth issues have their caveats:this solution:If someone iterates over the estimators and expects them to be of a certain type they will run into trouble(for example if they expect them all to have a coef to find some feature importance)LabelBinarizer solution:The number of estimators is unexpected and the mapping between classes and estimators is somewhere hidden in the LabelBinarizer where the user will probably not suspect itOh and the *trivial* solution by modifying fit_ovr and predict_ovr has the same drawbacks as doing it in label binarizer only it is more uglyIm open to suggestions though,,529,0.8431001890359168,0.2976827094474153,25755,323.58765288293534,31.294894195301882,89.76897689768977,1672,25,656,53,unknown,amueller,amueller,true,amueller,81,0.8888888888888888,328,25,590,true,true,false,false,151,356,942,58,95,1,297
320959,scikit-learn/scikit-learn,python,885,1338757984,,1338830744,1212,,unknown,false,false,false,16,3,0,0,19,2,21,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,65,8,0,0.0,0,,,0,0.0,0,3,false,WIP add fit_predict to outlier detection classes  Actually halves the computation for EllipticEnvelopeCloses issue #846,,528,0.8446969696969697,0.2968197879858657,25512,318.67356538099716,30.92662276575729,89.25211665098777,1672,25,656,41,unknown,amueller,amueller,true,,80,0.9,328,25,590,true,true,true,false,155,356,978,58,96,0,684
42333,scikit-learn/scikit-learn,python,882,1338666818,,1348942995,171269,,unknown,false,true,false,99,4,0,1,2,1,4,0,4,0,0,0,2,0,0,0,1,0,2,3,3,0,0,0,0,181,0,0,0.0,0,,,0,0.0,2,2,false,WIP: Calibration plot Ive started to play with calibration plots I added an example: examples/plot_calibrationpyI would like to get feedback from people who are familiar with the subject (especially @alextp and maybe @paolo-losi)On small and even not so small datasets Im often getting empty bins (meaning that some ranges of probabilities are never predicted by the classifier) To get smoother plots I was thinking I could use polynomial regression but theres a risk of overfitting since the number of bins is smallThis is a work-in-progress: I wont have time to add tests and documentation anytime soon,,527,0.8462998102466793,0.2975778546712803,25457,319.2834976627254,30.875594139136584,89.2878186746278,1671,25,655,72,unknown,mblondel,mblondel,true,,17,0.8823529411764706,147,27,795,true,true,false,false,64,146,88,17,34,0,1549
320728,scikit-learn/scikit-learn,python,881,1338663568,1338814974,1338814974,2523,2523,github,false,false,false,94,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,6,0,0,0.0,0,,,0,0.0,0,0,false,FIX renamed what was components_ to sources_ This PR fixes a bug I introduced in 02d1d85566bd8b6a35bc744d25b7b3a797f6a364There was already an undocumented attribute named components_ that contained the sourcesThis broke the whole thingInterestingly none of the tests found that only noticed it through an exampleShould I add another warning that the meaning of components_ changedThat would be a warning that would be issued every time FastICA is used though as components_is used internally This would be quite uglyAny idea on how to add tests that would have got that,,526,0.8460076045627376,0.303886925795053,25457,319.2834976627254,30.875594139136584,89.2878186746278,1671,25,655,41,unknown,amueller,agramfort,false,agramfort,79,0.8987341772151899,327,25,589,true,true,true,false,154,357,1134,59,107,0,-1
320690,scikit-learn/scikit-learn,python,878,1338644988,1338807210,1338807210,2703,2703,merged_in_comments,false,false,false,52,1,1,0,3,0,3,0,3,0,0,3,3,2,0,0,0,0,3,3,2,0,0,24,28,24,28,14.039459469118615,0.425211397436684,55,robertlayton@gmail.com,doc/whats_new.rst|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_svmlight_format.py,49,0.015817223198594025,0,0,false,MRG support opening compressed files in SVMlight reader Compressed file support for the SVMlight/LibSVM reader supports gzip and bzip2 Many (all) of the [datasets](http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/) distributed by LibSVMs authors are compressed with bzip2If there are other places where compressed file support is useful then the _gen_open function might be moved to utils,,525,0.8457142857142858,0.3022847100175747,25457,319.2834976627254,30.875594139136584,89.2878186746278,1671,25,655,41,unknown,larsmans,larsmans,true,larsmans,39,0.7435897435897436,71,28,685,true,true,false,false,25,47,411,22,33,1,4
320152,scikit-learn/scikit-learn,python,877,1338529363,1338530253,1338530253,14,14,github,false,false,false,23,3,1,0,0,0,0,0,1,0,0,2,3,2,0,0,0,0,3,3,3,0,0,22,33,25,33,9.553070685789722,0.2905281489899357,26,peter.prettenhofer@gmail.com,sklearn/metrics/pairwise.py|sklearn/neighbors/tests/test_neighbors.py,15,0.026690391459074734,0,0,true,SVM Radial Basis Function Parameters Example Bugfix Fixed a bug referenced [here](https://githubcom/scikit-learn/scikit-learn/issues/744#issuecomment-6029574) due to using Python 27-style string formatting instead of Python 26-style,,524,0.8454198473282443,0.29359430604982206,25191,320.3922035647652,31.042832757730935,89.95276090667302,1667,25,654,39,unknown,duckworthd,mblondel,false,mblondel,3,1.0,17,1,469,false,true,false,false,5,19,12,6,0,0,-1
395121,scikit-learn/scikit-learn,python,876,1338502665,1338912767,1338912768,6835,6835,github,false,false,true,25,6,0,0,14,0,14,0,4,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,48,30,0,0.0,0,,,0,0.0,0,1,false,Warm restart for the Sparse Elastic Net Added warm restart capabilities for the sparse version of ElasticNet as required for merging sparse and dense versions,,523,0.8451242829827916,0.2925531914893617,25187,320.4033826974233,31.04776273474411,89.92734347083812,1666,25,653,42,unknown,alexis-mignon,agramfort,false,agramfort,1,1.0,1,0,65,true,true,false,false,2,17,4,0,42,0,5
319713,scikit-learn/scikit-learn,python,875,1338458763,1338936473,1338936473,7961,7961,github,false,false,false,101,4,0,0,2,0,2,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,49,17,0,0.0,0,,,0,0.0,0,0,false,Ward clustering fails if connectivity is not item assignable and there is more than one component In the ward prior to launch the algorithm itself the number of components of the connectivity matrix is checked and if there is more than one _fix_connectivty is called to join them inplaceThe problem is raised if the connectivity matrix does not support item assignment like coo matrix It is necessary to transform it into a lil matrix before so I have put the copy of the matrix before component checking I have also put apart the non-connected case for the sake of clarity,,522,0.8448275862068966,0.2880143112701252,25172,320.5943111393612,31.066264102971555,89.98093119338948,1664,25,653,43,unknown,AlexandreAbraham,amueller,false,amueller,0,0,6,1,45,false,true,false,false,0,0,0,0,0,0,1899
319359,scikit-learn/scikit-learn,python,874,1338394952,1338492153,1338492153,1620,1620,github,false,false,false,39,2,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,52,0,0,0.0,0,,,0,0.0,0,0,false,MISC privatize/deprecate internal function of gaussian process  _arg_max_reduced_likelihood_function looks like an accessorbut it actually does work and is mainly called during fit()The return values are now stored using sklearn naming conventionAttributes of GaussianProcess are now documented,,521,0.8445297504798465,0.289048473967684,25172,320.5943111393612,31.066264102971555,89.98093119338948,1661,25,652,40,unknown,temporaer,amueller,false,amueller,0,0,3,2,1332,false,true,true,true,0,0,0,0,0,0,3
624892,scikit-learn/scikit-learn,python,873,1338385479,1338385680,1338385681,3,3,github,false,false,false,21,1,0,0,3,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,2,false,[MRG] Remove deprecation warning in sparse_encode Minor change just so its one click away from merging when the word is given,,520,0.8442307692307692,0.2870036101083033,25176,320.5433746425167,31.061328249126152,89.96663489037178,1660,25,652,39,unknown,vene,vene,true,vene,18,0.9444444444444444,35,19,780,true,true,false,false,15,13,2,3,1,0,0
625973,scikit-learn/scikit-learn,python,872,1338378915,1338383908,1338383908,83,83,github,false,false,false,28,2,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.549760452910535,0.13837448491430365,6,jaquesgrobler@gmail.com,doc/tutorial/index.rst,6,0.010869565217391304,0,0,false,Just a note for the tutorial section Mentions to users that if they wish to copy the code into iPython they can use%doctest_mode to make it easier,,519,0.8439306358381503,0.286231884057971,25176,320.50365427391165,31.061328249126152,89.96663489037178,1660,25,652,38,unknown,jaquesgrobler,agramfort,false,agramfort,14,0.9285714285714286,6,9,126,true,true,true,true,18,65,24,4,63,0,-1
317702,scikit-learn/scikit-learn,python,869,1337960018,1338376797,1338376797,6946,6946,github,false,false,false,24,2,0,0,3,0,3,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,14,0,0.0,0,,,0,0.0,1,3,true,MRG: PLS scale by zero bug fixes the problem when std of a covariate is zero @duchesnay could you please check if you agree,,518,0.8436293436293436,0.2504288164665523,25171,320.09058042985976,31.027770052838584,90.02423423781336,1647,25,647,42,unknown,pprett,duchesnay,false,duchesnay,21,0.8571428571428571,61,25,1025,true,true,false,false,17,46,188,38,39,0,6154
317678,scikit-learn/scikit-learn,python,868,1337957197,1338754163,1338754163,13282,13282,github,false,false,false,44,3,0,0,8,0,8,0,4,0,0,0,8,0,0,0,0,0,8,8,7,0,1,0,0,1206,0,0,0.0,0,,,0,0.0,0,2,true,MRG import liblinear 191 Heres liblinear 191 with our changes reapplied Im not going to implement LinearSVR myself but however would like to try that will need thisOn my box this builds without warnings all tests pass and examples/document_classification_20newsgroupspy seems to work fine,,517,0.8433268858800773,0.2504288164665523,25171,320.09058042985976,31.027770052838584,90.02423423781336,1647,25,647,48,unknown,larsmans,amueller,false,amueller,38,0.7368421052631579,70,28,677,true,true,false,true,24,44,409,22,31,1,82
316004,scikit-learn/scikit-learn,python,866,1337703668,1338572998,1338572998,14488,14488,merged_in_comments,false,false,false,67,8,3,0,26,0,26,0,5,0,0,1,4,1,0,0,0,0,4,4,3,0,0,110,0,289,16,8.884080591681702,0.2701840854460439,16,peter.prettenhofer@gmail.com,sklearn/hmm.py|sklearn/hmm.py,16,0.027257240204429302,0,4,false,WIP - Hmm API fix Regarding the Issue #832 - Ive made the following changes so far to hmmpy* fit() no longer contains any params beyond the observations* __init__ is now used to set up the params that were done when calling fit()* the previous decision logic that was in __init__ is now in fit() and some setter methods* examples within file adjusted,,516,0.8430232558139535,0.23850085178875638,25172,320.5943111393612,31.066264102971555,89.98093119338948,1636,27,644,45,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,13,0.9230769230769231,6,8,118,true,true,true,false,17,63,26,4,70,0,1
222382,scikit-learn/scikit-learn,python,865,1337702816,1343059666,1343059666,89280,89280,merged_in_comments,false,true,false,117,7,1,0,9,5,14,0,3,0,0,5,10,5,0,0,0,0,10,10,10,0,0,27,6,530,71,22.414792624837418,0.6817184669073711,14,peter.prettenhofer@gmail.com,examples/covariance/plot_outlier_detection.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_covariance.py|sklearn/covariance/tests/test_robust_covariance.py,11,0.005110732538330494,0,3,false,Ledoitwolf computation LedoitWolf shrinkage estimation with large number of features does not create memory troubles againThe construction of empirical covariance matrices is avoided by performing the computation with blocksAs a consequence the ledoit_wolf method will not always be able to return the shrunk covariance matrix (because it does not fit in memory) and will return None instead The maximum size of a memory-allowed covariance matrix is controled by a new parameter of the ledoit-wolf methodI tested up to n_features  100000 on my laptop using a block size of 1000 and 1000 observations No memory usage increase was observed while I could barely reach 10000 features x 1000 observations before on the same computer,,515,0.8427184466019417,0.23850085178875638,25171,319.3754717730722,30.988041794128165,89.98450597910293,1636,27,644,56,unknown,VirgileFritsch,GaelVaroquaux,false,GaelVaroquaux,6,0.6666666666666666,4,0,750,true,true,false,false,1,1,0,0,2,0,407
37492,scikit-learn/scikit-learn,python,862,1337199784,1346036749,1346036749,147282,147282,commit_sha_in_comments,false,false,false,283,63,9,68,49,0,117,0,10,5,0,3,32,6,0,1,17,1,26,44,23,0,2,1163,84,7480,489,60.374065423651196,1.8359539188743075,30,peter.prettenhofer@gmail.com,sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/kf_vars.mat|sklearn/datasets/descr/kf.rst|sklearn/kalman.py|sklearn/tests/test_kalman.py|examples/plot_kalman.py|sklearn/metrics/pairwise.py|sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/kf_vars.mat|sklearn/datasets/descr/kf.rst|sklearn/kalman.py|sklearn/tests/test_kalman.py|examples/plot_kalman.py,19,0.0,0,36,false,WIP: Kalman Filter Smoother EM This is a stub for a work in progress  The goal is to provide a module for efficiently and easily applying algorithms for inferring the hidden state of a system denoted x generated from the following bit of codepythonx[0]  nprandommultivariate_normal(mu_0 sigma_0)for t in range(T)  e1[t]  nprandommultivariate_normal(0 Q)  e2[t]  nprandommultivariate_normal(0 R)  x[t+1]  Adot(x[t]) + b[t] + e1[t]  z[t]  Cdot(x[t]) + d[t] + e2[t]e2[T-1]  nprandommultivariate_normal(0 R)z[T-1]  Cdot(z[T-1]) + d[T-1] +  e2[T-1]The initial goal is to provide 3 algorithms  The first two the [Kalman Filter][1] and [Kalman Smoother][2] are for estimating x given observations z state transition matrix A transition offsets b observation matrix C observation offsets d and covariance matrices Q and R  The two algorithms differ in one aspect: while the Filter estimates x[t] using z[0:t] the Smoother estimates the same using z[0:T] at additional computational cost  The third algorithm the [EM algorithm][3] as applied to the Linear-Gaussian system iteratively approximates A C b d Q R mu_0 and sigma_0 by maximizing the likelihood of the observationsCurrent Progress:+ Kalman Filter Kalman Smoother implemented tested+ Example provided+ Test dataset added+ Estimation of all model parameters including transition matrices offsets covariance matrices and initial state distribution+ Handle missing observations+ Users Guide documentationFuture:- Cython implementation- Handle multiple independent observation sequences- Get module reference documentation working- Implement Extended Kalman Filter or Unscented Kalman FilterAll comments and suggestions are appreciated  This is my first significant contribution to an open source project so I can use all the help I can get[1]: http://enwikipediaorg/wiki/Kalman_filter[2]: http://insteecsberkeleyedu/~cs294-40/fa08/scribes/lecture14pdf[3]: http://enwikipediaorg/wiki/Expectation%E2%80%93maximization_algorithm,,514,0.8424124513618677,0.2262295081967213,25556,318.0075129128189,30.990765377993426,89.4506182501174,1625,27,638,83,unknown,duckworthd,duckworthd,true,duckworthd,2,1.0,17,1,453,false,true,false,false,4,11,4,0,0,0,600
312973,scikit-learn/scikit-learn,python,859,1337127097,1338363804,1338363804,20611,20611,merged_in_comments,false,false,false,17,5,0,0,7,0,7,0,4,0,0,0,18,0,0,0,0,0,18,18,16,0,0,0,0,178,130,0,0.0,0,,,0,0.0,0,3,false,MRG Cluster k rename Fixes issue #844 Renaming parameter k in KMeans MiniBatchKMeans and SpectralClustering to n_clusters,,513,0.8421052631578947,0.21669341894060995,25140,319.0135242641209,31.026252983293556,89.97613365155131,1621,27,637,44,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,78,0.8974358974358975,316,24,571,true,true,true,false,161,352,1348,61,117,0,466
312932,scikit-learn/scikit-learn,python,858,1337123528,1338510529,1338510529,23116,23116,github,false,false,false,10,2,0,0,7,0,7,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,17,1,0,0.0,0,,,0,0.0,0,0,false,MRG rename unmixing_matrix_ to components_ in FastICA Fixing issue #854,,512,0.841796875,0.21634615384615385,25140,319.0135242641209,31.026252983293556,89.97613365155131,1621,27,637,46,unknown,amueller,amueller,true,amueller,77,0.8961038961038961,316,24,571,true,true,false,false,160,352,1346,61,117,0,527
312342,scikit-learn/scikit-learn,python,855,1337033381,1337122278,1337122278,1481,1481,commits_in_master,false,false,false,72,1,0,0,5,0,5,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,22,33,0,0.0,0,,,0,0.0,0,1,false,EasyFix: KNeighborsClassifier doesnt convert input to float In reference to [this issue](https://githubcom/scikit-learn/scikit-learn/issues/852)  Ive decided to place a conversion is check_pairwise_arrays() in sklearn/metrics/pairwisepy  While I can imagine cases where this could result in a loss of information I feel they must be extremely carefully constructed and cant imagine where they might appear in a real dataset  If you believe there is a better way let me know and I will happily change it,,511,0.8414872798434442,0.21236133122028525,25140,317.7406523468576,30.946698488464598,89.8170246618934,1618,27,636,37,unknown,duckworthd,amueller,false,amueller,1,1.0,17,1,451,false,true,false,false,3,9,3,0,0,0,584
311358,scikit-learn/scikit-learn,python,851,1336783470,1336987393,1336987393,3398,3398,github,false,false,false,48,5,0,0,6,0,6,0,2,0,0,0,3,0,0,0,0,1,2,3,2,0,0,0,0,231,0,0,0.0,0,,,0,0.0,0,0,true,EasyFix: Merge SVM grid search and parameter explanation examples In fulfillment of https://githubcom/scikit-learn/scikit-learn/issues/744 Deleted plot_svm_parameters_selectionpy and merged its contents into plot_rbf_parameterspy  Reformatted plots to only make one figure and show the contents of both side-by-sideLet me know if theres anything youd like to see cleaned up,,510,0.8411764705882353,0.2127329192546584,25150,317.61431411530816,30.934393638170974,89.78131212723659,1611,26,633,39,unknown,duckworthd,agramfort,false,agramfort,0,0,17,1,448,false,true,false,false,1,2,0,0,0,0,2234
35446,scikit-learn/scikit-learn,python,849,1336730002,,1374667406,632290,,unknown,false,false,false,105,42,33,9,32,0,41,0,5,0,0,9,10,9,0,0,0,0,10,10,10,0,0,5247,0,6312,0,227.17173607957125,6.898775656612531,195,vlad@vene.ro,sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/base.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx|sklearn/utils/seq_dataset.c|sklearn/utils/weight_vector.c|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.c|sklearn/utils/weight_vector.c|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/perceptron.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/weight_vector.c|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/base.py|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/utils/weight_vector.c|sklearn/utils/weight_vector.pxd|sklearn/utils/weight_vector.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx,127,0.027439024390243903,0,20,true,WIP: Multinomial Logistic Regression via SGD This is an early PR for multinomial logistic regression via stochastic gradient descent SGDClassifier now has a new parameter multi_class (similar to LinearSVC) If losslog and multi_classmultinomial SGDClassifier trains a multinomial LR model instead n_classes binary LR models Refactorings:  * sgd_fastpyx: LossFunctions now have a new method weight_update (see MultinomialLogLoss for the new weight update) Ive also added a multi-class Perceptron  * New learning rate schedule: Exponential decay - according to LingPipes Bob Carpenter this works fine for Multinomial LR Buttou-style learning rate calibration is a bit tricky and Ive had mixed results in the past (comments/remarks much appreciated) ,,509,0.8428290766208252,0.21951219512195122,25134,315.6680194159306,30.87451261239755,89.59974536484442,1611,26,633,131,unknown,pprett,pprett,true,,20,0.9,61,25,1011,true,true,false,false,18,45,188,35,48,0,2
310179,scikit-learn/scikit-learn,python,847,1336608721,1336811793,1336811793,3384,3384,github,false,false,false,26,1,0,0,8,0,8,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,3,false,Bug in plot_hmm_stock_analysis Fixing the bug makes the plot output ofplot_hmm_stock_analysis look less pretty Thehidden states are no longer a direct function of price,,508,0.84251968503937,0.2261904761904762,25109,316.61953881078495,30.586642239834323,89.21103986618344,1607,26,631,40,unknown,kwgoodman,lucidfrontier45,false,lucidfrontier45,1,1.0,19,3,702,false,false,false,false,0,0,0,0,0,0,2317
222712,scikit-learn/scikit-learn,python,842,1336514507,1343068368,1343068368,109231,109231,merged_in_comments,false,false,false,35,1,0,0,30,0,30,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,17,0,0,0.0,0,,,0,0.0,0,7,false,ENH: sklearnsetup_module to preseed RNGs to reproduce failures Could easily be controlled from the environment via SKLEARN_SEEDBesides adding setup_module() to the sklearn space should be of noeffect for any regular imports of sklearn,,507,0.8422090729783037,0.22686567164179106,25661,324.9288804021667,31.5654105451853,90.05884416039905,1603,26,630,53,unknown,yarikoptic,GaelVaroquaux,false,GaelVaroquaux,5,1.0,40,7,1244,true,true,false,false,0,0,28,0,1,0,14
309057,scikit-learn/scikit-learn,python,840,1336434096,1336434995,1336434995,14,14,github,false,false,false,15,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,DOC: updated testing instructions Recent commits have made the testing situation on Windows much easier,,506,0.841897233201581,0.23224852071005916,25109,313.6723883866342,30.586642239834323,89.05173443785097,1600,26,629,37,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,17,0.9411764705882353,35,19,757,true,true,true,false,10,14,1,3,1,0,1
308686,scikit-learn/scikit-learn,python,838,1336391889,1336656641,1336656641,4412,4412,merged_in_comments,false,false,false,57,14,0,0,12,1,13,0,3,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,177,75,0,0.0,0,,,0,0.0,0,5,false,MRG: LabelNormalizer This PR adds LabelNormalizer a simple transformer for normalizing label encoding I needed this to make my life easier when writing some Cython code The PR also adds minimalistic documentation for LabelBinarizer which was absent I dont need this in 011 but I would like this PR to be merged in master soon if possible,,505,0.8415841584158416,0.2344213649851632,25109,313.6723883866342,30.586642239834323,89.05173443785097,1599,26,629,39,unknown,mblondel,mblondel,true,mblondel,16,0.875,140,27,769,true,false,false,false,66,126,114,17,23,0,1046
206222,scikit-learn/scikit-learn,python,837,1336337225,,1342229226,98200,,unknown,false,false,false,147,24,13,0,16,0,16,0,4,31,4,15,65,22,0,0,52,5,29,86,32,11,0,3139,161,3317,162,319.91336823847126,9.714839358837864,14,vlad@vene.ro,doc/tutorial/astronomy/big_toc_css.rst|doc/tutorial/astronomy/classification.rst|doc/tutorial/astronomy/data/sdss_colors/fetch_data.py|doc/tutorial/astronomy/data/sdss_colors/scatter_colors.py|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/index.rst|doc/tutorial/astronomy/placeholder.txt|doc/tutorial/astronomy/regression.rst|doc/tutorial/astronomy/setup.rst|doc/tutorial/astronomy/solutions/classification_naive_bayes.py|doc/tutorial/index.rst|examples/tutorial/plot_ML_flow_chart.py|examples/tutorial/plot_gui_example.py|examples/tutorial/plot_iris_projections.py|examples/tutorial/plot_sdss_filters.py|doc/tutorial/astronomy/regression.rst|examples/tutorial/plot_neighbors_photoz.py|examples/tutorial/plot_sdss_filters.py|doc/tutorial/astronomy/data/sdss_photoz/fetch_data.py|doc/tutorial/astronomy/solutions/photoz_nearest_neighbors.py|doc/tutorial/astronomy/classification.rst|doc/tutorial/astronomy/data/sdss_photoz/fetch_data.py|doc/tutorial/astronomy/data/sdss_spectra/fetch_data.py|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/index.rst|doc/tutorial/astronomy/regression.rst|doc/tutorial/astronomy/setup.rst|doc/tutorial/astronomy/skeletons/exercise_01.py|doc/tutorial/astronomy/solutions/classification_naive_bayes.py|doc/tutorial/astronomy/solutions/exercise_01.py|doc/tutorial/astronomy/solutions/photoz_nearest_neighbors.py|examples/tutorial/plot_sdss_filters.py|doc/tutorial/astronomy/solutions/generate_skeletons.py|doc/tutorial/astronomy/classification.rst|doc/tutorial/astronomy/data/sdss_photoz/fetch_data.py|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/exercises.rst|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/index.rst|doc/tutorial/astronomy/regression.rst|examples/tutorial/plot_neighbors_photoz.py|examples/tutorial/plot_sdss_photoz.py|examples/tutorial/plot_sdss_specPCA.py|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/index.rst|doc/tutorial/astronomy/practical.rst|doc/tutorial/astronomy/solutions/exercise_01.py|examples/tutorial/plot_bias_variance_examples.py|doc/tutorial/astronomy/classification.rst|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/exercises.rst|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/practical.rst|doc/tutorial/astronomy/regression.rst|doc/tutorial/astronomy/setup.rst|doc/tutorial/astronomy/skeletons/exercise_01.py|doc/tutorial/astronomy/skeletons/exercise_02.py|doc/tutorial/astronomy/skeletons/exercise_03.py|doc/tutorial/astronomy/solutions/exercise_01.py|doc/tutorial/astronomy/solutions/exercise_02.py|doc/tutorial/astronomy/solutions/exercise_03.py|examples/tutorial/plot_ML_flow_chart.py|doc/tutorial/astronomy/classification.rst|doc/tutorial/astronomy/dimensionality_reduction.rst|doc/tutorial/astronomy/exercises.rst|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/index.rst|doc/tutorial/astronomy/setup.rst|doc/tutorial/astronomy/solutions/exercise_02.py|examples/tutorial/plot_sdss_images.py|doc/tutorial/astronomy/general_concepts.rst|doc/tutorial/astronomy/setup.rst|doc/tutorial/astronomy/practical.rst,14,0.0,1,6,false,WIP: Sklearn tutorial Initial PR  Several things need to be addressed before merge:1 Typos and proof-reading2 Pieces of this material duplicate what @jaquesgrobler has added in other tutorial sections  This needs to  be addressed3 This uses several data loaders which are built in to the example files  These should be moved to sklearn/datasets  I also need to double-check that these datasets will exist in long-term http locations4 Currently the tutorial is a bit of a nightmare when it comes to doctests  The doctest code which works with large datasets needs to be changed to prevent the datasets from being inadvertently downloaded  Also several snippets create matplotlib plots which shouldnt happen in doc testsIll have more significant time to invest in this around the end of June  If others could help with review and feedback in the meantime I would greatly appreciate it,,504,0.8432539682539683,0.23476968796433878,25573,323.34884448441716,31.243890040276856,89.97771086692995,1598,26,628,58,unknown,jakevdp,jakevdp,true,,23,0.9565217391304348,563,0,361,true,true,false,false,23,41,5,9,20,0,24
308516,scikit-learn/scikit-learn,python,836,1336333583,1336335485,1336335485,31,31,merged_in_comments,false,false,false,12,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,13,0,0,0.0,0,,,0,0.0,0,0,false,MRG added random_state to Gaussian Process So the user has more control,,503,0.8429423459244533,0.23476968796433878,25085,313.13533984452863,30.576041459039267,89.05720550129558,1598,26,628,36,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,76,0.8947368421052632,314,24,562,true,true,true,false,155,353,1351,60,144,0,-1
308502,scikit-learn/scikit-learn,python,833,1336329666,1336334480,1336334480,80,80,merged_in_comments,false,false,false,14,2,0,0,2,0,2,0,2,0,0,0,3,0,0,0,1,0,3,4,0,1,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,WIP Added page with links to various tutorials/presentations Contains Jakes Gaels and Oliviers talks,,502,0.8426294820717132,0.2344213649851632,25077,313.15548111815605,30.585795749092792,89.0856163017905,1598,26,628,37,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,75,0.8933333333333333,314,24,562,true,true,true,false,153,352,1349,60,144,0,31
308475,scikit-learn/scikit-learn,python,831,1336321669,1336329412,1336329412,129,129,merged_in_comments,false,false,false,8,2,0,0,1,0,1,0,1,0,0,0,22,0,0,0,0,0,22,22,22,0,0,0,0,9,189,0,0.0,0,,,0,0.0,0,0,false,MRG use random states everywhere never call nprandom ,,501,0.8423153692614771,0.2344213649851632,25077,312.0389201260119,30.506041392511065,88.5273358057184,1598,26,628,37,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,74,0.8918918918918919,314,24,562,true,true,true,false,151,352,1347,60,144,0,50
308464,scikit-learn/scikit-learn,python,829,1336317723,1336319286,1336319286,26,26,merged_in_comments,false,false,false,8,1,0,0,0,0,0,0,1,0,0,0,25,0,0,0,0,0,25,25,25,0,0,0,0,0,250,0,0.0,0,,,0,0.0,0,0,false,MRG backport assert_less and assert_greater Fixes issue #586,,500,0.842,0.2344213649851632,25069,310.5429015916072,30.55566636084407,87.8774582153257,1598,26,628,41,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,73,0.8904109589041096,314,24,562,true,true,true,false,149,352,1345,60,144,0,-1
308436,scikit-learn/scikit-learn,python,828,1336309282,1336320778,1336320778,191,191,merged_in_comments,false,false,false,8,1,0,0,2,0,2,0,2,0,0,0,8,0,0,0,0,0,8,8,7,0,0,0,0,127,24,0,0.0,0,,,0,0.0,0,1,false,MRG rename out_dim to  n_components in manifold module ,,499,0.8416833667334669,0.2344213649851632,25069,310.5429015916072,30.55566636084407,87.8774582153257,1597,26,628,39,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,72,0.8888888888888888,314,24,562,true,true,true,false,148,351,1344,60,144,0,141
308432,scikit-learn/scikit-learn,python,827,1336308391,1336318410,1336318410,166,166,merged_in_comments,false,false,false,8,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,9,0,0,0.0,0,,,0,0.0,0,0,false,Fix missing links to the C math libray ,,498,0.8413654618473896,0.2344213649851632,25069,310.5429015916072,30.55566636084407,87.8774582153257,1597,26,628,40,unknown,grenoya,GaelVaroquaux,false,GaelVaroquaux,0,0,0,2,258,false,true,false,false,0,1,0,0,0,0,-1
308424,scikit-learn/scikit-learn,python,826,1336307187,1336327083,1336327083,331,331,merged_in_comments,false,false,false,25,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0,1,0,1,2,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,MRG Added pipeline user guide Some introduction to pipelining I would like an image but I cant think of a useful oneFixes issue #718,,497,0.8410462776659959,0.2344213649851632,25069,310.5429015916072,30.55566636084407,87.8774582153257,1597,26,628,38,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,71,0.8873239436619719,314,24,562,true,true,true,false,147,351,1343,60,144,0,-1
308408,scikit-learn/scikit-learn,python,825,1336302601,1336318793,1336318793,269,269,merged_in_comments,false,false,false,54,6,5,0,1,0,1,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,129,118,133,118,38.84865494654407,1.1864979021541748,14,olivier.grisel@ensta.org,sklearn/neighbors/classification.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py,7,0.010385756676557863,2,0,false,MRG: Radius-based classifier: raise exception if no neighbors found This is a continuation of @joonazzz s PR #775 to close issue #760 I was waiting for a review by @jakevdp to merge it in but Jake seems busy this week end and I think that Id like to merge this in before the release,,496,0.8407258064516129,0.2344213649851632,25069,310.5429015916072,30.55566636084407,87.8774582153257,1597,26,628,41,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,14,0.6428571428571429,222,2,804,true,true,false,false,113,244,950,21,91,2,248
308267,scikit-learn/scikit-learn,python,823,1336247992,1336250667,1336250667,44,44,merged_in_comments,false,false,false,21,2,0,0,2,0,2,0,2,0,0,0,24,0,0,0,0,0,24,24,24,0,0,0,0,155,0,0,0.0,0,,,0,0.0,0,0,false,MRG adjusted examples to new matplotlib 111 Apparently one can not set a cmap without there being something to draw on,,495,0.8404040404040404,0.23546944858420268,25111,312.57217952291825,30.624029309864206,87.92959260881686,1595,26,627,40,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,70,0.8857142857142857,314,24,561,true,true,true,false,143,338,1340,60,141,0,5
308262,scikit-learn/scikit-learn,python,822,1336246402,1336250533,1336250533,68,68,merged_in_comments,false,false,false,31,1,0,0,3,0,3,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,16,39,0,0.0,0,,,0,0.0,0,1,false,Add warnings and clean up tests Not sure if this is the right way to do stuff This should probably be mirrored in LassoLars Enet etc Just sending it for review,,494,0.840080971659919,0.23582089552238805,25109,312.5970767453901,30.62646859691744,87.93659643952367,1595,26,627,40,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,16,0.9375,35,19,755,false,true,true,false,7,12,0,3,0,0,26
308186,scikit-learn/scikit-learn,python,821,1336225152,1336255125,1336255125,499,499,merged_in_comments,false,false,false,55,9,0,0,2,0,2,0,2,0,0,0,34,0,0,0,0,0,34,34,26,0,0,0,0,305,163,0,0.0,0,,,0,0.0,2,0,false,MRG Scale c removal Im trying to manually remove scale_C as this seemed easier than reverting the commitstodo:- ~~The decision function test doesnt pass~~ (thanks @agramfort)- ~~need to see if doctests pass~~- ~~need to see if docs build~~- ~~check if all examples work~~ ~~need to check example output~~ (thanks @vene),,493,0.8397565922920892,0.2398190045248869,25105,312.56721768572,30.631348336984665,87.9506074487154,1594,26,627,39,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,69,0.8840579710144928,314,24,561,true,true,true,false,140,332,1332,60,133,0,414
308010,scikit-learn/scikit-learn,python,818,1336170883,,1336175720,80,,unknown,false,false,false,12,1,0,0,0,5,5,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,6,0,0,0.0,0,,,0,0.0,0,0,true,MRG bug in preprocessingscale Either Im blind or this is a bug,,492,0.8414634146341463,0.2394578313253012,25103,312.63195633987965,30.63378879018444,87.95761462773375,1594,27,626,38,unknown,amueller,amueller,true,,68,0.8970588235294118,314,24,560,true,true,false,false,139,331,1330,60,133,0,-1
389552,scikit-learn/scikit-learn,python,816,1336145106,,1336152003,114,,unknown,false,false,true,9,3,0,0,34,0,34,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,15,0,0,0.0,0,,,0,0.0,0,3,true,Adds a warning in MiniBatchKMeans where k  batch_size ,,491,0.8431771894093686,0.24617737003058104,25103,312.63195633987965,30.63378879018444,87.95761462773375,1592,27,626,39,unknown,BenoitDamota,BenoitDamota,true,,0,0,1,5,555,false,true,false,false,0,0,0,0,0,0,4
306869,scikit-learn/scikit-learn,python,814,1336000968,1336001078,1336001078,1,1,github,false,false,false,10,1,0,0,0,0,0,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,6,0,0,0.0,0,,,0,0.0,0,0,false,Cleaned cruft from example code Cleaned cruft from example code,,490,0.8428571428571429,0.2507836990595611,25097,312.3480894130772,30.680957883412358,87.8192612662868,1587,27,624,41,unknown,invisibleroads,agramfort,false,agramfort,1,1.0,34,33,727,false,true,false,false,1,0,1,0,0,0,-1
306695,scikit-learn/scikit-learn,python,813,1335983921,1336001189,1336001189,287,287,github,false,false,false,10,1,0,0,4,0,4,0,2,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Fixed some typos Fixed some typos in the documentationhttp://scikit-learnorg/dev/tutorial/statistical_inference/supervised_learninghtml,,489,0.8425357873210634,0.2519561815336463,25097,312.3480894130772,30.680957883412358,87.8192612662868,1586,27,624,41,unknown,invisibleroads,agramfort,false,agramfort,0,0,34,33,727,false,true,false,false,0,0,0,0,0,0,10
306625,scikit-learn/scikit-learn,python,812,1335976983,1336107705,1336107705,2178,2178,github,false,false,false,29,2,0,0,6,0,6,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,12,52,0,0.0,0,,,0,0.0,0,1,false,ENH add decision_function to Pipeline This implements a decision_function for Pipeline predict_proba and predict_log_proba were already thereI needed this and I see no reason not to include it,,488,0.8422131147540983,0.2519561815336463,25097,312.3480894130772,30.680957883412358,87.8192612662868,1585,27,624,41,unknown,amueller,ogrisel,false,ogrisel,67,0.8955223880597015,313,24,558,true,true,true,false,137,331,1329,60,134,0,27
306534,scikit-learn/scikit-learn,python,810,1335962483,1336746780,1336746780,13071,13071,github,false,false,false,218,9,2,0,19,0,19,0,3,0,0,3,7,3,0,0,0,0,7,7,5,0,0,39,19,97,65,21.841866442601237,0.6678526344014014,48,peter.prettenhofer@gmail.com,sklearn/linear_model/base.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py,27,0.031007751937984496,0,3,false,MRG: New losses for SGD - Renamed p to epsilon- Allowed regression losses in SGDClassifier (the squared loss works surprisingly well if the learning rate is well set)- Added the epsilon-insensitive loss (used by SVR)- Added a sparse version of the log lossI came up with this sparse log loss today I changed the logistic function from 1 / (1 + e^-z) where z  y * w^T x to 1 / (1 + e^-(az)) where a is chosen so that values of z1-epsilon are squashed to a probability of 1 Effectively this results in sparse updates like the hinge loss but with probability support Since I developed this loss just today it may be too early to include it in scikit-learn WDYT Anyway I tested it on the news20 dataset and it seems to work well (have a look at the density and training time)log loss________________________________________________________________________________Training:SGDClassifier(alpha00001 class_weightNone epsilon01 eta000       fit_interceptTrue learning_rateoptimal losslog n_iter50       n_jobs1 penaltyl2 power_t05 rho085 seed0 shuffleFalse       verbose0 warm_startFalse)train time: 0369stest time:  0002sf1-score:   0896dimensionality: 33807density: 1000000    sparse_log loss________________________________________________________________________________Training:SGDClassifier(alpha00001 class_weightNone epsilon01 eta000       fit_interceptTrue learning_rateoptimal losssparse_log       n_iter50 n_jobs1 penaltyl2 power_t05 rho085 seed0       shuffleFalse verbose0 warm_startFalse)train time: 0255stest time:  0002sf1-score:   0899dimensionality: 33807density: 0722668,,487,0.8418891170431212,0.2589147286821705,25094,312.3854307802662,30.684625806965812,87.82976010201642,1585,27,624,45,unknown,mblondel,mblondel,true,mblondel,15,0.8666666666666667,140,27,764,true,false,false,false,63,118,112,17,23,0,94
306463,scikit-learn/scikit-learn,python,809,1335943811,1335945405,1335945405,26,26,github,false,false,false,27,1,0,0,0,0,0,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,false,Fixed typo in documentation I am not sure what the procedure is for minor commits like this Tell me if I should just be emailing the list,,486,0.8415637860082305,0.2589147286821705,25097,312.3480894130772,30.680957883412358,87.8192612662868,1585,27,624,38,unknown,zaxtax,ogrisel,false,ogrisel,0,0,55,8,1467,false,true,false,false,0,0,0,0,0,0,-1
388940,scikit-learn/scikit-learn,python,808,1335905622,1337156111,1337156111,20841,20841,commits_in_master,false,false,true,23,147,6,0,72,2,74,0,4,0,0,4,152,4,0,0,8,0,152,160,130,1,1,151,118,5054,1216,43.71639069835767,1.3367144306937533,20,peter.prettenhofer@gmail.com,sklearn/utils/arpack.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/classification.py,7,0.01078582434514638,0,3,false,Positive constraint for sparse elastic net Positiveness constraints were recently added to the elastic net I added similar constraints for sparse elastic net,,485,0.8412371134020619,0.2696456086286595,25099,312.68178015060363,30.638670863381012,87.97163233594964,1585,26,623,47,unknown,alexis-mignon,agramfort,false,agramfort,0,0,1,0,35,false,true,false,false,0,0,0,0,0,0,19
107608,scikit-learn/scikit-learn,python,804,1335795248,,1376504806,678492,,unknown,false,false,false,42,4,0,0,19,2,21,0,6,0,0,0,2,0,0,0,1,0,2,3,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,3,false,WIP: Estimator summary Heres a pull request to add a summary of the estimator capabilities to the doc This is a work-in-progress and I need your helpTo do:- add more columns (time complexity space complexity )- add more estimators,,484,0.8429752066115702,0.2823179791976226,25092,312.21106328710346,30.607364897178385,87.79690738083852,1583,27,622,131,unknown,mblondel,mblondel,true,,14,0.9285714285714286,140,27,762,true,false,false,true,64,118,111,17,27,0,2
65437,scikit-learn/scikit-learn,python,803,1335741634,1346063987,1346063987,172039,172039,github,false,false,false,57,32,13,28,56,0,84,0,5,0,0,14,18,14,0,0,0,0,18,18,18,0,0,263,142,671,410,97.33601482812837,2.959925899063343,182,virgile.fritsch@gmail.com,examples/cluster/plot_cluster_comparison.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_spectral.py|examples/cluster/plot_affinity_propagation.py|examples/cluster/plot_cluster_comparison.py|sklearn/cluster/affinity_propagation_.py|sklearn/metrics/pairwise.py|sklearn/cluster/spectral.py|sklearn/cluster/tests/test_affinity_propagation.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/tests/test_kernel_pca.py|sklearn/manifold/isomap.py|sklearn/svm/base.py|sklearn/grid_search.py|sklearn/tests/test_grid_search.py|sklearn/svm/base.py|sklearn/decomposition/kernel_pca.py|sklearn/svm/base.py,81,0.034328358208955224,0,24,false,MRG Fit pairwise This is supposed to be (a draft of) the fit_pairwise proposal~~It creates fit_pairwise and predict_pairwise functions for SVM KernelPCA SpectralClustering and AffinityPropagationIt also makes all these usable using GridSearchCV with a new fit_pairwise for the grid search~~Now uses a property to check whether the estimator was fit using a pairwise X,,483,0.8426501035196687,0.2835820895522388,25555,318.019956955586,30.99197808648014,89.45411856779495,1583,27,621,92,unknown,amueller,amueller,true,amueller,66,0.8939393939393939,313,24,555,true,true,false,false,139,332,1287,71,136,0,296
304613,scikit-learn/scikit-learn,python,802,1335551654,1336080782,1336080782,8818,8818,github,false,false,false,49,2,0,0,8,0,8,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,42,0,0,0.0,0,,,0,0.0,0,2,true,MRG check if backport of sparse scipy ARPACK is needed  Fixes Issue #768The arpack backport is only necessary for scipy 09 and belowIn scipy 011 it breaks Therefore I introduced a check to see if we can import from scipyTestet with scipy 09 010 0101 011-git,,482,0.8423236514522822,0.2875722543352601,25101,311.9796024062787,30.596390582048524,87.7654276722043,1578,28,619,42,unknown,amueller,agramfort,false,agramfort,65,0.8923076923076924,313,24,553,true,true,true,false,146,344,1213,71,131,0,928
303945,scikit-learn/scikit-learn,python,800,1335452687,1335742100,1335742100,4823,4823,github,false,false,false,22,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,27,0,0.0,0,,,0,0.0,1,0,false,MRG less warnings during tests Getting rid of some warnings in nosetests that were caused by @GaelVaroquaux fix for the neighbor warnings,,481,0.841995841995842,0.28776978417266186,25086,312.5249142948258,30.61468548194212,87.85776927369848,1574,28,618,37,unknown,amueller,amueller,true,amueller,64,0.890625,313,24,552,true,true,false,false,146,339,1212,71,131,0,-1
303667,scikit-learn/scikit-learn,python,797,1335400164,,1338576114,52932,,unknown,false,false,false,103,34,5,0,47,1,48,0,7,3,0,4,18,6,0,1,5,2,13,20,10,0,2,307,78,1188,97,36.96914067659063,1.1243967374673287,15,satra@mit.edu,sklearn/manifold/nmds.py|sklearn/manifold/tests/test_nmds.py|sklearn/manifold/nmds.py|sklearn/manifold/nmds.py|sklearn/manifold/tests/test_nmds.py|sklearn/manifold/nmds.py|sklearn/datasets/__init__.py|sklearn/datasets/base.py|sklearn/datasets/data/france_distances.csv|sklearn/manifold/mds.py|sklearn/manifold/tests/test_mds.py,9,0.0,0,13,false,MRG - adds a MDS algorithm: SMACOF This is an early PR for MDS (http://enwikipediaorg/wiki/Multidimensional_scaling)Multidimensional scaling is a set of statistical algorithms used in information visualization for exploring similarities or dissimilarities in data Given a similarity or dissimilarity matrix MDS finds a representation in a small geometric spaceThe algorithm implemented here is the SMACOF which works both for metric and nonmetric MDS MDS minimises a objective function called stress the SMACOF algorithm uses the Guttman transform for the metric step (http://enwikipediaorg/wiki/Stress_majorization http://cranr-projectorg/web/packages/smacof/vignettes/smacofpdf)The nonmetric step is here implemented using the PAVA (Pool Adjancent Violator Algorithm) for the isotonic regressionThanksN,,480,0.84375,0.28111273792093705,25171,320.09058042985976,31.027770052838584,90.02423423781336,1574,29,617,56,unknown,NelleV,GaelVaroquaux,false,,6,1.0,22,13,828,true,true,true,false,6,4,2,4,12,0,449
303603,scikit-learn/scikit-learn,python,796,1335391573,1335431558,1335431558,666,666,github,false,false,false,28,1,0,0,1,0,1,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,5,10,0,0.0,0,,,0,0.0,0,0,false,MRG convert X to float in k_means predict Fixes issue #795Warns users about copying / converting data Should really not happen often but will prevent weird behaviour,,479,0.8434237995824635,0.28046989720998533,25079,312.6121456198413,30.623230591331392,87.88229195741457,1574,29,617,35,unknown,amueller,agramfort,false,agramfort,63,0.8888888888888888,312,24,551,true,true,true,false,142,333,1211,71,131,0,4
301472,scikit-learn/scikit-learn,python,789,1334965085,1335111651,1335111651,2442,2442,github,false,false,false,18,4,0,1,16,0,17,0,6,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,67,28,0,0.0,0,,,0,0.0,1,6,true,FIX : fix SVC pickle with callable kernel for #787@bwhite tell me if it fixes your problem,,478,0.8430962343096234,0.2554278416347382,25015,311.333200079952,30.581651009394363,87.38756745952428,1567,29,612,37,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,20,0.8,68,163,870,true,true,true,false,81,119,246,37,39,1,1
301402,scikit-learn/scikit-learn,python,787,1334954149,,1334966071,198,,unknown,false,false,false,12,1,0,0,7,0,7,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,10,0,0,0.0,0,,,0,0.0,0,4,true,Fix sklearnbaseclone so that it properly copies the kernel_function Fix for https://githubcom/scikit-learn/scikit-learn/issues/786,,477,0.8448637316561844,0.25287356321839083,25015,311.333200079952,30.581651009394363,87.38756745952428,1567,29,612,37,unknown,bwhite,bwhite,true,,2,1.0,46,2,1400,false,true,false,false,6,1,2,0,0,0,34
301380,scikit-learn/scikit-learn,python,785,1334951591,1338764198,1338764198,63543,63543,merged_in_comments,false,false,false,33,1,0,0,13,0,13,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,9,0,0,0.0,0,,,0,0.0,0,6,true,Faster confusion_matrix implementation The current implementation is unnecessarily slow O(L^2M) with the proposed implementation as O(M) (if we ignore the O(L^2) memory creation overhead) whereL: Number of labelsM: Number of inputs,,476,0.8445378151260504,0.25287356321839083,25015,311.333200079952,30.581651009394363,87.38756745952428,1567,29,612,57,unknown,bwhite,amueller,false,amueller,1,1.0,46,2,1400,false,true,false,false,4,1,1,0,0,0,72
300301,scikit-learn/scikit-learn,python,783,1334784886,1334825740,1334825740,680,680,github,false,false,false,24,4,0,10,2,0,12,0,5,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,13,0,0,0.0,0,,,0,0.0,0,2,false,Minor tweaks to the SVM documentation  * Fix up some bad-looking math * Add a link to the kernel equations in the SVC docstring,,475,0.8442105263157895,0.24713375796178344,25015,311.333200079952,30.581651009394363,87.38756745952428,1558,28,610,34,unknown,dwf,agramfort,false,agramfort,4,0.75,69,60,1141,false,true,true,true,2,5,0,2,0,0,5
300059,scikit-learn/scikit-learn,python,782,1334757444,1334759200,1334759200,29,29,github,false,false,false,13,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Added the name change for the recent change EllipticEnvelope before it gets forgotten,,474,0.8438818565400844,0.24744897959183673,25015,311.333200079952,30.581651009394363,87.38756745952428,1557,28,610,34,unknown,jaquesgrobler,agramfort,false,agramfort,12,0.9166666666666666,6,8,84,true,true,true,true,17,62,31,4,80,0,-1
93032,scikit-learn/scikit-learn,python,778,1334677201,,1351614548,282289,,unknown,false,false,false,668,5,1,10,3,0,13,0,3,1,0,0,3,1,0,0,1,0,2,3,2,0,0,142,0,313,0,4.589607993407236,0.14035156162553528,0,,sklearn/linear_model/nngarotte.py,0,0.0,0,1,false,WIP - Add a Non-negative Garotte module to sklearn Im still working on this and theres a lot to still do but I thought Id leave it open to reviewing so long seeing as Im still finding my feet with a lot of this stuffThe code still has lots of comments that will removed later and cleaned upThe __main__ function currently houses a little script that Ill turn into an example laterAll the documentation still needs to be doneId appreciate any inputs and suggestions with this :)Heres some exerts from the discussion which lead to this (*from the Scikit-learn mailing list*)From **Alexandre Gramfort**hias soon as we have Immanuels branch with positive lasso [1] merged wecould have a non-negativeGarotte in the scikit A quick gist (hopefully not too buggy):https://gistgithubcom/2351057Feed back welcome and if someone is willing to cleanly merge this …AlexFrom **Jaques Grobler**Heres a wee summary on the non-negative garrote (NG) i pieced together:The original non-negative garrote from Breiman (1995) is basically a scaled version of the least square estimateBasically take a OLS estimator and then shrink that estimator to obtain a more sparse representationThe shrinkage is done by multiplying the OLS estimator by some shrinkage factor say dwhich is found by minimising the sum of square residuals under the restriction that the ds are positiveand that some are bound by a certain shrinkage parameterThe algorithm proposed in this paper is rather similar to that of the  Lars LASSO but with a complicatingfactor being a non-negative constraint on the shrinkage factor (See eq (2) in this paper)Once youve computed  your shrinkage factor you basically have your regression coefficientsseeing as your NG coefficient  shrinkage factor * regression coefficientHe showed it to be a stable selection method and often outperforms its competitors likesubset regression and ridge regressionThe solution path of the NG is piece-wise linear and its whole path can be computed quicklyIt is also path-consistent (A solution that contains at least one desirable estimate) given an appropriate initial estimate The path-consistency of the NG is highlighted to be in contrast to the fact that the LASSO is not always path consistent (Peng Zhao & Hui Zou personal communication) It is argued that the NG has the ability to turna consistent estimate into an estimate that is both consistent in terms of estimation and in terms of variable selectionA drawback is the NGs explicit reliance on the full least square estimate as a small sample size may cause it to perform poorly - however a ridge regression is suggested as an initial estimate for defining the NG estimate insteadof the least square estimateFrom **Jaques Grobler**Appart from trying it ourselves to see how it fares herere some findings:The simulations that are done in the last mentioned paper by Ming Yuan and Yi Linhttp://www2isyegatechedu/statistics/papers/05-25pdf  they find that the NG seems to dogenerally better than the LASSO (figure 1)Their second simulations they consider the 4 different models used in the original LASSO paper (Tibshirani 1996)which they use to compare the NG with several other popular methods inluding the LASSOThe results are shown in Table 2 of the above-mentioned paper from which the NG does very well often outperforming the other models and being the most successful in variable selectionThey also include one real example using the prostate cancer dataset from Stamey (1989) - the results of whichthey use to confirm the theory that the path consistency of LASSO depends on the correlationo of the design matrix whilst that of the NG is always path consistentThought it may be of interestfrom **Alexandre Gramfort**if you can reproduce this figure 1 using my gistit would be a sufficient argument for adding this estimator to the scikit,,473,0.8456659619450317,0.24523506988564167,25030,311.14662405113864,30.563324011186577,87.33519776268479,1552,28,609,82,unknown,jaquesgrobler,jaquesgrobler,true,,11,1.0,6,8,83,true,true,false,false,15,61,30,0,80,0,30
299079,scikit-learn/scikit-learn,python,776,1334599607,1338754111,1338754111,69241,69241,github,false,false,false,60,17,0,2,33,0,35,0,3,0,0,0,7,0,0,0,0,0,7,7,5,0,0,0,0,152,2,0,0.0,0,,,0,0.0,0,7,false,MRG Normalized mutual information This is a supervised clustering metric that is quite similar to adjusted mutual information in some sense but is a bit more well-known (1000+ citations vs 100+ citations)Still needs narrative and maybe inclusion in one of the examplesI prefer to use this for my paper instead of AMI Do you think its worth including,,472,0.8453389830508474,0.24873737373737373,25457,319.2834976627254,30.875594139136584,89.2878186746278,1547,29,608,62,unknown,amueller,amueller,true,amueller,62,0.8870967741935484,309,24,542,true,true,false,false,137,361,1078,90,127,0,26
308466,scikit-learn/scikit-learn,python,775,1334505000,1336318607,1336318607,30226,30226,merged_in_comments,false,false,false,92,3,0,0,15,2,17,0,4,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,49,110,0,0.0,0,,,0,0.0,0,5,false,MRG: Radius-based classifier: raise exception if no neighbors found Previously radius based classifier continued execution and crashed later see issue # 760 Now it is better so user will know why program crashesIt would be even better if it could print in the error message: what is the minimum radius so that all samples have at least 1 neighbor I dont know how it should be done one possibility is using brute force like in RadiusNeighborsMixinradius_neighbors() but I guess if there are lots of samples it can take long to calculate,,471,0.8450106157112527,0.2521957340025094,25006,310.96536831160523,30.512676957530193,87.17907702151484,1544,28,607,55,unknown,joonazzz,GaelVaroquaux,false,GaelVaroquaux,0,0,0,0,8,false,false,false,false,2,1,0,0,0,0,37
298003,scikit-learn/scikit-learn,python,773,1334344603,1334345164,1334345164,9,9,github,false,false,false,26,2,0,0,0,0,0,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,121,0,0,0.0,0,,,0,0.0,0,3,true,MRG pre_dispatch for foresters This adds a pre_dispatch option to the forests in the ensemble moduleBasically the same as in GridSearchNecessary for large datasets,,470,0.8446808510638298,0.25492610837438423,24991,311.1520147253011,30.53099115681645,87.23140330518987,1537,28,605,35,unknown,amueller,agramfort,false,agramfort,61,0.8852459016393442,309,24,539,true,true,true,false,134,360,1041,89,134,3,-1
89063,scikit-learn/scikit-learn,python,772,1334339802,1347148375,1347148375,213476,213476,commits_in_master,false,false,false,20,16,0,3,18,0,21,0,5,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,306,138,0,0.0,0,,,0,0.0,0,7,true,rfepy: Added proper support of warm_start to rfe As discussed on the mailing list and as listed on issue 769,,469,0.8443496801705757,0.25766871165644173,25006,310.96536831160523,30.512676957530193,87.17907702151484,1537,28,605,82,unknown,conradlee,amueller,false,amueller,2,1.0,4,0,749,true,true,false,false,5,6,1,0,1,0,6
297915,scikit-learn/scikit-learn,python,770,1334335145,1334578336,1334578336,4053,4053,github,false,false,false,144,5,0,4,12,0,16,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,52,0,0,0.0,0,,,0,0.0,0,6,true,MRG grid_search forgets estimators This should address #565 in the least intrusive wayIt is still possible to set refitFalse but then it is not possible to use predict or best_estimator_Now best_estimator_ is a property that returns the best estimator when fit was called with refitTrue and raises an appropriate error if notThis should keep the API as consistent as possible It only breaks if someone used the best estimator without refitting - which I dont feel is a very good idea any way And at least it gives sensible feedbackMy reasoning was that maybe someone has a big dataset and uses GridSearch with ShuffleSplit and a low train_size Then they might not want to fit to the whole datasetThis option is now available without really changing anything for the average userStill need to document the changes in whatsnew,,468,0.844017094017094,0.2594859241126071,24991,311.1520147253011,30.53099115681645,87.23140330518987,1537,28,605,36,unknown,amueller,amueller,true,amueller,60,0.8833333333333333,309,24,539,true,true,false,false,133,354,1038,88,133,3,3
297780,scikit-learn/scikit-learn,python,767,1334314026,1334314929,1334314929,15,15,github,false,false,false,29,4,0,2,2,0,4,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,26,10,0,0.0,0,,,0,0.0,1,0,true,MRG: Fix issue 762 Fixes the SGDRegressor crash reported in https://githubcom/scikit-learn/scikit-learn/issues/762 @mblondel: It would be great if you could review the modifications I did to SGDClassifier_partial_fit and SGDClassifierfit,,467,0.8436830835117773,0.2618757612667479,24989,309.17603745648086,30.253311457041097,87.35843771259354,1537,28,605,33,unknown,pprett,pprett,true,pprett,19,0.8947368421052632,58,24,983,true,true,false,false,20,49,117,35,59,0,3
297690,scikit-learn/scikit-learn,python,766,1334292996,1336256427,1336256427,32723,32723,merged_in_comments,false,false,false,87,1,0,0,7,0,7,0,5,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,12,0,0,0.0,0,,,0,0.0,0,3,true,BUG: fix atlas/blas detection ATLAS detection in numpy is not necessarily reliable and does not always work (eg see Gentoos patches [12])Removing the (NO_ATLAS_INFO 1) in blas_infoget(define_macros [])) part enables sklearn to be properly compiled with ATLAS support on Gentoo & Gentoo Prefix (ie Gentoo in non-root user space)Is this line just copied from numpys setup Does it have a purpose for sklearn If not then I believe we should merge this PR Of course I am open to discussion -)[1] http://gpozugainaorg/AJAX/Ebuild/2461610/View[2] https://githubcom/npinto/sekyfsr-gentoo-overlay/blob/master/dev-python/numpy/numpy-161-r1ebuild#L79,,466,0.8433476394849786,0.2630937880633374,24977,309.3245786123233,30.26784641870521,87.40040837570565,1537,28,605,51,unknown,npinto,npinto,true,npinto,5,1.0,57,39,1212,false,true,false,false,11,14,2,4,0,1,224
297551,scikit-learn/scikit-learn,python,763,1334269757,1334308266,1334308267,641,641,github,false,false,false,22,4,0,6,6,0,12,0,3,0,0,0,9,0,0,0,1,0,9,10,10,0,0,0,0,170,14,0,0.0,0,,,0,0.0,0,1,false,ENH: add verbose option to LinearSVC This PR enable the verbose option of LibLinear Mostly adapted from the work done with LibSVM,,465,0.843010752688172,0.2630937880633374,24977,309.3245786123233,30.26784641870521,87.40040837570565,1537,29,604,34,unknown,npinto,GaelVaroquaux,false,GaelVaroquaux,4,1.0,57,39,1211,false,true,false,false,8,12,1,0,0,0,1
297261,scikit-learn/scikit-learn,python,761,1334228632,1334245189,1334245189,275,275,github,false,false,false,44,3,0,0,8,0,8,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,45,0,0,0.0,0,,,0,0.0,0,4,false,MRG: Parallel npargsort in sklearnensembleforest This is just a very small PR to speed up the parallel computation of a forest of treesThe idea is to compute X_argsort in parallel by dispatching the features to sort to different processesTests still all pass,,464,0.8426724137931034,0.2616136919315403,24961,309.5228556548215,30.28724810704699,87.45643203397299,1536,29,604,33,unknown,glouppe,glouppe,true,glouppe,19,1.0,66,19,518,true,true,false,false,9,16,70,10,7,0,34
293760,scikit-learn/scikit-learn,python,757,1333586402,1335112958,1335112958,25442,25442,merged_in_comments,false,false,false,87,8,0,1,19,0,20,0,5,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,245,75,0,0.0,0,,,0,0.0,0,2,false,ENH: train_size and test_size in ShuffleSplit (#721) #721* I have deprecated arguments test_fraction and train_fraction in ShuffleSplit and train_test_split and renamed them to test_size and train_size * I have also modified documentation and tests (added one more to test deprecation warning) * I had to rewrite the argument checking a little to take in account that these values can now be integers The deprecation warning is: test_fraction is deprecated in 011 use test_size instead Is it correct I am not sure which version should be there,,463,0.8423326133909287,0.23915737298636927,24892,310.6218865498955,30.371203599550054,87.49799132251326,1515,29,596,45,unknown,davidmarek,GaelVaroquaux,false,GaelVaroquaux,2,1.0,7,4,1227,true,true,false,false,6,11,7,0,7,0,41
293660,scikit-learn/scikit-learn,python,756,1333574627,1334589194,1334589194,16909,16909,github,false,false,false,10,7,0,3,24,0,27,0,4,0,0,0,5,0,0,0,0,0,5,5,4,0,0,0,0,187,36,0,0.0,0,,,0,0.0,0,6,false,MRG re-allow zero-based indexes in SVMlight files Fixes issue #750,,462,0.841991341991342,0.23945409429280398,24892,303.5111682468263,29.72842680379238,86.09191708179335,1515,29,596,41,unknown,larsmans,larsmans,true,larsmans,37,0.7297297297297297,62,27,626,true,true,false,false,40,73,126,30,42,0,74
293427,scikit-learn/scikit-learn,python,754,1333549074,1335123679,1335123679,26243,26243,merged_in_comments,false,false,false,169,12,0,1,28,0,29,0,4,0,0,0,7,0,0,0,0,0,7,7,6,0,0,0,0,228,35,0,0.0,0,,,0,0.0,0,5,false,WIP: Changed GMMs API to suite rest of sklearn - parameters not to be set via fit anymore- parameters are to be set in __init__- previous way of doing it set as deprecated with warningNotes: * fixed three docstrings to match new signature * no errors/failures during make * Examples that still use fit in the old way will still  work - must still fix * must still update the documentation on this change-----------------It seems to be working well and passes all the testsThe examples still work - these Ill still change to match this update if you guys are all happy with it - but its still compatible with themPlease let me know if you can advise on whether Ive done this correctly or if you have suggestions on how to improveOnce everyone is happy with it  ill fix up the examples too and get going on updating the docsJust want some confirmation that code-wise its correctRegardsJ,,461,0.841648590021692,0.23850931677018633,24892,303.5111682468263,29.72842680379238,86.09191708179335,1514,29,596,45,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,10,1.0,6,8,70,true,true,true,false,14,52,24,0,58,0,1196
292787,scikit-learn/scikit-learn,python,751,1333459457,1333471511,1333471512,200,200,github,false,false,false,14,1,0,0,4,0,4,0,2,0,0,0,1,0,0,0,1,0,1,2,1,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Removed _plot from the face recognition example updated tutorial page to use static image,,460,0.841304347826087,0.22832722832722832,24889,303.266503274539,29.6516533408333,85.98175901000442,1512,29,595,34,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,9,1.0,6,8,69,true,true,true,false,13,51,23,0,58,0,1
291955,scikit-learn/scikit-learn,python,748,1333305173,1333580258,1333580258,4584,4584,github,false,false,false,41,2,0,0,3,0,3,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,291,0,0.0,0,,,0,0.0,0,0,false,MRG: refactored the HMM tests to ease PY3K transition Here is a refactoring of the HMM test suite to simplify the inheritance hierarchy and that uses setUp method extensively instead of class scoped statements that do not work on python 3,,459,0.840958605664488,0.20176405733186328,24836,302.78627798357223,29.63440167498792,85.88339507167015,1506,29,593,37,unknown,ogrisel,ogrisel,true,ogrisel,28,0.7857142857142857,489,114,1040,true,false,false,false,154,437,357,58,129,0,5
291722,scikit-learn/scikit-learn,python,747,1333227749,1334135167,1334135167,15123,15123,github,false,false,false,81,19,0,5,24,0,29,0,4,0,0,0,10,0,0,0,1,1,9,11,7,0,0,0,0,345,45,0,0.0,0,,,0,0.0,0,8,false,positivity constraint for lasso and elastic net Hallo please have a look at my changes Currently the warning Objective did not converge is always issued Im not sure why but I suspect some issue with the dual gap calculation Im also not sure of the theoretical justification of just skipping updates that make the coefficients negative I would have expected that the constraint gets somehow included in the objective function via a penalty Fortunately the plot suggests that the approach works,,458,0.8406113537117904,0.20446927374301677,24836,302.78627798357223,29.63440167498792,85.88339507167015,1504,29,592,38,unknown,ibayer,agramfort,false,agramfort,4,1.0,2,0,28,true,true,false,false,6,14,11,1,9,0,50
291620,scikit-learn/scikit-learn,python,742,1333202664,1333617361,1333617361,6911,6911,github,false,false,false,61,7,0,0,9,0,9,0,4,0,0,0,7,0,0,0,0,0,7,7,5,0,0,0,0,176,85,0,0.0,0,,,0,0.0,0,4,false,minkowski p-distance in sklearnneighbors Issue #351I have added new value p to classes in sklearnneighbors to support arbitrary Minkowski metrics for searches For p1 and p2 sklearn implementations of manhattan and euclidean distances are used For other values the minkowski distance from scipy is usedI have also modified tests to check if the distances are same for all algorithms,,457,0.8402625820568927,0.1875681570338059,24753,302.83197996202483,29.57217306993092,85.76738173150729,1504,29,592,39,unknown,davidmarek,ogrisel,false,ogrisel,1,1.0,7,4,1223,true,true,false,false,4,5,3,0,2,0,296
291396,scikit-learn/scikit-learn,python,741,1333145363,1333388578,1333388578,4053,4053,github,false,false,false,47,1,0,4,4,0,8,0,3,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,31,20,0,0.0,0,,,0,0.0,1,3,true,MRG: deterministic vocabulary_ for DictVectorizer @larsmans I changed the default behavior of the DictVectorizer class to make sure that it generates a deterministic vocabulary The old & faster behavior is still possible with sort_featureFalseAlso I rewrote a bit the narrative documentation to make it more noob-friendly,,456,0.8399122807017544,0.18230277185501065,24836,302.78627798357223,29.63440167498792,85.88339507167015,1504,29,591,36,unknown,ogrisel,larsmans,false,larsmans,27,0.7777777777777778,489,114,1038,true,false,true,true,152,434,356,48,151,1,1133
290849,scikit-learn/scikit-learn,python,739,1333062460,1333374096,1333374097,5193,5193,github,false,false,false,90,17,7,13,1,0,14,0,2,3,0,7,11,8,0,0,3,0,8,11,9,0,0,836,0,3033,73,50.136331495094744,1.5486015786271334,67,scottblanc@scottblanc.visibletech.net,sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/setup.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/ensemble/gradient_boosting.py|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/_gradient_boosting.c|sklearn/ensemble/_gradient_boosting.pyx|sklearn/ensemble/gradient_boosting.py|examples/ensemble/plot_gradient_boosting_regression.py|examples/ensemble/plot_gradient_boosting_regularization.py,45,0.00816326530612245,1,0,false,MRG: Gradient boosting Speed Enhancement Improved test time performance for GBRT in particular for predicting single data points For single data points its about 5x faster than the original code for batches its about 2x fasterSummary:  - estimators_ is now a 2d array of dtype object  - I added an extension module (_gradient_boosting) which contains two utility functions (one for fast regression tree prediction and one for iterating over the estimators_ array)  - I added a staged_predict method to iterate over the predictions at each stage (stolen from @ndawe),,455,0.8395604395604396,0.17448979591836736,24753,302.83197996202483,29.57217306993092,85.76738173150729,1501,29,590,36,unknown,pprett,pprett,true,pprett,18,0.8888888888888888,58,24,968,true,true,false,false,23,63,84,30,53,0,2061
289676,scikit-learn/scikit-learn,python,736,1332891170,1332949309,1332949309,968,968,github,false,false,false,23,1,0,0,3,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,92,0,0,0.0,0,,,0,0.0,0,2,false,BENCHMARK covertype: select classifier via cmd line opt As discussed on the mlIt should be uncontroversial Im waiting one +1 to merge,,454,0.8392070484581498,0.17307692307692307,24698,303.30391124787434,29.6380273706373,85.95837719653414,1493,29,588,31,unknown,paolo-losi,paolo-losi,true,paolo-losi,3,1.0,5,1,693,true,true,false,false,5,4,0,1,1,0,867
69967,scikit-learn/scikit-learn,python,734,1332867365,,1353523825,344274,,unknown,false,false,false,61,5,0,15,22,1,38,0,6,0,0,0,3,0,0,0,2,0,3,5,3,0,0,0,0,554,0,0,0.0,0,,,0,0.0,0,18,false,WIP: diffusion map embedding this is a very early PR for diffusion map embedding into scikits-learn (started incorporating the basic matrix functions)i would really like some advice as to where this should go:it seems it could go with affinity_propagation as a pure matrix based method or it could go with the manifold learning (which is where it is now),,453,0.8410596026490066,0.17307692307692307,25079,312.6121456198413,30.623230591331392,87.88229195741457,1490,29,588,104,unknown,satra,satra,true,,7,1.0,52,2,800,true,true,false,false,13,14,3,1,2,0,44
289384,scikit-learn/scikit-learn,python,733,1332862232,1332869112,1332869112,114,114,github,false,false,false,16,1,0,0,2,0,2,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Fixed broken image link Fixed wrong path of one of the examples in the tutorial section,,452,0.8407079646017699,0.17307692307692307,24699,303.2916312401312,29.555852463662497,86.07635936677599,1490,29,588,32,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,8,1.0,6,8,62,true,true,true,false,12,47,22,0,58,0,114
289030,scikit-learn/scikit-learn,python,731,1332804127,1332889122,1332889123,1416,1416,github,false,false,false,109,1,1,0,7,0,7,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.332954636975779,0.1338325465028545,34,satra@mit.edu,sklearn/naive_bayes.py,34,0.034482758620689655,0,7,false,Fixed a bug in GaussianNB (naive_bayespy) In GaussianNB the computation of the class priors was:    selfclass_prior_[i]  npfloat(npsum(y  y_i)) / n_classesWhich is wrong Finding the probability of the class should be using    selfclass_prior_[i]  npfloat(npsum(y  y_i)) / n_sampleswith n_samples simply be:    n_samples n_features  XshapeIve fixed this issue in the code The effect of this bug can be quite significant because instead of probability (a value between 0 and 1) the class_prior_ is a (possible) large number (much larger than the probabilities of the different features) thus it can bias the results towards the class priors with little value assigned to the actual features ,,451,0.8403547671840355,0.17139959432048682,24693,303.3653262058073,29.563034058235125,86.09727453124367,1488,29,587,32,unknown,udi,pprett,false,pprett,0,0,0,0,1196,false,true,false,false,0,0,0,0,0,0,1
288817,scikit-learn/scikit-learn,python,730,1332782443,1332785297,1332785297,47,47,github,false,false,false,30,2,0,0,8,0,8,0,4,0,0,0,5,0,0,0,0,0,5,5,5,0,0,0,0,24,4,0,0.0,0,,,0,0.0,0,0,false,Made old EllipticEnvelop deprecated class -renamed to **EllipticEnvelope**-Examples updatedPlease have a look if I did this correctly Seems fine on my side- for Issue #722regardsJ,,450,0.84,0.16902834008097167,24689,303.41447608246585,29.567823727165944,86.11122362185588,1488,29,587,30,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,7,1.0,6,7,61,true,true,true,false,11,46,21,0,58,0,1
288808,scikit-learn/scikit-learn,python,729,1332781259,1334506865,1334506865,28760,28760,github,false,false,false,31,1,0,0,10,0,10,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,8,10,0,0.0,0,,,0,0.0,0,1,false,WIP fit_predict convenience method on KMeans and MiniBatchKMeans Doing fit and predict on the same set makes sense for clustering algorithms so heres a method that does both in one go,,449,0.8396436525612472,0.1691995947315096,25006,310.96536831160523,30.512676957530193,87.17907702151484,1488,29,587,45,unknown,larsmans,larsmans,true,larsmans,36,0.7222222222222222,61,27,617,true,true,false,false,37,67,15,24,38,0,10
288757,scikit-learn/scikit-learn,python,728,1332775059,1332777641,1332777641,43,43,github,false,false,false,6,3,0,3,1,0,4,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,10,17,0,0.0,0,,,0,0.0,0,0,false,fix: convert input arrays to float ,,448,0.8392857142857143,0.1691995947315096,24689,303.33346834622705,29.567823727165944,86.11122362185588,1488,29,587,29,unknown,satra,agramfort,false,agramfort,6,1.0,52,2,799,true,true,false,true,12,13,2,0,1,0,5
288486,scikit-learn/scikit-learn,python,726,1332717604,1332717679,1332717679,1,1,github,false,false,false,6,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,18,0,0,0.0,0,,,0,0.0,0,0,false,ENH: a few typos in docstrings ,,447,0.8389261744966443,0.1691995947315096,23981,312.2888953755056,30.440765606104833,88.65351736791627,1488,29,586,30,unknown,emmanuelle,GaelVaroquaux,false,GaelVaroquaux,3,0.6666666666666666,25,6,692,false,true,false,false,2,4,1,0,0,0,-1
287299,scikit-learn/scikit-learn,python,720,1332467856,1332495451,1332495451,459,459,github,false,false,false,84,1,0,1,0,0,1,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,38,0,0,0.0,0,,,0,0.0,0,0,false,#685 combat for sp_linalglsqr #685Is this going in the right directionIm not sure what the best name for the function isThere is one additional problem:scipysparselinalglsqr soves Ax  b or min ||b - Ax||^2 or min ||Ax - b||^2 + d^2 ||x||^2while scipysparselinalgspsolve solves only Axb I dont see how we could support the none square case for scipy 07Right now the test would fall for scipy 07 with:ValueError: matrix must be square (has shape (100 10)),,446,0.8385650224215246,0.17065868263473055,23986,310.9313766363712,30.392729091970317,88.42658217293422,1481,31,583,34,unknown,ibayer,mblondel,false,mblondel,3,1.0,2,0,19,true,true,false,false,5,14,10,1,9,0,454
286286,scikit-learn/scikit-learn,python,717,1332361589,1332641568,1332641568,4666,4666,github,false,false,false,45,2,2,0,8,0,8,0,4,0,0,2,2,1,0,1,0,0,2,2,1,0,1,12,0,12,0,13.949963537479807,0.4549223562475482,32,satra@mit.edu,doc/themes/scikit-learn/static/sidebar.js|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/sidebar.js,32,0.031496062992125984,0,1,false,fix for Issue715 - dev website format issue Fixes the problem with the **Next** **Prev** and **Up** buttons freaking out on some machines with the collapsible sidebar-button ( #715 )-small tweak: changes buttons colour to darker shade of gray when mouse hovers over it,,445,0.8382022471910112,0.17618110236220472,23588,309.8609462438528,30.43920637612345,87.79888078684077,1478,31,582,38,unknown,jaquesgrobler,ogrisel,false,ogrisel,6,1.0,6,6,56,true,true,true,true,9,41,20,0,58,0,1
284650,scikit-learn/scikit-learn,python,714,1332255704,1332388777,1332388777,2217,2217,github,false,false,false,97,5,2,0,54,0,54,0,5,0,0,2,2,0,1,1,0,0,2,2,0,1,1,0,0,0,0,13.378602474735082,0.43628969672599566,50,satra@mit.edu,doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/layout.html,34,0.02913453299057412,0,7,false,Next button This addon puts a fixed next-page-button in the bottom-right corner of the screenIts purpose - Whilst presenting a tutorial or working through documentation upon reaching the end of a page one has to tediously scroll back to the top in order to choose *next* from the sidebarThis will thus allow one to work through the tutorials current page and when the bottom of the page is reached simply hit the *next* button to move ahead Ill be adding a online-html-build shortlyI hope you find this useful**Update**: Online html-build available here: http://jaquesgroblergithubcom/Next_button-online/installhtml,,444,0.8378378378378378,0.21165381319622964,23588,309.8609462438528,30.43920637612345,87.79888078684077,1474,31,581,38,unknown,jaquesgrobler,ogrisel,false,ogrisel,5,1.0,6,6,55,true,true,true,true,7,23,19,0,56,0,10
283955,scikit-learn/scikit-learn,python,712,1332164255,1332184005,1332184005,329,329,github,false,false,false,10,5,0,4,9,0,13,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,9,42,0,0.0,0,,,0,0.0,0,3,false,Fix y centering with multiple y see #708 see:https://githubcom/scikit-learn/scikit-learn/issues/708,,443,0.837471783295711,0.23266563944530047,23588,308.4619297948109,30.31202306257419,87.41733084619298,1470,34,580,38,unknown,agramfort,mblondel,false,mblondel,19,0.7894736842105263,66,163,838,true,true,true,true,94,113,113,32,79,0,4
283736,scikit-learn/scikit-learn,python,711,1332121432,1332169972,1332169973,809,809,github,false,false,false,101,2,0,0,5,0,5,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,13,0,0.0,0,,,0,0.0,0,0,false,fixed SparsePCAtransform returning NaN for 0 feature in all samples (fixes #615) I fixed the division (#615) by first checking if there are any zeros in the denominator This error happens only when all samples have output feature which is always 0 I set all zeros to 1 and the resulting division then will be 0/1 which yields the correct value: 0I know there are many ways how to fix this bug At first I tried to replace the NaNs after the division but numpy would print RuntimeWarning and I think it would be bad idea to turn them off ,,442,0.8371040723981901,0.23712121212121212,23582,308.5404121787804,30.319735391400222,87.43957255533883,1469,33,579,40,unknown,davidmarek,amueller,false,amueller,0,0,7,4,1210,false,true,false,false,2,2,0,0,0,0,3
54210,scikit-learn/scikit-learn,python,710,1332114001,,1349993297,297988,,unknown,false,true,false,25,7,0,26,21,0,47,0,8,0,0,0,3,0,0,0,2,0,3,5,3,0,0,0,0,269,75,0,0.0,0,,,0,0.0,0,8,false,WIP Factor analysis Implements factor analysis following Bishops bookTodo:- less naive optimization- better stopping criterion- narrative doc + example- tests,,441,0.8390022675736961,0.23712121212121212,23582,308.5404121787804,30.319735391400222,87.43957255533883,1469,33,579,107,unknown,amueller,amueller,true,,59,0.8983050847457628,289,22,513,true,true,false,false,159,394,885,56,214,8,3
54172,scikit-learn/scikit-learn,python,709,1332111752,1332378759,1332378759,4450,4450,github,false,false,false,54,1,0,0,4,0,4,0,2,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,43,0,0,0.0,0,,,0,0.0,0,1,false,cleaned some examples #680 #680In the new version I moved some of the examples and used coef in plot_lasso_and_elasticnetpy Can you see if you can get the coefficients from the dataset generatorCurrently the make_sparse_uncorrelated dataset generator cant return coef I will look into it to see if this can be changed easily,,440,0.8386363636363636,0.23712121212121212,23582,308.5404121787804,30.319735391400222,87.43957255533883,1469,33,579,39,unknown,ibayer,GaelVaroquaux,false,GaelVaroquaux,2,1.0,2,0,15,true,true,false,false,4,12,9,0,9,0,3
53950,scikit-learn/scikit-learn,python,707,1332098129,1332694207,1332694207,9934,9934,github,false,false,false,41,4,0,3,9,0,12,0,3,0,0,0,3,0,0,0,0,0,3,3,2,0,1,0,0,68,20,0,0.0,0,,,0,0.0,0,1,false,MRG Graphviz dot refactoring This PR tries to simplify the dot export in the tree moduleMy refactored version is shorter and produces shorter dot files (internal nodes are not duplicated any more)Also I feel it is easier to understand,,439,0.8382687927107062,0.23684210526315788,23482,307.72506600800614,30.15075376884422,86.91763904267098,1469,33,579,37,unknown,amueller,amueller,true,amueller,58,0.896551724137931,289,22,513,true,true,false,false,159,381,886,56,214,8,32
53004,scikit-learn/scikit-learn,python,706,1332010875,1333636373,1333636373,27091,27091,commit_sha_in_comments,false,false,false,122,1,0,1,2,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,72,0,0.0,0,,,0,0.0,0,0,false,Should there be many assert_*() in utilstesting I saw a lonely nonstandard assert_in() in utilstesting that -I was wongfully led to think- could be have been used instead of assert_true in https://githubcom/scikit-learn/scikit-learn/commit/7af5f6a549485b745edf7e8ddd1fd065de1ea51cby default assert_true() gives a quite poor output when things failIn this pull request I renamed assert_in to something else (assert_contained) I conditionnally used nosetools in the case where it is available to provide assert_inId like to spawn a discussion about the choice between * the direct -and quite natural- usage of nosetools in the lowest version accepted in dependencies* re implementation (and usage) of whats useful If the latter is prefered wed need small refactors in order to always import from sklearns util/testingpy instead of nosetools,,438,0.8378995433789954,0.24153166421207659,23480,307.7512776831346,30.153321976149915,86.83986371379898,1467,33,578,49,unknown,feth,larsmans,false,larsmans,2,0.5,11,4,433,false,true,false,false,3,2,1,0,0,0,1671
52887,scikit-learn/scikit-learn,python,705,1332002037,1332380720,1332380720,6311,6311,github,false,false,false,49,2,0,3,3,0,6,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,13,13,0,0.0,0,,,0,0.0,1,0,false,FIX : ICA with small n_components and whitenFalse (fix for #697) here is a fix for #697 (this code is not super clean but at least it does not break anymore)I am wondering if PCA should not be used to whiten the data@GaelVaroquaux please take a look,,437,0.8375286041189931,0.24135393671817512,23588,309.8609462438528,30.43920637612345,87.79888078684077,1467,33,578,40,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,18,0.7777777777777778,66,163,836,true,true,true,false,97,117,118,24,87,0,5349
51796,scikit-learn/scikit-learn,python,704,1331928872,1332491433,1332491433,9376,9376,github,false,false,false,28,3,1,0,9,0,9,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,22,0,22,41,4.276834976548318,0.13947189464588267,57,vlad@vene.ro,sklearn/cross_validation.py,57,0.04163623082542001,0,5,true,KFold has now an option to shuffle the data closes #703Ive added an option to KFold to shuffle the data The default is to NoneThanksN,,436,0.8371559633027523,0.2483564645726808,23588,309.8609462438528,30.43920637612345,87.79888078684077,1466,34,577,39,unknown,NelleV,mblondel,false,mblondel,5,1.0,22,13,788,true,true,false,false,10,7,1,4,2,0,179
50797,scikit-learn/scikit-learn,python,702,1331873187,,1374752456,714654,,unknown,false,false,false,134,29,17,0,21,0,21,0,6,1,0,184,210,184,0,0,1,0,209,210,209,0,0,2025,477,2372,1136,919.1691415381509,28.147389881220274,2750,wardefar@iro.umontreal.ca,sklearn/utils/__init__.py|sklearn/metrics/pairwise.py|sklearn/cross_validation.py|sklearn/datasets/mldata.py|sklearn/lda.py|sklearn/naive_bayes.py|sklearn/qda.py|sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/utils/validation.py|sklearn/ensemble/forest.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_text.py|sklearn/linear_model/tests/test_sgd.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/cross_validation.py|sklearn/neighbors/base.py|sklearn/tests/test_naive_bayes.py|sklearn/decomposition/pca.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/stochastic_gradient.py|sklearn/mixture/gmm.py|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/unsupervised.py|sklearn/svm/classes.py|sklearn/svm/sparse/classes.py|sklearn/cross_validation.py|doc/modules/feature_extraction.rst|examples/applications/face_recognition.py|examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_species_distribution_modeling.py|examples/applications/plot_stock_market.py|examples/applications/plot_tomography_l1_reconstruction.py|examples/applications/svm_gui.py|examples/applications/topics_extraction_with_nmf.py|examples/applications/wikipedia_principal_eigenvector.py|examples/cluster/plot_adjusted_for_chance_measures.py|examples/cluster/plot_affinity_propagation.py|examples/cluster/plot_cluster_comparison.py|examples/cluster/plot_color_quantization.py|examples/cluster/plot_dbscan.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/cluster/plot_kmeans_digits.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/cluster/plot_lena_segmentation.py|examples/cluster/plot_lena_ward_segmentation.py|examples/cluster/plot_mean_shift.py|examples/cluster/plot_mini_batch_kmeans.py|examples/cluster/plot_segmentation_toy.py|examples/cluster/plot_ward_structured_vs_unstructured.py|examples/covariance/plot_covariance_estimation.py|examples/covariance/plot_lw_vs_oas.py|examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_outlier_detection.py|examples/covariance/plot_robust_vs_empirical_covariance.py|examples/covariance/plot_sparse_cov.py|examples/decomposition/plot_faces_decomposition.py|examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_image_denoising.py|examples/decomposition/plot_kernel_pca.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_sparse_coding.py|examples/document_classification_20newsgroups.py|examples/document_clustering.py|examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_forest_importances_faces.py|examples/ensemble/plot_forest_iris.py|examples/feature_selection_pipeline.py|examples/gaussian_process/gp_diabetes_dataset.py|examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.py|examples/gaussian_process/plot_gp_regression.py|examples/grid_search_digits.py|examples/grid_search_text_feature_extraction.py|examples/linear_model/lasso_dense_vs_sparse_data.py|examples/linear_model/plot_ard.py|examples/linear_model/plot_bayesian_ridge.py|examples/linear_model/plot_lasso_and_elasticnet.py|examples/linear_model/plot_lasso_coordinate_descent_path.py|examples/linear_model/plot_lasso_lars.py|examples/linear_model/plot_lasso_model_selection.py|examples/linear_model/plot_logistic_l1_l2_sparsity.py|examples/linear_model/plot_logistic_path.py|examples/linear_model/plot_ols.py|examples/linear_model/plot_omp.py|examples/linear_model/plot_polynomial_interpolation.py|examples/linear_model/plot_ridge_path.py|examples/linear_model/plot_sgd_iris.py|examples/linear_model/plot_sgd_loss_functions.py|examples/linear_model/plot_sgd_ols.py|examples/linear_model/plot_sgd_penalties.py|examples/linear_model/plot_sgd_separating_hyperplane.py|examples/linear_model/plot_sgd_weighted_classes.py|examples/linear_model/plot_sgd_weighted_samples.py|examples/linear_model/plot_sparse_recovery.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_swissroll.py|examples/mixture/plot_gmm_classifier.py|examples/mixture/plot_gmm_selection.py|examples/mixture/plot_gmm_sin.py|examples/mlcomp_sparse_document_classification.py|examples/neighbors/plot_classification.py|examples/neighbors/plot_regression.py|examples/plot_classification_probability.py|examples/plot_confusion_matrix.py|examples/plot_digits_classification.py|examples/plot_feature_selection.py|examples/plot_hmm_stock_analysis.py|examples/plot_kernel_approximation.py|examples/plot_lda_qda.py|examples/plot_multilabel.py|examples/plot_permutation_test_for_classification.py|examples/plot_pls.py|examples/plot_precision_recall.py|examples/plot_random_dataset.py|examples/plot_rfe_digits.py|examples/plot_rfe_with_cross_validation.py|examples/plot_roc.py|examples/plot_roc_crossval.py|examples/plot_train_error_vs_test_error.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|examples/svm/plot_custom_kernel.py|examples/svm/plot_iris.py|examples/svm/plot_oneclass.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_separating_hyperplane.py|examples/svm/plot_separating_hyperplane_unbalanced.py|examples/svm/plot_svm_anova.py|examples/svm/plot_svm_nonlinear.py|examples/svm/plot_svm_parameters_selection.py|examples/svm/plot_svm_regression.py|examples/svm/plot_weighted_samples.py|examples/tree/plot_iris.py|examples/tree/plot_tree_regression.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/robust_covariance.py|sklearn/cross_validation.py|sklearn/datasets/base.py|sklearn/datasets/olivetti_faces.py|sklearn/datasets/samples_generator.py|sklearn/datasets/species_distributions.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_lfw.py|sklearn/datasets/twenty_newsgroups.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/fastica_.py|sklearn/decomposition/nmf.py|sklearn/decomposition/tests/test_nmf.py|sklearn/ensemble/forest.py|sklearn/externals/six.py|sklearn/feature_extraction/image.py|sklearn/feature_extraction/tests/test_text.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/hmm.py|sklearn/kernel_approximation.py|sklearn/lda.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/tests/test_sgd.py|sklearn/manifold/tests/test_isomap.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/tests/test_metrics.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pipeline.py|sklearn/pls.py|sklearn/preprocessing.py|sklearn/svm/base.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_hmm.py|sklearn/tests/test_pls.py|sklearn/tests/test_preprocessing.py|sklearn/tree/tree.py|sklearn/utils/extmath.py|sklearn/utils/fixes.py|sklearn/utils/sparsetools/csgraph.py|sklearn/utils/tests/test_murmurhash.py,230,0.01967930029154519,1,3,true,WIP: Single codebase python 3 support I am currently working on single codebase python 2 + 3 support I used python modernize and six and reused the work of @smoitra87 in #689 (cherry-picked into this branch)I plan to rebase this branch against master on a regular basis so please let me know in advance if you plan to work on this tooIts still not there yet but the amount of failing test is less than 20 I am not sure what the status of the examples is Also I had to do some changes in joblib which will have to be upstreamedI used the temporary sklearnexternalssix package as a convenience I am sure we will be able to stream it down to a few factorized utility functions once all test pass,,435,0.8390804597701149,0.24927113702623907,24892,310.6218865498955,30.371203599550054,87.49799132251326,1465,34,577,155,unknown,ogrisel,ogrisel,true,,26,0.8076923076923077,475,114,1024,true,false,false,false,153,440,362,44,190,1,2334
50491,scikit-learn/scikit-learn,python,701,1331857227,1332097073,1332097073,3997,3997,github,false,false,false,4,0,0,0,5,0,5,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Clean examples #680 #680,,434,0.8387096774193549,0.252356780275562,23482,307.72506600800614,30.15075376884422,86.91763904267098,1464,34,576,36,unknown,ibayer,ibayer,true,ibayer,1,1.0,2,0,12,true,true,false,false,3,11,6,0,9,0,78
49339,scikit-learn/scikit-learn,python,699,1331805654,1331809555,1331809555,65,65,github,false,false,false,238,2,1,0,1,0,1,0,1,0,0,6,6,4,0,0,0,0,6,6,4,0,0,103,0,119,0,28.195645510416604,0.9325865187252823,140,vlad@vene.ro,doc/modules/svm.rst|doc/tutorial.rst|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/sparse/base.py|sklearn/svm/sparse/classes.py,96,0.04020100502512563,0,0,false,Add verbose parameter to SVMs (fixes #250) Add a verbose parameter to SVMs to get verbose output from libsvmprinted to stdout while fitting a modelNote that this setting takes advantage of a per-process runtime settingin libsvm that if enabled may not work properly in a multithreadedcontext (the output may come out interleaved)It works like this:     from sklearn import svm     X  [[-2 -1] [-1 -1] [-1 -2] [1 1] [1 2] [2 1]]     Y  [1 1 1 2 2 2]     clf  svmSVC()fit(X Y)               # verboseFalse     clf  svmSVC(verboseTrue)fit(X Y)   # verboseTrue    **    optimization finished #iter  17    obj  -1492063 rho  0000025    nSV  6 nBSV  0    Total nSV  6Similarly for sparse SVMs:     clf  svmsparseSVC()fit(X Y)     clf  svmsparseSVC(verboseTrue)fit(X Y)    **    optimization finished #iter  17    obj  -1492063 rho  0000025    nSV  6 nBSV  0    Total nSV  6I tried to write a test that captures and verifies stdout but I wasunable to capture stdout with sysstdout  StringIO() (I tried stderrtoo) Im assuming this is because the output is coming from C code(svm/src/libsvm/libsvm_helperc) rather than Python codeA possible future improvement may be to allow the user to provide aPython callback function that is passed the string to be printed Thiswould provide more control over what gets printed where,,433,0.8383371824480369,0.25340990667623836,23469,307.98074055136567,30.167454940559885,86.88056585282713,1462,34,576,33,unknown,njwilson,agramfort,false,agramfort,5,1.0,7,0,417,false,false,false,true,9,5,5,0,0,0,28
48470,scikit-learn/scikit-learn,python,698,1331763701,1331763866,1331763867,2,2,github,false,false,false,12,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,FIX: Add NORMALIZE_WHITESPACE to broken doctest Broken at least on OS X,,432,0.8379629629629629,0.2525179856115108,23469,307.98074055136567,30.167454940559885,86.88056585282713,1461,35,575,34,unknown,njwilson,ogrisel,false,ogrisel,4,1.0,7,0,416,false,false,false,false,7,5,4,0,0,0,2
624806,scikit-learn/scikit-learn,python,696,1331711628,1331716610,1331716610,83,83,github,false,false,false,18,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,6,14,0,0.0,0,,,0,0.0,0,0,false,BUG: Fix metricsauc() w/ duplicate values Fixes #691 Thanks to Olivier Grisel for basically handing me thesolution,,431,0.8375870069605569,0.2525179856115108,23469,307.8103029528314,30.167454940559885,86.88056585282713,1461,35,575,33,unknown,njwilson,ogrisel,false,ogrisel,3,1.0,7,0,416,false,false,false,false,6,5,3,0,0,0,-1
624804,scikit-learn/scikit-learn,python,695,1331703364,1331704047,1331704047,11,11,github,false,false,false,33,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,9,0,0,0.0,0,,,0,0.0,0,0,false,FIX: Delete temporary cache directory The cluster/plot_feature_agglomeration_vs_univariate_selectionpyexample was leaving temporary files in the current directory Theexamples/cluster/joblib/ directory remained after running the exampleThis commit moves the cache to a temporary folder,,430,0.8372093023255814,0.2525179856115108,23465,307.8627743447688,30.172597485616876,86.89537609205199,1461,35,575,33,unknown,njwilson,ogrisel,false,ogrisel,2,1.0,7,0,416,false,false,false,false,4,5,2,0,0,0,-1
624805,scikit-learn/scikit-learn,python,694,1331692750,1331695093,1331695093,39,39,github,false,false,false,76,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,11,0,0.0,0,,,0,0.0,0,0,false,Skip k-means parallel test on Mac OS X Lion (107) There is a bug that occurs in the BLAS DGEMM function after a fork onMac OS X Lion that causes this test to hang See issue #636 for detailsThis commit causes the test_k_means_plus_plus_init_2_jobs test to beskipped on this platformThe Mac version is determined from platformmac_ver() similar to how itis described in the following link I couldnt find a cleaner wayhttp://stackoverflowcom/questions/1777344/how-to-detect-mac-os-version-using-python,,429,0.8368298368298368,0.25306416726748376,23465,307.5644577029619,30.172597485616876,86.89537609205199,1461,35,574,34,unknown,njwilson,ogrisel,false,ogrisel,1,1.0,7,0,415,false,false,false,false,3,5,1,0,0,0,-1
50185,scikit-learn/scikit-learn,python,693,1331689157,1331844120,1331844120,2582,2582,github,false,false,false,139,5,2,1,13,0,14,0,4,4,0,6,12,5,2,2,4,2,6,12,5,2,2,440,0,678,0,56.175378394470386,1.8580178730559411,68,vlad@vene.ro,doc/conf.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/doctools.js|doc/themes/scikit-learn/static/jquery.js|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/pygments.css|doc/themes/scikit-learn/static/sidebar.js|doc/themes/scikit-learn/theme.conf|doc/sphinxext/gen_rst.py|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/themes/scikit-learn/static/sidebar.js,39,0.0050578034682080926,0,6,false,Collapsable sidebar for documentation This is a new little add-on I though could be quite useful for the tutorials especially:Basically the side-bar will be collapsable allowing for the actual content to cover a larger areaof the screenWhen giving tutorials to a class directly from the documentation for example this could beusefulIt works very well without problems as far as I can tell The Examples section is set to ignoring thisfunction due to it not really being necessary there and the layout goes a tad crazy as well when includedIll make an online version of the html build for those who want to have a quick look at it**Note**: Im by no means a wizard with javascripts CSS or html so if you see something there I can do better please say ,,428,0.8364485981308412,0.25289017341040465,23465,307.5644577029619,30.172597485616876,86.89537609205199,1461,35,574,33,unknown,jaquesgrobler,ogrisel,false,ogrisel,4,1.0,6,6,48,true,true,true,true,6,16,18,0,52,0,66
624803,scikit-learn/scikit-learn,python,692,1331685944,1331695800,1331695800,164,164,github,false,false,false,7,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.687148356583816,0.15502887647045835,49,vlad@vene.ro,doc/developers/index.rst,49,0.03545586107091172,0,0,false,DOC: Various minor fixes to Contributing docs ,,427,0.8360655737704918,0.2532561505065123,23465,307.5644577029619,30.172597485616876,86.89537609205199,1461,35,574,33,unknown,njwilson,ogrisel,false,ogrisel,0,0,7,0,415,false,false,false,false,2,5,0,0,0,0,-1
74898,scikit-learn/scikit-learn,python,690,1331644727,1333206561,1333206561,26030,26030,github,false,false,false,78,1,0,39,36,0,75,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,4,false,MRG: NearestCentroid classifier The NearestCentroid classifier trains by taking the centroid of each class New instances are assigned to the closest centroidThe shrink_threshold parameter turns the model into the nearest shrunken centroid classifier In this instance centroids are shrunk such that only the top shrink_threshold features remain (optionally you can shrink by a static parameter)TODO---------* ~~Shrinking parameter (including tests)~~* ~~Sparse tests~~* ~~Narrative docs~~* ~~Reference~~* ~~Example~~* ~~Move to neighbors namespace~~,,426,0.8356807511737089,0.2536231884057971,23550,307.43099787685776,30.233545647558387,87.00636942675159,1459,35,574,41,unknown,robertlayton,mblondel,false,mblondel,10,0.9,5,5,298,true,true,false,false,8,19,4,7,13,0,86
91431,scikit-learn/scikit-learn,python,689,1331619518,1334574244,1334574244,49245,49245,merged_in_comments,false,false,false,19,16,5,3,10,0,13,0,4,0,0,7,26,7,0,0,0,0,26,26,26,0,0,89,0,151,71,40.895427720019576,1.352630906876851,114,vlad@vene.ro,sklearn/utils/__init__.py|sklearn/metrics/pairwise.py|sklearn/cross_validation.py|sklearn/datasets/mldata.py|sklearn/lda.py|sklearn/naive_bayes.py|sklearn/qda.py|sklearn/cross_validation.py|sklearn/cross_validation.py,56,0.023878437047756874,0,3,false,MRG: P3K fixes : Fixing doctests and other minor changes to make tests under P3K build PyCon sprint fixes,,425,0.8352941176470589,0.2539797395079595,23465,307.5644577029619,30.172597485616876,86.89537609205199,1459,35,574,51,unknown,smoitra87,GaelVaroquaux,false,GaelVaroquaux,0,0,4,2,536,true,true,false,false,0,0,0,0,6,0,99
624801,scikit-learn/scikit-learn,python,687,1331228168,,1331576499,5805,,unknown,false,false,false,24,12,4,0,16,0,16,0,7,0,0,2,5,2,0,0,0,0,5,5,2,0,0,165,36,344,42,18.36595815513018,0.611423579101768,54,vlad@vene.ro,sklearn/cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_cross_validation.py|sklearn/tests/test_cross_validation.py,48,0.03455723542116631,0,9,false,Stratified shuffle split Basically a merge of the ShuffleSplit and the StratifiedKFold to have randomized but balanced folds And theres a test a well,,424,0.8372641509433962,0.27141828653707706,23292,309.8059419543191,30.353769534604155,87.1973209685729,1454,35,569,34,unknown,schwarty,GaelVaroquaux,false,,0,0,2,0,633,false,true,false,false,1,0,0,0,0,0,64
53996,scikit-learn/scikit-learn,python,686,1331172886,1332101425,1332101425,15475,15475,github,false,false,false,79,14,1,5,24,0,29,0,5,1,0,1,9,2,0,0,4,0,8,12,5,0,0,194,0,326,82,8.68081485366951,0.28617477931468216,8,satra@mit.edu,sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/dictvectorizer.py,8,0.005792903692976104,0,8,false,MRG DictVectorizer Heres a very simple class that converts dict-based feature-value mappings to nparray and scipysparse matrices and back Useful when working with symbolic features esp in the sparse case Its essentially a rewrite of my NLTK/scikit-learn bridge following scikit-learn APIs* delNot optimized/del* delOnly one doctest that doesnt cover the whole class/del* delDocumentation is incomplete/delPlease let me know if this is interesting at all So far this is just me scratching my own itch :),,423,0.8368794326241135,0.27299058653149894,23480,307.8364565587734,30.153321976149915,86.83986371379898,1452,35,568,41,unknown,larsmans,larsmans,true,larsmans,35,0.7142857142857143,58,27,598,true,true,false,false,47,76,10,15,75,1,56
286475,scikit-learn/scikit-learn,python,685,1331077363,1332371318,1332371318,21565,21565,github,false,false,false,9,11,0,5,16,0,21,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,59,75,0,0.0,0,,,0,0.0,0,4,false,Make linear regression support multiple outputs Closes issue #675,,422,0.8364928909952607,0.2757863935625457,23279,303.66424674599426,30.027063018170885,85.87138622793076,1450,34,567,46,unknown,ibayer,amueller,false,amueller,0,0,2,0,3,false,true,false,false,1,3,0,0,0,0,9
624800,scikit-learn/scikit-learn,python,684,1331073289,1331074786,1331074786,24,24,github,false,false,false,61,3,1,0,1,0,1,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,Decision tree graphviz export fix I ended up with a perfectly valid DT that would fail when I tried to call the export_graphviz functionThere was originally an assertion statement assert node_id  -1 which was changed to a more readable if node_id  1: raise Error statement It seems that the minus sign was left out in the update Oops,,421,0.836104513064133,0.2757863935625457,23279,303.66424674599426,30.027063018170885,85.87138622793076,1450,34,567,32,unknown,clayw,glouppe,false,glouppe,2,1.0,11,12,1044,true,true,false,false,2,9,2,9,1,0,24
624799,scikit-learn/scikit-learn,python,683,1331040853,,1331141870,1683,,unknown,false,true,false,30,3,1,0,9,0,9,0,7,0,0,8,8,6,0,0,0,0,8,8,6,0,0,99,20,154,22,37.989849404604485,1.2663809669114177,186,vlad@vene.ro,doc/modules/svm.rst|doc/tutorial.rst|sklearn/grid_search.py|sklearn/linear_model/logistic.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/sparse/classes.py|sklearn/svm/tests/test_svm.py,94,0.04681784930504755,0,7,false,API: use CNone by default in libsvm/liblinear bindings use CNone by default in libsvm/liblinear bindings so:(C1 scale_CFalse) which is libsvm default   (CNone scale_CTrue) which is the scikit default,,420,0.8380952380952381,0.2765179224579371,23279,303.66424674599426,30.027063018170885,85.87138622793076,1450,34,567,33,unknown,agramfort,GaelVaroquaux,false,,17,0.8235294117647058,66,162,825,true,true,true,false,88,115,116,19,89,0,108
54726,scikit-learn/scikit-learn,python,682,1331026552,1332154414,1332154414,18797,18797,github,false,false,false,13,20,8,0,11,0,11,0,4,3,1,7,15,9,0,0,7,1,11,19,18,0,0,892,0,3971,0,54.870621089725695,1.8034529634919152,105,vlad@vene.ro,benchmarks/bench_sgd_covertype.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/utils/largescale.pxd|sklearn/utils/largescale.c|sklearn/utils/largescale.pyx|sklearn/linear_model/stochastic_gradient.py|sklearn/linear_model/sgd_fast.pxd|sklearn/utils/largescale.pxd|sklearn/utils/setup.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|benchmarks/bench_sgd_covertype.py,70,0.012426900584795321,0,4,false,MRG: Sgd weight vector Move WeightVector and Dataset into the utils package (sklearnutilslargescale),,419,0.837708830548926,0.27631578947368424,23582,308.5404121787804,30.319735391400222,87.43957255533883,1450,34,567,45,unknown,pprett,pprett,true,pprett,17,0.8823529411764706,55,24,945,true,true,false,false,20,45,12,2,21,0,1
624797,scikit-learn/scikit-learn,python,679,1330906028,1330906486,1330906486,7,7,github,false,false,false,21,2,0,0,0,0,0,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,42,0,0,0.0,0,,,0,0.0,0,4,false,MRG Logistic l1 l2 sample I modified another short example to include coefficient plot (and measure accuracy)I like images ),,418,0.8373205741626795,0.2774524158125915,23258,303.93842978759994,30.05417490755869,85.94892080144466,1447,34,565,30,unknown,amueller,agramfort,false,agramfort,57,0.8947368421052632,272,22,499,true,true,true,false,157,347,830,34,260,8,-1
624798,scikit-learn/scikit-learn,python,678,1330890989,1330891284,1330891284,4,4,github,false,false,false,22,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,25,0,0,0.0,0,,,0,0.0,0,0,false,MRG closes #677 - improved affinity propagation docstrings Ive slightly improved the docstring for affinity propagation to explain what preferences are for,,417,0.8369304556354916,0.2732688011913626,23181,304.73232388594107,30.154005435485956,86.23441611664725,1447,33,565,33,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,4,1.0,22,13,776,false,true,true,false,10,8,1,4,0,0,3
624795,scikit-learn/scikit-learn,python,674,1330829426,1330898591,1330898591,1152,1152,github,false,false,false,59,14,0,0,35,0,35,0,4,0,0,0,8,0,0,0,2,0,8,10,6,0,1,0,0,368,0,0,0.0,0,,,0,0.0,0,6,false,MRG Cluster sample datasets This is a PR with some toy dataset generators for clustering and classificationI factored the circles out of the label propagation example by clay and added a moons datasetThere is also an example to show off properties of different clustering algorithms but Im not satisfied with that yetComments welcome :)CheersAndy,,416,0.8365384615384616,0.27706283118849356,23239,300.8735315633203,29.82056026507165,85.33069409182839,1442,33,564,37,unknown,amueller,amueller,true,amueller,56,0.8928571428571429,271,22,498,true,true,false,false,153,309,618,34,230,8,536
624796,scikit-learn/scikit-learn,python,673,1330820050,1330879895,1330879895,997,997,github,false,false,false,93,7,0,0,5,0,5,0,4,0,0,0,5,0,0,0,0,0,5,5,3,0,0,0,0,68,26,0,0.0,0,,,0,0.0,0,1,false,MRG rename parameter multi_class of LinearSVC to crammer_singer After reading issue #657 I felt like multi_class is a bad name for the Crammer-Singer option of the LinearSVC Many people want to do multi-class classification Only few want or even know about Crammer-Singer SVMsTo avoid confusion I think it would be good to rename the option and make a bit more explicit that this is not needed for multi-class classificationAlso I had the impression that the Crammer-Singer SVM was not tested that much so I tried to come up with some tests,,415,0.8361445783132531,0.2747336377473364,23229,300.228163072022,29.70424899909596,85.2813293727668,1442,33,564,38,unknown,amueller,amueller,true,amueller,55,0.8909090909090909,271,22,498,true,true,false,false,152,307,467,34,223,8,395
624793,scikit-learn/scikit-learn,python,672,1330816889,1330881520,1330881520,1077,1077,merged_in_comments,false,false,false,44,1,1,0,4,0,4,0,2,0,0,28,28,28,0,0,0,0,28,28,28,0,0,341,10,341,10,130.4368231845273,4.355082108799299,363,vlad@vene.ro,sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/datasets/samples_generator.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/pca.py|sklearn/ensemble/forest.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/ridge.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/metrics.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pls.py|sklearn/tree/tree.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py,83,0.0258751902587519,0,0,false,MRG DOC move references from Notes to References section in docstrings For some reason having aReferences----------------section in the docstrings didnt work at some pointNow it does Dont ask meIt looks better and is the right way to do it,,414,0.8357487922705314,0.2747336377473364,23229,300.228163072022,29.70424899909596,85.2813293727668,1442,33,564,38,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,54,0.8888888888888888,271,22,498,true,true,true,false,151,307,466,34,223,8,1069
624807,scikit-learn/scikit-learn,python,671,1330809882,,1350400073,326503,,unknown,false,false,false,14,1,1,0,8,0,8,0,3,0,0,6,6,5,0,0,0,0,6,6,5,0,0,68,2,68,2,24.85987568033643,0.8300331417410224,303,vlad@vene.ro,benchmarks/bench_plot_fastkmeans.py|doc/whats_new.rst|examples/decomposition/plot_faces_decomposition.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/sparse_pca.py|sklearn/decomposition/tests/test_dict_learning.py,230,0.02986217457886677,0,1,false,DONTKNOW rename chunk_size to batch_size in sparse_pca and dictionary learning See discussion at 2c496e7765f44547738f034cd458562ea68bb407,,413,0.837772397094431,0.27411944869831545,23227,300.25401472424335,29.706806733542862,85.28867266543247,1442,33,564,113,unknown,amueller,amueller,true,,53,0.9056603773584906,271,22,498,true,true,false,false,148,303,430,34,220,8,2
624794,scikit-learn/scikit-learn,python,670,1330808956,1330892688,1330892688,1395,1395,merged_in_comments,false,false,false,13,2,1,0,0,2,2,0,1,0,0,0,1,1,0,0,0,0,1,1,1,0,0,25,0,35,0,0,0.0,0,,examples/linear_model/plot_lasso_and_elasticnet.py,0,0.0,0,0,false,MRG lasso/enet regression example plot Plotting coefficients of sparse problem corrected r2 score,,412,0.837378640776699,0.27411944869831545,23227,300.25401472424335,29.706806733542862,85.28867266543247,1442,33,564,38,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,52,0.9038461538461539,271,22,498,true,true,true,false,147,303,429,34,220,8,-1
624791,scikit-learn/scikit-learn,python,669,1330807065,1330869018,1330869018,1032,1032,github,false,false,false,60,1,0,0,3,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,MRG WEBSITE: make example gallery look even better This is a minor css enhancement for the example galleryIt makes the thumbnails fit in case they are too big (look at Comparison of Manifold Learning methods before) and centers the thumbnailsTested on Firefox nightly (130a1) and chromium Would be great if someone could test it under opera/safari/ie before merging,,411,0.8369829683698297,0.27300613496932513,23227,300.25401472424335,29.706806733542862,85.28867266543247,1441,33,564,38,unknown,amueller,amueller,true,amueller,51,0.9019607843137255,271,22,498,true,true,false,false,146,303,428,34,218,8,6
624792,scikit-learn/scikit-learn,python,668,1330803253,1331234046,1331234046,7179,7179,github,false,false,false,92,41,0,0,18,1,19,0,6,0,0,0,11,0,0,0,1,0,11,12,9,0,0,0,0,1067,472,0,0.0,0,,,0,0.0,0,5,false,MRG: Text feature extraction simplification Early PR to demonstrate ongoing work on simplification of the sklearnfeature_extractiontext module The goal is to simplify the usage by avoiding the nested preprocessor into analyzer into vectorizer constructor patternThe CountVectorizer has now many parameters but that makes it much more straightforwardTODO before merging:- delnarrative documentation/del    - delTDIDF/del    - delsection on customizing the vectorizer/del- delcheck docstrings completeness/del- delflatten the Vectorizer class as well The semi-hidden tc and tfidf attributes are a source of confusion even for myself when I forget about them/del,,410,0.8365853658536585,0.2727272727272727,23279,303.66424674599426,30.027063018170885,85.87138622793076,1440,33,564,44,unknown,ogrisel,ogrisel,true,ogrisel,25,0.8,461,114,1011,true,false,false,false,133,366,202,45,170,1,1
624789,scikit-learn/scikit-learn,python,667,1330801379,1330889080,1330889080,1461,1461,merged_in_comments,false,false,false,66,5,3,0,5,0,5,0,2,1,1,6,9,4,0,0,1,1,7,9,4,0,0,119,0,119,0,40.5635851362278,1.3543716117229017,116,vlad@vene.ro,sklearn/lda.py|sklearn/qda.py|examples/plot_lda_qda.py|examples/plot_lda_vs_qda.py|doc/index.rst|doc/modules/classes.rst|doc/modules/ldaqda.rst|doc/supervised_learning.rst|sklearn/lda.py|sklearn/qda.py,90,0.01078582434514638,0,0,false,MRG LDA and QDA docs Here are some improvements on the LDA and QDA docsI wrote a very short narrative added QDA to the references and removed an example that I feltwas a bit redundantI am not entirely happy with the organization of these classifiers each in their own moduleIf any on has an idea how to improve that let me know,,409,0.8361858190709046,0.2727272727272727,23226,300.26694221992597,29.70808576595195,85.29234478601568,1440,33,564,38,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,50,0.9,271,22,498,true,true,false,false,145,302,361,34,216,8,1422
624790,scikit-learn/scikit-learn,python,664,1330619451,1330619567,1330619567,1,1,github,false,false,false,105,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,0,false,cross_validationpy: fixed bug in text of error message This branch solves issue 663  I make a small change to correct an error messageIn the initialization method of StratifiedKFold (in cross_validationpy) there is some code that makes sure that k is smaller than the number of entries in the least-populated class I guess the idea is that each fold should contain at least one example from every classIn the case that the least populated class has fewer entires than k the error message says k should be smaller than the number of labels of the least populated class  In fact k should be larger,,408,0.8357843137254902,0.27639751552795033,23217,300.38333979411635,29.719602015764313,85.32540810612913,1435,32,562,32,unknown,conradlee,ogrisel,false,ogrisel,1,1.0,3,0,706,false,true,false,false,1,0,0,0,0,0,-1
624786,scikit-learn/scikit-learn,python,662,1330565153,1330892486,1330892486,5455,5455,merged_in_comments,false,false,false,24,2,1,0,2,0,2,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,34,0,37,0,4.487022463751613,0.14940458660127048,44,vlad@vene.ro,sklearn/decomposition/pca.py,44,0.033950617283950615,0,1,false,MRG issue #661 plus some renaming and minor cleanup Dont think mle makes sense if n_features  n_samples so I simply raise a warning,,407,0.8353808353808354,0.2770061728395062,23227,300.25401472424335,29.706806733542862,85.28867266543247,1432,32,561,42,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,49,0.8979591836734694,271,22,495,true,true,true,false,143,288,263,33,208,8,5451
624787,scikit-learn/scikit-learn,python,660,1330563518,1330605091,1330605091,692,692,github,false,false,false,17,1,0,0,2,0,2,0,2,0,0,0,2,0,0,0,0,2,0,2,2,0,0,0,0,11,0,0,0.0,0,,,0,0.0,0,0,false,MRG remove ball_tree and cross_val namespaces Sort of reissue of this: #647 but not removing scikitslearn namespace,,406,0.8349753694581281,0.27722007722007724,23227,300.25401472424335,29.706806733542862,85.28867266543247,1432,32,561,32,unknown,amueller,amueller,true,amueller,48,0.8958333333333334,271,22,495,true,true,false,false,141,288,262,33,207,8,598
624785,scikit-learn/scikit-learn,python,659,1330513739,1330703044,1330703044,3155,3155,merged_in_comments,false,false,false,71,2,1,0,4,1,5,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,2,0,17,4.032630280459721,0.13427641318071537,22,vanderplas@astro.washington.edu,sklearn/cluster/spectral.py,22,0.016936104695919937,0,1,false,ENH: use shift-invert in spectral clustering This enhancement speeds up the spectral clustering by a significant factor when using arpack  On my box it speeds the plot_lena_segmentationpy example from ~200sec to ~30secThis enhancement is based on the fact () that for a normalized laplacian matrix L the largest eigenvalue of I - L is 10  I believe this is correct but Id feel better if someone could confirm that property,,405,0.8345679012345679,0.27867590454195534,23226,300.26694221992597,29.70808576595195,85.29234478601568,1431,33,561,32,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,22,0.9545454545454546,496,0,294,true,true,false,false,30,48,15,3,55,5,2
44350,scikit-learn/scikit-learn,python,654,1330269098,1330368872,1330368872,1662,1662,github,false,false,false,17,7,4,1,8,0,9,0,3,0,0,1,3,1,0,0,0,0,3,3,2,0,0,158,0,208,22,16.667879110485178,0.5549885051635799,41,vlad@vene.ro,sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/coordinate_descent.py,41,0.03220738413197172,0,0,false,Add the option to set rho by CV in ElasticNetCV This PR also adds parallel to ElasticNetCV,,404,0.8341584158415841,0.2906520031421838,23202,300.40513748814755,29.738815619343157,85.29437117489871,1423,33,558,32,unknown,GaelVaroquaux,agramfort,false,agramfort,13,0.6153846153846154,202,2,734,true,true,false,true,96,193,295,15,158,4,54
43791,scikit-learn/scikit-learn,python,651,1330060211,1330209253,1330209253,2484,2484,merged_in_comments,false,false,false,60,3,0,0,3,0,3,0,2,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,304,9,0,0.0,0,,,0,0.0,1,0,true,BUG: fibonacci heap implementation This fixes a small bug in the shortest paths pyx file which causes rare errors on small graph problems  I remember @GaelVaroquaux saying that hed prefer a particular version of cython when these pyx fixes are compiled  I couldnt remember which so I havent done that yet  This should be converted to c code before merge,,403,0.8337468982630273,0.29635499207606975,23205,299.46132299073474,29.64878258995906,85.19715578539108,1418,33,556,33,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,21,0.9523809523809523,493,0,289,true,true,false,false,29,46,12,3,46,5,448
40896,scikit-learn/scikit-learn,python,650,1330019976,1330440516,1330440516,7009,7009,merged_in_comments,false,false,false,126,9,1,7,22,0,29,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,44,6,223,22,9.079625340411834,0.30232208145225703,42,vlad@vene.ro,sklearn/linear_model/ridge.py|sklearn/linear_model/tests/test_ridge.py,35,0.027624309392265192,0,9,false,optimisations to Ridge Regression GCV Hi allIve made a few optimisations to the ridge regression generalised cross validation (GCV) code:These changes eliminate unnecessary construction of the explicit matrix Gdot(Qdot(diag(1/(v+alpha))QT)) when we just want dot(G y) and diag(G) inside the _error method Similarly inside the _values method we can do the same thing to avoid explicitly constructing dot(KG)  dot(Q dot(diag(v/(v+alpha)) QT))Ive tested the speedup on the communities and crime dataset (http://archiveicsuciedu/ml/datasets/Communities+and+Crime) and I get a ~ 11x speedup when using a small training set consisting of the first 398 rows I expect the speedup is even more convincing for larger training setsThe scripts Im using to benchmark the code are here: https://gistgithubcom/1893136I should probably add a new test for this too,,402,0.8333333333333334,0.2975532754538279,23205,299.46132299073474,29.64878258995906,85.19715578539108,1418,33,555,34,unknown,fcostin,mblondel,false,mblondel,0,0,3,1,847,true,true,false,false,0,0,0,0,2,0,307
278816,scikit-learn/scikit-learn,python,649,1330014837,1330823114,1330823114,13471,13471,github,false,true,false,17,3,0,5,12,0,17,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,26,51,0,0.0,0,,,0,0.0,0,5,false,FIX: make GridSearchCV work with precomputed kernels Simple fix (+test) based on checking whether base_clfkernel  precomputed,,401,0.8329177057356608,0.29825949367088606,23205,299.46132299073474,29.64878258995906,85.19715578539108,1418,31,555,43,unknown,daien,amueller,false,amueller,2,0.5,3,0,476,false,true,false,false,1,1,1,0,0,0,7
288363,scikit-learn/scikit-learn,python,648,1329956474,1332694485,1332694485,45633,45633,github,false,false,false,15,10,0,8,29,0,37,0,6,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,166,80,0,0.0,0,,,0,0.0,0,6,false,MRG class_weights constructor parameter in RidgeCV Rest of issue #64 Just for consistencyFinally ready,,400,0.8325,0.2998417721518987,23239,300.8735315633203,29.82056026507165,85.33069409182839,1414,31,554,55,unknown,amueller,amueller,true,amueller,47,0.8936170212765957,261,22,488,true,true,false,false,137,282,198,32,211,8,18
221218,scikit-learn/scikit-learn,python,647,1329955283,,1342990805,217258,,unknown,false,true,false,62,2,0,0,17,0,17,0,4,0,0,0,24,0,0,0,0,23,1,24,24,0,0,0,0,82,0,0,0.0,0,,,0,0.0,0,1,false,MRG After 012: Remove scikitslearn namespace I thought it might be time to remove scikitslearn - I added a warning for 012 so maybe just keep the pr openThe deprecation warning said that sklearnball_tree and sklearncross_val will go in 011 so I guess they can go now tooThis PR is more to open discussion - after ICML of course ),,399,0.8345864661654135,0.2998417721518987,25140,319.0135242641209,31.026252983293556,89.97613365155131,1414,31,554,109,unknown,amueller,amueller,true,,46,0.9130434782608695,261,22,488,true,true,false,false,136,282,197,32,211,8,643
278144,scikit-learn/scikit-learn,python,645,1329906222,,1329908195,32,,unknown,false,false,false,23,1,0,0,3,0,3,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,1,false,Change of exception text because of ambiguous message rom: Multiple X are not allowed to: Multiple input features cannot have the same value,,398,0.8366834170854272,0.2998417721518987,23204,299.4742285812791,29.6500603344251,85.20082744354423,1411,31,554,28,unknown,leonpalafox,agramfort,false,,0,0,2,2,245,false,true,false,false,1,1,0,0,0,0,18
277462,scikit-learn/scikit-learn,python,643,1329792543,1329824605,1329824605,534,534,github,false,false,false,39,2,0,1,4,0,5,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,63,0,0,0.0,0,,,0,0.0,0,2,false,MRG verbosity parameter for forests: better control over tree building Since I have big datasets building trees often takes a whileThis PR provides a verbose keyword for forests to monitor progressIf no-one objects Id merge it quickly,,397,0.836272040302267,0.30546875,23189,299.6679460088835,29.669239725732027,85.25594031652939,1406,31,552,30,unknown,amueller,amueller,true,amueller,45,0.9111111111111111,260,22,486,true,true,false,false,132,277,77,32,207,8,412
276446,scikit-learn/scikit-learn,python,641,1329525639,1329792581,1329792582,4449,4449,github,false,false,false,90,2,0,0,5,0,5,0,2,0,0,0,4,0,0,0,8,0,4,12,6,0,0,0,0,5,0,0,0.0,0,,,0,0.0,0,0,true,MRG Some doc / sphinx cleanups This removes some more warnings from the doc building process and cleans some things up a bitThe two main parts are:- Moving the dataset rst files to txt files Since the files are included they should not be rsts Before they had separate html files that were unreachable- Include a hidden toc tree that contains the documents that are referred to using the html themeBtw when this PR gets merged the docs should build without **any** warnings or errors :),,396,0.8358585858585859,0.3069908814589666,23186,299.5773311481066,29.629949107219876,85.22384197360476,1395,31,549,30,unknown,amueller,amueller,true,amueller,44,0.9090909090909091,259,22,483,true,true,false,false,130,271,78,31,239,8,272
276084,scikit-learn/scikit-learn,python,637,1329469815,1329469968,1329469968,2,2,github,false,false,false,22,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,2,0,0.0,0,,,0,0.0,0,0,true,MRG: Fix a test case in SpectralClusteirng Though the test case is not used it may be misleading for people reading it ,,395,0.8354430379746836,0.3086890243902439,23189,298.2879813704774,29.496744145931263,84.82470136702747,1395,31,549,30,unknown,fannix,GaelVaroquaux,false,GaelVaroquaux,4,0.75,8,0,401,true,true,false,false,4,19,2,1,2,0,2
288526,scikit-learn/scikit-learn,python,635,1329392047,1332724925,1332724925,55547,55547,github,false,false,false,1404,55,7,9,78,0,87,0,5,48,2,19,118,22,0,0,101,21,49,171,62,0,1,1224,0,3643,0,283.8804069651124,9.258438524482095,52,vlad@vene.ro,doc/tutorial/big_toc_css.rst|doc/tutorial/diabetes_cv_excercice.rst|doc/tutorial/digits_classification_excercice.rst|doc/tutorial/digits_cv_excercice.rst|doc/tutorial/finding_help.rst|doc/tutorial/index.rst|doc/tutorial/iris_classification_excercice.rst|doc/tutorial/model_selection.rst|doc/tutorial/putting_together.rst|doc/tutorial/settings.rst|doc/tutorial/supervised_learning.rst|doc/tutorial/unsupervised_learning.rst|examples/tutorial/README.txt|examples/tutorial/_plot_pca_3d.py|examples/tutorial/_plot_pca_3d_mayavi.py|examples/tutorial/plot_cluster_iris.py|examples/tutorial/plot_cv_diabetes.py|examples/tutorial/plot_cv_digits.py|examples/tutorial/plot_digits_agglomeration.py|examples/tutorial/plot_digits_classification_excercice.py|examples/tutorial/plot_digits_pipe.py|examples/tutorial/plot_face_recognition.py|examples/tutorial/plot_iris_classifiers.py|examples/tutorial/plot_iris_dataset.py|examples/tutorial/plot_iris_exercice.py|examples/tutorial/plot_lena_compress.py|examples/tutorial/plot_logistic.py|examples/tutorial/plot_ols_3d.py|examples/tutorial/plot_ols_variance.py|examples/tutorial/plot_pca_iris.py|examples/tutorial/plot_ridge_variance.py|examples/tutorial/plot_stock_market.py|examples/tutorial/plot_svm_kernels.py|examples/tutorial/plot_svm_margin.py|doc/tutorial.rst|doc/tutorial_menu.rst|doc/user_guide.rst|doc/tutorial/digits_classification_excercice.rst|doc/tutorial/index.rst|doc/tutorial/tutorial_basic/tutorial.rst|doc/tutorial/tutorial_statInfer/big_toc_css.rst|doc/tutorial/tutorial_statInfer/diabetes_cv_excercice.rst|doc/tutorial/tutorial_statInfer/digits_classification_excercice.rst|doc/tutorial/tutorial_statInfer/digits_cv_excercice.rst|doc/tutorial/tutorial_statInfer/finding_help.rst|doc/tutorial/tutorial_statInfer/index.rst|doc/tutorial/tutorial_statInfer/iris_classification_excercice.rst|doc/tutorial/tutorial_statInfer/model_selection.rst|doc/tutorial/tutorial_statInfer/putting_together.rst|doc/tutorial/tutorial_statInfer/settings.rst|doc/tutorial/tutorial_statInfer/supervised_learning.rst|doc/tutorial/tutorial_statInfer/unsupervised_learning.rst|doc/tutorial_menu.rst|doc/user_guide.rst|examples/tutorial/plot_cv_diabetes.py|doc/tutorial/index.rst|doc/tutorial/tutorial.rst|examples/tutorial/_plot_pca_3d.py|examples/tutorial/plot_cluster_iris.py|examples/tutorial/plot_cv_diabetes.py|examples/tutorial/plot_cv_digits.py|examples/tutorial/plot_digits_agglomeration.py|examples/tutorial/plot_digits_first_image.py|examples/tutorial/plot_digits_pipe.py|examples/tutorial/plot_iris_classifiers.py|examples/tutorial/plot_iris_dataset.py|examples/tutorial/plot_lena_compress.py|examples/tutorial/plot_logistic.py|examples/tutorial/plot_ols_3d.py|examples/tutorial/plot_ols_variance.py|examples/tutorial/plot_pca_iris.py|examples/tutorial/plot_ridge_variance.py|examples/tutorial/plot_svm_kernels.py|examples/tutorial/plot_svm_margin.py,32,0.0,14,25,false,WIP - Scikit tutorials  Work in progress - Merging the scikit-learn tutorials into the main documentationStill lots to be done here This is just to keep it open to feedback and guidance from the communityBelow follows the original discussion from the #472 pull request___________________________________________________**GaelVaroquaux**   **WIP: merge in statistical learning tutorial**--------------------------------------------------------------------The goal of this pull request is to merge in the statistical learning tutorial into the main documentationThe pros would be:   * To make the tutorial more easily maintainable   * To be able to link from the tutorial to the docs and vice-versa   * To have examples and images shared between the docs and the tutorial   * To make the tutorial more visibleThe tutorial should be clearly advertise as a tutorial with a specific focus (using the scikit for inference and feature discovery more than simple prediction) It should not replace the docs and be kept short There should be room for other tutorials with other focuses For instance @ogrisels tutorial should be merged in later onRight now this pull request it not ready at all to be merged I am putting it up so that people can pitch in for instance @Balu-Varanasi or @amueller have expressed interested There is a lot of work Specifically (in order of priority and difficulty):   * The indexrst should be added in the tutorial   * I do not want any images stored in the git tree: as a result I have removed some images necessary to build to avoid polluting the tree These should all be generated by the Python code in the examples following the rest of the scikit documentation   * I have created a examples/tutorial folder to host these examples but each one should be moved in the right example folder and this folder should be deleted Also redundant examples should be deleted or merged   * The tutorial needs to be fully doctested I believe that this is the case currently but the doctests take way too long to run partly because of the model selection part (grid search) This model selection part is important as it is important to teach it to people but well need to figure out way to make it run faster: simpler data on some of these and delegating the more complexes ones to examples that get ran only once in a while and using literalinclude directives to show the code in the docsWhat we be great is if we could iterate on the above todo list and update it as we go For the sake of simplicity I can give right permissions to me repo to people working on this------------------------------------------------------*Balu-Varanasi commented*@GaelVaroquaux  I would like to work on this *GaelVaroquaux commented*Great Just try plowing your way through it and ask for any help when you need it Gael*Balu-Varanasi commented*Thank you*mblondel commented*I got feedback that the main documentation is lacking information for beginners so +1*jaquesgrobler commented*Im gonna get going on this Im a tad new to Scikit so I my call on your assistance at some point if I need an experienced opinion :) @mblondel*ogrisel commented*@jaquesgrobler I think it would be great if you could directly make some room for the other tutorial currently hosted at: http://scikit-learngithubcom/scikit-learn-tutorial/ / https://githubcom/scikit-learn/scikit-learn-tutorialThe goal would be to have a tutorials/ container with a indexrst file that gives an overview of the two tutorials and their target audience: one (Gaels) would be for people with a Scientific Computing background (they already have their own data in numerical form and are mostly aware of their statistical properties) the other for a more general audience with a larger focus on defining what a feature is how to deal with categorical feature and how to extract features from textBoth tutorials should be self contained with cross-refences to the narrative doc reference doc and examples (and soon the FAQ)I can deal with the refactoring of my part but it would be great to plan the placeholder structure in advance to make it easier to mere later*jaquesgrobler commented*@ogrisel That sounds like a good idea Ill have a look at it later today and Ill get also get Gaels inputs on it Ill be in touch All the bestJ*GaelVaroquaux commented*I agree what @ogrisel s comment As he said you should leave a placeholder structure One option would be to have a section Tutorials and a few sub-sections below it one of them would be Oliviers tutorial and the other mine*jaquesgrobler commented*Okay Ill get going on this Thanks for the inputs*jaquesgrobler commented*@ogrisel @GaelVaroquaux Ive had a quick look at the Olivers scikit tutorial Just to be sure on the main desire here:Basically neither of these two tutorials are in the documentation yet and we want a main Tutorials section that will refer to them as mentioned above (obviously along the above suggested specifications)Do I understand this correctlyJust one other thing - Section 2 in the User Guide titled Getting started: an introduction to machine learning with scikit-learn is also like a mini-tutorial (its rst file is called tutorial as well) Perhaps it would be good to within this chapter once the tutorials section is complete also give a link to it there or something of this effectSeeing as the Section 2 tutorial is more like an introduction tot he machine learning vocabulary and a simple learning example it may be good to then mention that there are tutorials available too once the basic idea has been discussed in this section (2) Just a thought*GaelVaroquaux commented* @ogrisel @GaelVaroquaux Ive had a quick look at the Olivers scikit tutorial Just to be sure on the main desire here:  Basically neither of these two tutorials are in the documentation yet and we want a main Tutorials section that will refer to them as mentioned above (obviously along the above suggested specifications)  Do I understand this correctlyYes Just one other thing - Section 2 in the User Guide titled Getting started: an introduction to machine learning with scikit-learn is also like a mini-tutorial (its rst file is called tutorial as well) Perhaps it would be good to within this chapter once the tutorials section is complete also give a link to it there or something of this effect or something to that effect  Seeing as the Section 2 tutorial is more like an introduction tot he machine learning vocabulary and a simple learning example it may be good to then mention that there are tutorials available too Just a thoughtActually we could break the section 2 into 3 subsections the first one being A quick introduction that would include the current content of section 2 and the 2 others being the tutorials that we are discussing How does that sound G*jaquesgrobler commented*Sounds good to me Perhaps assuming that section 3 will be yours( @GaelVaroquaux )  one could just start it with a note saying that this subsection is more suited to people with Scientific Computing background and that people that are new to the field can leave it until later Or would that be unnecessary*GaelVaroquaux commented* Sounds good to me Perhaps assuming that section 3 will be yours( @GaelVaroquaux )  one could just start it with a note saying that this subsection is more suited to people with Scientific Computing background and that people that are new to the field can leave it until later Or would that be unnecessaryA note is useful In general every section of a documentation should tell the reader why they want to read or not to read it The exact content of the note can be discussed I would say that it should rather say that the tutorial is targeted toward people wanting to do scientific inference ie reach conclusions on their data using the scikit-learn G*jaquesgrobler commented*noted thank you :)*ogrisel commented*I agree Also we should cross link the tutorials in their intro and make it explicit which audience is target there as well (if not already the case)*GaelVaroquaux commented* Also we should cross link the tutorials in their intro and make it  explicit which audience is target there as well (if not already the  case)Yes this is very important,,394,0.8350253807106599,0.31441717791411045,23584,311.35515603799183,30.529172320217096,88.02578018995929,1394,31,548,63,unknown,jaquesgrobler,GaelVaroquaux,false,GaelVaroquaux,3,1.0,6,6,22,true,true,true,false,4,6,3,0,7,0,88
275501,scikit-learn/scikit-learn,python,634,1329354303,1329475952,1329475952,2027,2027,github,false,false,false,46,7,0,3,11,0,14,0,4,0,0,0,4,0,0,0,0,0,4,4,3,0,0,0,0,35,84,0,0.0,0,,,0,0.0,0,1,false,FIX flip sign in decision function of LibSVM in binary case Addressing issue #630This is a dirty hack Should be done in the libsvmcpp but couldnt figure out how Im open to suggestionsAlso: Im not sure if the OneClassSVM case is handled right atm,,393,0.8346055979643766,0.315668202764977,23085,300.28156811782543,29.586311457656485,85.29348061511804,1394,31,547,33,unknown,amueller,ogrisel,false,ogrisel,43,0.9069767441860465,258,22,481,true,true,true,false,125,265,47,31,230,8,13
275164,scikit-learn/scikit-learn,python,633,1329312397,1329440157,1329440157,2129,2129,github,false,false,false,98,9,0,5,21,0,26,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,96,17,0,0.0,0,,,0,0.0,0,7,false,MRG: ignore in feature_extraction/textpy Fixes issue #629:When analysing documents if a bytes array is given to one of the n-gram analyzers it is decoded using a given charset However the errors are set to ignore which I feel may hide problems in codes (ie using the incorrect encoding may screw with results)Location: in feature_extractiontextpy lines 156 and 208 for classes WordNGramAnalyzer and CharNGramAnalyzerFix: Id like to add a keyword to the classes on what to do default being error rather than ignore (ie if people want to ignore errors they have to explicitly state so),,392,0.8341836734693877,0.31712962962962965,23085,299.848386398094,29.586311457656485,85.29348061511804,1393,31,547,33,unknown,robertlayton,ogrisel,false,ogrisel,9,0.8888888888888888,5,5,271,true,true,true,false,7,12,3,12,6,0,2
275152,scikit-learn/scikit-learn,python,632,1329310442,,1329311352,15,,unknown,false,false,false,113,10,6,0,0,0,0,0,1,60,2,6,69,56,0,1,60,2,7,69,57,0,1,6289,1524,6366,1524,276.13147373480865,9.19421598458757,418,vlad@vene.ro,sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/setup.py|sklearn/datasets/svmlight_format.py|doc/modules/clustering.rst|benchmarks/bench_plot_parallel_pairwise.py|doc/data_transforms.rst|doc/developers/debugging.rst|doc/developers/utilities.rst|doc/logos/identity.pdf|doc/modules/ensemble.rst|doc/modules/kernel_approximation.rst|doc/modules/outlier_detection.rst|examples/applications/plot_outlier_detection_housing.py|examples/applications/plot_stock_market.py|examples/applications/plot_tomography_l1_reconstruction.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/covariance/plot_outlier_detection.py|examples/covariance/plot_sparse_cov.py|examples/decomposition/plot_sparse_coding.py|examples/ensemble/README.txt|examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_forest_importances_faces.py|examples/ensemble/plot_forest_iris.py|examples/plot_kernel_approximation.py|examples/plot_multilabel.py|examples/plot_random_dataset.py|examples/svm/plot_rbf_parameters.py|examples/svm/plot_svm_parameters_selection.py|sklearn/covariance/graph_lasso_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/tests/test_graph_lasso.py|sklearn/covariance/tests/test_robust_covariance.py|sklearn/datasets/_svmlight_format.c|sklearn/datasets/_svmlight_format.pyx|sklearn/datasets/species_distributions.py|sklearn/datasets/tests/data/svmlight_multilabel.txt|sklearn/ensemble/__init__.py|sklearn/ensemble/base.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/__init__.py|sklearn/ensemble/tests/test_base.py|sklearn/ensemble/tests/test_forest.py|sklearn/feature_selection/selector_mixin.py|sklearn/feature_selection/tests/test_selector_mixin.py|sklearn/kernel_approximation.py|sklearn/preprocessing.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/tests/test_kernel_approximation.py|sklearn/tests/test_preprocessing.py|sklearn/utils/arraybuilder.c|sklearn/utils/arraybuilder.pyx|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/src/MurmurHash3.cpp|sklearn/utils/src/MurmurHash3.h|sklearn/utils/tests/test_logsumexp.py|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/tests/test_validation.py|sklearn/utils/validation.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,90,0.012355212355212355,0,0,false,WIP: ignore in feature_extraction/textpy Fixes issue #629:When analysing documents if a bytes array is given to one of the n-gram analyzers it is decoded using a given charset However the errors are set to ignore which I feel may hide problems in codes (ie using the incorrect encoding may screw with results)Location: in feature_extractiontextpy lines 156 and 208 for classes WordNGramAnalyzer and CharNGramAnalyzerFix: Id like to add a keyword to the classes on what to do default being error rather than ignore (ie if people want to ignore errors they have to explicitly state so)TODO- I need to rebase first I thought it worked but it didnt,,391,0.8363171355498721,0.3173745173745174,23085,299.848386398094,29.586311457656485,85.29348061511804,1393,31,547,31,unknown,robertlayton,robertlayton,true,,8,1.0,5,5,271,true,true,false,false,6,11,1,12,6,0,-1
275141,scikit-learn/scikit-learn,python,631,1329308087,1330205275,1330205275,14953,14953,merged_in_comments,false,false,false,51,1,0,1,5,0,6,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,4,23,0,0.0,0,,,0,0.0,0,1,false,FIX : Fixed bug in single_source_shortest_path_length in sklearnutilsgraph I believe the restriction of node vs neighbors to nodes with index w  v (neighbors  neighbors[neighbors  v]) in single_source_shortest_path_length is a bug (or I get something totally wrong) I also added a unit test test_shortest_path which fails with the restriction,,390,0.8358974358974359,0.31712962962962965,23085,299.848386398094,29.586311457656485,85.29348061511804,1393,31,547,39,unknown,jmetzen,GaelVaroquaux,false,GaelVaroquaux,2,0.5,5,1,128,true,true,false,false,1,4,1,9,43,0,3654
274799,scikit-learn/scikit-learn,python,628,1329248623,1329252855,1329252855,70,70,github,false,false,false,40,2,0,0,3,0,3,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,14,0,0.0,0,,,0,0.0,0,0,false,FIX: typo for default init_size in MiniBatchKMeans According to the docstring the default value for init_size should be 3 * batch_sizeThe typo made the default to be batch_size and if init_size was not None it was muliplied by 3,,389,0.8354755784061697,0.31786542923433875,23085,299.41520467836256,29.4996751137102,85.20684427117176,1391,31,546,30,unknown,daien,ogrisel,false,ogrisel,1,0.0,3,0,467,false,true,false,false,0,0,0,0,0,0,7
274723,scikit-learn/scikit-learn,python,627,1329226948,1329414695,1329414695,3129,3129,github,false,false,false,59,6,0,2,26,0,28,0,6,0,0,0,8,0,0,0,0,0,8,8,6,0,0,0,0,391,28,0,0.0,0,,,0,0.0,3,17,false,MRG trees: Min leaf cherrypick Cherry picked the min_leaf parameter from PR #522 by @ndaweAlso added a testIm not sure if we should keep the min_split parameter since I feel min_leaf makes more senseI know the ExtraTree paper uses min_split but I havent found a reason whyHow do you feel about that (expecially @glouppe @bdholt1),,388,0.8350515463917526,0.32076923076923075,23085,299.41520467836256,29.4996751137102,85.20684427117176,1390,31,546,33,unknown,amueller,glouppe,false,glouppe,42,0.9047619047619048,257,22,480,true,true,true,true,124,257,47,31,232,8,1
274057,scikit-learn/scikit-learn,python,623,1329112011,1329131359,1329131359,322,322,github,false,false,false,7,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,30,0,0,0.0,0,,,0,0.0,0,0,false,DOC: fix a few incoherencies in ridgepy ,,387,0.834625322997416,0.3210445468509985,23006,300.13909414935233,29.600973659045465,85.49943493001825,1388,31,545,31,unknown,npinto,mblondel,false,mblondel,3,1.0,48,39,1152,true,true,false,false,2,1,1,0,1,0,-1
273609,scikit-learn/scikit-learn,python,620,1328983833,1329223628,1329223628,3996,3996,github,false,false,false,37,2,0,0,6,0,6,0,4,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,9,33,0,0.0,0,,,0,0.0,0,2,false,MISC added warning to coordinate descent if alpha0 dont call cd with  alpha0 in testsIm not sure if the way I modified the tests is sensible Please someone with more domain knowledge take a look,,386,0.8341968911917098,0.32150943396226417,23006,300.13909414935233,29.600973659045465,85.49943493001825,1382,31,543,30,unknown,amueller,amueller,true,amueller,41,0.9024390243902439,256,22,477,true,true,false,false,121,249,44,31,237,8,25
273412,scikit-learn/scikit-learn,python,618,1328923451,1328974810,1328974810,855,855,github,false,false,false,8,12,3,0,8,0,8,0,4,0,0,1,10,1,0,0,0,0,10,10,6,0,0,93,0,174,26,13.702613514527304,0.4548783309937809,37,vlad@vene.ro,sklearn/cross_validation.py|sklearn/cross_validation.py|sklearn/cross_validation.py,37,0.02803030303030303,0,1,true,MRG: finally the Train Test Split utility function ,,385,0.8337662337662337,0.32575757575757575,22938,299.19783764931555,29.296364111953963,84.13985526201064,1379,31,542,31,unknown,ogrisel,ogrisel,true,ogrisel,24,0.7916666666666666,440,112,989,true,false,false,false,107,330,42,35,150,1,4
271876,scikit-learn/scikit-learn,python,613,1328678007,1328692450,1328692450,240,240,github,false,false,false,38,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,0,false,Fixed docstring to reflect current code in precision_recall_curve 1 It has n + 1 values for precision and recall2 It adds a fake point with precision  1 and recall  0Signed-off-by: Brandyn A White bwhite@dappervisioncom,,384,0.8333333333333334,0.3290229885057471,22935,297.4493132766514,29.16939175931982,83.93285371702639,1374,31,540,32,unknown,bwhite,GaelVaroquaux,false,GaelVaroquaux,0,0,39,2,1328,false,true,false,false,0,0,0,0,0,0,92
271724,scikit-learn/scikit-learn,python,612,1328658891,1329134391,1329134391,7925,7925,github,false,false,false,144,11,7,9,11,0,20,0,5,0,0,7,10,7,0,0,0,2,8,10,8,0,0,1077,0,1691,5,70.51412716097684,2.34068673442721,77,vlad@vene.ro,sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast_sparse.c|sklearn/linear_model/sgd_fast_sparse.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast_sparse.c|sklearn/linear_model/sgd_fast_sparse.pyx|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/sgd_fast_sparse.c|sklearn/linear_model/sgd_fast_sparse.pyx|sklearn/linear_model/setup.py|sklearn/linear_model/sgd_fast.c|sklearn/linear_model/sgd_fast.pxd|sklearn/linear_model/sgd_fast.pyx|sklearn/linear_model/stochastic_gradient.py,65,0.015793251974156496,0,6,false,MRG: Major SGD extension module refactoring A major refactoring of the SGD extension module which allows us to dump sgd_fast_sparsepyx ::1 A new dataset abstraction is introduced Dataset which basically provides a next and a shuffle method    There are currently two concrete implementations ArrayDataset which is backed by a c-style numpy array and CSRDataset which is backed by a scipy CSR matrix2 A WeightVector abstraction is introduced which basically wraps wscale and the np array w and provides methods such as scale add and reset_wscale I timed the refactoring on 20news and covertype there are no noticeable differences in terms of runtime This refactoring reduces code-complexity and should allow us to add features more easily Im thinking about various hashing tricks (eg feature cross-products via a new WeightVector etc ) and different multi-class approaches such as multinomial LR and multi-class hinge loss,,383,0.8328981723237598,0.3295046661880833,23006,300.13909414935233,29.600973659045465,85.49943493001825,1373,31,539,32,unknown,pprett,pprett,true,pprett,16,0.875,52,23,917,true,true,false,false,23,50,10,5,21,0,14
271323,scikit-learn/scikit-learn,python,611,1328609096,,1328630354,354,,unknown,false,false,false,27,1,0,0,1,0,1,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,17,0,0,0.0,0,,,0,0.0,0,1,false,MRG: change naive bayes self_classes attr to selfclasses_ changed naive bayes self_classes attr to selfclasses_ to adhere to sklearn naming convention Set self_classes deprecated (rm in 012),,382,0.8350785340314136,0.32907801418439714,22912,296.7440642458101,29.15502793296089,83.88617318435753,1372,31,539,31,unknown,pprett,larsmans,false,,15,0.9333333333333333,52,23,917,true,true,true,true,22,49,9,5,13,0,36
271211,scikit-learn/scikit-learn,python,610,1328581639,1329500593,1329500593,15315,15315,github,false,false,false,20,8,0,0,21,0,21,0,4,0,0,0,14,0,0,0,0,0,14,14,11,0,0,0,0,295,17,0,0.0,0,,,0,0.0,0,19,false,MRG: moved class_weight parameter in svms from fit to __init__ Addressing issue #64 This is a new version of #536,,381,0.8346456692913385,0.330028328611898,23195,299.4610907523173,29.618452252640655,85.19077387367967,1372,31,538,32,unknown,amueller,amueller,true,amueller,40,0.9,254,21,472,true,true,false,false,114,234,41,31,226,8,459
624895,scikit-learn/scikit-learn,python,609,1328286394,,1328286420,0,,unknown,false,false,false,36,7,5,0,0,0,0,0,1,0,0,2,3,1,0,0,0,0,3,3,2,0,0,235,0,263,6,25.471782974954873,0.8483314930437099,80,vlad@vene.ro,sklearn/cluster/k_means_.py|doc/modules/clustering.rst|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py,61,0.04072096128170895,0,0,true,Simpler seeding for parallel kmeans in #596 I added a test to I check that the seed for each _kmeans_single call were different from one another using a print statement and it seems to work fine,,380,0.8368421052631579,0.32376502002670227,22949,294.9148111028803,28.977297485729228,83.5766264325243,1364,31,535,33,unknown,ogrisel,ogrisel,true,,23,0.8260869565217391,435,112,982,true,false,false,false,113,329,42,31,204,1,-1
625994,scikit-learn/scikit-learn,python,608,1328240698,1328260963,1328260963,337,337,github,false,false,false,15,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0.0,0,,,0,0.0,0,0,false,Fixes issue #559 Issue #559 is caused by a missing    configadd_subpackage(svm/sparse)This patch fixes that,,379,0.8364116094986808,0.3226879574184963,22948,294.927662541398,28.978560223113124,83.5802684329789,1364,31,534,33,unknown,cscheid,GaelVaroquaux,false,GaelVaroquaux,0,0,19,0,619,false,true,false,false,2,1,0,0,0,0,-1
275934,scikit-learn/scikit-learn,python,603,1328121408,1329440357,1329440357,21982,21982,github,false,false,false,40,4,0,9,9,0,18,0,4,0,0,0,4,0,0,0,1,1,3,5,3,0,0,0,0,254,0,0,0.0,0,,,0,0.0,0,0,false,ENH: noise in gaussian processes This pull request consists of two things:- adding the ability to specify nugget as an array in GaussianProcess- adding an example and documentation on how to use this feature to fit noisy data,,378,0.8359788359788359,0.315339038841343,22946,294.9533687788721,28.981086028065892,83.5875533862111,1362,31,533,39,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,20,0.95,469,0,266,true,true,false,false,25,40,11,1,39,5,75
269913,scikit-learn/scikit-learn,python,602,1328118520,1328124493,1328124493,99,99,github,false,false,false,142,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Contributors Documentaion - A note on remotes In the contributors documentation I inserted a note saying how to add an extra remote that links to the main master repository and not your own should you wish to pull from there instead of your github repositoryI didnt find it entirely obvious the first time I set it all up on my system why one would want to make remotes to other things so I thought itd be good to add this note within the context of the documentationI feel its in a good location too seeing as the line  (If any of the above seems like magic to you then look up the Git documentation http://git-scmcom/documentation_ on the web)follows it directlyPlease let me know if you think I can word it better or if you find this unnecessaryJ,,377,0.8355437665782494,0.3153034300791557,22610,298.8942945599292,29.234851835471034,84.16629809818664,1362,30,533,35,unknown,jaquesgrobler,ogrisel,false,ogrisel,2,1.0,6,1,7,false,true,false,true,3,2,2,0,0,0,-1
273590,scikit-learn/scikit-learn,python,601,1328117322,1328979530,1328979530,14370,14370,github,false,false,false,10,8,0,0,30,0,30,0,5,0,0,0,21,0,0,0,0,0,21,21,15,0,0,0,0,167,78,0,0.0,0,,,0,0.0,0,6,false,API: set scale_C to True by default in libsvm/liblinear models ,,376,0.8351063829787234,0.31488801054018445,22998,299.19993042873296,29.611270545264805,84.78998173754239,1362,30,533,36,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,16,0.8125,63,156,791,true,true,true,false,75,95,20,25,91,3,13
268814,scikit-learn/scikit-learn,python,596,1327814550,1328527855,1328527855,11888,11888,merged_in_comments,false,false,false,21,10,1,22,10,0,32,0,5,0,0,1,4,1,0,0,0,0,4,4,2,0,0,156,0,270,6,4.336874138721134,0.1478752727900406,72,vlad@vene.ro,sklearn/cluster/k_means_.py,72,0.04730617608409987,1,3,false,MRG: Parallel k-means Parallel version of k-means using joblib based on the recent commit by @mblondel to metrics/pairwisepy and issue #592,,375,0.8346666666666667,0.31274638633377133,22256,299.2900790797987,29.385334291876347,84.69626168224299,1349,29,530,36,unknown,robertlayton,GaelVaroquaux,false,GaelVaroquaux,7,1.0,5,5,254,true,true,false,false,4,23,2,5,20,0,1
268703,scikit-learn/scikit-learn,python,595,1327784558,1328026501,1328026501,4032,4032,github,false,false,false,103,9,0,13,14,0,27,0,5,0,0,0,12,0,0,0,2,0,12,14,11,0,0,0,0,309,63,0,0.0,0,,,0,0.0,0,4,false,MRG: Perceptron Heres an implementation of the good old linear Perceptron It supports:- dense and sparse data- multi-class classification- fit and partial_fit- warm_startActually I cheated Heres what I did:- modify sgd_fast to allow disabling regularization- add a threshold option to the hinge loss (by changing the threshold from 1 to 0 we get the Perceptron instead of SVM)- add a class Perceptron which basically just inherits from SGDClassifier and sets the right optionsWhen we implement averaging or multi-class hinge in sgd we should get the average perceptron and the multi-class perceptron for free,,374,0.8342245989304813,0.3123759099933819,22081,302.024364838549,29.61822381232734,85.32222272542005,1347,29,529,35,unknown,mblondel,mblondel,true,mblondel,13,0.9230769230769231,111,25,669,true,false,false,false,61,114,13,3,58,3,12
268336,scikit-learn/scikit-learn,python,593,1327696626,1327697954,1327697954,22,22,github,false,false,false,87,2,0,2,0,0,2,0,3,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,1,true,Edited the contributers documentation Under Retrieving the latest code I added that it is necessary to have git and included a link to a good tutorial that covers installation understanding it and using itI also removed the (you need the git program to do this) from under How to contribute as it is already mentioned (now) in the above section This will give anyone that is unfamiliar with git a quick run-through and theyll be able to grasp github easier (if they dont know it already),,373,0.8337801608579088,0.31287128712871287,22130,301.31043831902394,29.55264347040217,85.13330320831452,1347,29,528,36,unknown,jaquesgrobler,ogrisel,false,ogrisel,1,1.0,4,1,2,false,true,false,true,1,0,1,0,0,0,4
268236,scikit-learn/scikit-learn,python,591,1327685511,1327685737,1327685737,3,3,github,false,false,false,77,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,Added a note to the install documentation Just under the different ways of getting scikit-learn installed - added a note to just clarify that if you wish to contribute go straight for the development version Its just to be clear so that people dont straight away install from source from the Download page and then get confused when they want to try and set it up for contributing with git/github Especially if somebody is new to git-hub,,372,0.8333333333333334,0.31175693527080584,22130,301.31043831902394,29.55264347040217,85.13330320831452,1347,29,528,36,unknown,jaquesgrobler,agramfort,false,agramfort,0,0,4,1,2,false,true,false,false,0,0,0,0,0,0,-1
267567,scikit-learn/scikit-learn,python,588,1327602831,1327771212,1327771213,2806,2806,github,false,false,false,44,3,0,2,8,0,10,0,6,0,0,0,3,0,0,0,1,0,3,4,2,0,0,0,0,280,0,0,0.0,0,,,0,0.0,0,4,false,initialize indices and distances in balltree This fixes the problems discussed in issue #585  There were arrays which were not being initialized which led to invalid settings of the warning flag  This issue did not affect the neighbors and distances which were eventually returned,,371,0.8328840970350404,0.3107307439104674,22040,301.90562613430126,29.673321234119783,85.2087114337568,1342,29,527,38,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,19,0.9473684210526315,465,0,260,true,true,false,false,22,32,10,0,35,5,11
267547,scikit-learn/scikit-learn,python,587,1327601221,1327662645,1327662645,1023,1023,github,false,true,false,20,2,0,0,7,0,7,0,4,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,33,3,0,0.0,0,,,0,0.0,1,2,false,add random_state to LocallyLinearEmbedding This addresses the discussion in issue #575@mblondel can you check if this resolves the problem,,370,0.8324324324324325,0.3107307439104674,22040,301.90562613430126,29.673321234119783,85.2087114337568,1342,29,527,37,unknown,jakevdp,ogrisel,false,ogrisel,18,0.9444444444444444,465,0,260,true,true,false,false,21,31,9,0,35,5,5
168658,scikit-learn/scikit-learn,python,583,1327334169,1340291269,1340291269,215951,215951,merged_in_comments,false,false,false,11,6,1,0,1,0,1,0,2,1,0,0,5,1,0,0,2,0,4,6,4,0,0,354,0,947,91,4.321402410168242,0.14781085158604612,3,g.louppe@gmail.com,sklearn/ensemble/bagging.py,3,0.0020435967302452314,0,0,false,WIP: Bagging meta-estimator This is a work-in-progress of a Bagging meta-estimator,,369,0.8319783197831978,0.3181198910081744,21966,301.7390512610398,29.727761085313666,85.31366657561685,1332,29,524,114,unknown,glouppe,glouppe,true,glouppe,18,1.0,58,18,438,true,true,false,false,28,79,23,4,91,0,181386
624900,scikit-learn/scikit-learn,python,580,1327272635,,1327277089,74,,unknown,false,false,false,62,1,1,0,6,0,6,0,3,7,0,2,9,3,0,0,7,0,2,9,3,0,0,0,0,0,0,8.404580977371458,0.2874768513694707,43,vlad@vene.ro,doc/index.rst|doc/modules/datasets.rst|doc/modules/datasets/labeled_faces.rst|doc/modules/datasets/labeled_faces_fixture.py|doc/modules/datasets/mldata.rst|doc/modules/datasets/mldata_fixture.py|doc/modules/datasets/olivetti_faces.rst|doc/modules/datasets/twenty_newsgroups.rst|doc/modules/datasets/twenty_newsgroups_fixture.py|doc/user_guide.rst,33,0.0,0,0,false,WIP moving datasets user guide into the modules folder This PR moves the datasets folder under doc into the modules folderThe datasets module was the only module whose user guide wasnt in the modules folderActually I would be in favor of renaming modules into userguide and moving all user guide related files (supervisedrst unsupervisedrst model_selectionrst and data_transformsrst) into itwdyt,,368,0.8342391304347826,0.3193103448275862,21957,301.8627317028738,29.739946258596348,85.3486359703056,1332,29,523,35,unknown,amueller,amueller,true,,39,0.9230769230769231,248,21,457,true,true,false,false,102,175,42,14,248,8,29
272831,scikit-learn/scikit-learn,python,578,1327092499,1328827593,1328827593,28918,28918,github,false,false,false,67,10,0,1,12,0,13,0,5,0,0,0,4,0,0,0,0,0,4,4,1,1,2,0,0,4,0,0,0.0,0,,,0,0.0,0,3,true,DOC: add old version warning This extends the sphinx theme to allow a setting in confpy which when set to True adds a header at the top of each documentation page warning that the documentation is an old version  I think we should cherry-pick this into the older versions  This way if google directs people into old docs there will be a prominent warning on every page,,367,0.8337874659400545,0.3228454172366621,22081,302.024364838549,29.61822381232734,85.32222272542005,1329,29,521,42,unknown,jakevdp,amueller,false,amueller,17,0.9411764705882353,460,0,254,true,true,false,false,18,26,8,0,33,5,2
624903,scikit-learn/scikit-learn,python,577,1327085908,1327090875,1327090875,82,82,github,false,false,false,41,2,1,0,6,0,6,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,5,0,5,4,4.299700276223403,0.14707602390016367,27,vlad@vene.ro,sklearn/datasets/twenty_newsgroups.py,27,0.018442622950819672,0,2,true,Fixed datafilenames consistency issue when all specified for subset in fetch_20newsgroups Previously filenames was not modified leading to it always containing the test subset filenames when all was specified and not being modified at all during the category mask-ing and shuffling,,366,0.8333333333333334,0.3224043715846995,21904,302.2279035792549,29.766252739225713,85.46384222059898,1329,29,521,37,unknown,Epinoch,ogrisel,false,ogrisel,0,0,1,0,90,false,true,false,false,0,0,0,0,0,0,6
268340,scikit-learn/scikit-learn,python,576,1327083872,1327697180,1327697180,10221,10221,github,false,false,false,59,4,1,0,21,0,21,0,4,1,1,5,11,8,0,0,1,1,9,11,9,0,0,532,0,585,53,22.562090530559544,0.7693171833134459,113,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/libsvm_sparse.c|sklearn/svm/libsvm_sparse.pyx|sklearn/svm/setup.py|sklearn/svm/sparse/__init__.py|sklearn/svm/sparse/base.py|sklearn/svm/sparse/setup.py,66,0.01092896174863388,0,5,true,MRG: merge sparse and dense SVMs This branch merges support for sparse matrices into the vanilla SVM classesI propose a new convention for the SVMs: they support sparse data in predict iff they have been fit on sparse data This is different from the current behavior of svmsparse* which is to convert dense sample arrays to CSR matrices,,365,0.8328767123287671,0.3224043715846995,22130,301.31043831902394,29.55264347040217,85.13330320831452,1329,29,521,40,unknown,larsmans,larsmans,true,larsmans,34,0.7058823529411765,53,27,551,true,true,false,false,45,72,14,0,128,1,9
624905,scikit-learn/scikit-learn,python,574,1327007665,1327248229,1327248229,4009,4009,github,false,false,false,33,1,1,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,78,0,78,0,9.202208984215321,0.3147725683620133,10,olivier.grisel@ensta.org,sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx,10,0.006081081081081081,1,0,false,BUG promote type-safety in murmurhash Heres the PR I promised @ogrisel yesterday (I think)* be explicit about size of integers* functions renamed to reflect this* raise TypeError instead of ValueError,,364,0.8324175824175825,0.3222972972972973,21869,301.56843019799715,29.67671132653528,85.41771457314006,1327,29,520,38,unknown,larsmans,ogrisel,false,ogrisel,33,0.696969696969697,53,27,550,true,true,true,true,45,73,14,0,132,1,335
625996,scikit-learn/scikit-learn,python,573,1327002540,1328205961,1328205961,20057,20057,merged_in_comments,false,false,false,44,1,1,0,18,0,18,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.473858466737401,0.15303280815818693,37,vlad@vene.ro,sklearn/decomposition/fastica_.py,37,0.025,0,3,false,fix error in unwhitened case of ica scaling of variance by samplesize should not be applied to prewhitened data because it belongs to the whitening itselfeg https://githubcom/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/pcapy#L213see also my initial comment https://githubcom/scikit-learn/scikit-learn/pull/243#issuecomment-1555860(sorry that i did not brought this to an end),,363,0.8319559228650137,0.3222972972972973,21894,301.2240796565269,29.642824518132823,85.32017904448708,1327,29,520,40,unknown,jansoe,GaelVaroquaux,false,GaelVaroquaux,1,0.0,2,0,195,false,true,false,false,0,0,0,0,0,0,29
624908,scikit-learn/scikit-learn,python,571,1326922433,1327095556,1327095556,2885,2885,github,false,false,false,27,12,3,0,14,0,14,0,4,0,0,3,4,2,0,0,0,0,4,4,2,0,0,13,11,206,42,17.556044977185437,0.6005171449328861,104,vlad@vene.ro,sklearn/ensemble/forest.py|doc/modules/grid_search.rst|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py,89,0.02557200538358008,0,4,false,MRG Out of bag model selection for ensemble classifiers This is a minor improvement for ensemble classifiers (RandomForestClassifier and ExtraTreesClassifier)It makes model selection without cross-validation possible,,362,0.8314917127071824,0.3223418573351279,21895,301.2103219913222,29.641470655400774,85.31628225622288,1324,29,519,39,unknown,amueller,amueller,true,amueller,38,0.9210526315789473,246,21,453,true,true,false,false,98,140,45,1,259,8,45
624910,scikit-learn/scikit-learn,python,568,1326895664,1327037655,1327037655,2366,2366,github,false,false,false,185,9,0,0,3,0,3,0,2,0,0,0,11,0,0,0,0,0,11,11,11,0,0,0,0,124,26,0,0.0,0,,,0,0.0,0,1,false,More convenient warm start Here are a few tiny changes to SGD and ElasticNet / Lasso which should make using warm-start much more convenient on a regularization path Now you can do the following:pythonclf  SGDClassifier(warm_startTrue)best_alpha  Nonebest_score  -npinf# from more regularized to less regularizedfor alpha in (10 01 001):    clfset_params(alphaalpha)    clffit(X y)    score  clfscore(X_val y_val)    if score  best_score:        best_score  score        best_alpha  alpha   I think this way of doing is intuitive and importantly it doesnt rely on the internal parameters of the classifiers You can clone the best estimator if you need to tooOn the other hand if you set warm_startFalse which is the default the subsequent call to fit(X y) erase the previous modelSGDClassifier has now a partial_fit(X y) method too but it has the following differences:- X can be a subset of the entire dataset- It makes only one epoch over X- It saves the current state of the learning rate and start from there while with warm-start fit starts with a fresh learning rateMathieu,,361,0.8310249307479224,0.32238605898123324,21901,302.1779827405141,29.77033012191224,85.47554906168668,1324,29,519,37,unknown,mblondel,mblondel,true,mblondel,12,0.9166666666666666,107,25,659,true,false,false,false,51,95,12,3,58,3,520
624913,scikit-learn/scikit-learn,python,564,1326775968,1326876504,1326876504,1675,1675,github,false,false,false,128,12,6,0,12,0,12,0,8,6,1,7,16,8,0,0,6,1,9,16,9,0,0,3799,73,4393,99,89.53937173420404,3.0841946088750616,72,vlad@vene.ro,doc/developers/utilities.rst|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx|sklearn/utils/__init__.py|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pyx|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx|sklearn/utils/setup.py|sklearn/utils/src/MurmurHash3.cpp|sklearn/utils/src/MurmurHash3.h|sklearn/utils/tests/test_murmurhash.py|sklearn/utils/murmurhash.c|sklearn/utils/murmurhash.pxd|sklearn/utils/murmurhash.pyx|sklearn/utils/setup.py|sklearn/utils/src/MurmurHash3.h,49,0.0,0,5,false,MRG: Murmurhash cython utility wrapper Here is a cython wrapper for the non cryptographic [murmurhash](https://codegooglecom/p/smhasher/wiki/MurmurHash3) hash function ([Wikipedia entry](https://codegooglecom/p/smhasher/wiki/MurmurHash3))This hash function is commonly used to implement feature hashing (as in Vowpal Wabbit and in Apache Mahout for instance) As far as I know it could also be useful to implement Bloom filters or Count Min Sketch for instanceI plan to use it both for the Random Projection branch (as an alternative to explicit sparse matrix allocation) and to implement a very scalable vocabulary-less text feature extractorAn alternative would have been Googles [CityHash](https://codegooglecom/p/cityhash/) which could be even faster but its apparently less commonly used and its README mentions poor avalanching issue on small sized inputs (16 bytes) which is a very common case for feature hashing,,360,0.8305555555555556,0.3229235880398671,22068,295.35979699111834,29.273155700561897,84.23962298350553,1319,29,518,37,unknown,ogrisel,ogrisel,true,ogrisel,22,0.8181818181818182,423,109,965,true,false,false,false,100,265,31,16,252,3,16
624916,scikit-learn/scikit-learn,python,563,1326763348,,1326780736,289,,unknown,false,false,false,44,5,5,0,1,0,1,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,96,15,96,15,25.957193188772564,0.8940986936114387,43,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py|sklearn/metrics/__init__.py|sklearn/metrics/metrics.py,30,0.009302325581395349,0,2,false,Add Matthews Correlation Coefficient to sklearnmetrics This adds a new score mcc_score to metricsMatthews Correlation Coefficient (MCC) is a measure for the quality of a binary classifier Like the F1 score it considers both recall and precision http://enwikipediaorg/wiki/Matthews_correlation_coefficientIve also added some tests,,359,0.8328690807799443,0.3229235880398671,22068,295.35979699111834,29.273155700561897,84.23962298350553,1318,29,517,37,unknown,gwtaylor,gwtaylor,true,,0,0,17,5,148,true,true,false,false,0,0,0,0,3,0,6
624920,scikit-learn/scikit-learn,python,562,1326747356,,1326993677,4105,,unknown,false,false,false,67,1,1,0,8,0,8,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,123,0,123,0,4.277563336719622,0.14734119222318168,41,vlad@vene.ro,sklearn/cross_validation.py,41,0.02724252491694352,0,2,false,MRG: Implement next in cross_validation objects The Python docs define an iterator as an object that implement __iter__ and next()  This pull request implements the next() method on some generators which I find extremely useful for interactive purposesIt is not complete however as its a time-consuming task but whats already here is useful and this way I get some feedback before the task is completed,,358,0.835195530726257,0.3229235880398671,22068,295.35979699111834,29.273155700561897,84.23962298350553,1316,29,517,38,unknown,fabianp,fabianp,true,,23,0.7391304347826086,80,20,611,true,true,false,false,43,49,11,0,48,0,148
624921,scikit-learn/scikit-learn,python,561,1326745383,1326816137,1326816137,1179,1179,github,false,false,false,38,10,2,0,8,0,8,0,4,0,1,6,21,7,0,0,0,2,19,21,19,0,0,254,0,621,129,34.793776952061044,1.1982074220094392,121,vlad@vene.ro,sklearn/linear_model/base.py|sklearn/linear_model/sparse/base.py|sklearn/linear_model/sparse/logistic.py|sklearn/svm/sparse/classes.py|sklearn/linear_model/sparse/logistic.py|sklearn/svm/base.py|sklearn/svm/sparse/base.py|sklearn/svm/sparse/classes.py|sklearn/utils/validation.py,57,0.020639147802929428,0,4,false,MRG: Merge sparse and dense LinearSVC I took up the chore of refactoring the liblinear bindings so as to merge sparse and dense LinearSVC Its almost done only the two versions of LogisticRegression still have to be merged,,357,0.834733893557423,0.3215712383488682,21956,299.23483330296955,29.468026963016943,84.25942794680269,1316,29,517,37,unknown,larsmans,larsmans,true,larsmans,32,0.6875,53,27,547,true,true,false,false,41,64,11,0,138,1,10
624922,scikit-learn/scikit-learn,python,557,1326685753,1326710932,1326710932,419,419,commits_in_master,false,false,false,283,3,0,0,3,0,3,0,3,0,0,0,5,0,0,0,6,1,4,11,7,0,0,0,0,2574,10,0,0.0,0,,,0,0.0,1,0,false,HLP: trying to wrap MurmurHash3 Hi allI am trying to make a cython wrapper for the fast hash function called murmurhash which arguably (one of) the fastest (non-cryptographic) hash function used in machine learningI would like to use it to make some experiments in the Random Projection branch and build an efficient hashing vectorizer to implement the hashing trick:  http://metaoptimizecom/qa/questions/6943/what-is-the-hashing-trickHowever my cython / c++ / distutils fu is lessMaybe @fabian could have  a quick look When I run the test I get:nosetests sklearn/utils/tests/test_murmurhashpyEERROR: Failure: ImportError (/home/ogrisel/coding/scikit-learn/sklearn/utils/murmurhashso: undefined symbol: MurmurHash3_x86_32)----------------------------------------------------------------------Traceback (most recent call last):  File /usr/local/lib/python27/dist-packages/nose/loaderpy line 390 in loadTestsFromName    addrfilename addrmodule)  File /usr/local/lib/python27/dist-packages/nose/importerpy line 39 in importFromPath    return selfimportFromDir(dir_path fqname)  File /usr/local/lib/python27/dist-packages/nose/importerpy line 86 in importFromDir    mod  load_module(part_fqname fh filename desc)  File /home/ogrisel/coding/scikit-learn/sklearn/utils/tests/test_murmurhashpy line 1 in module    from sklearnutilsmurmurhash import murmurhash3ImportError: /home/ogrisel/coding/scikit-learn/sklearn/utils/murmurhashso: undefined symbol: MurmurHash3_x86_32----------------------------------------------------------------------Ran 1 test in 0001sFAILED (errors1)However my setuppy is building a library called libMurmurHash3a successfully and it is passed in the C flags used for building the murmurhashso extension:building MurmurHash3 librarycompiling C++ sourcesC compiler: g++ -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -fPICcompile options: -I/usr/lib/pymodules/python27/numpy/core/include -cg++: sklearn/utils/src/MurmurHash3cppar: adding 1 object files to build/templinux-x86_64-27/libMurmurHash3acustomize UnixCCompilercustomize UnixCCompiler using build_extresetting extension sklearnsvmliblinear language from c to c++customize UnixCCompilercustomize UnixCCompiler using build_extbuilding sklearnutilsmurmurhash extensioncompiling C sourcesC compiler: gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPICcompile options: -Isklearn/utils/src -I/usr/lib/pymodules/python27/numpy/core/include -I/usr/include/python27 -cgcc: sklearn/utils/murmurhashcg++ -pthread -shared -Wl-O1 -Wl-Bsymbolic-functions -Wl-Bsymbolic-functions build/templinux-x86_64-27/sklearn/utils/murmurhasho -Lbuild/templinux-x86_64-27 -lMurmurHash3 -o sklearn/utils/murmurhashsorunning sconsI dont get any build error What am I doing wrong,,356,0.8342696629213483,0.32066666666666666,22060,294.92293744333637,29.23844061650045,84.17951042611061,1316,29,516,35,unknown,ogrisel,ogrisel,true,ogrisel,21,0.8095238095238095,423,109,963,true,false,false,false,97,258,30,16,253,3,361
624923,scikit-learn/scikit-learn,python,554,1326479936,1326792148,1326792148,5203,5203,github,false,false,false,117,1,0,0,22,0,22,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,15,true,MRG: partial_fit in SGD I started to work on adding partial_fit to SGD This turned out to be a bit harder than I thought* For classifications I added a classes argument which is compulsory on the first call to partial_fit Inferring the classes from X is prone to error: I got bitten while writing tests (some labels may be missing from y)* When calling partial_fit I wonder if I we should use n_iter or just set it to 1 Currently Im doing the former but it will probably bias the training if X is small* There was a bug in the intercept handling in the binary case (intercept_ was an ndarray of shape ()),,355,0.8338028169014085,0.3160377358490566,21908,296.2388168705496,29.121781997443858,82.84644878583165,1311,29,514,38,unknown,mblondel,mblondel,true,mblondel,11,0.9090909090909091,107,25,654,true,false,false,false,46,74,9,3,45,0,19
624924,scikit-learn/scikit-learn,python,553,1326421227,1326631464,1326631464,3503,3503,merged_in_comments,false,false,false,22,4,2,0,12,0,12,0,5,1,0,1,3,1,0,0,1,0,2,3,1,0,0,169,0,223,0,9.586260697128026,0.3307656105897357,0,,examples/applications/plot_tomography_l1_reconstruction.py|examples/applications/plot_tomography_l1_reconstruction.py,0,0.0,0,3,false,WIP: example on tomography reconstruction with l1 prior for the gallery An example of compressive sensing in the field of tomography reconstruction,,354,0.8333333333333334,0.31304935767410413,22014,292.04142818206594,28.890705914418096,82.76551285545563,1309,29,513,35,unknown,emmanuelle,GaelVaroquaux,false,GaelVaroquaux,2,0.5,22,6,619,false,true,false,false,1,0,0,0,0,0,259
624925,scikit-learn/scikit-learn,python,551,1326365027,1326367409,1326367409,39,39,github,false,false,false,7,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Fix an error in naive bayes doc ,,353,0.8328611898016998,0.31271015467383995,22000,291.6363636363636,28.81818181818182,82.63636363636363,1309,29,513,34,unknown,fannix,ogrisel,false,ogrisel,3,0.6666666666666666,8,0,365,true,true,false,false,3,19,1,1,2,0,-1
270518,scikit-learn/scikit-learn,python,547,1326173206,1328196325,1328196325,33718,33718,github,false,false,false,13,115,95,55,44,1,100,1,7,16,5,22,50,24,0,0,21,7,27,55,29,0,0,3912,502,4293,515,792.5608164113834,27.34702987592652,102,vlad@vene.ro,doc/modules/label_propagation.rst|doc/unsupervised_learning.rst|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|doc/modules/label_propagation.rst|examples/semisupervised/label_propagation_versus_svm_iris.py|examples/semisupervised/plot_label_propagation_digits.py|examples/semisupervised/plot_label_propagation_digits_active_learning.py|examples/semisupervised/plot_label_propagation_structure.py|examples/semisupervised/plot_label_propagation_versus_svm_iris.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|sklearn/utils/fixes.py|sklearn/label_propagation.py|examples/semisupervised/label_propagation_versus_svm_iris.py|examples/semisupervised/plot_label_propagation_digits.py|examples/semisupervised/plot_label_propagation_digits_active_learning.py|examples/semisupervised/plot_label_propagation_structure.py|examples/semisupervised/plot_label_propagation_versus_svm_iris.py|sklearn/label_propagation.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|doc/modules/label_propagation.rst|doc/unsupervised_learning.rst|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|doc/modules/label_propagation.rst|doc/modules/label_propagation.rst|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|examples/svm/plot_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|doc/modules/label_propagation.rst|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|examples/label_propagation/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/label_propagation/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|examples/label_propagation/label_propagation_versus_svm_iris.py|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/label_propagation_versus_svm_iris.py|scikits/learn/tests/test_label_propagation.py|doc/modules/label_propagation.rst|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|examples/label_propagation/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/label_propagation_versus_svm_iris.py|examples/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/label_propagation_versus_svm_iris.py|examples/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|examples/plot_label_propagation_versus_svm_iris.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|examples/svm/plot_iris.py|scikits/learn/label_propagation.py|doc/modules/label_propagation.rst|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|examples/plot_digits_classification.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|scikits/learn/label_propagation.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_structure.py|doc/modules/label_propagation.rst|doc/modules/classes.rst|doc/modules/label_propagation.rst|examples/plot_digits_classification.py|doc/modules/classes.rst|doc/modules/label_propagation.rst|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_structure.py|scikits/learn/label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|sklearn/label_propagation.py|sklearn/utils/fixes.py|sklearn/label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_structure.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|sklearn/utils/fixes.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_digits.py|sklearn/label_propagation.py|sklearn/label_propagation.py|examples/semi_supervised/label_propagation_digits_active_learning.py|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|sklearn/label_propagation.py|sklearn/label_propagation.py|sklearn/label_propagation.py|sklearn/tests/test_label_propagation.py|examples/semi_supervised/plot_label_propagation_structure.py|sklearn/label_propagation.py,82,0.0,0,12,false,Label propagation Label propagation algorithms for semi-supervised classification Reissue of this PR https://githubcom/scikit-learn/scikit-learn/pull/301,,352,0.8323863636363636,0.30960376091336467,22110,290.09497964721845,28.855721393034823,82.76797829036634,1301,29,511,39,unknown,clayw,GaelVaroquaux,false,GaelVaroquaux,1,1.0,8,11,988,true,true,false,false,1,0,0,0,3,0,15473
268782,scikit-learn/scikit-learn,python,546,1326159076,1327803211,1327803211,27402,27402,github,false,false,false,207,14,1,2,23,6,31,0,5,0,0,4,9,4,0,0,0,0,9,9,7,0,0,24,0,313,35,12.68472685992176,0.4332138993617147,88,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/liblinear.c|sklearn/svm/liblinear.pyx|sklearn/svm/sparse/base.py,72,0.02824478816408877,0,5,false,MRG Svm coef sign in liblinear and libsvm fixing libsvm coef_ The issue was that the signs of the decision function are flipped for the two class case in liblinear and libsvmTo put it otherwise some wrapping code sorts classes in ascending order while the libsvm code expects the first class in a two class setting to be +1 and the second one -1I fixed this by swapping the roles of +1 and -1 in liblinearI felt this is the cleanest solution I am a bit weary of touching the liblinear code since that could complicate upgradesWhat do you thinkThe main reason that I did not do this in cython is the following:The Cython wrappers get their coefs for prediction from the Python classIf I want to switch the sign there Ill have to switch it back when doing predictionsSo I have to switch it after fit before calling predict or decision function and then switch again on the value of the decision functionThis seemed ugly and confusing to meThere is also a second thing this pr fixes (yeah I know shouldnt do that):the primal coef_ of libSVM are now calculated correctly in the one-vs-one case,,351,0.8319088319088319,0.31002017484868866,22065,302.24337185588035,29.63970088375255,85.38409245411286,1301,29,510,37,unknown,amueller,amueller,true,amueller,37,0.918918918918919,237,21,444,true,true,false,false,86,117,45,0,248,5,878
624926,scikit-learn/scikit-learn,python,545,1326107662,1326109292,1326109292,27,27,github,false,false,false,42,3,0,0,3,0,3,0,3,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,13,0,0,0.0,0,,,0,0.0,0,0,false,MRG Sgd fortran layout Enhanced test time performance for SGDClassifier by converting coef_ to fortran-style array after model fitting This only affects multi-class classification Without this fix there is a rather large overhead for making predictions for single data pointssee http://sourceforgenet/mailarchive/messagephpmsg_id28644874,,350,0.8314285714285714,0.3129161118508655,22120,289.9638336347197,28.661844484629295,82.36889692585896,1299,29,510,32,unknown,pprett,pprett,true,pprett,14,0.9285714285714286,50,23,888,true,true,false,false,19,39,6,4,15,0,5
624927,scikit-learn/scikit-learn,python,541,1325962662,1325968587,1325968588,98,98,github,false,false,false,134,5,0,0,3,0,3,0,3,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,56,20,0,0.0,0,,,0,0.0,0,0,false,MRG: Explicitly mark the array returned by the linear svm readonly property coef_ as immutable This pull-request #539 should be taken into account to  understand the context about the debate of readonly properties in the scikit-learn APIThis pull request is also related to issue #470 LogisticRegression and LinearSVC should have read-write coef_ and intercept_ attributes (it does not fix it but makes it a less dangerous behavior)The following is an example session were the unsuspecting user might overlook a bug:  from sklearnsvm import SVC from sklearndatasets import load_iris iris  load_iris() clf  SVC(kernellinear)fit(irisdata iristarget) clfcoef_array([[-10443518   001195155 -191966087 -081040704]       [ 15863679   166207276 -165274791 -195241805]]) clfcoef_[0 3]  0 clfcoef_array([[-10443518   001195155 -191966087 -081040704]       [ 15863679   166207276 -165274791 -195241805]]),,349,0.830945558739255,0.31568497683653207,22127,287.1152890134225,28.5623898404664,82.11687079134089,1299,29,508,37,unknown,ogrisel,ogrisel,true,ogrisel,20,0.8,418,109,955,true,false,false,false,92,229,25,11,284,3,51
624928,scikit-learn/scikit-learn,python,539,1325880365,1325966515,1325966515,1435,1435,commits_in_master,false,false,false,34,1,1,0,11,0,11,0,5,0,0,8,8,7,0,0,0,0,8,8,7,0,0,127,4,127,4,32.70153789273368,1.1283428603311871,188,vlad@vene.ro,doc/modules/feature_selection.rst|examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_forest_importances_faces.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/feature_selection/selector_mixin.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,99,0.02495075508864084,0,4,true,MRG Make feature_importances_ a property Modules involved: treetree ensembleforestEliminate compute_importance fit parameter - make feature_importances_ a property that will be computed on demandThis eliminates ~40 lines of code and one fit parameter,,348,0.8304597701149425,0.319763624425476,22032,287.6724763979666,28.594771241830063,82.51633986928104,1297,30,507,36,unknown,pprett,pprett,true,pprett,13,0.9230769230769231,50,23,885,true,true,false,false,16,39,6,4,17,0,1
624929,scikit-learn/scikit-learn,python,538,1325877406,1331344655,1331344655,91120,91120,github,false,false,false,41,1,0,21,92,0,113,0,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,41,true,added a cython module to the hmm This is a Cython module to the hmmIt contains the equivalent codes to the fortran90 module which I tried to commit beforePure python loops in Baum-Welch are replaced by fast cython loops,,347,0.829971181556196,0.319763624425476,22990,295.04132231404964,28.925619834710744,83.38408003479773,1296,30,507,63,unknown,lucidfrontier45,GaelVaroquaux,false,GaelVaroquaux,2,0.5,6,1,305,false,true,false,false,2,7,2,0,0,0,8
624930,scikit-learn/scikit-learn,python,537,1325868477,1325967484,1325967484,1650,1650,github,false,false,false,89,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,44,0,0,0.0,0,,,0,0.0,1,1,true,MRG ENH in GaussianNB let estimated parameters have underscores This PR is to address issue #108It is a renaming of attributes to have underscores and introduces deprecated read only propertiesFor GaussianNB there are not tests or examples working with these attributesLooking at tests and examples there seems to be very little for NB at allI am not sure what @larsmans means by formulating GNB as a linear classifierCan you explain Would that eliminate the need for renaming as the parameters would be different anyway,,346,0.8294797687861272,0.32020997375328086,22032,287.6724763979666,28.594771241830063,82.51633986928104,1296,30,507,36,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,36,0.9166666666666666,232,21,441,true,true,true,false,73,91,44,0,246,5,1650
624931,scikit-learn/scikit-learn,python,536,1325858761,1328578354,1328578354,45326,45326,commit_sha_in_comments,false,false,false,42,1,1,0,4,0,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,31,0,31,0,8.923939618967362,0.3079140704872003,79,vlad@vene.ro,sklearn/svm/base.py|sklearn/svm/classes.py,55,0.03608923884514436,0,0,true,WIP Adding class weight parameter to svm constructors Trying to address issue #64 (Make class_weights a constructor parameter for all classifier models)After this modification the grid search test fails The svm can not be cloned any more I dont get that,,345,0.8289855072463768,0.32020997375328086,22032,287.6724763979666,28.594771241830063,82.51633986928104,1296,30,507,38,unknown,amueller,amueller,true,amueller,35,0.9142857142857143,232,21,441,true,true,false,false,67,80,43,0,246,3,12
624932,scikit-learn/scikit-learn,python,535,1325813512,1325842683,1325842683,486,486,github,false,false,false,17,1,0,0,3,0,3,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,13,20,0,0.0,0,,,0,0.0,0,0,false,ENH: Make LinearRegression work with sparse Fixes #533Trivial PR to make LinearRegression work with sparse data,,344,0.8284883720930233,0.32105263157894737,22016,287.24563953488376,28.570130813953487,82.48546511627906,1296,30,506,34,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,12,0.5833333333333334,177,2,682,true,true,false,false,72,150,23,6,240,3,19
624933,scikit-learn/scikit-learn,python,534,1325812816,1325967160,1325967160,2572,2572,merged_in_comments,false,false,false,45,4,1,0,3,9,12,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.9517497016895,0.1708550856703679,37,vlad@vene.ro,doc/modules/linear_model.rst,37,0.024342105263157894,0,2,false,MRG DOC Reworking Bayesian regression documentation I wanted to address issue #45Well beta is w But I couldnt for the life of me understand the restSo I did a rewrite Hope it is better nowI think I should go to bed ,,343,0.8279883381924198,0.32105263157894737,22016,287.24563953488376,28.570130813953487,82.48546511627906,1296,30,506,37,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,34,0.9117647058823529,232,21,440,true,true,true,false,65,75,42,0,245,1,2561
624934,scikit-learn/scikit-learn,python,532,1325804459,1325849551,1325849551,751,751,github,false,false,false,16,2,0,0,5,0,5,0,4,0,0,0,9,0,0,0,0,0,9,9,9,0,0,0,0,45,20,0,0.0,0,,,0,0.0,0,0,false,MRG Add underscores to estimated attributes in GridSearchCV and deprecation warnings This should fix Issue #531,,342,0.827485380116959,0.32126398946675444,22016,287.24563953488376,28.570130813953487,82.48546511627906,1295,30,506,33,unknown,amueller,agramfort,false,agramfort,33,0.9090909090909091,232,21,440,true,true,true,false,64,75,41,0,244,1,148
624817,scikit-learn/scikit-learn,python,530,1325785712,,1405274452,1324752,,unknown,false,false,false,60,2,1,0,20,6,26,0,6,0,0,2,2,2,0,0,0,0,2,2,2,0,0,11,0,20,0,9.234036996424283,0.3186112199502954,108,vlad@vene.ro,sklearn/__init__.py|sklearn/setup.py,66,0.043449637919684,1,1,false,MRG: Inplace Import Fix This addresses the problem that @amueller was having running tests  It raises an ImportError if sklearn is imported within the scikit-learn source directory  The fix is based on a similar check within the scipy setup fileIm a bit of a distutils novice: this worked on my system but Id love some feedback before its merged,,341,0.8299120234604106,0.32126398946675444,22016,287.24563953488376,28.570130813953487,82.48546511627906,1294,30,506,269,unknown,jakevdp,larsmans,false,,16,1.0,438,0,239,true,true,false,false,17,25,7,0,32,5,2
624935,scikit-learn/scikit-learn,python,529,1325781111,1325785067,1325785067,65,65,github,false,false,false,56,2,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,11,0,0,0.0,0,,,0,0.0,0,1,false,Doc fix in cross_validationpy If you pass your own score_func to cross_val_score then this function takes one argument (X_test) in the non supervised setting and two arguments (y_true and y_pred which is estimatorpredict(X_test)) in the supervised settingAs opposed to the three arguments (estimator X_test y_test) that were mentioned beforeHope I got this right :),,340,0.8294117647058824,0.32126398946675444,22015,287.25868725868725,28.57142857142857,82.4892119009766,1294,30,506,31,unknown,eickenberg,ogrisel,false,ogrisel,0,0,2,2,0,false,true,true,false,0,0,0,0,0,0,48
624937,scikit-learn/scikit-learn,python,528,1325770630,1325778490,1325778490,131,131,github,false,false,false,12,2,0,0,5,0,5,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,13,0,0,0.0,0,,,0,0.0,0,2,false,MRG: Fix to issue #349 This is a fix to issue #349,,339,0.8289085545722714,0.32126398946675444,22015,287.25868725868725,28.57142857142857,82.4892119009766,1293,30,506,31,unknown,glouppe,glouppe,true,glouppe,17,1.0,56,17,420,true,true,false,false,24,74,20,0,99,0,1
624938,scikit-learn/scikit-learn,python,527,1325769987,1325770106,1325770106,1,1,github,false,false,false,25,1,0,0,1,0,1,0,3,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,6,0,0,0.0,0,,,0,0.0,0,1,false,Broken links to Rubinsteins K-SVD paper A fix for a couple of broken links Just found out about it while checking MDP website with linkchecker,,338,0.8284023668639053,0.32126398946675444,22015,287.25868725868725,28.57142857142857,82.4892119009766,1293,30,506,31,unknown,otizonaizit,glouppe,false,glouppe,0,0,5,0,526,false,true,false,false,0,0,0,0,0,0,1
624939,scikit-learn/scikit-learn,python,526,1325724131,1325967387,1325967387,4054,4054,github,false,false,false,34,2,0,0,2,0,2,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,90,0,0,0.0,0,,,0,0.0,0,0,false,MRG DOC uncommenting doctests in balltreepyx adding doctest: +SKIP this is not terribly pretty in the source but looks good in the docsI think this is a reasonable compromiseThis addresses Issue #342,,337,0.827893175074184,0.32481990831696134,22015,287.25868725868725,28.57142857142857,82.4892119009766,1293,30,505,35,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,32,0.90625,232,21,439,true,true,true,false,59,67,40,0,244,1,1982
624940,scikit-learn/scikit-learn,python,524,1325698738,,1325877776,2983,,unknown,false,false,false,59,1,1,0,18,0,18,0,5,1,0,2,3,3,0,0,1,0,2,3,3,0,0,235,0,235,0,13.340512802117926,0.46030198551194473,49,vlad@vene.ro,sklearn/_hmmf.f90|sklearn/hmm.py|sklearn/setup.py,45,0.0045632333767926985,0,6,false,Fortran 90 module for hmm I added f90 module for forward backward viterbi and some helper function for hmmThey replace slow python loop in hmmpy and significantly improve computational speedFor 3-state2D Gaussian HMM the original hmmpy need about 10 sec to run 10 cycles of Baum-Welch for 5000 points while the modified one only needs 1 sec ,,336,0.8303571428571429,0.32790091264667537,22015,287.1678401090166,28.57142857142857,82.44378832614127,1291,30,505,35,unknown,lucidfrontier45,lucidfrontier45,true,,1,1.0,6,1,303,false,true,false,false,1,2,1,0,0,0,2
624941,scikit-learn/scikit-learn,python,523,1325594043,1325595689,1325595689,27,27,github,false,false,false,42,5,0,0,5,0,5,0,3,0,0,0,6,0,0,0,0,0,6,6,5,0,0,0,0,369,15,0,0.0,0,,,0,0.0,1,0,false,MRG: Trees/Forest - Better doc better default values This PR addresses the issues discussed with @amueller on the mailing list - Better default values (max_features max_depth)- Consistency with the narrative documentation- Improved the chapter on forests of trees- Tests,,335,0.8298507462686567,0.3305626598465473,22004,286.90238138520266,28.585711688783856,82.39411016178877,1290,30,504,29,unknown,glouppe,glouppe,true,glouppe,16,1.0,56,15,418,true,true,false,false,24,71,15,0,100,0,10
485125,scikit-learn/scikit-learn,python,522,1325563946,1359991384,1359991384,573790,573790,github,false,true,false,51,603,184,299,269,6,574,0,15,14,1,16,60,26,0,0,17,5,41,63,40,0,0,3902,1287,17391,2238,616.8594779069333,18.732233140640165,782,vlad@vene.ro,examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/ensemble/bagging.py|sklearn/ensemble/boosting.py|sklearn/ensemble/boosting.py|sklearn/ensemble/bagging.py|sklearn/ensemble/tests/test_boosting.py|sklearn/ensemble/boosting.py|sklearn/ensemble/base.py|sklearn/ensemble/boosting.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/boosting.py|sklearn/ensemble/tests/test_boosting.py|sklearn/ensemble/boosting.py|sklearn/ensemble/base.py|sklearn/ensemble/boosting.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/tree/tree.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_importances.py|examples/ensemble/plot_boost_iris.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_iris.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pxd|sklearn/tree/_tree.pyx|sklearn/ensemble/boosting.py|sklearn/ensemble/tests/test_boosting.py|sklearn/ensemble/boosting.py|sklearn/ensemble/base.py|sklearn/ensemble/boosting.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/__init__.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/tree/tree.py|sklearn/ensemble/boost.py|sklearn/ensemble/tests/test_boost.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_importances.py|examples/ensemble/plot_boost_iris.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|examples/ensemble/plot_boost_iris.py|sklearn/ensemble/boost.py|examples/ensemble/plot_boost_error.py|sklearn/ensemble/boost.py|sklearn/ensemble/boost.py|sklearn/tree/tree.py|examples/ensemble/plot_adaboost_classification.py|examples/ensemble/plot_adaboost_error.py|examples/ensemble/plot_adaboost_iris.py|examples/ensemble/plot_adaboost_quantiles.py|examples/ensemble/plot_adaboost_regression.py|sklearn/base.py|sklearn/datasets/__init__.py|sklearn/datasets/samples_generator.py|sklearn/ensemble/__init__.py|sklearn/ensemble/tests/test_weight_boosting.py|sklearn/ensemble/weight_boosting.py|sklearn/grid_search.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,302,0.009596928982725527,0,181,false,MRG: AdaBoost for regression and multi-class classification This PR adds:* a new ensembleweight_boosting module with AdaBoostRegressor (using AdaBoostR2 [1]) and AdaBoostClassifier (using the multi-class SAMME algorithm [2])* a new Gaussian quantiles dataset in datasetssamples_generator as used in [2]Examples are provided:[hastie](https://fcloudgithubcom/assets/202816/74736/07057cd0-60a3-11e2-87e3-dd8572078d11png)[twoclass](https://fcloudgithubcom/assets/202816/121925/d7a0c70c-6e11-11e2-8927-e0471292262fpng)[multiclass](https://fcloudgithubcom/assets/202816/121927/dceefd78-6e11-11e2-8959-60c051ef6118png)[regression](https://fcloudgithubcom/assets/202816/121928/e1155136-6e11-11e2-9cf2-1d51cae3969apng)[1] http://citeseerxistpsuedu/viewdoc/summarydoi101131314[2] http://wwwstanfordedu/~hastie/Papers/sammepdf,,334,0.8293413173652695,0.3314139475367882,25581,322.50498416793715,31.15593604628435,89.98866346116257,1290,30,504,188,unknown,ndawe,GaelVaroquaux,false,GaelVaroquaux,1,1.0,13,32,690,true,true,true,false,1,0,0,0,184,0,15
624943,scikit-learn/scikit-learn,python,521,1325512607,1325538345,1325538345,428,428,github,false,false,false,50,3,0,0,7,0,7,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,40,0,0,0.0,0,,,0,0.0,0,9,false,MRG: improved  the speed of training dpgmm I improved  the speed of training dpgmm by calling fast scipy built-in function and replacing unnecessary double loop The original code needs over 10 sec to train 5000 points in 2D with 5 states while the modified one only needs about 1 sec,,333,0.8288288288288288,0.3359123146357189,21911,288.1201223129935,28.707042124960065,82.7438273013555,1288,30,503,28,unknown,lucidfrontier45,ogrisel,false,ogrisel,0,0,6,1,301,false,true,false,false,0,0,0,0,0,0,8
624944,scikit-learn/scikit-learn,python,520,1325296326,1325349199,1325349199,881,881,merged_in_comments,false,false,false,20,5,1,0,6,4,10,0,5,0,0,9,9,7,0,0,0,0,9,9,7,0,0,90,37,100,37,41.58950844850881,1.4350334446483033,190,vlad@vene.ro,benchmarks/bench_plot_svd.py|doc/developers/utilities.rst|doc/whats_new.rst|examples/applications/wikipedia_principal_eigenvector.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/nmf.py|sklearn/decomposition/pca.py|sklearn/utils/extmath.py|sklearn/utils/tests/test_svd.py,96,0.017615176151761516,0,0,true,MRG: better function and parameter names for randomized_svd Renamed fast_svd to randomized_svd and use explicit variable names and fixed docstring,,332,0.8283132530120482,0.35704607046070463,21737,290.2424437594884,28.982840318351197,83.40617380503289,1285,30,500,29,unknown,ogrisel,ogrisel,true,ogrisel,19,0.7894736842105263,409,109,947,true,false,false,false,84,211,27,8,310,4,76
624945,scikit-learn/scikit-learn,python,519,1325249123,1325272061,1325272061,382,382,github,false,false,false,37,1,0,0,3,0,3,0,7,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,40,9,0,0.0,0,,,0,0.0,0,7,true,ENH: make ShuffleSplit able to subsample the data Add the train_fraction option to ShuffleSplit This will be useful to easily implement the cv / training errors curves to qualitatively evaluate the under / overfitting of a model,,331,0.8277945619335347,0.3657694962042788,21669,289.1688587382897,28.935345424338916,83.06797729475288,1285,30,500,30,unknown,ogrisel,ogrisel,true,ogrisel,18,0.7777777777777778,409,109,947,true,false,false,false,84,210,24,8,309,4,309
624946,scikit-learn/scikit-learn,python,515,1324779954,1325104508,1325104508,5409,5409,github,false,false,false,40,23,5,0,9,0,9,0,4,0,3,3,13,6,0,0,0,3,10,13,11,0,0,1388,0,1754,87,32.77242546158745,1.130338563489027,53,vlad@vene.ro,sklearn/preprocessing/__init__.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/preprocessing/__init__.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/preprocessing/__init__.py|sklearn/preprocessing/setup.py|sklearn/preprocessing/src/_preprocessing.c|sklearn/preprocessing/src/_preprocessing.pyx|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx,39,0.010107816711590296,1,4,false,MRG: support for variance scaling on CSR matrices Leverage @mblondels work on Cython utility to compute mean and variance of CSR matrix to reuse it in the sklearnpreprocessing module for variance scalingAlso move cython code from sklearnpreprocessing to sklearnutilssparsefuncs,,330,0.8272727272727273,0.3793800539083558,21484,284.3045987711785,28.905231800409606,81.13014336250232,1276,30,494,31,unknown,ogrisel,ogrisel,true,ogrisel,17,0.7647058823529411,405,109,941,true,false,false,false,88,214,21,8,314,4,1304
624947,scikit-learn/scikit-learn,python,514,1324715373,,1331438024,112044,,unknown,false,false,false,24,4,1,3,15,0,18,0,6,0,0,1,2,1,0,0,0,0,2,2,2,0,0,1,0,7,11,4.118317975842983,0.14209853440057035,48,vlad@vene.ro,sklearn/feature_extraction/text.py,48,0.03236682400539447,0,10,false,MRG: Disallow negative tf-idf weight Add the same smoothing count to n_samples to avoid negative weightTODO before merge:- make all tests pass,,329,0.8297872340425532,0.38165879973027644,22015,287.25868725868725,28.57142857142857,82.4892119009766,1273,30,494,61,unknown,fannix,larsmans,false,,2,1.0,8,0,346,true,true,false,false,2,11,0,0,1,0,128
624948,scikit-learn/scikit-learn,python,513,1324680665,1325350003,1325350003,11155,11155,github,false,false,false,31,34,0,0,5,0,5,0,3,0,0,0,48,0,0,0,0,0,48,48,42,0,0,0,0,1142,2,0,0.0,0,,,0,0.0,0,1,true,MRG Doc underscores for real Fix (hopefully all) reference issue due to estimated_ attributes Also other minor stuffThis should fix all rst errorsThere are some sphinx errors left though,,328,0.8292682926829268,0.3836734693877551,21737,290.2424437594884,28.982840318351197,83.40617380503289,1272,29,493,30,unknown,amueller,amueller,true,amueller,31,0.9032258064516129,225,21,427,true,true,false,false,45,45,37,0,230,1,164
624949,scikit-learn/scikit-learn,python,512,1324638012,1324641429,1324641429,56,56,github,false,false,false,24,6,6,0,2,0,2,0,4,0,0,51,51,50,0,0,0,0,51,51,50,0,0,613,10,613,10,289.7230518793987,10.007654668615636,496,vlad@vene.ro,sklearn/svm/liblinear.pyx|sklearn/svm/libsvm.pyx|sklearn/svm/sparse/libsvm.pyx|sklearn/decomposition/kernel_pca.py|sklearn/linear_model/omp.py|sklearn/linear_model/sparse/coordinate_descent.py|sklearn/base.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/mean_shift_.py|sklearn/covariance/outlier_detection.py|sklearn/covariance/robust_covariance.py|sklearn/covariance/shrunk_covariance_.py|sklearn/datasets/descr/boston_house_prices.rst|sklearn/datasets/samples_generator.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/kernel_pca.py|sklearn/decomposition/pca.py|sklearn/feature_extraction/text.py|sklearn/feature_selection/rfe.py|sklearn/feature_selection/univariate_selection.py|sklearn/gaussian_process/gaussian_process.py|sklearn/grid_search.py|sklearn/linear_model/logistic.py|sklearn/linear_model/ridge.py|sklearn/linear_model/sparse/logistic.py|sklearn/manifold/isomap.py|sklearn/manifold/locally_linear.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/unsupervised.py|sklearn/metrics/metrics.py|sklearn/multiclass.py|sklearn/naive_bayes.py|sklearn/pls.py|sklearn/preprocessing/__init__.py|sklearn/tree/tree.py|sklearn/utils/arpack.py|sklearn/utils/extmath.py|sklearn/metrics/cluster/supervised.py|sklearn/utils/__init__.py|sklearn/utils/_csgraph.py|sklearn/cluster/affinity_propagation_.py|sklearn/cluster/dbscan_.py|sklearn/cluster/hierarchical.py|sklearn/cluster/k_means_.py|sklearn/cluster/mean_shift_.py|sklearn/cluster/spectral.py|sklearn/hmm.py|sklearn/linear_model/bayes.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/naive_bayes.py|sklearn/pipeline.py|sklearn/svm/classes.py|sklearn/cluster/_feature_agglomeration.py|sklearn/cluster/_k_means.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/spectral.py|sklearn/datasets/samples_generator.py|sklearn/ensemble/forest.py|sklearn/pls.py|sklearn/svm/liblinear.pyx|sklearn/svm/libsvm.pyx|sklearn/svm/sparse/libsvm.pyx|sklearn/utils/__init__.py,95,0.014314928425357873,0,4,true,MRG The consistency brigade strikes again These are some typo fixes Example-Examples making Reference **bold** removing Methods sections and pyflakes whenever I found some,,327,0.8287461773700305,0.3837764144512611,21464,278.838986209467,28.932165486395824,81.11256056653001,1272,28,493,30,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,30,0.9,225,21,427,true,true,true,false,42,42,36,0,230,1,10
624950,scikit-learn/scikit-learn,python,511,1324521723,1328492465,1328492465,66179,66179,github,false,false,false,56,50,13,0,40,0,40,0,7,0,0,6,16,6,0,0,2,0,16,18,14,0,0,1001,104,2927,709,118.91857945014684,3.9605541612368746,58,vlad@vene.ro,sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/hmm.py|sklearn/mixture/__init__.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/hmm.py|sklearn/mixture/gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/mixture/gmm.py|sklearn/hmm.py|sklearn/mixture/gmm.py|sklearn/mixture/dpgmm.py|sklearn/mixture/gmm.py|sklearn/mixture/tests/test_gmm.py,50,0.01423728813559322,0,9,false,MRG: GMM fixes The PR presents a refactoring of GMM stuff: some renaming a bit more tests more consistency with remainder of SLSo far I have worked only on the classical GMM but Ill consider the Bayesian GMM nextI whould also add  AIC/BIC model selection to the classical GMM soonComments and feedback welcome,,326,0.8282208588957055,0.38033898305084746,22949,294.9148111028803,28.977297485729228,83.5766264325243,1271,28,491,42,unknown,bthirion,GaelVaroquaux,false,GaelVaroquaux,0,0,8,1,630,true,true,true,false,0,0,0,0,13,0,10145
624951,scikit-learn/scikit-learn,python,510,1324505713,1324505730,1324505730,0,0,github,false,false,false,6,4,3,0,0,0,0,0,0,0,0,8,10,8,0,0,0,0,10,10,10,0,0,324,0,352,0,35.451891066385286,1.2245017909895772,112,vlad@vene.ro,sklearn/pipeline.py|sklearn/base.py|sklearn/cross_validation.py|sklearn/multiclass.py|sklearn/pls.py|sklearn/preprocessing/__init__.py|sklearn/utils/__init__.py|sklearn/svm/classes.py,50,0.021739130434782608,0,0,false,MRG Docstring fixes make me cry,,325,0.8276923076923077,0.37771739130434784,21409,279.22836190387216,29.006492596571537,81.3209397916764,1271,28,491,32,unknown,amueller,amueller,true,amueller,29,0.896551724137931,224,21,425,true,true,false,false,41,40,33,0,226,1,-1
624952,scikit-learn/scikit-learn,python,508,1324498622,,1324561276,1044,,unknown,false,false,false,16,3,3,0,1,1,2,0,1,0,0,8,8,8,0,0,0,0,8,8,8,0,0,324,0,324,0,35.45065498527491,1.224458768690834,112,vlad@vene.ro,sklearn/pipeline.py|sklearn/base.py|sklearn/cross_validation.py|sklearn/multiclass.py|sklearn/pls.py|sklearn/preprocessing/__init__.py|sklearn/utils/__init__.py|sklearn/svm/classes.py,50,0.021828103683492497,0,0,false,WIP: Doc fixes underscore and other errors Trying to get rid of all the documentation errors,,324,0.8302469135802469,0.37994542974079126,21397,279.3849605084825,29.022760200028042,81.36654671215591,1270,28,491,32,unknown,amueller,amueller,true,,28,0.9285714285714286,224,21,425,true,true,false,false,39,40,32,0,225,1,1040
624953,scikit-learn/scikit-learn,python,507,1324497964,1324557091,1324557091,985,985,github,false,false,false,56,5,1,0,0,0,0,0,0,0,0,1,4,1,0,0,0,0,4,4,4,0,0,33,0,135,4,4.422841153335059,0.1527627275536914,8,vanderplas@astro.washington.edu,sklearn/manifold/isomap.py,8,0.00546448087431694,0,0,false,MRG: Convert BallTree to Nearest Neighbors This pull request contains changes to three modules: - sklearnclusteringmean_shift - sklearnclusteringisomap - sklearnclusteringlocally_linear_embeddingPreviously all three used a BallTree object directly  With this pull request they use a NearestNeighbors object for more flexibility  All previous tests pass and example plots appear unchangedThis pull request also addresses issue #191,,323,0.8297213622291022,0.3790983606557377,21407,279.1610220955762,29.009202597281263,81.32853739431027,1270,28,491,32,unknown,jakevdp,agramfort,false,agramfort,15,1.0,428,0,224,true,true,false,true,12,29,5,0,50,3,-1
624822,scikit-learn/scikit-learn,python,506,1324497881,,1376504786,866781,,unknown,false,false,false,40,1,1,0,2,1,3,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,88,7,88,7,8.82227339788604,0.27115331649835084,242,vlad@vene.ro,sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py,206,0.1407103825136612,0,0,false,WIP : Approximate nearest neighbor search in KMeans This is an early PR for adding random-projection based nearest neighbor search in KMeans The for loop in _labels_inertia_random_projection cannot be done efficiently in pure Python as far as I see it,,322,0.8322981366459627,0.3790983606557377,26058,334.82999462736973,31.890398342159795,91.37309079745185,1270,28,491,186,unknown,mblondel,mblondel,true,,10,1.0,103,23,631,true,false,false,false,46,70,10,3,42,0,5843
624954,scikit-learn/scikit-learn,python,505,1324492786,,1330715898,103718,,unknown,false,false,false,54,2,1,0,11,0,11,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,6,6,25,4.343252460691162,0.15049966152283278,15,vanderplas@astro.washington.edu,sklearn/cluster/spectral.py,15,0.010351966873706004,0,3,false,HLP: Use shift-invert mode in spectral_embedding I need some help on this I checked empirically that the largest eigenvalue of  -laplacian in spectral_embedding is always 1 and swiching to shift-invert mode makes me win a 25x factor in the lena exampleThe question is: is this true or Im just lucky (no test failures),,321,0.8348909657320872,0.38164251207729466,21319,277.87419672592523,28.941319949340965,81.0544584642807,1270,28,491,49,unknown,fabianp,GaelVaroquaux,false,,22,0.7727272727272727,75,20,585,true,true,true,true,36,42,13,0,51,0,52
624955,scikit-learn/scikit-learn,python,504,1324479295,1324493043,1324493043,229,229,github,false,false,false,69,3,0,0,2,0,2,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,30,0,0,0.0,0,,,0,0.0,0,0,false,HACK: prevent proliferation of image files in documentation build This fixes the problem of image proliferation in the sphinx buildIve tested it on Sphinx 107  If sphinx changes the build layout between  versions this fix will not work  In that case it would not crash but the problem would come backIts a hack but it will clean things up in the absence of a more elegant solution,,320,0.834375,0.3827418232428671,21309,278.0045989957295,28.95490168473415,81.09249612839645,1270,28,491,31,unknown,jakevdp,fabianp,false,fabianp,14,1.0,428,0,224,true,true,false,false,10,28,4,0,45,3,57
624823,scikit-learn/scikit-learn,python,503,1324477194,,1405606416,1352093,,unknown,false,false,false,5,1,1,0,2,2,4,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,10,5,10,5,9.363404588491681,0.28778502925263705,214,vlad@vene.ro,sklearn/linear_model/least_angle.py|sklearn/linear_model/tests/test_least_angle.py,170,0.11896431070678797,0,1,false,Add return_cholesky keyword to lars_path ,,319,0.8369905956112853,0.3827851644506648,26058,334.82999462736973,31.890398342159795,91.37309079745185,1270,28,491,267,unknown,fabianp,fabianp,true,,21,0.8095238095238095,75,20,585,true,true,false,false,34,42,12,0,50,0,1352083
624956,scikit-learn/scikit-learn,python,502,1324468178,,1324469970,29,,unknown,false,false,false,38,1,1,0,1,0,1,0,1,0,0,10,10,10,0,0,0,0,10,10,10,0,0,83,0,83,0,44.915568573247896,1.5642104547532545,31,vlad@vene.ro,sklearn/cluster/setup.py|sklearn/linear_model/setup.py|sklearn/linear_model/sparse/setup.py|sklearn/neighbors/setup.py|sklearn/preprocessing/setup.py|sklearn/setup.py|sklearn/svm/setup.py|sklearn/svm/sparse/setup.py|sklearn/tree/setup.py|sklearn/utils/setup.py,13,0.0042583392476933995,0,0,false,ENH: Silence cython This pull requests makes gcc ignore all the unused symbols produced by cythonAs a result not only does the build look cleaner but also you can actually see if there are any real errors/warnings,,318,0.839622641509434,0.38892831795599714,21054,276.3845350052246,28.498147620404673,81.40970836895602,1270,28,491,35,unknown,amueller,amueller,true,,27,0.9629629629629629,224,21,425,true,true,false,false,34,32,23,0,219,0,29
624957,scikit-learn/scikit-learn,python,501,1324466985,1324656434,1324656434,3157,3157,github,false,false,false,64,5,2,0,7,0,7,0,3,1,0,3,4,3,0,0,1,0,3,4,3,0,0,1,883,1,920,23.125277381928537,0.8027365056814817,26,vlad@vene.ro,examples/applications/plot_species_distribution_modeling.py|sklearn/datasets/__init__.py|sklearn/datasets/species_distributions.py|examples/applications/plot_species_distribution_modeling.py|sklearn/datasets/species_distributions.py,20,0.0042583392476933995,0,4,false,MRG: Species dataset/example fix A few things:- wrote a dataset loader for the species distribution data- cleaned up and documented the species distribution exampleThis decreases run-time for the example from a few minutes to 15 seconds and decreases the memory footprint of the data from ~140MB to ~50MB  This should be reduced to ~15MB when Gaels joblib compressor can be used,,317,0.8391167192429022,0.38892831795599714,21058,276.3320353309906,28.49273435274005,81.39424446766074,1270,28,491,38,unknown,jakevdp,jakevdp,true,jakevdp,13,1.0,428,0,224,true,true,false,false,9,23,3,0,38,3,15
624958,scikit-learn/scikit-learn,python,500,1324439934,1328040942,1328040942,60016,60016,merged_in_comments,false,false,false,30,56,18,0,38,1,39,0,5,4,0,8,19,8,0,0,4,0,15,19,13,0,0,1225,79,2266,141,130.56231680465982,4.451809666600571,130,vlad@vene.ro,examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/randomized_l1.py,78,0.014164305949008499,1,6,false,MRG: Randomized lasso clean Cleaned up Randomized linear models for mergeThis is the final pull request: I am tired of cherr-picking/rebasing @agramfort  you have access to this repo,,316,0.8386075949367089,0.3873937677053824,22256,299.2900790797987,29.385334291876347,84.69626168224299,1269,28,490,47,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,11,0.5454545454545454,173,2,666,true,true,false,false,70,143,18,6,282,3,0
624962,scikit-learn/scikit-learn,python,499,1324439358,,1324439850,8,,unknown,false,false,false,18,17,17,0,1,0,1,0,1,4,0,8,12,8,0,0,4,0,8,12,8,0,0,1271,79,1271,79,126.20183537649446,4.395050821378009,57,vlad@vene.ro,sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py,35,0.007082152974504249,0,1,false,WIP: Randomized lasso rebase Randomized lasso and linear models for feature selection This pull request is cleaned up,,315,0.8412698412698413,0.3873937677053824,21054,276.3845350052246,28.498147620404673,81.40970836895602,1269,28,490,33,unknown,GaelVaroquaux,GaelVaroquaux,true,,10,0.6,173,2,666,true,true,false,false,69,141,17,6,282,3,1
624965,scikit-learn/scikit-learn,python,498,1324437369,,1324437549,3,,unknown,false,false,false,16,23,23,0,0,0,0,0,1,6,2,11,19,15,0,0,6,2,11,19,15,0,0,1454,79,1454,79,153.13350577991534,5.332960100769064,70,vlad@vene.ro,sklearn/utils/fixes.py|sklearn/linear_model/tests/test_randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/utils/fixes.py|sklearn/linear_model/randomized_lasso.py|sklearn/cluster/hierarchical.py|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/_inertia.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/linear_model/randomized_lasso.py|sklearn/cluster/_hierarchical.c|sklearn/cluster/_inertia.c|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/randomized_l1.py,35,0.0,1,0,false,WIP: Randomized lasso This is a rebased version of the Randomized lasso by @agramfort and myself,,314,0.8439490445859873,0.3873937677053824,21112,275.62523683213334,28.419856006062904,81.18605532398635,1269,28,490,34,unknown,GaelVaroquaux,GaelVaroquaux,true,,9,0.6666666666666666,173,2,666,true,true,false,false,67,139,16,6,282,3,-1
624967,scikit-learn/scikit-learn,python,497,1324435437,,1324437110,27,,unknown,false,false,false,19,18,18,0,0,2,2,2,1,4,1,10,15,13,0,0,4,1,10,15,13,0,0,994,0,994,0,110.64692223191437,3.8533407153090873,97,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/cluster/_hierarchical.c|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/least_angle.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/grid_search.py|sklearn/metrics/metrics.py|sklearn/utils/fixes.py|sklearn/linear_model/randomized_lasso.py|sklearn/cluster/hierarchical.py|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/_inertia.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/linear_model/randomized_lasso.py|sklearn/grid_search.py,45,0.0070921985815602835,1,0,false,WIP: Randomized lasso Randomized linear models to identify relevant features This is a cleaned up version of @agramforts PR,,313,0.8466453674121406,0.3872340425531915,21112,275.62523683213334,28.419856006062904,81.18605532398635,1269,28,490,34,unknown,GaelVaroquaux,GaelVaroquaux,true,,8,0.75,173,2,666,true,true,false,false,66,138,15,6,282,3,-1
624968,scikit-learn/scikit-learn,python,496,1324419233,,1324439273,334,,unknown,false,false,false,3,33,33,0,0,2,2,2,1,6,2,13,21,17,0,0,6,2,13,21,17,0,0,1475,79,1475,79,174.55637190148545,6.079006004631142,98,vlad@vene.ro,sklearn/metrics/metrics.py|sklearn/cluster/_hierarchical.c|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/least_angle.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/base.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/grid_search.py|sklearn/metrics/metrics.py|sklearn/utils/fixes.py|sklearn/linear_model/randomized_lasso.py|sklearn/cluster/hierarchical.py|sklearn/cluster/_hierarchical.pyx|sklearn/cluster/_inertia.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/setup.py|sklearn/linear_model/randomized_lasso.py|sklearn/grid_search.py|sklearn/utils/fixes.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/cluster/_inertia.c|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/randomized_lasso.py|sklearn/linear_model/__init__.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/randomized_l1.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|examples/linear_model/plot_randomized_lasso.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/randomized_l1.py|sklearn/linear_model/tests/test_randomized_l1.py|sklearn/linear_model/randomized_l1.py,44,0.005007153075822604,0,0,false,WIP Randomized lasso ,,312,0.8493589743589743,0.388412017167382,21107,275.6431515610935,28.426588335623254,81.15790969820438,1269,28,490,33,unknown,agramfort,GaelVaroquaux,false,,15,0.8666666666666667,58,153,748,true,true,false,false,54,79,17,10,72,3,-1
624969,scikit-learn/scikit-learn,python,495,1324415969,1324469435,1324469430,891,891,github,false,false,false,27,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,40,0,0,0.0,0,,,0,0.0,0,0,false,MRG: Replace BaseDictionaryLearning with a mixin This is so that we can have easily make a MiniBatchKMeans dictionary learning In the future maybe trees as well :),,311,0.8488745980707395,0.3891696750902527,21107,276.7802150945184,28.473965982849293,81.67906381769082,1269,28,490,35,unknown,vene,agramfort,false,agramfort,15,0.9333333333333333,29,17,618,true,true,true,true,14,26,4,0,45,2,-1
624970,scikit-learn/scikit-learn,python,493,1324406967,1324408160,1324408160,19,19,github,false,false,false,18,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,1,0,false,MRG trying to clarify the kernel_approx documentation Is that more clear than beforeWhat do you think @mbondel,,310,0.8483870967741935,0.38461538461538464,21106,276.79332891121004,28.475315076281625,81.68293376291102,1269,28,490,32,unknown,amueller,mblondel,false,mblondel,26,0.9615384615384616,224,21,424,true,true,true,false,30,28,22,0,218,0,19
624971,scikit-learn/scikit-learn,python,492,1324405886,,1324487083,1353,,unknown,false,false,false,63,1,1,0,5,0,5,0,3,2,0,0,2,2,0,0,2,0,0,2,2,0,0,128,50,128,50,8.938401662038988,0.3117842141697382,0,,sklearn/boosting.py|sklearn/tests/test_boosting.py,0,0.0,0,2,false,WIP: initial draft of FunctionalGradientBoosting Hi sklearnAs discussed at the sprint on Sunday Ive started writing up a general functional gradient ensemble learner for regression models This would be my first commit of a new component / file to sklearn and Im not sure what the expectations are am I done Whats nextNew code is in sklearn/boostingpy and sklearn/tests/test_boostingpy- James,,309,0.8511326860841424,0.3825454545454545,21106,276.79332891121004,28.475315076281625,81.68293376291102,1269,28,490,35,unknown,jaberg,jaberg,true,,2,1.0,58,15,727,false,true,false,false,1,0,0,0,0,0,2
624972,scikit-learn/scikit-learn,python,491,1324392438,1325277001,1325277001,14742,14742,github,false,false,false,15,23,1,0,12,0,12,0,5,0,0,2,7,2,0,0,0,0,7,7,5,0,0,119,0,486,39,8.371967247249088,0.2888866698774494,68,vlad@vene.ro,sklearn/ensemble/base.py|sklearn/ensemble/forest.py,62,0.04602821083890126,0,1,false,MRG: Parallel forest This PR implements parallelized forest of trees It parallelizes fit predict predict_proba,,308,0.8506493506493507,0.3726800296956199,21688,289.2382884544449,29.04832165252674,83.2257469568425,1269,29,490,40,unknown,glouppe,glouppe,true,glouppe,15,1.0,56,15,404,true,true,false,false,17,53,12,0,105,0,13478
624973,scikit-learn/scikit-learn,python,490,1324389772,1324475606,1324475606,1430,1430,github,false,false,false,30,9,1,0,9,1,10,0,4,0,0,3,5,3,0,0,0,0,5,5,3,0,0,68,18,229,60,13.249717761925437,0.4706834086058012,24,robertlayton@gmail.com,sklearn/datasets/__init__.py|sklearn/datasets/tests/test_20news.py|sklearn/datasets/twenty_newsgroups.py,19,0.007501875468867217,2,1,false,WIP : Vectorized news20 dataset loader This PR implements a vectorized loader for the news20 dataset This will be useful for the semi-supervised and multilabel branchesCC: @larsmans and @ogrisel,,307,0.8501628664495114,0.37134283570892723,20545,281.52835239717695,29.009491360428328,83.52397176928693,1269,29,490,36,unknown,mblondel,amueller,false,amueller,9,1.0,103,23,630,true,false,false,true,44,69,7,3,24,0,90
624974,scikit-learn/scikit-learn,python,489,1324389178,,1324477129,1465,,unknown,false,false,false,27,1,0,0,2,2,4,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,10,5,0,0.0,0,,,0,0.0,0,0,false,MRG: Add argument return_cholesky to lars_path Returns the cholesky factor so that it can be reused to dofor example least squares on the support (lasso debiasing),,306,0.8529411764705882,0.37142857142857144,21196,279.2036233251557,29.01490847329685,81.52481600301944,1269,29,490,35,unknown,fabianp,fabianp,true,,20,0.85,75,20,584,true,true,false,false,32,37,9,0,48,0,258
624975,scikit-learn/scikit-learn,python,488,1324386797,1324477501,1324477501,1511,1511,github,false,false,false,26,5,0,0,0,0,0,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,51,17,0,0.0,0,,,0,0.0,1,0,false,MRG : Sparse matrix support in KMeans This PR implements sparse matrix support in KMeans It depends on @ogrisels PR which needs to be merged first,,305,0.8524590163934426,0.36981132075471695,21196,279.2036233251557,29.01490847329685,81.52481600301944,1269,29,490,35,unknown,mblondel,amueller,false,amueller,8,1.0,103,23,630,true,false,false,true,43,69,6,3,21,0,-1
624976,scikit-learn/scikit-learn,python,486,1324333017,1324472637,1324472637,2327,2327,github,false,false,false,26,13,4,0,15,1,16,0,7,2,0,8,18,7,0,0,2,0,16,18,13,0,0,351,8,393,14,45.05765915847603,1.5691542894863475,60,vlad@vene.ro,doc/developers/index.rst|doc/developers/utilities.rst|sklearn/manifold/isomap.py|sklearn/utils/graph.py|sklearn/utils/tests/test_shortest_path.py|doc/developers/utilities.rst|sklearn/cluster/spectral.py|sklearn/utils/fixes.py|sklearn/utils/__init__.py|sklearn/utils/validation.py,33,0.005291005291005291,0,6,false,MRG: Utility Cleanup This pull request features a bit of cleanup in sklearnutils as well as some more documentation for developers regarding use of the utils,,304,0.8519736842105263,0.37037037037037035,21009,276.97653386643816,28.5591889190347,81.58408301204246,1268,29,489,36,unknown,jakevdp,amueller,false,amueller,12,1.0,427,0,222,true,true,false,false,8,22,2,0,29,3,740
624977,scikit-learn/scikit-learn,python,484,1324328461,1324475429,1324475429,2449,2449,github,false,false,false,40,84,59,0,4,0,4,0,2,2,1,13,18,12,0,1,4,1,15,20,14,0,1,5255,1299,5622,1375,450.89437409697285,15.651677094126079,77,vlad@vene.ro,.gitignore|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/__init__.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|examples/cluster/plot_mini_batch_kmeans.py|sklearn/cluster/_k_means.c|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|examples/document_clustering.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/cluster/plot_kmeans_stability.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/cluster/plot_mini_batch_kmeans.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|examples/cluster/plot_kmeans_stability.py|examples/document_clustering.py|sklearn/base.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/document_clustering.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability_high_dim_sparse.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/document_clustering.py|examples/cluster/plot_kmeans_stability_high_dim_sparse.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/cluster/plot_mini_batch_kmeans.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability_high_dim_sparse.py,39,0.005372217958557176,0,1,false,MRG: Minibatch KMeans optimization Here is a major refactoring of the Minibatch KMeans + KMeans module to factor out common code make kmeans++ init work on sparse data avoid unnecessary memory allocations and cythonize critical label assignments and means updates,,303,0.8514851485148515,0.37145049884881043,21058,276.3320353309906,28.49273435274005,81.39424446766074,1268,28,489,36,unknown,ogrisel,ogrisel,true,ogrisel,16,0.75,404,109,936,true,false,false,false,80,211,22,6,286,6,41
220131,scikit-learn/scikit-learn,python,483,1324326663,,1342892479,309430,,unknown,false,false,false,86,9,4,0,3,3,6,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,2087,14,2339,45,28.317234939045584,1.004384675086136,23,satra@mit.edu,sklearn/metrics/pairwise.py|sklearn/metrics/tests/test_pairwise.py|sklearn/utils/arrayfuncs.pyx|sklearn/utils/arrayfuncs.c|sklearn/metrics/pairwise.py|sklearn/metrics/pairwise.py,19,0.007692307692307693,0,1,false,WIP: Refactor euclidean_distance add optional argument for preallocated output array This pull request adds * Several optimized Cython routines for (squared) Euclidean distance now used in the dense-dense case by euclidean_distance * An explicit check that euclidean_distance is operating on floating point arrays (general consensus in the sprint room is that it doesnt usually make much sense in the integer case and if you really want that you can cast) * An optional out argument for euclidean_distance (only in the dense-dense case raises an error otherwise),,302,0.8543046357615894,0.37153846153846154,20553,281.46742567994943,28.99819977618839,83.49146110056927,1268,28,489,120,unknown,dwf,dwf,true,,3,1.0,56,57,1020,true,true,false,false,2,1,0,0,4,0,31
624982,scikit-learn/scikit-learn,python,482,1324326558,1324328912,1324328912,39,39,github,false,false,false,3,2,0,0,0,0,0,0,0,0,0,0,22,0,0,0,0,0,22,22,22,0,0,0,0,208,16,0,0.0,0,,,0,0.0,0,0,false,pep8 examples fixes ,,301,0.8538205980066446,0.371824480369515,20553,281.46742567994943,28.99819977618839,83.49146110056927,1268,28,489,29,unknown,DraXus,agramfort,false,agramfort,0,0,10,16,1383,false,true,false,false,0,0,0,0,0,0,-1
624983,scikit-learn/scikit-learn,python,481,1324319418,1324325809,1324325809,106,106,github,false,false,false,61,5,0,0,3,0,3,0,3,0,0,0,4,0,0,0,3,0,4,7,4,0,0,0,0,466,30,0,0.0,0,,,0,0.0,1,4,false,Mean var2 This PR implements a small utility to compute the mean and variance along axis 0 for sparse matrices This is part of the effort to add sparse matrix support to KMeans The utility is useful to scale the convergence tolerance with the variance over the entire (sparse) datasetI hope I didnt mess up the edge casesCC @ogrisel,,300,0.8533333333333334,0.3714733542319749,20551,280.7162668483286,28.952362415454235,83.35360809692959,1268,27,489,28,unknown,mblondel,ogrisel,false,ogrisel,7,1.0,102,23,629,true,false,true,true,39,62,5,3,16,0,29
624984,scikit-learn/scikit-learn,python,480,1324319126,1324319427,1324319427,5,5,commits_in_master,false,false,false,68,59,59,0,0,0,0,0,0,5,0,18,23,17,0,1,5,0,18,23,17,0,1,5443,1317,5443,1317,460.5758081972843,16.418867869981042,73,vlad@vene.ro,sklearn/linear_model/logistic.py|.gitignore|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/utils/__init__.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/tests/test_k_means.py|examples/cluster/plot_mini_batch_kmeans.py|sklearn/cluster/_k_means.c|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|examples/document_clustering.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/cluster/plot_kmeans_stability.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/cluster/plot_mini_batch_kmeans.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|examples/cluster/plot_kmeans_stability.py|examples/document_clustering.py|sklearn/base.py|sklearn/cluster/k_means_.py|sklearn/cluster/tests/test_k_means.py|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability.py|examples/document_clustering.py|sklearn/cluster/k_means_.py|examples/cluster/plot_kmeans_stability_high_dim_sparse.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|examples/document_clustering.py|examples/cluster/plot_kmeans_stability_high_dim_sparse.py|examples/cluster/plot_kmeans_stability_low_dim_dense.py|sklearn/cluster/_k_means.c|sklearn/cluster/_k_means.pyx|sklearn/cluster/k_means_.py|sklearn/cluster/k_means_.py|sklearn/cluster/setup.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_sparsefuncs.py|sklearn/utils/setup.py|sklearn/utils/sparsefuncs.c|sklearn/utils/sparsefuncs.pyx|sklearn/utils/tests/test_sparsefuncs.py,37,0.0031397174254317113,1,0,false,Mean and variance along axis 0 for sparse matrices This PR implements a small utility to compute the mean and variance along axis 0 for sparse matrices This is part of the effort to add sparse matrix support to KMeans The utility is useful to scale the convergence tolerance with the variance over the entire (sparse) datasetI hope I didnt mess up the edge casesCC @ogrisel,,299,0.8528428093645485,0.37205651491365777,20551,280.7162668483286,28.952362415454235,83.35360809692959,1268,27,489,28,unknown,mblondel,mblondel,true,mblondel,6,1.0,102,23,629,true,false,false,false,38,62,4,3,16,0,-1
624783,scikit-learn/scikit-learn,python,478,1324307633,1325197182,1325197182,14825,14825,github,false,false,false,29,33,3,0,46,0,46,0,5,2,0,4,13,6,0,0,2,0,11,13,8,0,0,139,36,705,63,29.86418707825049,1.0351263813878016,165,vlad@vene.ro,examples/ensemble/plot_forest_importances.py|examples/ensemble/plot_forest_importances_pixel.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tree.py,115,0.0380952380952381,0,23,false,MRG: Feature importances with trees This PR implements feature importances using single tree and/or forest of treesIt comes with doc tests and two examplesReviews are welcome :),,298,0.8523489932885906,0.3746031746031746,21523,289.4577893416345,29.038702783069276,83.21330669516331,1268,27,489,35,unknown,glouppe,GaelVaroquaux,false,GaelVaroquaux,14,1.0,55,14,403,true,true,false,false,14,47,11,0,96,0,191
624784,scikit-learn/scikit-learn,python,477,1324300908,1324301224,1324301224,5,5,github,false,false,false,19,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,9,0,0,0.0,0,,,0,0.0,0,0,false,remove unused return_log keyword in GMM This keyword was unused - I think its cleaner to simply remove it,,297,0.8518518518518519,0.3736968724939856,20527,279.63170458420615,28.937496955229697,83.25619915233595,1268,27,489,29,unknown,jakevdp,agramfort,false,agramfort,11,1.0,427,0,222,true,true,false,true,4,21,1,0,23,1,-1
138656,scikit-learn/scikit-learn,python,476,1324299227,,1338217411,231969,,unknown,false,false,false,76,17,10,0,4,0,4,0,3,3,0,5,9,3,0,0,3,0,6,9,3,0,0,308,0,469,0,72.91752737910475,2.6008892970809034,48,vlad@vene.ro,examples/polynomial_classifier.py|sklearn/__init__.py|sklearn/polynomial_classifier.py|examples/polynomial_classifier.py|sklearn/polynomial_classifier.py|sklearn/polynomial_classifier.py|examples/polynomial_classifier.py|sklearn/polynomial_classifier.py|sklearn/polynomial_classifier.py|examples/polynomial_classifier.py|examples/polynomial_classifier.py|examples/polynomial_classifier.py|doc/modules/polynomial_classifier.rst|doc/supervised_learning.rst|sklearn/polynomial_classifier.py|doc/modules/classes.rst,36,0.0056406124093473006,0,1,false,WIP: Polynomial Classifer Hi thereIve implemented a (simple) Polynomial Classifier into the sklearn package together with an example (working on iris and digits datasets) and documentation (API and math background) Its not a very sophisticated version if the classifier is merged into the sklearn package I will add further improvements like speedup numerical stability or a probabilistic interpretation of the classifier outputIf you find any errors or have suggestions just drop me a line,,296,0.8547297297297297,0.37308622078968573,20527,279.63170458420615,28.937496955229697,83.25619915233595,1268,27,489,111,unknown,chermes,agramfort,false,,0,0,0,0,26,true,true,false,false,0,0,0,0,10,0,15
624781,scikit-learn/scikit-learn,python,475,1324244430,1324291862,1324291862,790,790,github,false,false,false,8,1,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,21,4,0,0.0,0,,,0,0.0,0,0,false,Fixed some doctests on datasets See commit message,,295,0.8542372881355932,0.371033360455655,20521,279.71346425612785,28.94595779932752,83.28054188392377,1268,26,488,30,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,25,0.96,222,21,422,true,true,true,false,32,30,23,0,169,0,-1
624780,scikit-learn/scikit-learn,python,473,1324238399,1324238468,1324238468,1,1,github,false,false,false,8,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,68,0,0,0.0,0,,,0,0.0,0,0,false,Dataset doc beautification Use tables in dataset docstrings,,294,0.8537414965986394,0.37315875613747956,20521,279.71346425612785,28.94595779932752,83.28054188392377,1267,26,488,30,unknown,amueller,agramfort,false,agramfort,24,0.9583333333333334,222,21,422,true,true,true,false,31,29,22,0,168,0,-1
32095,scikit-learn/scikit-learn,python,472,1324223334,1329394067,1329394067,86178,86178,commits_in_master,false,false,false,394,1,1,0,16,0,16,0,5,34,0,0,34,21,0,0,34,0,0,34,21,0,0,1169,0,1169,0,163.18839351907772,5.8113360350398935,0,,doc/tutorial/big_toc_css.rst|doc/tutorial/diabetes_cv_excercice.rst|doc/tutorial/digits_classification_excercice.rst|doc/tutorial/digits_cv_excercice.rst|doc/tutorial/finding_help.rst|doc/tutorial/index.rst|doc/tutorial/iris_classification_excercice.rst|doc/tutorial/model_selection.rst|doc/tutorial/putting_together.rst|doc/tutorial/settings.rst|doc/tutorial/supervised_learning.rst|doc/tutorial/unsupervised_learning.rst|examples/tutorial/README.txt|examples/tutorial/_plot_pca_3d.py|examples/tutorial/_plot_pca_3d_mayavi.py|examples/tutorial/plot_cluster_iris.py|examples/tutorial/plot_cv_diabetes.py|examples/tutorial/plot_cv_digits.py|examples/tutorial/plot_digits_agglomeration.py|examples/tutorial/plot_digits_classification_excercice.py|examples/tutorial/plot_digits_pipe.py|examples/tutorial/plot_face_recognition.py|examples/tutorial/plot_iris_classifiers.py|examples/tutorial/plot_iris_dataset.py|examples/tutorial/plot_iris_exercice.py|examples/tutorial/plot_lena_compress.py|examples/tutorial/plot_logistic.py|examples/tutorial/plot_ols_3d.py|examples/tutorial/plot_ols_variance.py|examples/tutorial/plot_pca_iris.py|examples/tutorial/plot_ridge_variance.py|examples/tutorial/plot_stock_market.py|examples/tutorial/plot_svm_kernels.py|examples/tutorial/plot_svm_margin.py,0,0.0,3,11,false,WIP: merge in statistical learning tutorial The goal of this pull request is to merge in the statistical learning tutorial into the main documentationThe pros would be: * To make the tutorial more easily maintainable * To be able to link from the tutorial to the docs and vice-versa * To have examples and images shared between the docs and the tutorial * To make the tutorial more visibleThe tutorial should be clearly advertise as a tutorial with a specific focus (using the scikit for inference and feature discovery more than simple prediction) It should not replace the docs and be kept short There should be room for other tutorials with other focuses For instance @ogrisels tutorial should be merged in later onRight now this pull request it not ready at all to be merged I am putting it up so that people can pitch in for instance @Balu-Varanasi or @amueller have expressed interested There is a lot of work Specifically (in order of priority and difficulty):  * The indexrst should be added in the tutorial  * I do not want any images stored in the git tree: as a result I have removed some images necessary to build to avoid polluting the tree These should all be generated by the Python code in the examples following the rest of the scikit documentation  * I have created a examples/tutorial folder to host these examples but each one should be moved in the right example folder and this folder should be deleted Also redundant examples should be deleted or merged  * The tutorial needs to be fully doctested I believe that this is the case currently but the doctests take way too long to run partly because of the model selection part (grid search) This model selection part is important as it is important to teach it to people but well need to figure out way to make it run faster: simpler data on some of these and delegating the more complexes ones to examples that get ran only once in a while and using literalinclude directives to show the code in the docsWhat we be great is if we could iterate on the above todo list and update it as we go For the sake of simplicity I can give right permissions to me repo to people working on this,,293,0.8532423208191127,0.37138130686517784,20521,279.71346425612785,28.94595779932752,83.28054188392377,1267,26,488,46,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,7,0.7142857142857143,172,2,664,true,true,false,false,54,118,12,5,268,2,59
624778,scikit-learn/scikit-learn,python,471,1324219672,1324220045,1324220045,6,6,github,false,false,false,13,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,19,4,0,0.0,0,,,0,0.0,0,0,false,Doctest mess Renamed linnerud bunch items to be more consistent with dataset interface,,292,0.8527397260273972,0.37271214642262895,20518,279.5106735549274,28.901452383273224,83.29271858855638,1267,26,488,31,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,23,0.9565217391304348,221,21,422,true,true,true,false,27,26,21,0,157,0,-1
624779,scikit-learn/scikit-learn,python,469,1324214739,1324215675,1324215675,15,15,github,false,false,false,9,4,1,0,0,0,0,0,0,0,0,1,2,1,0,0,0,0,2,2,1,0,0,0,2,0,2,4.751435999259142,0.16920465612460914,7,robertlayton@gmail.com,sklearn/cluster/tests/test_k_means.py,7,0.005833333333333334,0,0,false,Doctest mess Doctest eps mess with pull api :),,291,0.852233676975945,0.37333333333333335,20518,279.5106735549274,28.901452383273224,83.29271858855638,1267,26,488,31,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,22,0.9545454545454546,221,21,422,true,true,true,false,26,26,20,0,155,0,-1
624775,scikit-learn/scikit-learn,python,468,1324172573,1324212521,1324212521,665,665,commits_in_master,false,false,false,15,3,2,0,0,0,0,0,1,0,0,2,3,1,0,0,0,0,3,3,2,0,0,0,2,41,2,9.292810769397253,0.3309250576809429,19,robertlayton@gmail.com,sklearn/cluster/tests/test_k_means.py|doc/datasets/index.rst,14,0.011725293132328308,0,0,false,Dataset documentation Im trying to get the dataset API a little more consistent and documented,,290,0.8517241379310345,0.37018425460636517,20470,287.98241328773815,29.702002931118713,84.904738641915,1267,25,487,31,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,21,0.9523809523809523,221,21,421,true,true,false,false,25,26,19,0,153,0,-1
626023,scikit-learn/scikit-learn,python,467,1324074114,1324129973,1324129973,930,930,github,false,false,false,12,1,0,0,4,0,4,0,2,0,0,0,19,0,0,0,0,0,19,19,19,0,0,0,0,414,0,0,0.0,0,,,0,0.0,0,0,true,pep8 compliant I modified some of the files to be pep8 compliant,,289,0.8512110726643599,0.36541737649063033,20401,288.95642370472035,29.80246066369296,85.19190235772756,1266,25,486,31,unknown,Balu-Varanasi,glouppe,false,glouppe,1,1.0,8,49,247,true,true,true,false,1,0,1,0,1,0,84
624776,scikit-learn/scikit-learn,python,466,1324062731,1324073247,1324073247,175,175,github,false,false,false,76,2,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,22,0,24,0,4.908192813167914,0.1750760104920362,0,,examples/svm/plot_iris.py,0,0.0,0,0,true,RBF Svm iris example This is a somewhat trivial change in the DOCsI changed the NuSVC in the SVM examples to a rbf-Kernel SVCThe result from the NuSVC should be the same as from a SVC with some C so imho there is nothingreally to be seen from the difference of NuSVC and SVCOn the other hand Id really like to have an image of an rbf-Kernel SVM quite at the top,,288,0.8506944444444444,0.3648763853367434,20401,288.95642370472035,29.80246066369296,85.19190235772756,1266,25,486,31,unknown,amueller,fabianp,false,fabianp,20,0.95,221,21,420,true,true,true,false,19,21,18,0,135,0,175
624772,scikit-learn/scikit-learn,python,465,1323974383,1324220359,1324220359,4099,4099,github,false,false,false,47,6,2,0,0,0,0,0,0,0,0,2,3,2,0,0,0,0,3,3,3,0,0,2,8,17,11,8.88901111813982,0.3176244489495939,11,robertlayton@gmail.com,sklearn/decomposition/tests/test_fastica.py|sklearn/decomposition/fastica_.py,6,0.005172413793103448,0,0,false,Fastica wowhiten As nothing happened here:https://githubcom/scikit-learn/scikit-learn/pull/243I fixed issue 238 since it seemed pretty trivialfastica now returns none as whitening matrix when whitening is falseThis seems to be the agreed solution as discussed in the pull request aboveI also added a non-regression test,,287,0.8501742160278746,0.3612068965517241,20371,289.3819645574591,29.846350203720974,85.31736291787345,1265,25,485,30,unknown,amueller,agramfort,false,agramfort,19,0.9473684210526315,221,21,419,true,true,true,false,18,21,17,0,121,0,-1
624774,scikit-learn/scikit-learn,python,464,1323950685,1323951571,1323951571,14,14,github,false,false,false,33,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,FIX - error in the bibtex entry - extra comma that makes bibtex fail There is an error in the bibtex entry for the article: an extra comma that made bibtex failThanks,,286,0.8496503496503497,0.3624567474048443,20342,289.74535443909156,29.888899813194378,85.43899321600628,1264,25,485,30,unknown,NelleV,agramfort,false,agramfort,3,1.0,20,13,696,false,true,true,true,1,1,0,0,0,0,-1
626025,scikit-learn/scikit-learn,python,463,1323911868,1323914510,1323914510,44,44,github,false,false,false,9,1,0,0,0,0,0,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,3,0,0,0.0,0,,,0,0.0,0,0,false,Add arXiv link to Halko et al 2009 paper ,,285,0.8491228070175438,0.3620689655172414,20341,289.7595988397817,29.890369205053833,85.44319354997296,1264,24,484,30,unknown,npinto,GaelVaroquaux,false,GaelVaroquaux,2,1.0,43,37,1091,false,true,false,false,0,0,0,0,0,0,-1
626027,scikit-learn/scikit-learn,python,461,1323764056,1323764391,1323764391,5,5,github,false,false,false,13,1,0,0,0,0,0,0,2,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,modified gid to git in doc/developers/indexrst I fixed a bug in doc/developers/indexrst file,,284,0.8485915492957746,0.3652246256239601,20341,289.7595988397817,29.890369205053833,85.44319354997296,1257,24,483,30,unknown,Balu-Varanasi,glouppe,false,glouppe,0,0,8,49,244,false,true,true,false,0,0,0,0,0,0,-1
624770,scikit-learn/scikit-learn,python,460,1323715416,,1323715946,8,,unknown,false,false,false,70,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,7,0,0,0.0,0,,,0,0.0,0,0,false,make BaseEstimator an abstract base class Now that we have Python 26 as a requirement we can use abstract base classes to prevent users from instantiating what they shouldnt instantiate Ive already used this in the Naive Bayes moduleHeres a patch that does the same trick for BaseEstimator Im putting this up as a PR because I wasnt sure if this is the direction that we want to take,,283,0.8515901060070671,0.3642329778506973,20366,289.40390847490914,29.853677698124326,85.33830894628302,1257,24,482,30,unknown,larsmans,larsmans,true,,31,0.7096774193548387,48,26,512,true,true,false,false,35,61,16,0,133,0,-1
624771,scikit-learn/scikit-learn,python,459,1323478509,1323526686,1323526686,802,802,merged_in_comments,false,false,false,41,1,1,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,5,12,5,12,8.528537823305573,0.30533376988817357,35,vlad@vene.ro,sklearn/linear_model/tests/test_logistic.py|sklearn/svm/base.py,33,0.027071369975389663,0,0,true,Check for consistent input in Logistic Regression Currently BaseLibLinear the base class of Logistic Regression does not check whether the target and training vectors have the same number of samplesThis pull request adds that validation and the corresponding test case,,282,0.851063829787234,0.35931091058244463,20383,288.2303880684884,29.877839375950547,85.11995290192807,1252,24,479,31,unknown,cavorite,larsmans,false,larsmans,0,0,8,0,1002,false,true,false,false,0,0,0,0,0,0,-1
624768,scikit-learn/scikit-learn,python,458,1323372264,1323520041,1323520041,2462,2462,merged_in_comments,false,false,false,35,3,2,0,3,0,3,0,2,0,0,8,12,8,0,0,0,0,12,12,10,0,0,25,7,32,7,36.03484289180821,1.2900984977568692,63,vlad@vene.ro,sklearn/datasets/samples_generator.py|sklearn/feature_extraction/image.py|sklearn/grid_search.py|sklearn/manifold/tests/test_isomap.py|sklearn/manifold/tests/test_locally_linear.py|sklearn/utils/fixes.py|sklearn/datasets/mldata.py|sklearn/utils/tests/test___init__.py,37,0.013018714401952807,0,0,false,Drop some Python 25 compatibility stuff Some patches to remove Py25 compat as discussed on the ML Merge when the buildbots are ready for itDont merge if we decide to keep supporting 25 :),,281,0.8505338078291815,0.3531326281529699,20384,288.21624803767656,29.876373626373628,85.1157770800628,1246,24,478,31,unknown,larsmans,GaelVaroquaux,false,GaelVaroquaux,30,0.7,48,26,508,true,true,false,false,34,61,15,0,117,0,4
624769,scikit-learn/scikit-learn,python,457,1323364369,1324306970,1324306970,15710,15710,commit_sha_in_comments,false,false,false,37,2,2,0,1,2,3,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,13,5,13,5,8.873833086140772,0.3176958136919728,27,vlad@vene.ro,sklearn/manifold/tests/test_locally_linear.py|sklearn/manifold/locally_linear.py,26,0.021207177814029365,0,0,false,Make locally_linear_embedding robust to zero eigenvalues arpack in shift-invert mode raises an Exception with zero eigenvaluesso degrade gratefully to the non shirt-invert mode for those casesIssue reported by Timmy Wilson in the mailing list:    http://permalinkgmaneorg/gmanecomppythonscikit-learn/1123,,280,0.85,0.3523654159869494,20384,288.21624803767656,29.876373626373628,85.1157770800628,1246,24,478,31,unknown,fabianp,jakevdp,false,jakevdp,19,0.8421052631578947,75,20,572,true,true,false,false,32,41,7,0,67,0,15709
624765,scikit-learn/scikit-learn,python,456,1323180530,1324395530,1324395530,20250,20250,github,false,false,false,49,36,4,0,3,3,6,0,3,0,0,2,13,2,0,0,1,0,13,14,11,0,0,83,14,643,205,17.757482221758373,0.6255927158527614,24,vlad@vene.ro,sklearn/decomposition/dict_learning.py|sklearn/decomposition/tests/test_dict_learning.py|sklearn/decomposition/dict_learning.py|sklearn/decomposition/dict_learning.py,24,0.01932367149758454,0,0,false,WIP: Sparse coder This quick and dirty pull request aims to add an estimator (transformer) object that implements sparse coding against a fixed dictionary in the form of the SparseCoder object At the same time this will address the small inconsistencies and missing info in docs that came up,,279,0.8494623655913979,0.3639291465378422,20782,279.18390915215093,28.726782792801462,82.71581176017708,1245,25,476,38,unknown,vene,fabianp,false,fabianp,14,0.9285714285714286,28,17,604,true,true,false,true,12,32,3,0,33,0,1206
350909,scikit-learn/scikit-learn,python,455,1323125062,1362351700,1362351700,653777,653777,merged_in_comments,false,true,false,40,1,0,3,0,0,3,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,28,0,0,0.0,0,,,0,0.0,0,0,false,WIP: First draft of a random search in GridSearchCV HiSo far no tests so this is just to get feedback on the API and goals of an extension to sklearns GridSearchCV that does random search when given a budget,,278,0.8489208633093526,0.36276083467094705,20382,288.2445294868021,29.879305269355314,85.1241291335492,1238,25,475,149,unknown,alextp,amueller,false,amueller,2,1.0,29,3,1340,false,false,false,true,2,4,0,0,0,0,392872
624767,scikit-learn/scikit-learn,python,453,1322674941,1322677368,1322677364,40,40,github,false,false,false,6,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,DOC: fix name for line_profiler_extpy extension ,,277,0.8483754512635379,0.3574561403508772,20242,290.23811876296804,30.085959885386817,85.71287422191483,1229,26,470,28,unknown,yarikoptic,ogrisel,false,ogrisel,4,1.0,31,7,1084,true,true,false,true,1,0,0,0,2,0,40
624764,scikit-learn/scikit-learn,python,452,1322591960,1322705804,1322705804,1897,1897,github,false,false,false,113,13,5,0,5,0,5,0,5,0,0,0,35,0,0,0,0,0,35,35,31,0,0,0,0,175,0,0,0.0,0,,,0,0.0,0,5,false,Reorganization of the class/function reference This pull request is an attempt at reorganizing the class and function reference page of the documentation Straightly put in my opinion and as it is in its current state the reference is indeed a complete mess (no offense) There is no clear apparent logical order and it is not always so easy to quickly find what we are looking for This pull request tries to solve that issueCurrent changes:- Sorted the reference by modules alphabetically- Added a table of content to quickly jump to a module reference- Standardized the module-level documentationDo you have any other ideas or suggestions to improve that page,,276,0.8478260869565217,0.3623082542001461,20242,290.23811876296804,30.085959885386817,85.71287422191483,1227,26,469,27,unknown,glouppe,ogrisel,false,ogrisel,13,1.0,52,14,383,true,true,true,false,13,58,9,0,97,0,50
63633,scikit-learn/scikit-learn,python,448,1322134618,1332465084,1332465084,172174,172174,github,false,false,false,172,67,0,71,60,0,131,0,10,0,0,0,35,0,0,0,11,1,34,46,37,0,0,0,0,4554,151,0,0.0,0,,,0,0.0,1,46,false,MRG: Gradient boosting This is a PR for Gradient Boosted Regression Trees [1] (aka Gradient Boosting MART TreeNet)GBRTs have been advertised as one of the best off-the-shelf data-mining procedures they share many properties with random forests including little need for tuning and data preprocessing as well as high predictive accuracy GBRT have been used very successfully in areas such as learning of ranking functions and ecologyThis should be an alternative to Rs gbm package [2] Currently it feature three loss functions (binary classification least squared regression and robust regression) stochastic gradient boosting and variable importance  Ive benchmarked the code against Rs gbm package (via rpy2) using a variety of datasets (about 4 classification and 3 regression datasets) - the results are remarkably similar gbm however is usually a bit faster for least-squares regressionSome features are still on my TODO list:    * Multi-class classification (done thanks to @scottblanc)    * Partial dependency plots    * Quantile loss function for robust regressionI havent benchmarked it against OpenCVs implementation[1] http://enwikipediaorg/wiki/Gradient_boosting[2] http://cranr-projectorg/web/packages/gbm/indexhtml,,275,0.8472727272727273,0.35773480662983426,23888,307.602143335566,30.01507032819826,87.36604152712658,1219,27,464,73,unknown,pprett,pprett,true,pprett,12,0.9166666666666666,48,23,842,true,true,false,false,27,90,8,0,54,1,1
624762,scikit-learn/scikit-learn,python,447,1321831647,1321833109,1321833109,24,24,github,false,false,false,52,9,0,0,0,0,0,0,2,0,0,0,41,0,0,0,0,0,41,41,41,0,0,0,0,189,60,0,0.0,0,,,0,0.0,0,0,false,Pep8 This pull request fixes nearly all pep8 issues in the modules (not the tests)Some remain inside doctests I havent looked into the whitespace issues there and didnt want to break anythingOh and I also left out hmm since it is orphanedIm a little bored Can you tell ),,274,0.8467153284671532,0.34032476319350474,19888,290.124698310539,30.420353982300885,86.48431214802896,1208,27,460,28,unknown,amueller,amueller,true,amueller,18,0.9444444444444444,214,21,394,true,true,false,false,16,22,14,0,96,0,-1
624763,scikit-learn/scikit-learn,python,446,1321566498,1321635443,1321635443,1149,1149,github,false,false,false,60,1,0,0,6,0,6,0,3,0,0,0,3,0,0,0,2,0,3,5,5,0,0,0,0,74,0,0,0.0,0,,,0,0.0,0,3,false,ENH incrementally build arrays in SVMlight loader to reduce memory usage Memory use on [covtypebinary](http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/binaryhtml#covtypebinary) reduced by nearly 2/3 at the expense of of a 40% slowdown Should fix issue #369Method: classical dynamic arrays with O(1) amortized append The growth factor may be tweaked further for better performanceTest script:    from bz2 import BZ2File    from sklearndatasets import load_svmlight_file    load_svmlight_file(BZ2File(covtypelibsvmbinarybz2)),,273,0.8461538461538461,0.33954451345755693,19886,290.15387709946697,30.42341345670321,86.49301015790003,1205,27,457,28,unknown,larsmans,larsmans,true,larsmans,29,0.6896551724137931,45,26,487,true,true,false,false,33,61,12,0,114,0,3
624825,scikit-learn/scikit-learn,python,444,1321379979,,1388684410,1121680,,unknown,false,false,false,92,69,29,21,20,0,41,0,6,2,0,7,11,7,0,0,4,1,8,13,10,0,0,2989,263,4604,439,180.5650078689144,5.54968073636652,166,vlad@vene.ro,sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/_inertia.c|sklearn/cluster/_inertia.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/_inertia.c|sklearn/cluster/_inertia.pyx|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_linkage.py|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/tests/test_linkage.py|sklearn/cluster/__init__.py|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/tests/test_hierarchical.py|sklearn/cluster/tests/test_linkage.py|sklearn/cluster/hierarchical.py|sklearn/cluster/hierarchical.py|sklearn/cluster/linkage.py|sklearn/cluster/hierarchical.py,120,0.01520387007601935,0,11,false,Hierarchical clustering refactoring * Refactored cluster/hierarchicalpy (more modular allows different linkage criteria and decouples dendrogram representation from clustering algorithm)* Added complete linkage criterion* Added unittests for linkage criteriaThis is probably too premature to be pulled directly I would like to get feedback if this refactoring is interesting for scikit-learn at all what needs to be reconsidered etc I am willing to write some further examples/unittests if you are interested in pulling these changesI tried my best to avoid breaking backward-compatibility if you find anything which breaks please report ,,272,0.8492647058823529,0.33517622667588115,26058,334.82999462736973,31.890398342159795,91.37309079745185,1202,27,455,220,unknown,jmetzen,jmetzen,true,,1,1.0,3,1,36,true,true,false,false,2,3,1,0,33,0,299
19300,scikit-learn/scikit-learn,python,443,1321338685,1327685278,1327685278,105776,105776,github,false,false,false,29,20,2,24,27,0,51,0,5,0,0,3,3,3,0,0,0,0,3,3,3,0,0,27,4,367,137,17.672163730307823,0.6025653023828909,83,vlad@vene.ro,sklearn/metrics/__init__.py|sklearn/metrics/metrics.py|sklearn/metrics/metrics.py|sklearn/metrics/tests/test_metrics.py,61,0.029819694868238558,0,22,false,WIP: Enh/metrics adding a couple of metricsTODO before merge:- devise an API refactoring to be able to pass parameterized scoring callables to the GridSearchCV and cross_val_score utilities,,271,0.8487084870848709,0.3363384188626907,22048,301.8414368650218,29.66255442670537,85.177793904209,1200,27,455,49,unknown,satra,satra,true,satra,5,1.0,44,2,667,true,true,false,false,8,10,2,0,9,1,190
624761,scikit-learn/scikit-learn,python,440,1321285324,1324398265,1324398265,51882,51882,merged_in_comments,false,false,false,112,51,9,0,15,0,15,0,4,3,0,5,16,4,0,0,9,1,12,22,8,0,0,357,56,992,134,51.400790571683714,1.810840059933592,89,vlad@vene.ro,sklearn/feature_extraction/__init__.py|sklearn/feature_extraction/kernel_approximation.py|sklearn/feature_extraction/tests/test_kernel_approximation.py|sklearn/feature_extraction/kernel_approximation.py|examples/kernel_approximation.py|examples/kernel_approximation.py|examples/kernel_approximation.py|sklearn/feature_extraction/kernel_approximation.py|sklearn/feature_extraction/kernel_approximation.py|doc/modules/classes.rst|doc/modules/feature_extraction.rst,89,0.009749303621169917,0,7,false,MRG: Kernel approximation See mailing listContains code for rbf kernel approximation and skewed chi2 kernel approximationI am planning to add others though I think for most people rbf is the most interesting kernelIncludes an example on the toy digits datasets that illustrates the useI put the module under feature extraction Not sure this is a good place It is certainly quite different than other feature extraction functions available at the moment On the other hand it is about computing a feature mapThe narrative documentation is yet to comeI could not get the reference to include the module Do I have to add it somewhere beside modules/classesrst,,270,0.8481481481481481,0.3363509749303621,20782,279.18390915215093,28.726782792801462,82.71581176017708,1200,27,454,39,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,17,0.9411764705882353,212,21,388,true,true,false,false,15,17,13,0,72,0,161
624758,scikit-learn/scikit-learn,python,439,1321105130,1322075180,1322075180,16167,16167,github,false,false,false,87,57,2,0,48,0,48,0,8,0,0,0,21,0,0,0,11,2,19,32,17,0,0,0,0,2195,389,0,0.0,0,,,0,0.0,0,32,false,Ensemble: Forests of randomized trees  This is a work-in-progress pull request for forests of randomized trees It includes the following changes:- Added BaseEnsemble from which ensemble classes should be derived- Added RandomForestClassifier/Regressor (ensemble)- Added ExtraTreeClassifier/Regressor (tree)- Added ExtraTreesClassifier/Regressor (ensemble)- Added tests (coverage of the ensemble module is 100%)- Added an example comparing the surface decision of a decision tree of a random forest and of an extra-trees classifier- Added narrative documentation- Improved some minor details in the tree module,,269,0.8475836431226765,0.3317107093184979,19882,291.11759380344034,30.530127753747106,86.3595211749321,1198,27,452,28,unknown,glouppe,ogrisel,false,ogrisel,12,1.0,50,14,366,true,true,true,false,11,40,8,0,80,0,105
624760,scikit-learn/scikit-learn,python,438,1321058693,1324306351,1324306351,54127,54127,commits_in_master,false,false,false,15,11,1,0,13,0,13,0,5,0,0,11,12,8,0,0,0,0,12,12,9,0,0,169,51,319,130,51.31575106460725,1.8303789625839864,215,vlad@vene.ro,doc/modules/svm.rst|doc/tutorial.rst|doc/whats_new.rst|sklearn/grid_search.py|sklearn/linear_model/logistic.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/sparse/base.py|sklearn/svm/sparse/classes.py|sklearn/svm/tests/test_sparse.py|sklearn/svm/tests/test_svm.py,96,0.02924791086350975,0,1,true,MRG: Support for scaling by n_samples of the C parameter in libsvm and liblinear models ,,268,0.8470149253731343,0.3293871866295265,20524,279.6725784447476,28.94172675891639,83.26836873903723,1197,27,451,33,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,14,0.8571428571428571,58,151,709,true,true,false,false,53,82,13,0,33,4,55
624756,scikit-learn/scikit-learn,python,436,1320885015,1321047375,1321047375,2706,2706,github,false,false,false,188,6,3,0,9,0,9,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,432,0,626,0,21.54215703140457,0.7898502310451841,145,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py,122,0.045422781271837874,0,4,false,Tree Array Representation This PR replaces the composite tree representation with a representation based on numpy arrays The reason for this was two-fold: First storage is more compact (no structure padding) and writing/reading to disc is more efficient Second traversing the composite structure in cython is inefficient compared to pure C due to memory managementThe major advantage of the new representation is a speedup in test-time performance by a factor of ~4 (see timings on covertype below) and the fact that (de-)serialization should be significantly faster Two other modifications are piggybacked in this PR:  1 Nodes now store best_error and initial_error Best error is the error of the split Initial error is the error before the split For leaves both are the same This information is needed eg for variable importance   2 We now compute the error for the leaf nodes Brian did this in his first PR we forgot to re-implement it during our refactoring Covertype timings:                      Train           Test           ERRCART          272195s   01028s     00476  (old)CART          285280s   00266s     00476  (new)For further information on the topic see this [1] thread on the mailing list[1] http://sourceforgenet/mailarchive/messagephpmsg_id28293709,,267,0.846441947565543,0.31306778476589797,19329,296.23881214755033,31.093176056702365,88.20942625071136,1196,27,449,25,unknown,pprett,pprett,true,pprett,11,0.9090909090909091,47,23,827,true,true,false,false,26,89,7,0,43,1,48
624986,scikit-learn/scikit-learn,python,432,1320758195,1320880751,1320880751,2042,2042,github,false,false,false,70,64,55,0,11,0,11,0,3,8,2,15,31,20,0,0,8,8,15,31,20,0,0,5391,764,7557,952,350.1034067695123,12.837482439758757,147,vlad@vene.ro,sklearn/tree/_tree.pyx|sklearn/ensemble/forest.py|doc/conf.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/tree/_tree.c|sklearn/ensemble/tests/test_forest.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/random_forest.py|sklearn/ensemble/tests/test_random_forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/tests/test_tree.py|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/tree/__init__.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/ensemble/__init__.py|sklearn/ensemble/forest.py|sklearn/ensemble/tests/test_forest.py|sklearn/tree/__init__.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/forest.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/ensemble/random_forest.py|sklearn/ensemble/tests/test_random_forest.py|sklearn/ensemble/random_forest.py|sklearn/ensemble/random_forest.py|sklearn/ensemble/tests/test_random_forest.py|sklearn/ensemble/bagging.py|sklearn/ensemble/base.py|sklearn/ensemble/boosting.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/random_forest.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/ensemble/random_forest.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py,93,0.0014388489208633094,3,5,false,Changes on tree module CC: @pprett @bdholt1This pull request is a factorization of my changes on the tree module Those were part of a pull request to the ensemble branch of bdholt1 (see [1]) I submit this here because Brian is away for this week and because @pprett is also ready to merge his array representation of the tree See the discusion in [1] for further details[1]: https://githubcom/bdholt1/scikit-learn/pull/24,,266,0.8458646616541353,0.302158273381295,19322,296.34613394058584,31.1044405341062,88.2413828796191,1193,27,448,28,unknown,glouppe,glouppe,true,glouppe,11,1.0,50,14,362,true,true,false,false,15,40,7,0,84,2,1666
624987,scikit-learn/scikit-learn,python,431,1320757523,,1321135469,6299,,unknown,false,false,false,29,40,14,0,22,4,26,0,4,3,0,7,24,6,0,0,6,1,20,27,15,0,0,1175,65,2211,92,132.50902391110105,4.858486687251249,84,vlad@vene.ro,sklearn/covariance/__init__.py|sklearn/covariance/g_lasso_.py|examples/covariance/plot_cov_l1.py|sklearn/covariance/g_lasso_.py|sklearn/datasets/samples_generator.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/g_lasso_.py|examples/covariance/plot_cov_l1.py|sklearn/covariance/g_lasso_.py|sklearn/datasets/samples_generator.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/tests/test_g_lasso.py|examples/covariance/plot_cov_l1.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/tests/test_g_lasso.py|examples/covariance/plot_cov_l1.py|sklearn/covariance/__init__.py|sklearn/covariance/g_lasso_.py|examples/covariance/plot_cov_l1.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/tests/test_g_lasso.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/tests/test_g_lasso.py|sklearn/covariance/g_lasso_.py|sklearn/covariance/tests/test_g_lasso.py|doc/modules/classes.rst|doc/modules/covariance.rst|examples/covariance/plot_sparse_cov.py|sklearn/covariance/g_lasso_.py,75,0.00935251798561151,0,2,false,Glasso The implements the standard GLasso algorithm to estimate sparse inverse covariance I believe that the code is ready to be merged with examples tests and documentation Please review,,265,0.8490566037735849,0.302158273381295,19329,296.23881214755033,31.093176056702365,88.20942625071136,1193,27,448,29,unknown,GaelVaroquaux,mblondel,false,,6,0.8333333333333334,162,2,624,true,true,false,false,61,155,13,0,182,5,297
624827,scikit-learn/scikit-learn,python,430,1320714875,,1379090170,972921,,unknown,false,false,false,98,20,2,0,34,1,35,0,5,1,0,3,8,4,0,0,1,0,7,8,5,0,0,343,17,440,34,18.033263523025305,0.5542538743730971,194,vlad@vene.ro,sklearn/naive_bayes.py|sklearn/preprocessing/__init__.py|sklearn/preprocessing/tests/test_preprocessing.py|examples/semisupervised_document_classification.py,152,0.06493506493506493,0,18,false,WIP: Semisupervised Naive Bayes using Expectation Maximization Heres the EM algorithm for semisupervised Naive Bayes The implementation checks for convergence based on the coefficients following the advice of Bishop 2006 so it could be used more generally for linear classifier self-training but I only implemented the necessary machinery (fitting on a 1-of-K vector) on the discrete NB estimators and we might want to switch to log-likelihood based convergence checking laterNarrative documentation follows if theres interest in this pull request Ive adapted the document classification example script into a new semisupervised example We might also merge these scripts,,264,0.8522727272727273,0.3023088023088023,26058,334.82999462736973,31.890398342159795,91.37309079745185,1193,27,447,191,unknown,larsmans,larsmans,true,,28,0.7142857142857143,45,26,477,true,true,false,false,32,63,12,0,110,4,62
624989,scikit-learn/scikit-learn,python,429,1320690395,1320759027,1320759027,1143,1143,merged_in_comments,false,false,false,58,2,2,0,1,0,1,0,2,1,0,3,4,1,0,1,1,0,3,4,1,0,1,2,0,2,0,9.193569067372389,0.3380825462463074,59,vlad@vene.ro,doc/images/iris.png|doc/conf.py|doc/images/iris.png|doc/modules/tree.rst,35,0.015873015873015872,0,1,false,fix for Issue #356 (build of pdf documentation is broken) This pull request fixes build of pdf documentation The problems were:* Lack of bm package in doc/confpy* irissvg - Latex doesnt seem to support this format So Ive converted it to pngI had to additionally include morefloats package to fix “Too many unprocessed floats” error,,263,0.8517110266159695,0.30158730158730157,19323,296.33079749521295,31.10283082337111,88.23681622936397,1189,27,447,28,unknown,bvtrach,GaelVaroquaux,false,GaelVaroquaux,0,0,0,1,262,false,false,false,false,0,0,0,0,1,0,134
624990,scikit-learn/scikit-learn,python,428,1320633647,1324394352,1324394352,62678,62678,merged_in_comments,false,false,false,106,10,4,0,13,0,13,0,4,4,1,9,17,8,0,0,6,1,12,19,11,0,0,554,2,851,21,64.82046724408333,2.3120765405641324,141,vlad@vene.ro,sklearn/covariance/tests/test_covariance.py|examples/covariance/outlier_detection.py|sklearn/covariance/__init__.py|sklearn/covariance/empirical_covariance_.py|sklearn/covariance/outlier_detection.py|doc/modules/classes.rst|doc/modules/covariance.rst|doc/modules/outlier_detection.rst|doc/unsupervised_learning.rst|examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_outlier_detection.py|examples/svm/plot_oneclass.py|sklearn/covariance/__init__.py|sklearn/covariance/outlier_detection.py|examples/covariance/outlier_detection.py,101,0.01177336276674025,0,1,false,MRG: Outlier and novelty detection This pull-request is about providing tools for outlier and novelty detection:- modification of the OneClassSVM example in order to illustrate novelty detection - implementation of a mixin to be combined with a covariance object in order to perform outlier detection - creation of an example to illustrate outlier detection and comparison between OneClassSVM- and covariance-based methods - documentation about robust covariance estimation novelty detection outlier detection + link to the examples/\ This pull-request use things that are introduced in a previous pull-request about refactoring the robust covariance estimator objects This latter PR should be merged first,,262,0.851145038167939,0.3016924208977189,20524,279.6725784447476,28.94172675891639,83.26836873903723,1185,27,446,41,unknown,VirgileFritsch,GaelVaroquaux,false,GaelVaroquaux,5,0.6,1,0,552,true,true,false,false,8,11,2,0,18,0,42
624991,scikit-learn/scikit-learn,python,427,1320583284,1320590654,1320590654,122,122,commits_in_master,false,false,false,39,1,0,0,2,0,2,0,3,0,0,0,4,0,0,0,0,0,4,4,2,0,0,0,0,65,48,0,0.0,0,,,0,0.0,0,1,false,ENH: use integer indexing instead of boolean masks by default for CV As discussed on the mailing list:- use integer indices by default- update check_cv to raise ValueError when cvindices  False and input data is sparse,,261,0.8505747126436781,0.30402384500745155,19337,294.0476806122977,30.976883694471738,87.81093240937064,1185,27,446,24,unknown,ogrisel,larsmans,false,larsmans,15,0.7333333333333333,379,106,893,true,false,false,false,91,334,41,0,337,7,2
624992,scikit-learn/scikit-learn,python,425,1320422644,1320508151,1320508151,1425,1425,github,false,false,false,7,13,6,0,1,0,1,0,1,0,0,5,8,5,0,0,0,0,8,8,6,0,0,96,1,158,1,30.776066745732287,1.1317989647637674,60,vlad@vene.ro,sklearn/svm/sparse/base.py|sklearn/svm/sparse/classes.py|sklearn/svm/base.py|sklearn/svm/classes.py|sklearn/svm/classes.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,42,0.020316027088036117,0,0,true,Svm cache size See discussion here:https://githubcom/scikit-learn/scikit-learn/commit/7769275dc4fb9f5f8ea6448ecfbaffe4fdcbf1a6,,260,0.85,0.3054928517682468,19331,293.46645284775747,30.986498370492992,87.78645698618799,1183,26,444,24,unknown,amueller,amueller,true,amueller,16,0.9375,206,21,378,true,true,false,false,13,16,10,0,35,0,12
624993,scikit-learn/scikit-learn,python,423,1320359035,1324291777,1324291777,65545,65545,merged_in_comments,false,false,false,34,3,3,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,47,0,47,0,13.768314284485687,0.5048197449702961,38,vlad@vene.ro,sklearn/utils/extmath.py|sklearn/utils/extmath.py|sklearn/datasets/samples_generator.py,32,0.024224072672218017,0,0,false,Random projections: factor out randomized range finder As mentioned on the mailing list: I found the randomized range finder more generally useful so I factored it out of fast_svdAlso includes a random cosmit,,258,0.8527131782945736,0.30734292202876606,19329,296.23881214755033,31.093176056702365,88.20942625071136,1180,26,443,36,unknown,kcarnold,GaelVaroquaux,false,GaelVaroquaux,0,0,10,0,1172,true,true,false,false,0,0,0,0,5,0,770
624994,scikit-learn/scikit-learn,python,422,1320338817,1320422557,1320422557,1395,1395,commits_in_master,false,false,true,7,5,4,0,2,0,2,0,2,0,0,3,3,3,0,0,0,0,3,3,3,0,0,48,1,72,1,13.407051719575719,0.49304928986191277,56,vlad@vene.ro,sklearn/svm/classes.py|sklearn/svm/base.py|sklearn/svm/tests/test_svm.py,42,0.02243829468960359,0,0,false,Svm cache size See discussion here:https://githubcom/scikit-learn/scikit-learn/commit/7769275dc4fb9f5f8ea6448ecfbaffe4fdcbf1a6,,257,0.8521400778210116,0.3081525804038893,19331,293.46645284775747,30.986498370492992,87.78645698618799,1180,26,443,24,unknown,amueller,amueller,true,amueller,15,0.9333333333333333,206,21,377,true,true,false,false,12,14,9,0,28,0,387
427031,scikit-learn/scikit-learn,python,418,1320058644,,1348183084,468740,,unknown,false,true,false,118,3,1,0,15,7,22,0,6,0,0,1,1,1,0,0,0,0,1,1,1,0,0,100,0,104,0,4.435680101545668,0.16263566122653683,140,vlad@vene.ro,sklearn/linear_model/ridge.py,140,0.10424422933730454,0,1,false,WIP: Improve Ridges conjugate gradient descent Reorder terms in the algorithm so that XX is not explicitly computed and add LinearOperator as accepted types This improves existing code in two ways:* Speed: I get speedups ranging from 25x to more than 10x    Here are some experimental benchmarks speed plotted as   a function of matrix density for three types of design matrix :    square tall and flat Code lives [here](https://gistgithubcom/1322711)[Square](https://lh3googleusercontentcom/-2PPuoNXc_Bs/Tq5SAE2AyyI/AAAAAAAAAIs/oBAoGKB6St4/s640/cg1png)[Flat](https://lh3googleusercontentcom/-xdmwymdllow/Tq5SAe9DnGI/AAAAAAAAAIw/jppeZ16m9hM/s640/cg2png)[Tall](https://lh4googleusercontentcom/-XGX2oFlqww0/Tq5SAOK1h6I/AAAAAAAAAI0/Jk-fGlPVGdE/s640/cg3png)* Interface Accepting the LinearOperator type enables the use of custom   matrix-vector products effectively enabling to use dense matrices with   special structure pytables etc I have tried using memmap and while it works it is exhasperately slow Probably PyTables is much better but havent tried,,256,0.85546875,0.3075204765450484,19329,296.23881214755033,31.093176056702365,88.20942625071136,1175,27,440,130,unknown,fabianp,larsmans,false,,18,0.8888888888888888,72,20,534,true,true,false,true,47,58,17,0,85,0,14
624995,scikit-learn/scikit-learn,python,417,1319719494,1324494623,1324494623,79585,79585,github,false,false,false,36,32,2,0,11,1,12,0,4,0,0,2,15,2,0,0,2,0,15,17,12,0,0,35,9,1426,153,13.860686851437427,0.4827050753584584,23,vlad@vene.ro,sklearn/multiclass.py|sklearn/tests/test_multiclass.py|sklearn/multiclass.py,23,0.01658255227108868,0,0,false,MRG : ENH multilabel learning in OneVsRestClassifier Multilabel learning with the OvR strategy I had this feature planned for quite some time but only decided to act on it after this weeks discussion on the ML,,255,0.8549019607843137,0.29776496034607064,21009,276.97653386643816,28.5591889190347,81.58408301204246,1168,26,436,36,unknown,larsmans,amueller,false,amueller,27,0.7037037037037037,45,26,466,true,true,false,true,33,65,11,0,87,4,20
624997,scikit-learn/scikit-learn,python,415,1319555078,,1336255586,278341,,unknown,false,false,false,56,1,1,0,9,0,9,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,10,0,10,0,4.167909804572089,0.15281777573701538,40,vlad@vene.ro,sklearn/grid_search.py,40,0.028308563340410473,0,0,false,Fixed GridSearchCV to work when using precomputed kernel matrices HelloI wrote a small fix of grid_searchpy to allow for the use of precomputed kernel matrices in conjunction with GridSearchCV I tested it successfullyHope this helpsAdrienPS: Its the first time I really use github so I hope I didnt make any newbie mistake,,254,0.8582677165354331,0.2922859164897382,19329,296.23881214755033,31.093176056702365,88.20942625071136,1165,26,434,99,unknown,daien,GaelVaroquaux,false,,0,0,2,0,355,false,true,false,false,0,0,0,0,0,0,15
624998,scikit-learn/scikit-learn,python,414,1319300823,1319300980,1319300980,2,2,github,false,false,false,67,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,3,0,0,0.0,0,,,0,0.0,0,0,false,ENH: Adds more verbosity to grid_search verbose  2 gives scores ENH: Adds more verbosity to grid_search verbose  2 gives scores while running gridWhen the classifier takes long I found it annoying to have to wait to the end until I get resultsDont know if this is small enough to merge myself Since Im still pretty new around Id rather get your ok ),,253,0.857707509881423,0.29694019471488176,19579,288.77879360539356,30.645078911078194,85.90837121405588,1162,26,431,21,unknown,amueller,amueller,true,amueller,14,0.9285714285714286,193,21,365,true,true,false,false,10,10,6,0,20,0,1
624999,scikit-learn/scikit-learn,python,413,1319299877,,1319551391,4191,,unknown,false,false,false,40,5,5,0,6,0,6,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,59,0,59,0,22.834525904886377,0.836882364908907,19,vlad@vene.ro,benchmarks/bench_svm.py|benchmarks/bench_svm.py|benchmarks/bench_svm.py|benchmarks/bench_svm.py|benchmarks/bench_svm.py,19,0.01321279554937413,0,2,false,Svm bench shogun This adds the shogun libsvm bindings to the svm benchmarkIt also corrects a bug in the benchmark: as far as I can see sklearn has gamma1/n_features as default while the other libs have gamma1 as default,,252,0.8611111111111112,0.29763560500695413,19579,288.77879360539356,30.645078911078194,85.90837121405588,1162,26,431,20,unknown,amueller,fabianp,false,,13,1.0,193,21,365,true,true,false,false,9,10,5,0,19,0,59
625000,scikit-learn/scikit-learn,python,412,1319231915,1319252677,1319252677,346,346,github,false,false,false,9,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,11,0,0,0.0,0,,,0,0.0,0,0,true,fix: permutation test score averages across folds closes #411,,251,0.8605577689243028,0.29896193771626295,19578,288.7935437736234,30.646644192460926,85.91275921953213,1160,26,430,20,unknown,satra,agramfort,false,agramfort,4,1.0,43,2,642,true,true,false,true,7,11,2,0,8,1,220
625001,scikit-learn/scikit-learn,python,410,1319204967,1319575956,1319575956,6183,6183,github,false,false,false,41,4,0,0,12,0,12,0,5,0,0,0,55,0,0,0,0,0,55,55,54,0,0,0,0,526,70,0,0.0,0,,,0,0.0,0,4,true,ENH accept matrix input throughout Simple solution: replace npasanyarray with npasarray throughout This does not copy its input:     X  npmatrix(nparange(10))     Y  npasarray(X)     Ybase is X    TrueAll tests pass Added some tests for the utils module,,250,0.86,0.30103806228373703,19414,291.2331307304008,30.905532090244154,86.63850829298444,1160,26,430,20,unknown,larsmans,larsmans,true,larsmans,26,0.6923076923076923,45,26,460,true,true,false,false,34,72,9,0,80,7,7
625003,scikit-learn/scikit-learn,python,408,1319132833,1319169286,1319169286,607,607,github,false,false,false,65,7,2,0,4,0,4,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,15,0,18,17,4.646385655219008,0.17088476757321744,24,vlad@vene.ro,sklearn/linear_model/omp.py,24,0.016666666666666666,0,0,false,FIX for issue #403: Orthogonal Matching Pursuit bug in column swapping This fixes issue #403OMP would behave wrongly if the maximally correlated atom at one iteration has had its position swapped The effect was that OMP solutions would never have nonzero values for their first n_nonzero_atoms indices (0-n_nonzero_atoms) The column swapping is an optimization trick for having the currently used atoms in contiguous memory,,249,0.8594377510040161,0.3055555555555556,19606,287.87105987962866,30.55187187595634,85.68805467713965,1159,26,429,20,unknown,vene,vene,true,vene,13,0.9230769230769231,27,17,557,true,true,false,false,24,60,5,0,188,1,6
625004,scikit-learn/scikit-learn,python,407,1319126966,1319127038,1319127038,1,1,github,false,false,false,9,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,DOCS: Typo in url Broken link on reference paper,,248,0.8588709677419355,0.3055555555555556,19606,287.87105987962866,30.55187187595634,85.68805467713965,1159,26,429,20,unknown,amueller,mblondel,false,mblondel,12,1.0,193,21,363,true,true,true,false,8,10,4,0,15,0,-1
625005,scikit-learn/scikit-learn,python,405,1319125195,1319144372,1319144372,319,319,commit_sha_in_comments,false,false,false,14,1,0,0,3,0,3,0,2,0,0,0,2,0,0,0,2,0,2,4,4,0,0,0,0,105,0,0,0.0,0,,,0,0.0,0,0,false,ENH Cython version of SVMlight loader Speedup of ~30% on MNIST dataset fromhttp://wwwcsientuedutw/~cjlin/libsvmtools/datasets/multiclasshtml,,247,0.8582995951417004,0.30619345859429364,19606,287.87105987962866,30.55187187595634,85.68805467713965,1159,26,429,20,unknown,larsmans,larsmans,true,larsmans,25,0.68,44,25,459,true,true,false,false,33,72,9,0,77,7,15
625006,scikit-learn/scikit-learn,python,404,1319065212,1319065590,1319065590,6,6,github,false,false,false,7,2,0,0,0,0,0,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,16,0,0,0.0,0,,,0,0.0,0,0,false,DOC Added documentation for attributes of GridSearchCV ,,246,0.8577235772357723,0.3076923076923077,19596,288.0179628495612,30.56746274749949,85.73178199632578,1159,26,428,19,unknown,amueller,agramfort,false,agramfort,11,1.0,193,21,362,true,true,true,false,7,9,3,0,11,0,-1
625007,scikit-learn/scikit-learn,python,402,1319027388,1320972727,1320972727,32422,32422,github,false,false,false,70,38,3,0,41,0,41,0,5,0,0,3,11,3,0,0,0,0,11,11,9,0,0,442,0,1013,56,26.029137726441874,0.9543668453054887,37,vlad@vene.ro,sklearn/metrics/cluster/__init__.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/__init__.py|sklearn/metrics/cluster/__init__.py|sklearn/metrics/cluster/supervised.py|sklearn/metrics/cluster/supervised.py,28,0.0069881201956673656,0,9,false,Adjusted Mutual Information Mutual Information adjusted for chance See [1] for details (specifically look at the references for detail)I have tested this against the Matlab code and it works Took me a while as I had log for entropy and log2 for Expected Mutual Information (should be the other way around) I think that the _expected_mutual_information can be optimised but I went with get it right first[1]  http://enwikipediaorg/wiki/Adjusted_Mutual_Information,,245,0.8571428571428571,0.31027253668763105,19329,296.23881214755033,31.093176056702365,88.20942625071136,1159,26,428,27,unknown,robertlayton,robertlayton,true,robertlayton,6,1.0,4,5,152,true,true,false,false,15,88,7,0,67,2,78
625008,scikit-learn/scikit-learn,python,400,1318957675,1318958538,1318958538,14,14,github,false,false,false,12,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,DOCS: Image is aligned to the right Minor typo in user guide,,244,0.8565573770491803,0.3110181311018131,19596,288.0179628495612,30.56746274749949,85.73178199632578,1157,26,427,18,unknown,amueller,ogrisel,false,ogrisel,10,1.0,193,21,361,true,true,true,false,6,9,2,0,9,0,-1
625009,scikit-learn/scikit-learn,python,398,1318896052,,1320759083,31050,,unknown,false,false,false,28,2,2,0,7,0,7,0,5,5,1,4,10,9,0,0,5,1,4,10,9,0,0,1365,14,1365,14,46.22362631987636,1.699890968465563,72,vlad@vene.ro,sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/utils/_fast_math.h|sklearn/utils/fast_math.c|sklearn/utils/fast_math.h|sklearn/utils/fast_math.pyx|sklearn/utils/setup.py|sklearn/utils/tests/test_fast_math.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/setup.py|sklearn/utils/fast_math.h,49,0.0,0,3,false,Fast log This branch uses a fast and ugly approximation of the log in the entropy criteria of the decision trees It achieve a times 4 speed up,,243,0.8600823045267489,0.31472435450104674,19331,293.46645284775747,30.986498370492992,87.78645698618799,1156,26,426,27,unknown,GaelVaroquaux,GaelVaroquaux,true,,5,1.0,154,2,602,true,true,false,false,69,200,24,0,178,9,510
625010,scikit-learn/scikit-learn,python,396,1318868068,1322486695,1322486696,60310,60310,github,false,false,false,104,6,0,0,7,0,7,0,4,0,0,0,7,0,0,0,0,0,7,7,7,0,0,0,0,1308,75,0,0.0,0,,,0,0.0,0,0,false,Refactor MCD robust covariance estimator: it is easier to regularize I need to reuse my MInimum Covariance Determinant code outside from sklearn I propose a change in the code structure so that it will be easier to me to base my new code on the existing oneWhat I want to do is to be able to plug any covariance estimator within the MCD computation thats why I introduced a method nonrobust_covariance which only aims at computing a non-robust covariance in a consistent way accross the MCD computation Just by changing what is inside this method one can do what I want to do,,242,0.859504132231405,0.31567944250871083,20187,291.5242482785951,30.167929855847824,86.34269579432308,1156,26,426,29,unknown,VirgileFritsch,VirgileFritsch,true,VirgileFritsch,4,0.5,1,0,532,true,true,false,false,7,6,1,0,8,0,19818
625011,scikit-learn/scikit-learn,python,391,1318412951,1318804768,1318804768,6530,6530,merged_in_comments,false,false,false,64,3,1,0,7,0,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,10,0,14,45,4.019502654075587,0.14783263517693834,13,vlad@vene.ro,sklearn/cluster/hierarchical.py,13,0.008660892738174551,0,5,false,Fixed bug in updating structure matrix in ward_tree algorithm Change: Formerly when merging two clusters the merge was not connected to all other clusters the two childs had been connected to This could result in raised Exceptions when connected components of the graph become unconnected Now the connections are propagated up through the ward tree during merging such that connected components remain always connected,,241,0.8589211618257261,0.31379080612924715,19589,288.0187860533973,30.57838582878146,85.76241768339374,1154,28,421,16,unknown,jmetzen,GaelVaroquaux,false,GaelVaroquaux,0,0,3,1,2,true,true,false,false,1,1,0,0,2,0,62
626040,scikit-learn/scikit-learn,python,386,1317991762,1318005483,1318005483,228,228,github,false,false,false,228,3,0,0,22,0,22,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,553,0,0,0.0,0,,,0,0.0,0,11,true,_find_best_split using samples not in sample_mask I noticed a problem with _find_best_split in _treepyx The variables a and b should iterate over pairs of consecutive samples that are in the sample mask The variable a is initialized to zero but this sample is not necessarily in the sample_mask I have added code so it initializes to the first sample in the sample maskI found this problem by adding the following test at line 625 in _treepyxpython            if not sample_mask[X_argsorted_i[a]] or not sample_mask[X_argsorted_i[b]]:                raise ValueError(Attempting to split on sample that is not within sample_mask)and I use the following test case:pythonimport numpy as npfrom sklearntree import DecisionTreeRegressornpseterr(raiseraiseraiseignoreraise) #We are unforgiving #(except for underflows)features  nparray([[ 15208097839  14040744019  12975102234  15990493774]  [ 14250700378  1358193512   11782884979  1627578125 ]  [ 12728772736  14040744019  12975102234  15990493774]  [ 13237025452  14371923828  13835694885  15784558105]  [ 10310237122  14371928406  13835696411  15784559631]  [ 12771276855  14371923828  13835694885  15784558105]  [ 12091514587  14040744019  12975102234  15990493774]])labels  nparray([ 1 070209277053896582  0          090914464 048026916  049622521])dt  DecisionTreeRegressor()dtfit(featureslabels)dtfit(-featureslabels)This results in the exception being raisedI also added a test in treepy which checks for empty sample masks while the tree is being recursively built This is a possible side effect of invalid thresholds being used An empty mask in recursive_partition would result in a tree leaf node having no label which is invalid,,240,0.8583333333333333,0.3142292490118577,19522,290.4927773793668,30.580883106239114,86.36410203872553,1147,26,416,17,unknown,TimSC,pprett,false,pprett,1,1.0,3,2,46,false,false,false,false,3,6,1,0,0,0,53
625014,scikit-learn/scikit-learn,python,385,1317991247,1320880757,1320880757,48158,48158,github,false,true,false,41,12,4,0,12,0,12,0,5,6,0,1,12,6,0,0,6,0,6,12,11,0,0,501,154,751,174,30.558509454985938,1.1238004970190545,7,satra@mit.edu,scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/random_forest.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/ensemble/random_forest.py,5,0.002635046113306983,0,8,true,Enh/ensemble Now that Decision Trees have been successfully included in scikit-learn/master here is an early Pull Request for ensemble learningThere is a random forest implementation if folks want to give it a tryNoel has already added boosting and bagging,,239,0.8577405857740585,0.3142292490118577,19331,293.46645284775747,30.986498370492992,87.78645698618799,1147,26,416,26,unknown,bdholt1,glouppe,false,glouppe,7,1.0,3,14,74,true,true,true,false,21,124,7,0,166,0,73
625015,scikit-learn/scikit-learn,python,383,1317741840,1317745631,1317745631,63,63,github,false,false,false,21,1,0,0,4,0,4,0,3,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,1,3,false,Sparse SVM mem leak this commit fixes #367Testing shows that memory is now released Thanks @tehf0x for your hard work,,238,0.8571428571428571,0.308300395256917,18987,295.5706536050982,30.65255174593143,87.0595670722073,1136,26,413,20,unknown,bdholt1,fabianp,false,fabianp,6,1.0,3,14,71,true,true,true,false,18,117,6,0,160,0,1
625016,scikit-learn/scikit-learn,python,381,1317696752,1317697849,1317697849,18,18,github,false,false,false,19,5,0,0,2,0,2,0,3,0,0,0,4,0,0,0,0,0,4,4,4,0,0,0,0,40,2,0,0.0,0,,,0,0.0,0,0,false,Doc/permutation docstring for permutation_test_score updated to reflect the difference in p-value depending on what sort of function is used,,237,0.8565400843881856,0.3112449799196787,18663,297.80849809784064,31.13111504045437,87.76723999357017,1136,26,412,22,unknown,satra,ogrisel,false,ogrisel,3,1.0,40,2,624,true,true,false,true,3,5,1,0,2,1,15
625017,scikit-learn/scikit-learn,python,380,1317680820,1317741494,1317741494,1011,1011,commits_in_master,false,false,false,21,250,246,0,2,3,5,3,3,37,33,50,121,67,0,6,37,33,51,121,67,0,6,18140,1233,18246,1237,1535.1016370382554,57.93930252201642,152,x006@x006-icsl.(none),sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/modules/tree.rst|sklearn/tree/__init__.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/setup.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|doc/modules/tree.rst|examples/tree/plot_iris.py|examples/tree/plot_tree_regression.py|examples/tree/plot_iris.py|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|doc/modules/tree.rst|doc/supervised_learning.rst|sklearn/tree/tree.py|doc/modules/tree.rst|sklearn/tree/tests/test_tree.py|doc/modules/tree.rst|doc/modules/tree.rst|doc/modules/tree.rst|sklearn/tree/tests/test_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/images/iris.svg|sklearn/tree/tests/test_tree.dot|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/modules/tree.rst|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|benchmarks/bench_sgd_covertype.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|benchmarks/bench_sgd_covertype.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|doc/images/iris.svg|doc/modules/tree.rst|examples/tree/plot_tree_regression.py|doc/images/boston.svg|doc/images/iris.svg|doc/modules/tree.rst|doc/modules/tree.rst|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/images/boston.png|doc/images/boston.svg|doc/images/iris.png|doc/images/iris.svg|sklearn/tree/__init__.py|sklearn/tree/tests/test_tree.dot|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|benchmarks/bench_sgd_covertype.py|sklearn/tree/_tree.c|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/modules/tree.rst|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|benchmarks/bench_tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|doc/images/boston.png|doc/images/iris.png|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tests/test_tree.dot|sklearn/tree/tests/test_tree.py|benchmarks/bench_tree.py|sklearn/tree/__init__.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/tree.py|benchmarks/bench_tree.py|sklearn/tree/_tree.c|sklearn/tree/_tree.pyx|sklearn/tree/tree.py|sklearn/tree/_tree.pyx|benchmarks/bench_tree.py|examples/tree/plot_iris.py|doc/modules/classes.rst|doc/modules/tree.rst|sklearn/tree/__init__.py|sklearn/tree/_tree.c|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|benchmarks/bench_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|sklearn/svm/src/libsvm/libsvm_sparse_helper.c|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/base.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/base.py|scikits/learn/tree_model/classifier.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/setup.py|scikits/learn/tree_model/tests/__init__.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/base.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/datasets/base.py|examples/tree_model/plot_decision_tree_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|examples/tree_model/plot_decision_tree_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/bagging/__init__.py|scikits/learn/ensemble/bagging/bagging.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/boosting/__init__.py|scikits/learn/ensemble/boosting/gradboost.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/committee.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|testbdt.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/ensemble/bagging.py|scikits/learn/tree_model/classifier.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|benchmarks/bench_tree.py|scikits/learn/datasets/data/boston_house_prices.csv|scikits/learn/datasets/descr/boston_house_prices.rst|scikits/learn/tree_model/optimisation/__init__.py|scikits/learn/tree_model/optimisation/profile_tree.py|scikits/learn/tree_model/tests/test_randomforest.py|scikits/learn/tree_model/tests/test_tree.py|examples/tree_model/plot_decision_tree_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/base.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|benchmarks/bench_tree.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tests/test_randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/__init__.py|scikits/learn/decisiontree/__init__.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/random_forest.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/setup.py|scikits/learn/tree/__init__.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/decision_tree.py|scikits/learn/tree/libdecisiontree.cpp|scikits/learn/tree/libdecisiontree.pyx|scikits/learn/tree/libdecisiontree_helper.cpp|scikits/learn/tree/setup.py|scikits/learn/tree/src/Histogram.h|scikits/learn/tree/src/Node.cpp|scikits/learn/tree/src/Node.h|scikits/learn/tree/tests/__init__.py|scikits/learn/tree/tests/test_decision_tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/optimisation/__init__.py|scikits/learn/tree_model/optimisation/profile_tree.py|scikits/learn/tree_model/setup.py|scikits/learn/tree_model/tests/__init__.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|benchmarks/bench_tree.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/tree/__init__.py|scikits/learn/tree/tree.py|scikits/learn/ensemble/random_forest.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/datasets/base.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.pyx|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|benchmarks/bench_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/random_forest.py|scikits/learn/ensemble/tests/test_random_forest.py|scikits/learn/tree/__init__.py|scikits/learn/tree/decision_tree.py|scikits/learn/tree/libdecisiontree.cpp|scikits/learn/tree/libdecisiontree.pyx|scikits/learn/tree/libdecisiontree_helper.cpp|scikits/learn/tree/setup.py|scikits/learn/tree/src/Histogram.h|scikits/learn/tree/src/Node.cpp|scikits/learn/tree/src/Node.h|scikits/learn/tree/tests/test_decision_tree.py|doc/modules/classes.rst|doc/modules/tree.rst|doc/supervised_learning.rst|examples/tree/README.txt|examples/tree/plot_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/datasets/base.py|scikits/learn/setup.py|scikits/learn/tree/__init__.py|scikits/learn/tree/_tree.pyx|doc/modules/tree.rst|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|benchmarks/bench_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/tree.py|doc/modules/tree.rst|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tests/test_tree.py|scikits/learn/tree/tree.py|scikits/learn/tree/_tree.c|scikits/learn/tree/_tree.pyx|scikits/learn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py|sklearn/tree/tests/test_tree.py|sklearn/tree/tree.py,87,0.0,1,1,false,Sparse SVM mem leak this commit fixes #367Testing shows that memory is now released  Thanks @tehf0x for your hard work,,236,0.8559322033898306,0.31078365706630945,18987,295.5706536050982,30.65255174593143,87.0595670722073,1135,26,412,21,unknown,bdholt1,bdholt1,true,bdholt1,5,1.0,3,14,70,true,true,false,false,16,109,5,0,159,0,837
625022,scikit-learn/scikit-learn,python,377,1317662696,1317743258,1317743258,1342,1342,github,false,false,false,64,3,0,0,9,0,9,0,4,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,81,15,0,0.0,0,,,0,0.0,0,5,false,Allow sparse input to NMF I patched up the NMF code to accept scipysparse matrices Allows the topic detection example to scale up a bit: I can run it with 10000 examples and 10000 features in just over half a minute using on the scale of 200MB of memory That used to be 15GB of memory and no end in sight after several minutes,,235,0.8553191489361702,0.30913978494623656,18595,298.8975531056736,31.244958322129605,88.08819575154611,1134,26,412,22,unknown,larsmans,ogrisel,false,ogrisel,24,0.6666666666666666,43,25,442,true,true,true,true,41,90,19,0,99,8,14
625023,scikit-learn/scikit-learn,python,376,1317645937,1317656057,1317656057,168,168,github,false,false,false,23,2,0,0,1,0,1,0,1,0,0,0,2,0,0,0,0,0,2,2,1,0,0,0,0,0,26,0,0.0,0,,,0,0.0,0,0,false,Fast tests in coordindate_descent  Trivial changes (that hopefully dont impact on the test quality) to win a factor 3 in the coordinate_descent tests,,234,0.8547008547008547,0.30899256254225826,18597,298.8116362854224,31.241598107221595,88.07872237457654,1134,26,412,21,unknown,fabianp,agramfort,false,agramfort,17,0.8823529411764706,71,20,506,true,true,true,true,51,66,20,0,104,0,168
625024,scikit-learn/scikit-learn,python,375,1317645931,1317727416,1317727416,1358,1358,github,false,false,false,87,6,0,0,0,0,0,0,1,0,0,0,8,0,0,0,3,0,8,11,9,0,0,0,0,1672,182,0,0.0,0,,,0,0.0,0,0,false,Implements a robust covariance estimator: Rousseeuws MCD Minimum Covariance Determinant (MCD) is a robust estimator ofcovariance introduced by Rousseeuw [1] The main idea behind the MCDis to find a fixed proportion of observations whose scatter matrix hasthe minimum determinantThe MCD estimator is computed with the FastMCD algorithm [2][1] P J Rousseeuw Least median of squares regression J Am StatAss 79:871 1984[2] P J Rousseeuw and K Van Driessen A fast algorithm for theminimum covariance determinant estimator Technometrics 41(3):2121999,,233,0.8540772532188842,0.30899256254225826,18663,297.80849809784064,31.13111504045437,87.76723999357017,1134,26,412,22,unknown,VirgileFritsch,fabianp,false,fabianp,3,0.3333333333333333,1,0,518,true,true,false,false,6,8,0,0,3,1,-1
625025,scikit-learn/scikit-learn,python,374,1317581605,1317675503,1317675503,1564,1564,github,false,false,false,76,10,0,0,4,0,4,0,4,0,0,0,9,0,0,0,0,0,9,9,8,0,0,0,0,199,0,0,0.0,0,,,0,0.0,0,2,false,SGD refactoring + enhancements This pull request includes::  * refactoring of sgd unifying of sparse and dense prediction now uses only dense coef - which is much faster at test time  * added input size check to sparsesvm naive_bayes linear_modelsparse I just got bitten by the CV - sparse matrix problem (sparse matrices cannot be indexed by boolean masks Id propose to swap to index arrays completely)  * some love to svmsparseclasses (pep8 mutable default values),,232,0.853448275862069,0.31245698554714385,18595,298.8975531056736,31.244958322129605,88.08819575154611,1132,26,411,22,unknown,pprett,pprett,true,pprett,10,0.9,46,23,789,true,true,false,false,22,73,2,0,35,1,28
625026,scikit-learn/scikit-learn,python,373,1317572526,1317573503,1317573503,16,16,github,false,false,false,110,1,1,0,2,0,2,0,2,0,0,4,4,3,0,0,0,0,4,4,3,0,0,84,33,84,33,17.96810297624269,0.6818207789375182,57,vlad@vene.ro,doc/whats_new.rst|sklearn/datasets/__init__.py|sklearn/datasets/svmlight_format.py|sklearn/datasets/tests/test_svmlight_format.py,52,0.011088011088011088,2,0,false,API change in SVMlight reader: handle multiple files with svmlight_load_files API change in SVMlight reader: handle multiple files with svmlight_load_files Documented this in whats_newrstThis might not seem as convenient as having svmlight_load_file handle multiple files itself but its much easier to keep consistent and document (): I found it very hard to write a proper and concise docstring for an svmlight_load_file that could handle multiple filesAlso I removed the buffer_mb parameter which was unused anyway and introduced a dtype parameter to determine the underlying type of the returned sample vectors I admit right away that this is kind of a package deal@ogrisel @mblondel what do you think,,231,0.8528138528138528,0.31392931392931395,18585,298.35889157923054,31.20796341135324,87.81275221953189,1132,26,411,20,unknown,larsmans,ogrisel,false,ogrisel,23,0.6521739130434783,43,25,441,true,true,true,true,39,89,18,0,97,8,11
574674,scikit-learn/scikit-learn,python,372,1317521882,,1356107970,643101,,unknown,false,true,false,82,58,1,0,28,0,28,0,7,2,0,0,10,2,0,0,3,0,8,11,8,0,0,247,28,1195,621,9.381833619332385,0.2883514470688775,0,,sklearn/random_projection.py|sklearn/tests/test_random_projection.py,0,0.0,0,11,false,WIP: Sparse Random Projections Early pull requests for sparse random projectionMain paper used as a reference for this PR:Li Hastie and Church KDD 2006 Very Sparse Random Projections- http://wwwstanfordedu/~hastie/Papers/Ping/KDD06_rppdf- http://csewebucsdedu/~akmenon/VerySparseRPTalkpdf (slides)## TODO before merge- write narrative doc- delempirical checks for johnson lindenstrauss in test suite/del- delexample explaining johnson lindenstrauss embedding/del- delupdate manifold example to use RP module instead of random unitary/del- example on dim-reduction (eg ANN search on faces data with ball-tree),,230,0.8565217391304348,0.31674842326559216,26058,334.82999462736973,31.890398342159795,91.37309079745185,1131,26,410,142,unknown,ogrisel,amueller,false,,14,0.7857142857142857,351,106,857,true,false,false,true,96,357,42,1,156,9,410
624830,scikit-learn/scikit-learn,python,371,1317383491,,1376564642,986352,,unknown,false,false,false,116,1,1,0,4,0,4,0,2,2,0,1,3,3,0,0,2,0,1,3,3,0,0,100,30,100,30,14.03328877443203,0.4313143133240581,11,peter.prettenhofer@gmail.com,sklearn/semi_supervised/__init__.py|sklearn/semi_supervised/pu.py|sklearn/semi_supervised/tests/test_pu.py,11,0.0,0,1,true,WIP: PU learning: the 1-DNF method Early pull request: the 1-DNF method for binary classifier learning from positive and unlabeled data only (PU learning) Issues:* this works more or less on the dataset Im currently working on but thats not public yet huge and XML-encoded* I tried to come up with an example based on 20news (my pu-learning-example branch) but that performs very poorly* the implementation is a transformer that takes X and y and returns y_transformed rather than X_transformed this is different from other transformers* Ive adopted the convention that -1 means unlabeledIf someone can devise an example that makes this work on a benchmark dataset Id be very glad,,229,0.8602620087336245,0.3126760563380282,26058,334.82999462736973,31.890398342159795,91.37309079745185,1126,25,409,205,unknown,larsmans,larsmans,true,,22,0.6818181818181818,43,25,439,true,true,false,false,38,88,17,0,95,8,9
625027,scikit-learn/scikit-learn,python,370,1317373121,1318197525,1318197525,13740,13740,merged_in_comments,false,false,false,106,8,4,0,11,3,14,0,3,0,0,9,9,9,0,0,0,0,9,9,9,0,0,161,202,161,419,62.51867167457802,2.372412971778499,39,vlad@vene.ro,sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/tests/test_ball_tree.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/base.py|sklearn/neighbors/classification.py|sklearn/neighbors/regression.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/utils/testing.py|sklearn/neighbors/ball_tree.c|sklearn/neighbors/ball_tree.pyx|sklearn/neighbors/base.py|sklearn/neighbors/tests/test_neighbors.py|sklearn/neighbors/unsupervised.py|sklearn/utils/testing.py,18,0.0077628793225123505,0,2,true,Equidistant warning for Nearest Neighbors As noted in issue #365 there can be a problem with k-neighbors algorithms when some of the distances are equal  When neighbor k and neighbor k+1 have identical distances one must be discarded  The results will depend on the ordering of the training data  This PR addresses this by adding a warning when this situation is detected  The warning is present with algorithm  brute and algorithm  ball_tree but not with kd_treeIt also adds an assert_warns function to sklearn/utils/testingpy which is copied from the numpy source  This is required for users with python  26 and numpy  14,,228,0.8596491228070176,0.3112208892025406,18533,298.6024928505908,31.2415690929693,87.95122214428316,1126,25,409,20,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,10,1.0,378,0,142,true,true,false,false,19,81,6,0,58,8,40
625028,scikit-learn/scikit-learn,python,364,1317032807,1317942601,1317942601,15163,15163,github,false,false,false,105,21,5,0,29,0,29,0,3,2,0,2,17,2,0,0,7,0,15,22,16,0,0,168,38,415,81,22.908992111061583,0.8693422617976987,0,,sklearn/metrics/tests/test_unsupervised.py|sklearn/metrics/tests/test_unsupervised.py|sklearn/metrics/unsupervised.py|sklearn/metrics/tests/test_unsupervised.py|sklearn/metrics/unsupervised.py,0,0.0,0,3,false,Silhouette Coefficient Implementation of the Silhouette Coefficient [1]As this metric is unsupervised I put it in its own module in metrics/ Its arguments are a distance matrix of the pairwise distances and the labels array labels is the output from a cluster algorithm such as the output from kmeanspredictSparse matrices are supported so long as numpy supports them (uses npmean and npmaximum)[1] Peter J Rousseeuw (1987) Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis Computational and Applied Mathematics 20: 53–65 doi:101016/0377-0427(87)90125-7# TODO before merging- confirm the final module location and naming conventions- check narrative documentation,,227,0.8590308370044053,0.304097771387491,18663,297.80849809784064,31.13111504045437,87.76723999357017,1120,26,405,20,unknown,robertlayton,robertlayton,true,robertlayton,5,1.0,4,5,129,true,true,false,false,13,80,5,0,53,2,14
625029,scikit-learn/scikit-learn,python,360,1316722308,1316737839,1316737839,258,258,github,false,false,false,21,1,1,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.284697857910199,0.16259339405646403,15,vlad@vene.ro,doc/install.rst,15,0.010932944606413994,0,1,false,Corrected Easy Install section of documentation When searching or installing scikit-learn with pip or easy_install the right package name is scikit-learn,,226,0.8584070796460177,0.29956268221574345,18466,299.1985270226362,31.35492256038124,88.1620275100184,1114,25,401,18,unknown,cmd-ntrf,ogrisel,false,ogrisel,0,0,4,1,85,false,true,false,false,0,0,0,0,0,0,-1
625030,scikit-learn/scikit-learn,python,357,1316630269,1317049705,1317049705,6990,6990,github,false,false,false,60,2,1,0,6,0,6,0,3,0,0,10,11,9,0,0,0,0,11,11,9,0,0,419,6,419,6,45.123615339705296,1.712341520829018,115,vlad@vene.ro,doc/modules/linear_model.rst|sklearn/decomposition/dict_learning.py|sklearn/linear_model/base.py|sklearn/linear_model/bayes.py|sklearn/linear_model/coordinate_descent.py|sklearn/linear_model/least_angle.py|sklearn/linear_model/omp.py|sklearn/linear_model/ridge.py|sklearn/utils/__init__.py|sklearn/utils/tests/test___init__.py,84,0.01312910284463895,1,0,false,rename overwrite_Foo params to copy_Foo (and inversed their meaning) As promised heres a patch to change all overwrite_ params to copy_ for consistency with older sklearn code that favored copy It also resolves the overwrite_Gram (least_angle) vs overwrite_gram (@venes code) issue in favor of copy_Gram I found that most consistent with copy_Xy etc All tests including doctests seem to pass,,225,0.8577777777777778,0.29686360320933625,18466,299.1985270226362,31.35492256038124,88.1620275100184,1110,25,400,18,unknown,larsmans,larsmans,true,larsmans,21,0.6666666666666666,43,25,430,true,true,false,false,41,102,18,0,76,8,5517
625031,scikit-learn/scikit-learn,python,354,1316537071,1316972646,1316972646,7259,7259,github,false,false,false,10,8,0,0,5,0,5,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,39,116,0,0.0,0,,,0,0.0,0,0,false,added hopefully more intelligible error messages Addresses issue 344:https://githubcom/scikit-learn/scikit-learn/issues/344,,224,0.8571428571428571,0.2944693572496263,18473,298.8144860066042,31.288908136198778,88.12862014832459,1108,24,399,18,unknown,amueller,mblondel,false,mblondel,9,1.0,179,21,333,false,true,true,false,4,5,1,0,0,0,4
625032,scikit-learn/scikit-learn,python,353,1316473164,1316599256,1316599256,2101,2101,github,false,false,false,18,2,2,0,4,0,4,0,3,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,16,4,16,8.497484724544526,0.32245936866281927,14,vlad@vene.ro,sklearn/linear_model/base.py|sklearn/linear_model/tests/test_sgd.py,14,0.010486891385767791,0,3,false,Sgd warm starts Issue 338:https://githubcom/scikit-learn/scikit-learn/issues/338Accepted intercepts of shape () and reshaped to (1)Non-regression test included,,223,0.8565022421524664,0.2898876404494382,18473,298.8144860066042,31.23477507713961,88.0744870892654,1107,24,398,18,unknown,amueller,ogrisel,false,ogrisel,8,1.0,179,21,332,false,true,true,false,2,3,0,0,0,0,3
625033,scikit-learn/scikit-learn,python,352,1316460555,1316532606,1316532606,1200,1200,merged_in_comments,false,false,false,45,1,1,0,2,1,3,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.802123674392625,0.1822291885683611,16,vlad@vene.ro,sklearn/utils/__init__.py,16,0.012021036814425245,1,1,false,Safer assert_all_finite Check for Inf/NaN when Xsum() is not finite This avoids the risk offalse positivs because of sum overflow while remaining performant forthe usual case where all values are finiteThis idea is taken from:   https://githubcom/tecki/numpy/commit/57167f7c02b02bfb49204233eaee0f289ad37c92@larsmans: what do you think ,,222,0.8558558558558559,0.2915101427498122,18473,298.8144860066042,31.23477507713961,88.0744870892654,1107,24,398,17,unknown,fabianp,larsmans,false,larsmans,16,0.875,71,20,492,true,true,false,false,51,71,24,0,101,0,57
625034,scikit-learn/scikit-learn,python,350,1315930465,1316021764,1316021764,1521,1521,github,false,false,false,86,6,1,0,2,0,2,0,1,0,0,1,2,1,0,0,0,0,2,2,2,0,0,36,0,76,0,4.40796029797911,0.17132720009028857,16,virgile.fritsch@gmail.com,scikits/learn/metrics/metrics.py,16,0.01233616037008481,0,1,false,Refactor of metricsroc_curve method The new implementation even if it looks very naive reduces thecomputation time of fpr/tpr vectorsroc_curve computation time depends now only on the length of y_scoreFor comparison here are the results between the old and the newimplementation for the following vectors:- 10^6 length vector  (y_score has 1000 unique values):    - old impl: 2829 seconds    - new impl: 314 seconds- 10^6 length vector (y_score has 10000 unique values):    - old impl: 26761 seconds    - new impl: 364 seconds,,221,0.8552036199095022,0.27062451811873556,17464,300.10306917086575,31.5506184150252,88.5822262940907,1098,23,392,19,unknown,ohe,agramfort,false,agramfort,0,0,12,20,770,false,false,false,true,0,0,0,0,0,0,52
348151,scikit-learn/scikit-learn,python,348,1315787825,,1346517678,512164,,unknown,false,false,false,73,7,0,0,15,0,15,0,6,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,123,40,0,0.0,0,,,0,0.0,0,10,false,min_df text feature_extraction comit: added min_df to control the minimum document frequency a term must have to be a part of the vocabulary in the text feature extractionThis is a complement to the max_df param in the text feature extraction Parameter min_df may be used to restrict terms that only appear in one or two documents By default min_df is zero and does not restrict termsAlso provided the unit test test_vectorizer_min_df(),,220,0.8590909090909091,0.26336477987421386,19329,296.23881214755033,31.093176056702365,88.20942625071136,1098,22,390,133,unknown,diogojc,amueller,false,,0,0,4,2,177,false,false,false,false,0,0,0,0,0,0,90
625035,scikit-learn/scikit-learn,python,347,1315579507,1315924039,1315924039,5742,5742,github,false,false,false,143,24,1,0,14,0,14,0,5,0,0,2,12,2,0,0,2,0,12,14,11,0,0,157,60,1454,93,9.353096735636303,0.36422260643991194,7,vanderplas@astro.washington.edu,sklearn/metrics/cluster.py|sklearn/metrics/tests/test_cluster.py,7,0.0052750565184626974,0,3,true,Adjusted Rand Index for clustering evaluation This is an early pull request to let you know that I have a working implementation of Hubert & Arabie 1985 clustering similarity measure ARI which is adjusted for chance (while V-Measure as currently implemented in sklearn is not)The implementation is based on pythonification of the Wikipedia article hence quite naive As usually not performance critical I prefer to keep the code readable for correctness verification rather than optimized but obfuscated## TODO before merge- delcheck for possible big-integer switch in intermediate results of combinations that might penalize the perf badly: if so we might want to switch to a stirling approx/del- delupdate the clustering doc chapter/del- deluse ARI in the clustering examples/del- delcheck values against another independent implementation/del- delwrite a new example that demonstrates the concept of adjustment for chance/del,,219,0.8584474885844748,0.25169555388093445,17205,303.5164196454519,31.676838128451035,89.45074106364429,1093,22,388,19,unknown,ogrisel,ogrisel,true,ogrisel,13,0.7692307692307693,344,106,835,true,false,false,false,91,285,31,1,122,6,3
626083,scikit-learn/scikit-learn,python,346,1315438503,1315438960,1315438960,7,7,github,false,false,false,12,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,Fix typo in SGDClassifiers docstring (via GitHub) -learning_rate: int+learning_rate: strHTH,,218,0.8577981651376146,0.2537878787878788,17194,303.71059671978594,31.697103640804933,89.5079678957776,1089,21,386,19,unknown,npinto,agramfort,false,agramfort,1,1.0,41,37,993,false,true,true,true,1,0,0,0,0,0,-1
625037,scikit-learn/scikit-learn,python,343,1315231691,1315316958,1315316958,1421,1421,github,false,false,false,150,2,0,0,7,0,7,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,16,0,0,0.0,0,,,0,0.0,0,1,false,Bug: euclidean distances  On my machine (32-bits) sklearnclusterteststest_k_meanstest_transform failsFAIL: sklearnclusterteststest_k_meanstest_transform----------------------------------------------------------------------Traceback (most recent call last):  File /usr/local/lib/python27/dist-packages/nose-100-py27egg/nose/casepy line 187 in runTest    selftest(*selfarg)  File /home/gilles/Sources/scikit-learn/sklearn/cluster/tests/test_k_meanspy line 280 in test_transform    assert_equal(X_new[c c] 0)  File /usr/lib/pymodules/python27/numpy/testing/utilspy line 313 in assert_equal    raise AssertionError(msg)AssertionError: Items are not equal: ACTUAL: 2384185791015625e-07 DESIRED: 0I tracked down the bug and it actually comes from the euclidean_distances metric Computing (x-y)^2 as x*x - 2*x*y + y*y seems to introduce rounding errors due to floating point arithmetic In my opinion it would be better if (x-y)^2 was actually computed as (x-y) and then squared In all cases if x  y then x-y would cancel itself and no rounding error would occur For now I have simply added if X is Y: npfill_diagonal(distances 00) which resolves the test issue but imo the euclidean metric should be rewritten properly from scratch What is your opinion,,217,0.8571428571428571,0.24981188863807374,17079,300.89583699279814,31.500673341530533,88.88108203056386,1086,22,384,20,unknown,glouppe,glouppe,true,glouppe,10,1.0,45,13,298,true,true,false,false,17,30,10,0,37,2,10
625038,scikit-learn/scikit-learn,python,341,1315166302,1315304719,1315304719,2306,2306,github,false,false,false,39,8,3,0,10,0,10,0,3,0,0,5,6,2,0,0,0,0,6,6,3,0,0,15,70,78,70,20.735017911705576,0.8157658402584448,12,vanderplas@astro.washington.edu,doc/model_selection.rst|doc/modules/cross_validation.rst|doc/modules/grid_search.rst|sklearn/cross_val.py|sklearn/tests/test_cross_val.py,10,0.0045627376425855515,0,3,false,Cross validation improvements Various improvements related to cross validation tools- reworked the cross_val_score function to better interoperate with the metrics module- more tests- add missing documentation for ShuffleSplit- add missing documentation for the cross_val_score function,,216,0.8564814814814815,0.25019011406844105,17079,300.89583699279814,31.500673341530533,88.88108203056386,1083,22,383,21,unknown,ogrisel,ogrisel,true,ogrisel,12,0.75,344,106,830,true,false,false,false,94,265,32,1,107,5,14
625039,scikit-learn/scikit-learn,python,340,1315068710,1315957304,1315957304,14809,14809,github,false,false,false,400,4,0,0,55,0,55,0,7,0,0,0,5,0,0,0,0,0,5,5,4,0,0,0,0,28,124,0,0.0,0,,,0,0.0,1,12,false,Refactor Neighbors module This attempts to address the changes to the neighbors module which were recently discussed on the mailing list (see discussion thread a hrefhttp://sourceforgenet/mailarchive/forumphpthread_nameCAC8MkjzwrHdZb5J1b8fiw5kE%2BBFPqVmzmjfr%3D%3DLB8VFTCBpy1Q%40mailgmailcom&forum_namescikit-learn-generalhere/a)The basic idea is this: Ive written a NearestNeighbors class which implements all unsupervised neighbors-based learning methods  It provides a uniform interface for brute force BallTree and cKDTree  Ive made NeighborsClassifier and NeighborsRegressor inherit from NearestNeighbors  I think this is the correct thing to do here it meets the LSP because both of these classes should be valid inputs into functions like kneighbors_graph where only the unsupervised functionality is usedIve made kneighbors_graph and radius_neighbors_graph into methods of NearestNeighbors and kept the old stand-alone versions as convenience functionsThe new version retains nearly complete backward compatibility with the old version (aside from the order of keyword arguments in some cases)  The test-suite passed with no modificationA few issues to think about:- Ive included cKDTree as an option in NearestNeighbors  This adds flexibility for free but breaks picklability  I did this mainly to ease the task of comparing execution times for the three neighbors search methods  If picklability is a concern we could still keep cKDTree and write __getstate__ and __setstate__ such that the KD-tree is rebuilt on deserialization- For completeness Id need to implement a new version of barycenter_weights which works in cases where k is different for each neighborhood (ie in radius_neighbors_graph)  @fabianp has mentioned in the past that barycenter_weights may be removed at some point  This would be a great time to make that decision- Im not completely happy with the classification_type keyword  It could be misleading: for example if someone creates a classifier using NeighborsClassifier(r_neighbors05) any prediction will still be performed using kneighbors() with the default n_neighbors5 not radius_neighbors() as the user may naively expect  We could try to anticipate this by checking which keywords are passed to the constructor but this could lead to other sources of confusion- The methodauto decision process should be more sophisticated as mentioned in issue #195  Im writing a profiler suite which will help address this  With the new cython version of BallTree in place initial tests make it look like methodauto will simply end up choosing BallTree in all casesIm waiting on peoples input before I fully update tests and documentation and comb the code-base to make sure the appropriate neighbors object is being used in all cases,,215,0.8558139534883721,0.2399403874813711,17792,304.57508992805754,32.14928057553957,90.88354316546763,1080,22,382,21,unknown,jakevdp,ogrisel,false,ogrisel,9,1.0,355,0,115,true,true,false,false,16,53,7,0,43,8,154
625040,scikit-learn/scikit-learn,python,335,1314964192,1315037996,1315037996,1230,1230,github,false,false,false,28,3,0,0,2,0,2,0,2,0,0,0,113,0,0,0,255,0,113,368,409,0,15,0,0,681,262,0,0.0,0,,,0,0.0,0,2,true,move sources from scikits/learn to sklearn Had to be done at some point I added a compatibility layer for some modules we can add more as needed,,214,0.8551401869158879,0.23646071700991608,17059,299.95896594173166,31.36174453367724,87.51978427809368,1077,22,381,19,unknown,fabianp,fabianp,true,fabianp,15,0.8666666666666667,69,20,475,true,true,false,false,46,60,21,0,65,0,1
625041,scikit-learn/scikit-learn,python,334,1314962658,1314963397,1314963397,12,12,github,false,false,false,28,2,0,0,1,0,1,0,2,0,0,0,112,0,0,0,255,0,112,367,408,0,15,0,0,677,262,0,0.0,0,,,0,0.0,0,0,true,move sources from scikits/learn to sklearn Had to be done at some point I added a compatibility layer for some modules we can add more as needed ,,213,0.8544600938967136,0.23587786259541985,17059,299.95896594173166,31.36174453367724,87.51978427809368,1077,22,381,19,unknown,fabianp,fabianp,true,fabianp,14,0.8571428571428571,69,20,475,true,true,false,false,45,59,18,0,64,0,3
625042,scikit-learn/scikit-learn,python,331,1314902107,1314912666,1314912666,175,175,github,false,false,false,37,1,0,0,8,0,8,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,15,0,0,0.0,0,,,0,0.0,1,1,false,Removed parameter iid in cross_val_score and _cross_val_score It was useless and tricky as there was no way to get the number of samples (by which was multiplied the score)@agramfort : could you please review this ,,212,0.8537735849056604,0.23000761614623,17064,300.9259259259259,31.3525550867323,87.49413970932957,1076,22,380,19,unknown,JeanKossaifi,GaelVaroquaux,false,GaelVaroquaux,6,1.0,2,7,87,true,true,true,false,6,2,6,0,10,0,54
625043,scikit-learn/scikit-learn,python,328,1314803186,1314875522,1314875522,1205,1205,github,false,false,false,66,4,0,0,8,0,8,0,5,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,48,0,0,0.0,0,,,0,0.0,0,0,false,Usability change in crossval I came across a failed assertion while using crossvalStratifiedKFold: over my dataset of 100 labels 2 of these classes had fewer instances than the k-fold validation I was hoping to do  It took me a good while to figure out that this was the problem This minor change is to raise more useful error messages and save others the time spent deciphering,,211,0.8530805687203792,0.2305319969159599,17061,300.9788406306781,31.35806810855167,87.50952464685541,1075,22,379,20,unknown,bdholt1,ogrisel,false,ogrisel,4,1.0,3,14,37,true,true,true,false,10,37,4,0,43,0,6
625044,scikit-learn/scikit-learn,python,327,1314727267,1314877994,1314877994,2512,2512,merged_in_comments,false,false,false,36,2,2,0,7,0,7,0,4,2,0,6,8,6,0,2,2,0,6,8,6,0,2,80,6,80,6,27.647995909995643,1.1231101288344094,12,vlad@vene.ro,doc/logos/scikit-learn-logo-thumb.png|doc/sphinxext/gen_rst.py|examples/covariance/plot_covariance_estimation.py|examples/manifold/plot_lle_digits.py.prof|examples/plot_precision_recall.py|examples/plot_roc.py|examples/plot_train_error_vs_test_error.py|examples/plot_pls.py,9,0.0,0,0,false,generate thumbnails for the example gallery Here is an screenshot on how the gallery looks now:[Screenshot](https://lh5googleusercontentcom/-sT3t3rMgUr0/Tl0IIljdxYI/AAAAAAAAAII/KbQJFrB8q5A/s640/Screenshot-Examples%252520%2525E2%252580%252594%252520scikit-learn%252520v09-git%252520documentation%252520-%252520Google%252520Chromepng)For the images that dont generate a plot (or are broken) I just put a nice skl logo :-),,210,0.8523809523809524,0.23551829268292682,16996,302.01223817368793,31.477994822311132,87.84419863497294,1074,22,378,19,unknown,fabianp,fabianp,true,fabianp,13,0.8461538461538461,68,20,472,true,true,false,false,41,52,17,0,64,0,1
625045,scikit-learn/scikit-learn,python,326,1314723055,1314726520,1314726520,57,57,github,false,false,false,40,1,0,0,1,0,1,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0.0,0,,,0,0.0,0,0,false,Fix/svm Fixed https://githubcom/scikit-learn/scikit-learn/issues/217Valgrind on linux does a much better job than anything (including DUMA) on windows:$ valgrind python mem_bugpyAt present there are still memory leaks but they are in python itself and not in the svm module,,209,0.8516746411483254,0.23551829268292682,16996,302.01223817368793,31.477994822311132,87.84419863497294,1074,22,378,19,unknown,bdholt1,fabianp,false,fabianp,3,1.0,3,14,36,true,true,true,false,9,34,3,0,41,0,57
625046,scikit-learn/scikit-learn,python,320,1314195832,1314681650,1314681650,8096,8096,github,false,false,false,147,26,8,0,21,0,21,0,5,4,0,5,15,6,0,0,6,2,9,17,8,0,0,421,131,676,149,68.43542502699748,2.795867117505161,71,vlad@vene.ro,scikits/learn/setup.py|scikits/learn/meta/multiclass.py|scikits/learn/meta/tests/test_multiclass.py|scikits/learn/meta/__init__.py|scikits/learn/meta/multiclass.py|scikits/learn/meta/tests/test_multiclass.py|scikits/learn/meta/__init__.py|scikits/learn/meta/multiclass.py|scikits/learn/meta/tests/test_multiclass.py|scikits/learn/meta/multiclass.py|scikits/learn/meta/tests/test_multiclass.py|scikits/learn/preprocessing/__init__.py|scikits/learn/meta/__init__.py|scikits/learn/meta/multiclass.py|scikits/learn/meta/tests/__init__.py|scikits/learn/meta/tests/test_multiclass.py,58,0.0,0,5,false,Meta-classifiers for multiclass problems This PR implements three meta-estimators for multiclass problems:* one-vs-the-rest* one-vs-one* error-correcting output codesI went for one-vs-the-rest because as a name its strictly speaking more correct than one-vs-all but Im ok to change that if more people prefer one-vs-allI introduced a new module called meta Well be able to put bagging and boosting there tooIm pretty happy with how the API turned out Like in the preprocessing module I used both classes and functions Classes are for users and functions are fore internal usage (todo: rewrite SGDClassifier with these functions) or power-users Look at unit tests for example of how grid search workTo be done before or after merge:* multi-label support in one-vs-rest (use sparse matrix for the indicator matrix Y)* n_jobs support (should be easy as I decomposed everything in functions and list comprehensions),,208,0.8509615384615384,0.22300469483568075,16827,300.82605336661317,31.25928567183693,87.7161704403637,1065,22,372,22,unknown,mblondel,mblondel,true,mblondel,5,1.0,83,22,512,true,false,false,false,59,141,24,2,76,2,86
625047,scikit-learn/scikit-learn,python,319,1314190787,1314202294,1314202294,191,191,github,false,false,false,8,1,0,0,0,0,0,0,0,0,0,0,19,0,0,0,0,0,19,19,19,0,0,0,0,186,0,0,0.0,0,,,0,0.0,0,0,false,Sorting parameters in BaseEstimtor__repr__ Also modified the doctests,,207,0.8502415458937198,0.22362204724409449,16689,300.91677152615495,30.918569117382706,86.94349571574091,1065,22,372,23,unknown,JeanKossaifi,ogrisel,false,ogrisel,5,1.0,2,7,79,true,true,false,false,5,2,5,0,6,0,-1
625048,scikit-learn/scikit-learn,python,318,1314184347,1314185663,1314185663,21,21,github,false,false,false,6,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,14,0,0,0.0,0,,,0,0.0,0,0,false,FIX in least_anglepy doctests Using ELLIPSIS,,206,0.8495145631067961,0.2222222222222222,16691,300.8807141573303,30.914864298124737,86.93307770654845,1065,22,372,23,unknown,JeanKossaifi,GaelVaroquaux,false,GaelVaroquaux,4,1.0,2,7,79,true,true,true,false,4,2,4,0,6,0,-1
625049,scikit-learn/scikit-learn,python,317,1314134132,1314179712,1314179712,759,759,github,false,false,false,18,24,20,0,2,0,2,0,2,1,0,10,14,10,0,0,1,0,13,14,13,0,0,629,91,847,122,159.88825700298716,6.636212770928605,169,vlad@vene.ro,scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py|scikits/learn/utils/__init__.py|scikits/learn/utils/tests/test___init__.py|scikits/learn/utils/__init__.py|scikits/learn/utils/tests/test___init__.py|scikits/learn/utils/tests/test___init__.py|scikits/learn/utils/__init__.py|scikits/learn/utils/tests/test___init__.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/ridge.py|scikits/learn/utils/__init__.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/sparse/coordinate_descent.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/base.py,67,0.02199528672427337,0,1,false,Normalize data and refactor of coordinate_descentpy its my refactoring on top of Jeans PR 307feed back welcome,,205,0.848780487804878,0.22623723487824038,16621,301.3657421334456,30.984898622224897,87.05854040069791,1063,22,371,24,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,13,0.8461538461538461,56,149,629,true,true,true,false,67,110,16,0,41,10,165
625050,scikit-learn/scikit-learn,python,316,1314039972,1314189431,1314189431,2490,2490,github,false,false,false,312,6,4,0,8,0,8,0,3,1,3,4,8,7,0,0,1,3,4,8,7,0,0,2240,0,2458,0,27.500354689493747,1.1413541551161501,42,vlad@vene.ro,scikits/learn/neighbors.py|scikits/learn/src/ball_tree.c|scikits/learn/src/ball_tree.pyx|scikits/learn/src/ball_tree.c|scikits/learn/neighbors.py|scikits/learn/setup.py|scikits/learn/src/BallTree.h|scikits/learn/src/BallTreePoint.h|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx,24,0.0096,0,1,false,Cython ball tree This is a complete re-write of BallTree as recently discussed on the mailing listMain advantages: - All ball tree information is stored in numpy arrays rather than in dynamically allocated C arrays  This allows a constructed BallTree to be pickled and unpickled without the need to rebuild the tree - Multiple distance metrics are supported  Currently it can efficiently use any minkowski p-distance (similar to scipyspatialcKDTree)  Because the distance functions are written in cython theres potential to more easily add support for other metrics - Compared to the current C++ BallTree implementation the new code is faster by a factor of 5-8 for building the tree and about 30-50% for querying the tree depending on the type of input dataDisadvantages: - The code is not as easy to understand as the c++ implementation  A class-based approach is much more intuitive  Pseudo-code for a class-basedimplementation is included in the implementation notes - The code uses mostly raw pointers because numpy array-slicing led to too much call overhead  This makes the code more difficult to modify and I couldnt use numpy routines for things like searching and sorting arrays - I wrote fast cython routines for a few of these basic algorithmsBecause theyre purpose-built for this implementation they perform well - Allocating all memory before building the tree leads to less flexibility  The old C++ implementation would easily allow extensions to different construction methods inline data addition and subtraction etc  Adding these sorts of extensions to the new module would be much more difficult - delMemory allocation is not exact/del (fixed in commit below)As noted on the mailing list I think the advantages far outweigh the disadvantages  Currently all tests pass: I think its ready to go barring any further input  For quick comparison of execution times between this implementation and the old implementation see http://githubcom/jakevdp/pyTree,,204,0.8480392156862745,0.2296,16624,301.79258902791145,30.979307025986525,87.64436958614053,1061,22,370,25,unknown,jakevdp,fabianp,false,fabianp,8,1.0,346,0,103,true,true,false,false,15,41,7,0,39,8,82
626092,scikit-learn/scikit-learn,python,315,1314023541,1314099025,1314099025,1258,1258,merged_in_comments,false,false,false,46,1,1,0,8,0,8,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.387230451999623,0.18208433172517952,17,vlad@vene.ro,scikits/learn/svm/classes.py,17,0.013578274760383386,0,1,false,FIX: Corrected NuSVR impl type and set epsilon to None The implementation type for NuSVR was passing through epsilon_svr to the base class instead of nu_svr  Also for NuSVR epsilon is not used (although I realise that it is passed into libsvms params but not used),,203,0.8472906403940886,0.2292332268370607,16624,301.79258902791145,30.979307025986525,87.64436958614053,1060,22,370,25,unknown,TimSC,,false,,0,0,3,2,0,false,false,false,false,0,0,0,0,0,0,39
625052,scikit-learn/scikit-learn,python,314,1313865124,1313873407,1313873407,138,138,github,false,false,false,37,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,4.708464124060705,0.19541778087934863,1,pietro.berkes@googlemail.com,examples/cluster/plot_segmentation_toy.py,1,0.0008019246190858059,0,0,false,In spectral clustering example forced the solver to be arpack (the amg solver is unstable for this example and results are notsatisfying)(The example is broken on the website the website will need to be refreshed) ,,202,0.8465346534653465,0.2301523656776263,16623,301.81074414967213,30.981170667147925,87.64964206220297,1059,22,368,24,unknown,emmanuelle,GaelVaroquaux,false,GaelVaroquaux,1,0.0,19,5,474,false,true,false,false,0,0,0,0,0,0,-1
625053,scikit-learn/scikit-learn,python,313,1313398637,1316261438,1316261438,47713,47713,github,false,false,false,235,47,2,0,55,0,55,0,6,0,0,2,14,2,0,0,3,1,13,17,13,0,0,172,0,1342,421,9.192908695169143,0.3616713001197963,64,vlad@vene.ro,scikits/learn/metrics/__init__.py|scikits/learn/metrics/pairwise.py,58,0.04370761115297664,0,16,false,Pairwise distance A number of consistency changes and improvements to the pairwise_distance function and the pairwisepy moduleThe main point of this PR is to centralise the calculation of the pairwise distances in a matrix and to allow YNone as an option to the metrics As an example see pdist and cdist in scipyspatialdistanceThis is a work in progress as its probably going to take quite a bit of back and forth At the moment Im looking for comments on:* The new pairwise_distance function which uses a dictionary to map strings to functions of the form metric_name(X YNone)* The use of YNone in metrics/kernels* Whether there should be a pairwise_kernel function of the same form as pairwise_distance but for kernels instead* The check_set_Y function in pairwisepy Is this appropriate/complete/inefficient* Branching of pairwise_distance depending on whether X or Y is sparse ie what to do if correlation given as metric considering that scipyspatialdistance does not take spare matrices One would expect this would error but is there a way around this or a fail-fast option* Implementing a pairwise_p_distance function which accepts a Minkowski metric p such that for vectors x and y dist(xy)  sum(abs(x - y) ** p) ** (1 / p)The big weakness for me is spare matrices I have not used them before in any non-trivial sense so I am not aware of normal pitfalls/compatability/etc,,201,0.845771144278607,0.24114544084400905,17079,300.89583699279814,31.500673341530533,88.88108203056386,1050,24,363,24,unknown,robertlayton,ogrisel,false,ogrisel,4,1.0,3,5,87,true,true,true,false,9,39,4,0,39,2,627
625055,scikit-learn/scikit-learn,python,311,1313081040,1313248404,1313248404,2789,2789,github,false,false,false,50,4,2,0,2,0,2,0,2,0,0,6,6,6,0,0,0,0,6,6,6,0,0,20,71,24,71,30.929860713566214,1.2837952540878101,115,vlad@vene.ro,scikits/learn/datasets/tests/test_samples_generator.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/tests/test_text.py|scikits/learn/feature_extraction/text.py|scikits/learn/datasets/samples_generator.py|scikits/learn/datasets/tests/test_samples_generator.py,46,0.020209580838323353,0,0,false,Improved test coverage This pull request includes the following list of changes:- Improved test coverage of datasetssamples_generator (- 100%)- Improved test coverage of features_extractionimage (- 100%)- Fixed a minor bug in features_extractionimage- Improved test coverage of features_extractiontext (- 99%) The only left uncovered line concerns RomanPreprocessor::__repr__,,200,0.845,0.2402694610778443,16422,300.14614541468757,31.055900621118013,87.32188527584947,1046,28,359,27,unknown,glouppe,ogrisel,false,ogrisel,9,1.0,40,12,273,true,true,true,false,15,22,9,0,26,2,7
625056,scikit-learn/scikit-learn,python,310,1313071646,1317949815,1317949815,81302,81302,github,false,false,false,299,1,0,0,238,0,238,0,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,1,123,false,Enh/tree (performance optimised) As @satra commented in https://githubcom/scikit-learn/scikit-learn/pull/288#issuecomment-1691949 here is a separate PR for a decision tree  This version is significantly faster than the alternatives (Orange and MILK) Furthermore Orange and scikitslearn support multiclass classification and regression Milk only supports classification  We would now welcome any comments with the aim of gaining acceptance for a merge into master Performance and scores-----------------------------------$python bench_treepy madelonTree benchmarksLoading data Done 2000 samples with 500 features loaded into memoryscikitslearn (initial): mean 8423 std 062Score: 076scikitslearn (now): mean 065 std 000Score: 076milk: mean 11531 std 157Score: 075Orange: mean 2582 std 002Score: 050$python bench_treepy arceneTree benchmarksLoading data Done 100 samples with 10000 features loaded into memoryscikitslearn (initial): mean 4095 std 044Score: 060scikitslearn (now): mean 020 std 000Score: 060milk: mean 7100 std 060Score: 060Orange: mean 1078 std 020Score: 051TODO before merge------------------------------- delincrease test coverage to over 95% /del- delfinish the documentation (fix broken example and plot links add practical usage tips)/del- deldemonstrate how to use a graphviz output in an example/del- delinclude a static grapvhiz output for the iris and boston datasets in the documentation/del- deladd feature_names to GraphvizExporter/del- delextract the graphviz exporter code out of the tree classes (use visitor pattern) assign node numbers (not mem addresses)/del- dels/dimension/feature/g /del- deladd a test for the pickling of a fitted tree/del- delcythonise prediction/del- delexplain in the documentation and in the docstrings how these classes relate to ID3 C45 and CART/delFuture enhancements-------------------------------- ability to provide instance weights (for boosting DTs)- support a loss matrix (ala Rs rpart) - support multivariate regression (ala Rs mvpart)- support Randomized Trees,,199,0.8442211055276382,0.2406576980568012,19394,292.1522120243374,30.731153965143857,86.93410333092709,1046,28,359,30,unknown,bdholt1,GaelVaroquaux,false,GaelVaroquaux,2,1.0,3,14,17,true,true,true,false,5,13,2,0,19,0,6
625057,scikit-learn/scikit-learn,python,309,1313057741,1313063986,1313063986,104,104,commits_in_master,false,false,false,7,9,9,0,3,5,8,5,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,356,74,356,74,43.32369473757393,1.7982541699198666,48,vlad@vene.ro,scikits/learn/datasets/samples_generator.py|scikits/learn/datasets/tests/test_samples_generator.py|scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/tests/test_rfe.py|scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/tests/test_rfe.py|scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/rfe.py,45,0.008988764044943821,0,0,false,Improved test coverage of samples_generator to 100% ,,198,0.8434343434343434,0.24194756554307117,16422,299.9634636463281,31.055900621118013,87.32188527584947,1045,28,359,26,unknown,glouppe,glouppe,true,glouppe,8,1.0,40,12,273,true,true,false,false,14,17,8,0,22,2,4
625058,scikit-learn/scikit-learn,python,308,1313006719,1313134567,1313134567,2130,2130,merged_in_comments,false,false,false,48,9,3,0,10,0,10,0,2,0,0,3,4,3,0,0,0,0,4,4,4,0,0,193,0,362,17,17.386837365697733,0.7201954745961007,68,vlad@vene.ro,scikits/learn/manifold/locally_linear.py|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|scikits/learn/manifold/locally_linear.py,43,0.02404207362885049,0,2,false,Cleanup/Speedup of LocallyLinearEmbedding I cleaned up the LocallyLinearEmbedding class: it now conforms to the standard of defining all parameters in __init__ rather than fit  I also implemented fit_transform and made some small improvements to the Hessian LTSA and MLLE algorithms that should speed up execution on high-dimensional data,,197,0.8426395939086294,0.24267468069120962,16378,300.5861521553303,31.017218219562828,87.31224813774575,1045,28,358,28,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,7,1.0,338,0,91,true,true,false,false,15,46,7,0,39,8,265
625059,scikit-learn/scikit-learn,python,307,1312997055,1314134144,1314134144,18951,18951,commits_in_master,false,false,false,69,25,11,0,7,0,7,0,4,0,0,7,12,7,0,0,1,0,12,13,11,0,0,500,0,678,91,83.59440996385369,3.4694580802341246,148,vlad@vene.ro,scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/sparse/coordinate_descent.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/base.py,78,0.01652892561983471,1,2,false,Added a parameter in LinearModel_center_data to normalize data Added a parameter to normalize data in LinearModel (as a consequence _set_intercept was modified and now also returns the standard deviation of X)Also added a parameter overwrite_X in this very class (default is False as most of the time we want to get a copy)@vene : can you please review omppy as there were merge conflicts with this file,,196,0.8418367346938775,0.24192336589030805,16623,301.81074414967213,30.981170667147925,87.64964206220297,1045,28,358,29,unknown,JeanKossaifi,agramfort,false,agramfort,3,1.0,2,7,65,true,true,false,false,3,0,3,0,2,0,300
625060,scikit-learn/scikit-learn,python,306,1312989672,1314114026,1314114026,18739,18739,commit_sha_in_comments,false,false,false,147,3,1,0,19,6,25,0,7,0,0,13,14,12,0,0,0,0,14,14,13,0,0,48,33,58,77,57.547199901751455,2.3837131545663004,167,vlad@vene.ro,doc/developers/index.rst|examples/svm/plot_svm_anova.py|scikits/learn/base.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/tests/test_dbscan.py|scikits/learn/feature_selection/univariate_selection.py|scikits/learn/grid_search.py|scikits/learn/tests/test_base.py|scikits/learn/tests/test_cross_val.py|scikits/learn/tests/test_grid_search.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/tests/test_pipeline.py,63,0.010494752623688156,1,8,false,ENH set_params method on BaseEstimator deprecate estimator params to fit As proposed on the mailing list heres a patch that introduces a public set_params method on all built-in estimators The rationale is summarized in the c94ac99f08505073f7c46b004c202c1a54e897a0s commit message Passing data-independent parameters to fit partial_fit or fit_transform is now deprecated set_params returns self so it can be chainedSome more remarks: estimators behave inconsistently in that some do _set_params first then input validation while other do both steps in reverse order Ive removed the params keyword from MiniBatchKMeans fit and partial_fit since that hasnt featured in any release yet (partial_fit accepted a params keyword arg but then ignored it btw)KMeans got a k parameter to fit The remaining problem is DBSCAN where could I not decide whether the metric eps and min_samples parameters should be given to __init__ or fit Maybe @robertlayton has an opinion on that,,195,0.841025641025641,0.24062968515742128,16378,300.5861521553303,31.017218219562828,87.31224813774575,1045,28,358,28,unknown,larsmans,fabianp,false,fabianp,20,0.65,39,25,388,true,true,false,false,52,137,25,0,206,6,4
625061,scikit-learn/scikit-learn,python,305,1312972618,1312983621,1312983621,183,183,github,false,false,false,59,1,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,9,0,0,0.0,0,,,0,0.0,0,3,false,batch k-means: calculate labels and intertia in chunks to prevent memory  While testing the mini-batch k-means on a relatively large dataset I got memory errors in the labeling phase While the clustering was done in small chunks the labeling was not I made a small change to also do the labeling in chunksPlease review and comment (or merge),,194,0.8402061855670103,0.23937360178970918,16381,300.5311031072584,31.011537757157683,87.29625785971552,1045,28,358,24,unknown,vincentschut,pprett,false,pprett,3,1.0,0,0,154,true,true,false,false,2,9,4,0,2,0,94
625062,scikit-learn/scikit-learn,python,304,1312922353,1315172830,1315172830,37507,37507,github,false,false,false,152,25,2,0,18,5,23,0,5,0,0,2,9,2,0,0,0,0,9,9,6,0,0,324,47,618,65,9.361352196838961,0.3683192103946245,4,fabian.pedregosa@inria.fr,scikits/learn/feature_selection/rfe.py|scikits/learn/feature_selection/tests/test_rfe.py,4,0.003003003003003003,0,6,false,Improvements on the RFE module This commit includes the following list of changes:- Documentation has been enhanced and completed- Examples have been added- The percentage (float) parameter has become step (int or float) and indicates the number of features to remove at each iteration (int) or the percentage of features to remove (float) with respect to the original number of features - Exactly n_features_to_select are now always selected It may not always have been the case before as too many features could have been removed at a time in the last step of the elimination- The ranking_ attribute is now a proper ranking of the features (ie best features are ranked 1)- The code of RFECV has been made simpler- The cv argument of RFECVfit has been moved into the constructor and is now passed through check_cv- Added predict and transform methods- TestsCheers,,193,0.8393782383419689,0.2424924924924925,17105,299.15229465068694,31.277404267757962,87.284419760304,1042,28,357,27,unknown,glouppe,ogrisel,false,ogrisel,7,1.0,40,12,271,true,true,true,false,13,17,7,0,17,0,2477
625065,scikit-learn/scikit-learn,python,303,1312889594,1312893029,1312893029,57,57,github,false,false,false,156,3,0,0,3,0,3,0,3,0,0,0,5,0,0,0,0,0,5,5,1,0,0,0,0,120,0,0,0.0,0,,,0,0.0,0,0,false,Small improvements in the documentation of the toy datasets This pull request includes small improvements (cosmits for most of them) in the documentation of the toy datasets It also fixes a broken test in the docstring of load_digits Also while making those changes I noticed that at the end of scikits/learn/datasets/basepy some code was used to enhance the docstring of the load_* functions I noticed two bugs with respect to this1) This code does not seem to be executed during the generation of the online version of the documentation (09-git) For instance in [1] one should see a full description of the Iris dataset[1]: http://scikit-learnsourceforgenet/dev/modules/generated/scikitslearndatasetsload_irishtml#scikitslearndatasetsload_iris2) The previous bug does not happen on my machine However the References sections in the decr/ files is never included in the generated documentation I dont understand why since they are actually in the docstrings of those functions (from scikitslearndatasets import load_iris print load_iris__doc__ indeed includes the references),,192,0.8385416666666666,0.24122479462285287,16293,296.93733505186276,30.197017123918247,86.3561038482784,1042,28,357,24,unknown,glouppe,ogrisel,false,ogrisel,6,1.0,40,12,271,true,true,true,false,11,15,6,0,15,0,5
625066,scikit-learn/scikit-learn,python,302,1312858362,1313317972,1313317972,7660,7660,github,false,false,false,101,7,4,0,10,0,10,0,3,0,0,4,5,2,0,0,0,0,5,5,2,0,0,104,0,104,0,23.807325692783134,0.9881925961722524,129,vlad@vene.ro,doc/modules/classes.rst|doc/modules/manifold.rst|examples/manifold/plot_compare_methods.py|examples/manifold/plot_lle_digits.py|doc/modules/manifold.rst,86,0.02765321375186846,0,2,false,Manifold Documentation Rewrite This pull request goes along with the a hrefhttps://githubcom/scikit-learn/scikit-learn/pull/282Isomap/a pull request that was recently merged  The only changes unique to this pull request are in the examples and doc subdirectories  Ive separated the two because this involves changes to the locally linear embedding documentation and examples as wellThis builds using make html in the doc subdirectory  I used Sphinx v 107: for some reason Sphinx 10 crashes on my system  I get a lot of autodoc cant find/import function  warnings which according to some of the dev docs is a problem with later versions of Sphinx,,191,0.837696335078534,0.2414050822122571,16584,302.52050168837434,31.054027978774723,87.8557645923782,1042,28,356,28,unknown,jakevdp,ogrisel,false,ogrisel,6,1.0,337,0,89,true,true,false,false,14,43,6,0,36,5,505
625067,scikit-learn/scikit-learn,python,301,1312847570,1327186473,1327186473,238981,238981,commits_in_master,false,true,false,87,94,16,0,40,1,41,0,8,10,0,7,32,10,0,0,16,0,22,38,24,0,0,1396,232,3487,498,187.24097482199787,6.460683428231661,53,vlad@vene.ro,doc/modules/label_propagation.rst|doc/unsupervised_learning.rst|examples/semi_supervised/label_propagation_versus_svm_iris.py|examples/semi_supervised/plot_label_propagation_digits.py|examples/semi_supervised/plot_label_propagation_digits_active_learning.py|examples/semi_supervised/plot_label_propagation_structure.py|examples/semi_supervised/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|doc/modules/label_propagation.rst|doc/unsupervised_learning.rst|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|doc/modules/label_propagation.rst|doc/modules/label_propagation.rst|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py|examples/label_propagation/plot_label_propagation_structure.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|scikits/learn/label_propagation.py|examples/label_propagation/plot_label_propagation_versus_svm_iris.py|examples/svm/plot_iris.py|scikits/learn/label_propagation.py|scikits/learn/tests/test_label_propagation.py,32,0.0,0,23,false,WIP : added a module for Label Propagation Label Propagation algorithms are a powerful family of semi-supervised learning algorithms I original wrote a demo for a workshop at the Noisebridge machine learning group but now I think its ready to join the scikitslearn codebaseI implemented two label propagation algorithms label spreading and label propagation including one example that demonstrates performance versus SVM with small number of labels and another showing how the algorithm can learn a complex  structureAlso included a little documentation describing the algorithm,,190,0.8368421052631579,0.24176646706586827,22110,290.09497964721845,28.855721393034823,82.76797829036634,1042,28,356,53,unknown,clayw,clayw,true,clayw,0,0,7,11,833,true,true,false,false,0,0,0,0,15,0,35
625069,scikit-learn/scikit-learn,python,300,1312823364,1314197517,1314197517,22902,22902,commit_sha_in_comments,false,false,false,43,4,2,0,2,4,6,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,22,0,46,34,8.472329415315073,0.3550229859704964,18,vlad@vene.ro,scikits/learn/pipeline.py|scikits/learn/pipeline.py,18,0.01350337584396099,0,0,false,Pipeline fit_params I made this into a pull request so the diff can be easily seenThis is the exact code supplied by Adrien only with some renaming Tests and doctests pass at the moment but I would like to add more tests,,189,0.8359788359788359,0.2423105776444111,16234,290.871011457435,29.93716890476777,86.05396082296416,1042,28,356,28,unknown,vene,fabianp,false,fabianp,12,0.9166666666666666,22,15,484,true,true,false,false,21,37,8,0,292,1,2562
625070,scikit-learn/scikit-learn,python,299,1312809943,1312913285,1312913285,1722,1722,github,false,false,false,6,20,9,0,13,0,13,0,4,0,0,2,6,2,0,0,1,0,6,7,3,0,0,359,154,448,203,53.88266208053012,2.2657936389640794,18,vlad@vene.ro,scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py,16,0.011976047904191617,0,3,false,Patch extraction 2D patch extraction utilities,,188,0.8351063829787234,0.24176646706586827,16234,290.5014167795984,29.875569791794998,85.56116791918195,1042,28,356,25,unknown,vene,ogrisel,false,ogrisel,11,0.9090909090909091,22,15,484,true,true,true,true,20,35,7,0,289,1,128
625071,scikit-learn/scikit-learn,python,298,1312763138,1313011127,1313011127,4133,4133,merged_in_comments,false,false,false,10,4,2,0,14,0,14,0,4,0,0,6,8,5,0,0,0,1,7,8,6,0,0,254,21,409,21,32.23790566284717,1.3556205050751744,143,vlad@vene.ro,doc/modules/classes.rst|examples/linear_model/plot_lasso_bic_aic.py|examples/linear_model/plot_lasso_path_crossval.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py|scikits/learn/linear_model/least_angle.py,80,0.025506376594148537,0,5,false,LassoLars with BIC / AIC as promised here is LassoLarsIC,,187,0.8342245989304813,0.2423105776444111,16234,290.5014167795984,29.875569791794998,85.56116791918195,1042,28,355,28,unknown,agramfort,GaelVaroquaux,false,GaelVaroquaux,12,0.8333333333333334,56,148,613,true,true,false,false,64,100,17,0,47,10,651
625072,scikit-learn/scikit-learn,python,297,1312563016,1314243503,1314243503,28008,28008,github,false,false,false,175,32,9,0,28,3,31,0,6,0,0,3,9,3,0,0,1,2,7,10,9,0,0,698,0,1054,139,44.430375745429956,1.8190303159488992,36,vlad@vene.ro,scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/mean_shift_.py|examples/cluster/plot_mean_shift.py|scikits/learn/cluster/mean_shift_.py|scikits/learn/cluster/mean_shift_.py,28,0.0106951871657754,0,17,true,Increased efficiency and and flexibility added to mean shift clustering I have tried to make the mean shift clustering implementation located in learn/cluster/mean_cluster_py more scalable  I have also tried to improve the initialization by allowing initial kernel positions to be specified as an argument and by providing the function get_bucket_seeds to automatically detect seeds in a scalable mannerIn cases of low dimensionality I have decresed the computational complexity of the algorithm by using cKDTree from scipyspatial to to look up all points within a given euclidean distance of the center of a cluster)I have also removed a bug/problem that was acknowledged in the comments of old mean_shift_implementation which in the case of duplicate clusters biased the cluster centers to be near the later-detected duplicate clusters  Instead if there is a duplicate the position cluster with the most support (points within its bandwidth) is now selectedI was in contact with Alexandre Gramfort one of the authors of the previous mean_shift implementation  Perhaps he would be a good person to look this code over,,186,0.8333333333333334,0.24216959511077157,16776,300.4887935145446,31.29470672389127,87.68478779208392,1034,28,353,28,unknown,conradlee,agramfort,false,agramfort,0,0,3,0,497,true,true,false,true,1,2,0,0,9,0,3
625073,scikit-learn/scikit-learn,python,296,1312550028,1313093126,1313093126,9051,9051,github,false,false,false,23,2,0,0,1,0,1,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,8,0,0.0,0,,,0,0.0,0,0,true,Fixed imports and run unit tests feature_extraction/tests/test_imagepy unit tests were not running  This change is to fix the imports and run the tests,,185,0.8324324324324325,0.24486692015209124,16422,299.9634636463281,31.055900621118013,87.32188527584947,1034,29,353,32,unknown,bdholt1,mblondel,false,mblondel,1,1.0,3,14,11,true,true,true,false,4,3,1,0,1,0,5026
625074,scikit-learn/scikit-learn,python,295,1312546106,1312817707,1312817707,4526,4526,github,false,false,false,25,7,1,0,14,0,14,0,3,2,0,2,8,2,0,1,2,0,6,8,2,0,1,61,0,101,0,17.078432723443218,0.7295310677526714,43,vlad@vene.ro,scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/datasets/data/boston_house_prices.csv|scikits/learn/datasets/descr/boston_house_prices.rst,27,0.016730038022813688,0,2,true,New Dataset: Boston House Prices Here is a dataset thats been around a long time and is often used as an example for regression problems,,184,0.8315217391304348,0.24486692015209124,15671,286.64411971156915,29.289770914427926,82.8281539148746,1034,29,353,25,unknown,bdholt1,agramfort,false,agramfort,0,0,3,14,11,true,true,true,false,3,3,0,0,1,0,198
625075,scikit-learn/scikit-learn,python,293,1312467226,1312736793,1312736793,4492,4492,github,false,false,false,128,13,4,0,13,0,13,0,5,0,0,7,26,7,0,0,0,0,26,26,24,0,0,975,46,1255,344,36.48707069700998,1.5345359150104907,73,vlad@vene.ro,scikits/learn/datasets/__init__.py|scikits/learn/datasets/samples_generator.py|scikits/learn/pipeline.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/tests/test_grid_search.py|scikits/learn/tests/test_hmm.py|scikits/learn/utils/tests/test_svd.py|scikits/learn/datasets/samples_generator.py,37,0.012030075187969926,0,6,false,Complete rewriting of samples_generatorpy This commit include the following changes:- Functions have been renamed to make_* for consistency- Interfaces have been standardized - Documentation has been improved and completed- make_classification: new function replacing test_dataset_classif This fixes issue #240- make_blobs: samples are now balanced between centers- make_friedman1: the output function was incorrect- make_friedman2: new function added- make_friedman3: new function added- make_sparse_correlated: a seed parameter is now used- make_s_curve: the noise parameter is now actually used- make_regression_dataset becomes make_regression and test_dataset_reg is removed to avoid redundancy and confusion- Various cosmits- Tests have been updatedRemark 1: This fixes issue #240Remark 2: I still have to update the examples and the benchmarks __This is a work in progress__,,183,0.8306010928961749,0.2503759398496241,16150,287.9876160990712,29.41176470588235,83.77708978328172,1034,28,352,23,unknown,glouppe,mblondel,false,mblondel,5,1.0,40,12,266,true,true,true,true,8,8,5,0,6,0,9
625076,scikit-learn/scikit-learn,python,292,1312356365,1312448154,1312448154,1529,1529,github,false,false,false,72,4,0,0,14,0,14,0,4,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,60,104,0,0.0,0,,,0,0.0,0,5,false,Add ARPACK support to KernelPCA As discussed in the a hrefhttps://githubcom/scikit-learn/scikit-learn/pull/282Isomap pull request/a Ive added arpack support to KernelPCA to facilitate the Isomap implementation  It leads to a factor of a few speedup for large matrices when only a few eigenvectors are soughtA few of the tests needed to be modified because they asked for more eigenvectors than the rank of the matrix  ARPACK rightly raises an error in this case,,182,0.8296703296703297,0.2480916030534351,15561,285.00739027054817,29.368292526187265,83.02808302808303,1029,29,351,21,unknown,jakevdp,GaelVaroquaux,false,GaelVaroquaux,5,1.0,336,0,84,true,true,false,false,12,31,5,0,30,3,29
625077,scikit-learn/scikit-learn,python,290,1312219784,1312223503,1312223503,61,61,github,false,false,false,64,1,1,0,3,0,3,0,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.48415352854011,0.20991766746209875,1,fabian.pedregosa@inria.fr,doc/support.rst,1,0.0007776049766718507,0,0,false,Irc chan Corrected the IRC chan on the 08 documentation currently the stable one Im not sure Im asking to pull in the correct branchIf someone could make sure the stable documentation at http://scikit-learnsourceforgenet/stable/supporthtmlis updatedPeople are trying to go on the IRC chan to ask for help but dont find it as there is a mistake in the stable documentationThanks,,181,0.8287292817679558,0.24494556765163297,12605,307.8936929789766,27.8460928203094,82.66560888536296,1026,29,349,22,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,2,1.0,19,13,560,true,true,true,false,8,5,1,0,18,0,39
625078,scikit-learn/scikit-learn,python,289,1312205343,1312283297,1312283297,1299,1299,github,false,false,false,39,5,0,0,7,0,7,0,3,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,33,65,0,0.0,0,,,0,0.0,0,1,false,LinearModelCV fix Here is a small fix into the LinearModelCV class The previous version does not pass correctly the fit parameters to the internal path function (eg in test_enet_path the max_iter parameter has no effect in the fit call),,180,0.8277777777777777,0.2459016393442623,15552,285.0437242798354,29.38528806584362,83.01183127572017,1026,28,349,21,unknown,sabba,ogrisel,false,ogrisel,1,1.0,0,0,3,false,true,false,false,2,4,1,0,0,0,41
625080,scikit-learn/scikit-learn,python,288,1312048233,1317949815,1317949815,98359,98359,github,false,false,false,99,64,64,0,11,0,11,0,5,51,15,34,100,57,0,1,51,15,34,100,57,0,1,8832,530,8832,530,835.2763699450395,35.66711545457716,59,vlad@vene.ro,scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/base.py|scikits/learn/setup.py|scikits/learn/tree_model/_tree.cpp|scikits/learn/tree_model/normalise.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/base.py|scikits/learn/tree_model/classifier.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/setup.py|scikits/learn/tree_model/tests/__init__.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/base.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/datasets/base.py|examples/tree_model/plot_decision_tree_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|examples/tree_model/plot_decision_tree_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/bagging/__init__.py|scikits/learn/ensemble/bagging/bagging.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/boosting/__init__.py|scikits/learn/ensemble/boosting/gradboost.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/committee.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|testbdt.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/ensemble/bagging.py|scikits/learn/tree_model/classifier.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|benchmarks/bench_tree.py|scikits/learn/datasets/data/boston_house_prices.csv|scikits/learn/datasets/descr/boston_house_prices.rst|scikits/learn/tree_model/optimisation/__init__.py|scikits/learn/tree_model/optimisation/profile_tree.py|scikits/learn/tree_model/tests/test_randomforest.py|scikits/learn/tree_model/tests/test_tree.py|examples/tree_model/plot_decision_tree_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/base.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|benchmarks/bench_tree.py|scikits/learn/boosting/__init__.py|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/base.py|scikits/learn/decisiontree/__init__.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/src/BDT.cpp|scikits/learn/decisiontree/src/BDT.h|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/bagging/__init__.py|scikits/learn/bagging/bagging.py|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/gradboost.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/decisiontree/src/tmp/BDT.cpp|scikits/learn/decisiontree/src/tmp/BDT.h|scikits/learn/decisiontree/tests/__init__.py|scikits/learn/setup.py|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.c|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/base.py|scikits/learn/bagging/bagging.py|scikits/learn/boosting/__init__.py|scikits/learn/boosting/gradboost.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/bagging/__init__.py|scikits/learn/ensemble/bagging/bagging.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/__init__.py|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/ensemble/boosting/gradboost.py|scikits/learn/ensemble/committee.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/base.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.c|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/setup.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/decisiontree/__init__.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/decisiontree.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|testbdt.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/ensemble/committee.py|testbdt.py|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/tmp/BDT.cpp|scikits/learn/decisiontree/src/tmp/BDT.h|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|testbdt.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/__init__.py,29,0.0,0,4,false,Enh/ensemble the goal of this branch is to implement a variety of ensemble learning techniques the intent is not to merge it immediately but to ensure that people are aware of the work going on and to get review we will try to make a push soon to merge a lot of the components into better namespaces so please hold off on any review till we push that updatenoel has already implemented boosting and bagging brian has already cleaned up a lot of the tree model code we will clean this further based on the feedback already received ,,179,0.8268156424581006,0.24881141045958796,15492,284.79215078750326,29.43454686289698,83.13968499870901,1023,27,347,26,unknown,satra,GaelVaroquaux,false,GaelVaroquaux,2,1.0,34,2,559,true,true,false,false,1,1,0,0,2,0,19
625082,scikit-learn/scikit-learn,python,287,1312037851,1312049171,1312049171,188,188,github,false,false,false,105,6,5,0,2,0,2,0,2,1,0,5,6,6,0,0,1,0,5,6,6,0,0,252,37,254,37,44.794377441183784,1.9127635946599744,71,vlad@vene.ro,scikits/learn/datasets/tests/test_20news.py|scikits/learn/datasets/twenty_newsgroups.py|examples/document_classification_20newsgroups.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/datasets/mlcomp.py|scikits/learn/datasets/twenty_newsgroups.py|scikits/learn/datasets/base.py|scikits/learn/datasets/twenty_newsgroups.py|scikits/learn/datasets/twenty_newsgroups.py,44,0.017446471054718478,0,0,false,Data compression in 20 news groups features This pull request implements several features: * Loading of file contents by default in load_files * Renaming load_filenames back to load_files as the new version of load_files now loads more than filenames (load_files had recently been depreciated in favor of load_filenames I am changing this back: there has been no release since the depreciation) * fetch_20newsgroups now supports a subsetall to load both the training and testing set * Compression of the 20 newsgroup dataset storage on the disk The storage requirement go from 80Mo to 15Mo which makes it usable in examples ran to generate the docs,,178,0.8258426966292135,0.24900872323552736,15492,284.79215078750326,29.43454686289698,83.13968499870901,1023,27,347,22,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,4,1.0,143,2,523,true,true,false,false,68,145,26,0,130,12,179
625083,scikit-learn/scikit-learn,python,284,1311942600,1311951587,1311951587,149,149,github,false,false,false,19,1,1,0,0,0,0,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,9,0,9,0,4.125472791091716,0.17616557736798466,7,vlad@vene.ro,scikits/learn/linear_model/cd_fast.c|scikits/learn/linear_model/cd_fast.pyx,7,0.0056772100567721,0,0,true,Coordinate descent stopping rule Tolerance scaled wrt the maximum absolute value of the coefficientsRefer to Issue #283: https://githubcom/scikit-learn/scikit-learn/issues/283,,177,0.8248587570621468,0.25385239253852393,15492,284.79215078750326,29.43454686289698,83.13968499870901,1020,27,346,21,unknown,sabba,ogrisel,false,ogrisel,0,0,0,0,0,false,true,false,false,1,0,0,0,0,0,-1
625084,scikit-learn/scikit-learn,python,282,1311797892,1312885450,1312885450,18125,18125,github,false,false,false,70,25,9,0,29,6,35,6,5,5,0,6,19,8,0,0,8,3,11,22,17,0,0,2484,0,4684,235,97.71887138377346,4.094794218178306,14,vlad@vene.ro,examples/manifold/plot_isomap.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/isomap.py|examples/manifold/plot_isomap.py|scikits/learn/manifold/graph_search.c|scikits/learn/manifold/graph_search.pyx|scikits/learn/manifold/isomap.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx|examples/manifold/plot_isomap.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx|examples/manifold/plot_isomap.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx|scikits/learn/manifold/isomap.py|scikits/learn/manifold/shortest_path.c|scikits/learn/manifold/shortest_path.pyx,14,0.0,0,9,false,Isomap This is an implementation of Isomap for manifold learning built on the scikit-learn infrastructureThe main computational cost of isomap is in a shortest-path search of a connected graph  Ive written a cython module that includes both the Floyd-Warshall algorithm and Dijkstras algorithm with Fibonacci Heaps  The latter performs very well when the number of edges is much less than N^2Examples have been incorporated in examples/manifold/plot_lle_digitspy and examples/manifold/plot_compare_methodspy,,176,0.8238636363636364,0.254320987654321,16234,290.871011457435,29.93716890476777,86.05396082296416,1019,27,344,29,unknown,jakevdp,ogrisel,false,ogrisel,4,1.0,333,0,77,true,true,false,false,11,26,4,0,22,3,17
625085,scikit-learn/scikit-learn,python,281,1311783989,1311786805,1311786805,46,46,github,false,false,false,7,1,0,0,0,0,0,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,1,false,FIX: copyNone in Scalertransform instead of copyFalse ,,175,0.8228571428571428,0.2551781275890638,15490,284.82892188508714,29.438347320852163,83.15041962556488,1019,27,344,21,unknown,glouppe,agramfort,false,agramfort,4,1.0,40,12,258,true,true,false,true,6,7,4,0,4,0,-1
625086,scikit-learn/scikit-learn,python,280,1311727491,1311850061,1311850061,2042,2042,github,false,false,false,72,5,0,0,11,0,11,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,39,20,0,0.0,0,,,0,0.0,0,4,false,Lars n_features LARS and LassoLARS classes docstrings say that they take a n_features parameter This used to be a lie They did indeed take a max_features fit parameter but max_features has different semantics (ie can be overridden by alpha in the lasso case as per lars_path function)This pull request adds the n_features class parameter makes it play nicely with the max_features fit parameter and adds a test to enforce this behavior,,174,0.8218390804597702,0.25540765391014975,15490,284.82892188508714,29.438347320852163,83.15041962556488,1016,27,343,22,unknown,vene,fabianp,false,fabianp,10,0.9,21,15,471,true,true,false,true,16,24,6,0,220,1,143
625087,scikit-learn/scikit-learn,python,279,1311677999,1311680347,1311680347,39,39,github,false,false,false,13,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,34,0,0,0.0,0,,,0,0.0,0,0,false,COSMIT in BayesianRidge Using implicit continuation inside parenthesis instead of backslashes in BayesianRidgefit,,173,0.8208092485549133,0.2574341546304163,15466,285.2709168498642,29.35471356523988,83.15013578171474,1016,26,343,21,unknown,JeanKossaifi,GaelVaroquaux,false,GaelVaroquaux,2,1.0,2,7,50,false,true,true,false,2,0,2,0,0,0,-1
625088,scikit-learn/scikit-learn,python,278,1311651759,1311735371,1311735371,1393,1393,github,false,false,false,13,7,0,0,16,0,16,0,4,0,0,0,11,0,0,0,0,0,11,11,8,0,0,0,0,92,12,0,0.0,0,,,0,0.0,0,3,false,API : renaming LARS to Lars follow up on the mailing list discussion,,172,0.8197674418604651,0.2576530612244898,15483,284.95769553704065,29.451656655686882,83.18801265904541,1016,26,342,21,unknown,agramfort,agramfort,true,agramfort,11,0.8181818181818182,55,147,600,true,true,false,false,51,58,10,0,30,10,0
625089,scikit-learn/scikit-learn,python,277,1311599430,1312643611,1312643611,17403,17403,github,false,false,false,56,58,0,0,11,0,11,0,2,0,0,0,12,0,0,0,6,0,12,18,11,0,0,0,0,1704,557,0,0.0,0,,,0,0.0,0,0,false,Orthogonal matching pursuit A subset of the larger Dictionary Learning pull request This branch is a little bit ahead of that one but thats the point: to improve on little pieces at a timeWhat I feel is left to do:Unify the gram precomputation interface: doneBuild an estimator object similar to Lasso etc: done,,171,0.8187134502923976,0.2546531302876481,15671,286.96318039691147,29.353583051496393,82.89196605194309,1014,26,342,25,unknown,vene,agramfort,false,agramfort,9,0.8888888888888888,20,15,470,true,true,true,true,15,23,5,0,214,1,265
625090,scikit-learn/scikit-learn,python,274,1311434271,1312402178,1312402178,16131,16131,commit_sha_in_comments,false,false,false,33,20,12,0,8,1,9,0,2,0,0,5,7,3,0,0,0,1,6,7,5,0,0,393,35,619,35,65.62506690174033,2.7970904575816182,132,vlad@vene.ro,scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|doc/modules/classes.rst|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|doc/modules/linear_model.rst|doc/modules/linear_model.rst|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py,76,0.03305785123966942,0,1,false,LarsCV: cross-validated Lasso using using Here is an implementation of a cross-validated Lasso using the lars_path On many datasets it is much faster and more stable than the LassoCV based on coordinate descent,,170,0.8176470588235294,0.24049586776859505,15573,284.7877737109099,29.345662364348552,82.96410453990882,1011,26,340,22,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,3,1.0,141,2,516,true,true,false,false,62,150,25,0,146,10,5933
625091,scikit-learn/scikit-learn,python,273,1311392115,1314191094,1314191094,46649,46649,github,false,false,false,61,42,3,0,34,5,39,0,5,1,0,2,15,3,0,0,6,1,13,20,11,0,3,133,0,999,83,18.079938298509088,0.7490053677811308,100,vlad@vene.ro,scikits/learn/datasets/twenty_newsgroups.py|examples/cluster/plot_kmeans_vq.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py,88,0.024026512013256007,0,8,true,Kmeans transform K-means transform function which has both vector quantisation and dot productCode does not have tests yet - I wanted feedback on the style before I worry about thatThird attempt at this sorry for the confusion As far as I know this works if it doesnt Ill just send the code for someone else to set up :P,,169,0.8165680473372781,0.23777961888980945,16691,300.8807141573303,30.914864298124737,86.93307770654845,1011,26,339,25,unknown,robertlayton,mblondel,false,mblondel,3,1.0,2,5,63,true,true,false,false,5,16,3,0,7,0,3556
625092,scikit-learn/scikit-learn,python,272,1311360646,1311361018,1311361018,6,6,github,false,false,false,14,1,0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,5,5,5,0,0,0,0,21,0,0,0.0,0,,,0,0.0,0,0,true,DOC: Missing import in doctests issue #271 This is a fix for issue #271,,168,0.8154761904761905,0.23651452282157676,15346,285.9377036361267,29.519092923237327,82.88804900299752,1011,26,339,19,unknown,glouppe,ogrisel,false,ogrisel,3,1.0,39,12,253,true,true,true,false,5,4,3,0,3,0,-1
625093,scikit-learn/scikit-learn,python,270,1311348394,1313139030,1313139030,29843,29843,merged_in_comments,false,false,false,48,42,9,0,12,5,17,3,4,0,0,5,6,4,0,0,0,0,6,6,4,0,0,409,54,645,168,54.63687428019974,2.3338782378781806,109,wardefar@iro.umontreal.ca,examples/decomposition/plot_faces_decomposition.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_faces_decomposition.py|doc/modules/decomposition.rst|doc/modules/decomposition.rst,65,0.02736318407960199,0,4,true,Mini Batch SparsePCA Because the Dictionary Learning pull request is huge we decided to break it off into smaller parts that will be improved upon during reviewThis pull request contains:the dict_learning_online function (in sparse_pcapy)the MiniBatchSparsePCA estimator (this is new)the faces decomposition exampledocs tests,,167,0.8143712574850299,0.2330016583747927,15671,286.96318039691147,29.353583051496393,82.89196605194309,1010,26,339,29,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,8,0.875,20,15,467,true,true,false,false,14,22,4,0,208,1,66
625095,scikit-learn/scikit-learn,python,269,1311347741,1311391290,1311391290,725,725,commits_in_master,false,false,false,41,2,2,0,2,1,3,1,2,1,0,2,3,3,0,0,1,0,2,3,3,0,0,407,0,407,0,14.10984522642,0.6037459650015445,71,vlad@vene.ro,scikits/learn/datasets/twenty_newsgroups.py|examples/cluster/plot_kmeans_vq.py|scikits/learn/cluster/k_means_.py,64,0.014937759336099586,0,0,true,Kmeans transform The k-means transform function which can use either vector quantisation or the dot product(this is a new pull request from the old one which I accidentally merged branches on my end ignore the other one use this one),,166,0.8132530120481928,0.23319502074688797,15346,285.9377036361267,29.519092923237327,82.88804900299752,1010,26,339,18,unknown,robertlayton,robertlayton,true,robertlayton,2,1.0,2,5,63,true,true,false,false,4,13,2,0,5,0,0
625096,scikit-learn/scikit-learn,python,268,1311344892,1311344920,1311344920,0,0,commits_in_master,false,false,false,16,20,20,0,0,1,1,1,2,4,0,3,7,5,0,0,4,0,3,7,5,0,0,850,93,850,93,112.13264041788052,4.798041942409135,12,vlad@vene.ro,examples/cluster/plot_kmeans_vq.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py|examples/cluster/plot_dbscan.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/dbscan_.py,12,0.0,0,0,true,k-means transform The k-means transform function which can use either vector quantisation or the dot product,,165,0.8121212121212121,0.23269391159299416,15346,285.9377036361267,29.519092923237327,82.88804900299752,1010,26,339,17,unknown,robertlayton,robertlayton,true,robertlayton,1,1.0,2,5,63,true,true,false,false,3,11,1,0,3,0,-1
625097,scikit-learn/scikit-learn,python,267,1311342670,,1311385717,717,,unknown,false,false,false,38,5,4,0,7,0,7,0,5,1,0,4,6,4,0,0,1,0,5,6,4,0,0,147,47,155,53,27.396551717428366,1.1722702148425557,42,vlad@vene.ro,scikits/learn/feature_selection/tests/test_chi2.py|scikits/learn/feature_selection/univariate_selection.py|scikits/learn/feature_selection/__init__.py|scikits/learn/feature_selection/univariate_selection.py|examples/document_classification_20newsgroups.py|doc/modules/feature_selection.rst,41,0.0,0,3,true,χ² feature selection second attempt Heres the χ² feature selection functionality again this time as a function to be used SelectKBest and friendsNo partial sorting in this pull request but a demo on document classification is included,,164,0.8170731707317073,0.23308270676691728,15346,285.9377036361267,29.519092923237327,82.88804900299752,1010,26,339,18,unknown,larsmans,larsmans,true,,19,0.6842105263157895,37,25,369,true,true,false,false,54,125,33,0,213,7,13
625098,scikit-learn/scikit-learn,python,266,1311268840,1311335063,1311335063,1103,1103,github,false,false,false,25,1,0,0,6,0,6,0,4,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,164,0,0,0.0,0,,,0,0.0,0,6,false,Fix for issue #110 + other doc fixes DOC: Fixed issue #110DOC: Added default valuesFIX: Removed nu in SVR and epsilon in NuSVR,,163,0.8159509202453987,0.226890756302521,15350,285.66775244299674,29.5114006514658,82.80130293159608,1009,26,338,18,unknown,glouppe,GaelVaroquaux,false,GaelVaroquaux,2,1.0,39,12,252,true,true,false,false,3,1,2,0,2,0,12
625099,scikit-learn/scikit-learn,python,265,1311264469,1311294700,1311294701,503,503,github,false,false,false,25,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,1,5,0,0.0,0,,,0,0.0,0,0,false,Fix for _to_graph The function failed when used with a mask wich data wasnt of type bool(a numpy array of type int16 for instance),,162,0.8148148148148148,0.22624053826745164,15350,285.66775244299674,29.5114006514658,82.80130293159608,1009,26,338,19,unknown,JeanKossaifi,agramfort,false,agramfort,1,1.0,2,7,45,false,true,true,true,1,0,1,0,0,0,-1
624757,scikit-learn/scikit-learn,python,264,1311254585,,1311342562,1466,,unknown,false,false,false,63,5,4,0,18,0,18,0,5,2,0,4,7,4,0,0,2,0,5,7,4,0,0,183,35,212,54,36.10706376674416,1.5450002029492047,41,vlad@vene.ro,examples/document_classification_20newsgroups.py|scikits/learn/feature_selection/__init__.py|scikits/learn/feature_selection/chi2.py|doc/modules/feature_selection.rst|scikits/learn/feature_selection/chi2.py|doc/modules/feature_selection.rst|scikits/learn/feature_selection/chi2.py|scikits/learn/feature_selection/tests/test_chi2.py,41,0.0,0,14,false,χ² feature selection Heres a feature selection transformer that picks the best features ie the ones having the highest mutual dependence with class labels by doing a χ² test Also included is some extra narrative docs for feature selection in generalIts demod optionally in the document classification example F1 doesnt go up due to χ² but memory usage can be drastically reduced,,161,0.8198757763975155,0.22605042016806723,15346,285.9377036361267,29.519092923237327,82.88804900299752,1009,26,338,16,unknown,larsmans,larsmans,true,,18,0.7222222222222222,37,25,368,true,true,false,false,53,120,32,0,211,7,50
624754,scikit-learn/scikit-learn,python,263,1311173255,1315441815,1315441815,71142,71142,merged_in_comments,false,false,false,23,1,1,0,16,0,16,0,5,0,0,2,2,2,0,0,0,0,2,2,2,0,0,47,0,47,0,8.92639520965848,0.38280649630751484,36,vlad@vene.ro,scikits/learn/gaussian_process/gaussian_process.py|scikits/learn/metrics/pairwise.py,32,0.02691337258200168,2,3,false,API: make l1_distances return 2D matrix of pairwise distances by default follow up on this discussion :https://githubcom/scikit-learn/scikit-learn/commit/eb92e5273ad897f287b21076655d5debe815532f#commitcomment-480269@mblondel @GaelVaroquaux feed back welcome,,160,0.81875,0.22035323801513879,15411,284.4721302965414,29.26481084939329,82.14911426902862,1007,26,337,20,unknown,agramfort,robertlayton,false,robertlayton,10,0.8,53,147,595,true,true,false,false,43,49,7,0,26,8,281
624755,scikit-learn/scikit-learn,python,261,1311087758,1311104105,1311104105,272,272,github,false,false,false,7,1,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,32,0,0,0.0,0,,,0,0.0,0,0,false,Enforce axis1 in Normalizertransform + doc fixes ,,159,0.8176100628930818,0.21386306001690616,15428,284.02903811252264,29.2325641690433,82.05859476276899,1006,26,336,17,unknown,glouppe,mblondel,false,mblondel,1,1.0,39,12,250,true,true,true,true,1,0,1,0,1,0,2
624753,scikit-learn/scikit-learn,python,260,1311075574,1311075722,1311075723,2,2,github,false,false,false,7,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,false,DOC: Missing dot in Pipeline class description ,,158,0.8164556962025317,0.2131979695431472,15428,284.02903811252264,29.2325641690433,82.05859476276899,1006,25,336,18,unknown,glouppe,mblondel,false,mblondel,0,0,39,12,250,false,true,true,true,0,0,0,0,0,0,-1
624751,scikit-learn/scikit-learn,python,259,1311027905,1311111672,1311111672,1396,1396,github,false,false,false,183,4,1,0,11,0,11,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,17,23,50,23,14.004036782167692,0.6005733922215604,63,vlad@vene.ro,scikits/learn/linear_model/tests/test_logistic.py|scikits/learn/svm/base.py|scikits/learn/utils/__init__.py,50,0.021240441801189464,0,0,false,Systematic check for NaN and infinity Heres a patch that enables checking for NaN and infinity in any estimator that uses the safe_asanyarray or atleast2d_or_csr functions in its input Specifically it fixes a href/issues/252issue 252/a: logistic regression would go into an infinite loop when fed NaN Thats fixed at the BaseLibLinear levelThe check is done by calling isfinite on the result of npsum(X) for the input array X This is not the most elegant solution but itll work for any non-floating point array and at least all FP arrays whose sum isnt greater than 34028235e+38 (the float32 maximum) I reckon thats pretty safeThe alternative not (npisfinite(npmin(X)) and npisfinite(npmax(X))) takes more than twice as long to compute and npisfinite(X) takes O(n) space because it constructs a boolean array of shape Xshape so I didnt even bother to measure thatRunning the full test suite is not significantly slower nor is running the document classification example with four classes Datasets too large to fit in RAM might suffer so we might want to do something about that eg assume finiteness for memory-mapped arrays,,157,0.8152866242038217,0.2107051826677995,15428,284.02903811252264,29.2325641690433,82.05859476276899,1005,25,335,17,unknown,larsmans,larsmans,true,larsmans,17,0.7058823529411765,37,25,365,true,true,false,false,49,109,29,0,203,7,921
624752,scikit-learn/scikit-learn,python,257,1311024553,1311025751,1311025751,19,19,github,false,false,false,9,19,19,0,1,0,1,0,1,8,4,12,24,13,0,0,8,4,12,24,13,0,0,2213,94,2213,94,207.78966136375024,9.00168915460719,75,vlad@vene.ro,scikits/learn/cluster/k_means_.py|scikits/learn/cluster/tests/test_k_means.py|scikits/learn/utils/__init__.py|examples/document_clustering.py|scikits/learn/cluster/_k_means.c|scikits/learn/cluster/_k_means.pyx|scikits/learn/cluster/k_means_.py|examples/document_clustering.py|scikits/learn/cluster/setup.py|scikits/learn/cluster/sparse/__init__.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/_fast_kmeans.pyx|scikits/learn/cluster/sparse/k_means_.py|scikits/learn/cluster/sparse/setup.py|examples/document_clustering.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/_fast_kmeans.pyx|scikits/learn/cluster/sparse/k_means_.py|examples/document_clustering.py|scikits/learn/cluster/sparse/__init__.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/_fast_kmeans.pyx|scikits/learn/cluster/sparse/k_means_.py|examples/document_clustering.py|scikits/learn/cluster/sparse/__init__.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/_fast_kmeans.pyx|scikits/learn/cluster/sparse/k_means_.py|examples/document_clustering.py|scikits/learn/cluster/sparse/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/k_means_.py|examples/document_clustering.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/sparse/k_means_.py|scikits/learn/cluster/_k_means.pyx|scikits/learn/cluster/sparse/__init__.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/k_means_.py|scikits/learn/cluster/sparse/setup.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/_k_means.pyx|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/setup.py|scikits/learn/cluster/tests/test_k_means.py|examples/document_clustering.py|scikits/learn/cluster/_k_means.pyx|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/_k_means.c|scikits/learn/cluster/_k_means.pyx|scikits/learn/cluster/k_means_.py|examples/document_clustering.py|examples/document_clustering.py|scikits/learn/cluster/tests/test_k_means.py,52,0.0,0,0,false,Sparse mbkm moved _gen_even_slices and added callable init test ,,156,0.8141025641025641,0.2107051826677995,15343,282.14821091051294,29.00345434400052,81.47037737078799,1005,25,335,18,unknown,lemin,ogrisel,false,ogrisel,0,0,1,0,3,true,true,false,false,1,2,0,0,2,0,10
624749,scikit-learn/scikit-learn,python,255,1310919858,1311028862,1311028862,1816,1816,github,false,false,false,11,1,0,0,1,0,1,0,1,0,0,0,3,0,0,0,2,0,3,5,5,0,0,0,0,424,127,0,0.0,0,,,0,0.0,0,0,false,KernelPCA gets its own module I hope I didnt miss anything,,155,0.8129032258064516,0.20479862896315337,14985,274.5412078745412,29.162495829162495,81.54821488154822,1002,24,334,19,unknown,vene,mblondel,false,mblondel,7,0.8571428571428571,20,15,462,true,true,true,false,13,20,3,0,198,1,14
624750,scikit-learn/scikit-learn,python,254,1310842789,1310860802,1310860802,300,300,merged_in_comments,false,false,false,8,6,1,0,0,2,2,0,1,1,0,0,3,1,0,0,2,0,2,4,2,0,0,99,0,122,0,4.865609856190881,0.21383729276154279,0,,scikits/learn/datasets/olivetti_faces.py,0,0.0,0,0,false,Added loader code for (Roweis) Olivetti faces dataset ,,154,0.8116883116883117,0.19649122807017544,14960,275.33422459893046,29.144385026737968,81.68449197860963,1000,23,333,18,unknown,dwf,GaelVaroquaux,false,GaelVaroquaux,2,1.0,51,56,864,true,true,false,false,4,1,2,0,12,0,-1
624745,scikit-learn/scikit-learn,python,251,1310829643,1310829670,1310829670,0,0,commits_in_master,false,false,false,9,117,111,0,0,0,0,0,1,3,0,7,13,5,0,0,4,2,8,14,7,0,0,3172,208,3560,218,570.4283628230156,25.167841308432138,55,vanderplas@astro.washington.edu,scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|doc/modules/classes.rst|doc/modules/classes.rst|scikits/learn/decomposition/sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|doc/modules/decomposition.rst|doc/modules/decomposition.rst|doc/modules/decomposition.rst|doc/modules/decomposition.rst|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/decomposition/plot_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/decomposition/plot_sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py,54,0.0,0,0,false,Dwf sparse pca Make the compound example more systematic,,153,0.8104575163398693,0.19839857651245552,14751,274.42207307979123,28.947190021015523,81.75716900549115,1000,23,333,19,unknown,GaelVaroquaux,GaelVaroquaux,true,GaelVaroquaux,2,1.0,139,2,509,true,true,false,false,52,129,22,0,105,10,-1
625100,scikit-learn/scikit-learn,python,249,1310771382,1310771431,1310771431,0,0,github,false,false,false,11,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,true,ImporError - ImportError Doesnt look like this was ever tested :P,,151,0.8079470198675497,0.1983695652173913,14760,274.2547425474255,28.929539295392953,81.70731707317073,1000,23,332,19,unknown,dwf,GaelVaroquaux,false,GaelVaroquaux,0,0,51,56,863,true,true,true,false,1,1,0,0,10,0,-1
624747,scikit-learn/scikit-learn,python,248,1310771382,1310771427,1310771427,0,0,github,false,false,false,11,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,2,0,0,0.0,0,,,0,0.0,0,0,true,ImporError - ImportError Doesnt look like this was ever tested :P,,151,0.8079470198675497,0.1983695652173913,14760,274.2547425474255,28.929539295392953,81.70731707317073,1000,23,332,20,unknown,dwf,GaelVaroquaux,false,GaelVaroquaux,0,0,51,56,863,true,true,true,false,1,1,0,0,10,0,-1
625101,scikit-learn/scikit-learn,python,247,1310756484,1310757105,1310757105,10,10,github,false,false,false,23,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.496690949994656,0.19840024468493014,8,vanderplas@astro.washington.edu,doc/support.rst,8,0.007386888273314866,0,0,true,FIX the IRC chan used is scikit-learn and not learn There is a small error in the documentation on the IRC chan used,,150,0.8066666666666666,0.1994459833795014,14748,274.4778953078383,28.68185516680228,81.2991592080282,999,22,332,20,unknown,NelleV,GaelVaroquaux,false,GaelVaroquaux,1,1.0,19,13,543,true,true,true,false,6,3,0,0,19,0,-1
625102,scikit-learn/scikit-learn,python,246,1310744731,,1324301416,225944,,unknown,false,false,false,77,4,3,0,13,0,13,0,4,0,0,2,2,2,0,0,0,0,2,2,2,0,0,95,2,137,10,18.553058485235788,0.6802539555252158,110,vlad@vene.ro,scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py,89,0.08210332103321033,0,6,true,WIP: Complement Naive Bayes I implemented [J Rennie et al](http://citeseeristpsuedu/viewdoc/summarydoi1011138572)s complement naive Bayes multiclass strategy and refactored the categorical naive Bayes classes a bit further The new strategy is not demod in the document classification example since it performs less well than ordinary multiclass naive Bayes I send this pull request anyway because it may be useful for some datasets(The fact that CNB doesnt perform very well is not unexpected given [Kibriya etals](http://wwwcswaikatoacnz/~eibe/pubs/kibriya_et_al_crpdf) criticism of it),,149,0.8120805369127517,0.1992619926199262,19329,296.23881214755033,31.093176056702365,88.20942625071136,998,22,332,39,unknown,larsmans,larsmans,true,,16,0.75,37,25,362,true,true,false,false,42,97,28,0,191,6,9
625103,scikit-learn/scikit-learn,python,244,1310442853,1312626049,1312626049,36386,36386,github,false,false,false,10,36,2,0,48,4,52,0,6,2,0,1,10,3,0,0,5,0,8,13,7,0,0,174,43,1156,190,13.797142521563526,0.589119842891713,13,vlad@vene.ro,scikits/learn/cluster/__init__.py|scikits/learn/cluster/dbscan_.py|scikits/learn/cluster/tests/test_dbscan.py,13,0.0,0,18,false,Dbscan DBSCAN density based clustering algorithm (Ester et al 1996),,148,0.8108108108108109,0.19539170506912443,15400,286.49350649350646,29.415584415584416,83.37662337662339,990,23,328,30,unknown,robertlayton,mblondel,false,mblondel,0,0,2,5,52,true,true,false,false,0,0,0,0,2,0,656
626127,scikit-learn/scikit-learn,python,243,1310405372,,1324221101,230262,,unknown,false,false,false,32,1,1,0,24,0,24,0,5,0,0,1,1,1,0,0,0,0,1,1,1,0,0,9,0,9,0,4.117355685240363,0.15096419242239006,46,vlad@vene.ro,scikits/learn/decomposition/fastica_.py,46,0.042279411764705885,0,5,false,Pull request regarding Issue #238  new if clause in method fit() of FastICA to handle that there are only 2 (instead of 3) outputs of the function fastica in case of whitenFalse,,147,0.8163265306122449,0.18933823529411764,19329,296.23881214755033,31.093176056702365,88.20942625071136,990,22,328,38,unknown,jansoe,,false,,0,0,2,0,3,false,true,false,false,1,0,0,0,0,0,5
625105,scikit-learn/scikit-learn,python,242,1310399303,1351741441,1351741441,689035,689035,merged_in_comments,false,false,false,125,3,1,0,6,0,6,0,4,0,0,6,6,6,0,0,0,0,6,6,6,0,0,131,25,220,25,27.060613826837276,0.8317102468606629,283,vlad@vene.ro,scikits/learn/metrics/pairwise.py|scikits/learn/naive_bayes.py|scikits/learn/neighbors.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/utils/__init__.py,111,0.08547794117647059,0,5,false,One-hot/1-of-K transformer in preprocessing module This is an early pull request for a transformer that builds the sparse one-hot/1-of-K representation of a feature space Theres no example yet the implementation cant handle sparse input and I havent optimized itWhat might further have to be done:* allow the user to mask out some features that must be left as-is* delrefactor LabelBinarizer and OneHotTransformer to factor out the basic operation/del* handle sparse matrices in a smarter way without requiring a dense intermediate representationI want to have this functionality because Im working on a program that has several features that depend on the presence of items from a vocabulary Using one-hot representation for such features is recommended by [Hsu Chang and Lin](http://wwwcsientuedutw/~cjlin/papers/guide/guidepdf) ao,,146,0.815068493150685,0.18933823529411764,26058,334.82999462736973,31.890398342159795,91.37309079745185,990,22,328,140,unknown,larsmans,larsmans,true,larsmans,15,0.7333333333333333,37,25,358,true,true,false,false,40,92,27,0,188,6,370369
624831,scikit-learn/scikit-learn,python,241,1310307500,,1374771464,1074399,,unknown,false,false,false,46,38,19,3,19,1,23,1,6,3,0,5,13,4,0,0,6,0,10,16,8,0,0,537,26,747,29,80.60742940762347,2.4774761371055543,198,wardefar@iro.umontreal.ca,scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/tests/test_kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/tests/test_kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/tests/test_kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/tests/test_kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/__init__.py|doc/modules/decomposition.rst|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/kmeans_coder.py|scikits/learn/decomposition/tests/test_kmeans_coder.py|examples/decomposition/kmeans_coder.py|examples/decomposition/kmeans_coder.py,173,0.0,0,6,false,WIP: K-Means Coder Dictionary learning using K-MeansThis was here for a while but I forgot to send the pull requestNote: my three pull requests are incremental The K-Means Coder branch includes the Dictionary Learning (sc) branch which includes the sparse pca branchBestVlad,,145,0.8206896551724138,0.18231046931407943,26058,334.82999462736973,31.890398342159795,91.37309079745185,989,22,327,185,unknown,vene,,false,,6,1.0,20,15,455,true,true,false,false,8,6,2,0,158,0,125
625106,scikit-learn/scikit-learn,python,236,1310126436,1310401055,1310401055,4576,4576,github,false,false,false,54,3,0,0,2,0,2,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,27,17,0,0.0,0,,,0,0.0,0,0,true,Changed the default return type of ward_tree from bool to int Changed the return type for ward_tree function from bool to int to avoidan error when the parameter return_as is set to numpyndarray or when applying todense method to the sparse matrix returnedfix : when edges is emptyadded tests for grid_to_graph,,144,0.8194444444444444,0.17863720073664824,14754,271.65514436762913,28.263521756811713,80.5205368035787,984,23,325,21,unknown,JeanKossaifi,fabianp,false,fabianp,0,0,2,7,32,false,true,true,false,0,0,0,0,0,0,19
625107,scikit-learn/scikit-learn,python,235,1310073801,1310131160,1310131160,955,955,github,false,false,false,71,1,0,0,2,0,2,0,2,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,41,6,0,0.0,0,,,0,0.0,2,0,false,Reverse coef_ in Ridge This is probably not so well-known but Ridge support Y of shape [n_samples n_responses] for multivariate regression There was as an inconsistency in that coef_ was [n_features n_responses] where as all other linear classifiers use [n_classes n_features] This is fixed in this pull request @vene and @fabianp can you check that it doesnt break your code The case when Y is [n_samples] shouldnt be affected at all,,143,0.8181818181818182,0.17407071622846781,14731,272.0792885751137,28.307650532889824,80.64625619441992,981,23,324,19,unknown,mblondel,mblondel,true,mblondel,4,1.0,76,22,464,true,false,false,false,34,98,11,0,52,4,839
625108,scikit-learn/scikit-learn,python,234,1310061408,1310136153,1310136153,1245,1245,github,false,false,false,36,3,1,0,10,0,10,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,0,12,58,26,4.753182773853974,0.21058546619385568,13,vlad@vene.ro,scikits/learn/feature_extraction/tests/test_text.py,13,0.011828935395814377,2,2,false,inverse_transform on vectorizers Since nothing had happened in @amuellers [pull request](/pull/201) for over a month I decided to cherry-pick his best commits and finish the work Special attention @turian who [requested](/issues/173) this in the first place,,142,0.8169014084507042,0.1737943585077343,14732,272.06081998370894,28.305729025251154,80.64078197121911,981,23,324,18,unknown,larsmans,larsmans,true,larsmans,14,0.7142857142857143,37,25,354,true,true,false,false,36,85,24,0,174,6,49
625109,scikit-learn/scikit-learn,python,233,1309960085,,1310039175,1318,,unknown,false,false,false,40,1,1,0,10,0,10,0,4,0,0,3,3,3,0,0,0,0,3,3,3,0,0,26,17,26,17,13.971837594679972,0.6190059439050041,2,pietro.berkes@googlemail.com,scikits/learn/hmm.py|scikits/learn/mixture.py|scikits/learn/tests/test_mixture.py,2,0.0018231540565177757,0,6,false,Replace mixturelogsum with numpylogaddexp Since Im venturing into other peoples area of expertise I decided to make this commit a pull request I suggest we drop mixturelogsum in favor of numpylogaddexp which has the same function but is maintained upstream,,141,0.8226950354609929,0.16773017319963537,14742,271.9441052774386,28.286528286528284,80.5860805860806,977,23,323,17,unknown,larsmans,larsmans,true,,13,0.7692307692307693,37,25,353,true,true,false,false,35,84,23,0,172,6,281
625110,scikit-learn/scikit-learn,python,230,1309709454,1310480895,1310480895,12857,12857,github,false,false,false,11,3,1,0,7,0,7,0,5,0,0,2,3,2,0,0,0,0,3,3,2,0,0,67,21,79,21,9.112004756277987,0.40448549019713775,40,vanderplas@astro.washington.edu,scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py,29,0.025732031943212066,0,4,false,radius_neighbors_graph add radius_neighbors_graph to build graph of nearest neighbor from radius,,140,0.8214285714285714,0.15527950310559005,14780,271.9891745602165,28.21380243572395,80.31123139377537,974,23,320,21,unknown,agramfort,agramfort,true,agramfort,9,0.7777777777777778,52,144,578,true,true,false,false,41,45,4,0,19,8,9
625111,scikit-learn/scikit-learn,python,228,1309348793,1309788941,1309788941,7335,7335,github,false,false,false,112,5,1,0,13,0,13,0,4,0,0,2,4,2,0,0,0,0,4,4,4,0,0,48,14,68,51,9.289894898430603,0.41239596239916587,25,vlad@vene.ro,scikits/learn/feature_extraction/tests/test_text.py|scikits/learn/feature_extraction/text.py,21,0.016067329762815608,0,5,false,TfidfTransfomer: user-selectable norm Heres an updated feature_extractiontext module as per [Issue 225](https://githubcom/scikit-learn/scikit-learn/issues/225) The main changes concern the TfidfTransformer which previously* had an boolean parameter use_tf to enable/disable L1 normalization* did normalization before idf reweighingand now* allows the user to select L1 L2 or no normalization with the norm parameter* performs L2 normalization by default as per the Information Retrieval literature (where tf-idf originated)* normalizes after applying idfThe first big result is that the k-NN document classifier example goes from 5 to 8 F1-score making it competitive I suspect similar improvements might be got from other similarity-based (as opposed to margin-based) learning methods on tf-idf vectors,,139,0.8201438848920863,0.18821729150726854,14716,272.28866539820604,28.33650448491438,80.32073933134004,967,29,316,18,unknown,larsmans,larsmans,true,larsmans,12,0.75,36,25,346,true,true,false,false,33,78,20,0,167,6,22
625113,scikit-learn/scikit-learn,python,227,1309338023,1310757634,1310757634,23660,23660,commit_sha_in_comments,false,false,false,33,3,2,0,13,0,13,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,32,0,34,0,9.147261433704358,0.4077551901005778,0,,scikits/learn/gaussian_process/gaussian_process.py|scikits/learn/gaussian_process/gaussian_process.py,0,0.0,0,10,false,Gaussian process new pull request now from a branch as proposed by larsmans- optimizations for GaussianProcess- fixed pep8 stuff- added explicit dtype keyword for npzeros- npnewaxis instead of None,,138,0.8188405797101449,0.18759571209800918,13684,292.8237357497808,30.181233557439345,85.79362759427069,967,29,316,22,unknown,vincentschut,vincentschut,true,vincentschut,2,1.0,0,0,112,true,true,false,false,1,1,3,0,0,0,71
625114,scikit-learn/scikit-learn,python,226,1309278761,1309336770,1309336770,966,966,github,false,false,false,40,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,optimizations of GaussianProcess I did some profiling on GaussianProcess (fit() and predict()) and optimized it somewhat Its mostly low-hanging fruit just simple numpy stuff however it makes a more that 2x difference in speed for mePlease review and comment,,137,0.8175182481751825,0.18634969325153375,13684,292.8237357497808,30.181233557439345,85.79362759427069,965,28,315,17,unknown,vincentschut,vincentschut,true,vincentschut,1,1.0,0,0,111,false,true,false,false,0,0,0,0,0,0,14
625115,scikit-learn/scikit-learn,python,224,1309121303,,1317645641,142072,,unknown,false,false,false,93,5,1,0,3,6,9,0,2,3,0,3,12,6,0,0,3,0,9,12,9,0,0,787,91,993,211,27.99249552374425,1.2478249883049484,36,virgile.fritsch@gmail.com,examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_robust_vs_empirical_covariance.py|scikits/learn/covariance/__init__.py|scikits/learn/covariance/empirical_covariance_.py|scikits/learn/covariance/robust_covariance.py|scikits/learn/covariance/tests/test_covariance.py,29,0.01156515034695451,0,0,false,Implements a robust covariance estimator: Rousseeuws MCD (in replacement of previous bad pull-request)Minimum Covariance Determinant (MCD) is a robust estimator ofcovariance introduced by Rousseeuw [1] The main idea behind the MCDis to find a fixed proportion of observations whose scatter matrix hasthe minimum determinantThe MCD estimator is computed with the FastMCD algorithm [2][1] P J Rousseeuw Least median of squares regression J Am StatAss 79:871 1984[2] P J Rousseeuw and K Van Driessen A fast algorithm for theminimum covariance determinant estimator Technometrics 41(3):2121999,,136,0.8235294117647058,0.18735543562066306,13672,298.34698654183734,30.500292568753657,86.8929198361615,961,28,313,31,unknown,VirgileFritsch,VirgileFritsch,true,,2,0.5,1,0,419,true,true,false,false,6,8,2,0,244,0,917
625116,scikit-learn/scikit-learn,python,223,1309114191,,1309121963,129,,unknown,false,false,false,82,250,250,0,1,0,1,0,2,31,8,118,157,108,0,2,31,8,118,157,108,0,2,9006,1805,9006,1805,1895.061923442823,84.47641334626148,627,vmic@crater2.logilab.fr,examples/covariance/plot_mahalanobis_distances.py|examples/covariance/plot_robust_vs_empirical_covariance.py|scikits/learn/covariance/empirical_covariance_.py|scikits/learn/covariance/robust_covariance.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/linear_model/ridge.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/mldata.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|doc/contents.rst|doc/index.rst|doc/modules/classes.rst|doc/modules/manifold.rst|doc/unsupervised_learning.rst|examples/manifold/README.txt|examples/manifold/plot_lle_digits.py|examples/manifold/plot_swissroll.py|scikits/learn/datasets/__init__.py|scikits/learn/manifold.py|scikits/learn/tests/test_manifold.py|scikits/learn/manifold.py|doc/modules/manifold.rst|doc/modules/neighbors.rst|scikits/learn/manifold.py|scikits/learn/tests/test_manifold.py|doc/modules/classes.rst|doc/modules/naive_bayes.rst|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/__init__.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|examples/manifold/plot_lle_digits.py|scikits/learn/manifold/locally_linear.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_swissroll.py|scikits/learn/manifold/locally_linear.py|examples/manifold/plot_lle_digits.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/sparse/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/setup.py|scikits/learn/preprocessing/sparse/setup.py|scikits/learn/preprocessing/src/_preprocessing.c|scikits/learn/preprocessing/src/_preprocessing.pyx|examples/manifold/plot_swissroll.py|scikits/learn/manifold/locally_linear.py|examples/manifold/plot_swissroll.py|doc/modules/manifold.rst|doc/modules/manifold.rst|examples/manifold/plot_lle_digits.py|scikits/learn/manifold/locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|scikits/learn/manifold/locally_linear.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/__init__.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|doc/modules/naive_bayes.rst|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|benchmarks/bench_sgd_covertype.py|doc/modules/classes.rst|doc/modules/naive_bayes.rst|examples/gaussian_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/metrics/metrics.py|scikits/learn/metrics/tests/test_metrics.py|scikits/learn/cluster/tests/common.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/tests/test_mldata.py|examples/document_classification_20newsgroups.py|scikits/learn/naive_bayes.py|scikits/learn/grid_search.py|scikits/learn/grid_search.py|scikits/learn/cluster/tests/common.py|scikits/learn/cluster/tests/common.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/grid_search.py|scikits/learn/datasets/mldata.py|scikits/learn/svm/src/libsvm/svm.cpp|scikits/learn/datasets/mldata.py|scikits/learn/datasets/tests/test_mldata.py|doc/modules/datasets.rst|examples/covariance/plot_lw_vs_oas.py|scikits/learn/covariance/empirical_covariance_.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/datasets/tests/test_mldata.py|doc/modules/classes.rst|scikits/learn/naive_bayes.py|scikits/learn/cross_val.py|scikits/learn/datasets/mldata.py|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/utils/testing.py|doc/modules/datasets.rst|doc/modules/datasets_fixture.py|scikits/learn/datasets/lfw.py|scikits/learn/datasets/twenty_newsgroups.py|scikits/learn/manifold/locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|doc/modules/datasets.rst|doc/modules/sgd.rst|doc/modules/datasets_fixture.py|scikits/learn/manifold/tests/test_locally_linear.py|doc/conf.py|doc/modules/datasets.rst|doc/modules/datasets_fixture.py|scikits/learn/utils/testing.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/datasets/mldata.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_lle_digits.py|examples/manifold/plot_lle_digits.py|scikits/learn/manifold/locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|doc/modules/manifold.rst|examples/manifold/plot_lle_digits.py|scikits/learn/manifold/locally_linear.py|doc/modules/neighbors.rst|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|doc/modules/linear_model.rst|doc/modules/linear_model.rst|scikits/learn/naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/sparse/__init__.py|scikits/learn/utils/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/feature_extraction/text.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/sparse/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/src/_preprocessing.c|scikits/learn/preprocessing/src/_preprocessing.pyx|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/setup.py|scikits/learn/preprocessing/__init__.py|scikits/learn/cross_val.py|scikits/learn/preprocessing/__init__.py|scikits/learn/utils/__init__.py|scikits/learn/src/BallTree.h|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|scikits/learn/tests/test_neighbors.py|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|scikits/learn/datasets/base.py|examples/decomposition/plot_kernel_pca.py|scikits/learn/decomposition/pca.py|scikits/learn/metrics/pairwise.py|scikits/learn/metrics/tests/test_pairwise.py|scikits/learn/lda.py|scikits/learn/tests/test_lda.py|examples/manifold/plot_lle_digits.py|scikits/learn/decomposition/pca.py|scikits/learn/lda.py|examples/manifold/plot_lle_digits.py|scikits/learn/feature_extraction/text.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/utils/__init__.py|scikits/learn/utils/tests/test_utils.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py|scikits/learn/preprocessing/__init__.py|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|scikits/learn/tests/test_neighbors.py|scikits/learn/utils/__init__.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/__init__.py|doc/modules/classes.rst|scikits/learn/preprocessing/__init__.py|scikits/learn/naive_bayes.py|doc/modules/preprocessing.rst|scikits/learn/preprocessing/__init__.py|examples/plot_roc.py|doc/contents.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/tests/test_fastica.py|examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|doc/modules/classes.rst|doc/modules/preprocessing.rst|scikits/learn/preprocessing/__init__.py|doc/modules/preprocessing.rst|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/fastica_.py|scikits/learn/naive_bayes.py|examples/decomposition/plot_ica_vs_pca.py|examples/plot_classification_probability.py|examples/plot_pls.py|examples/plot_rfe_digits.py|examples/decomposition/plot_ica_blind_source_separation.py|scikits/learn/src/BallTree.h|scikits/learn/tests/test_neighbors.py|examples/document_classification_20newsgroups.py|examples/document_classification_20newsgroups.py|scikits/learn/naive_bayes.py|examples/decomposition/plot_pca_vs_lda.py|doc/modules/manifold.rst|doc/themes/scikit-learn/static/nature.css_t|scikits/learn/decomposition/fastica_.py|examples/manifold/plot_swissroll.py|scikits/learn/decomposition/pca.py|scikits/learn/lda.py|doc/modules/preprocessing.rst|doc/modules/preprocessing.rst|doc/modules/classes.rst|scikits/learn/metrics/__init__.py|scikits/learn/metrics/metrics.py|scikits/learn/metrics/tests/test_metrics.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/grid_search.py|scikits/learn/manifold/locally_linear.py|scikits/learn/manifold/tests/test_locally_linear.py|scikits/learn/utils/__init__.py|scikits/learn/utils/fixes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/datasets/samples_generator.py|scikits/learn/metrics/cluster.py|scikits/learn/utils/__init__.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|doc/whats_new.rst|scikits/learn/feature_extraction/text.py|scikits/learn/svm/sparse/classes.py|doc/whats_new.rst|scikits/learn/naive_bayes.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/setup.py|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/tests/data/svmlight_classification.txt|scikits/learn/datasets/tests/test_svmlight_format.py|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/tests/test_svmlight_format.py|scikits/learn/naive_bayes.py|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/tests/data/svmlight_invalid.txt|scikits/learn/datasets/tests/test_svmlight_format.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/tests/test_mldata.py|scikits/learn/utils/testing.py|doc/modules/datasets.rst|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/svmlight_format.py|scikits/learn/tests/test_naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|doc/modules/classes.rst|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/tests/test_svmlight_format.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/__init__.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/svmlight_format.py|scikits/learn/datasets/tests/test_svmlight_format.py|doc/modules/datasets.rst|scikits/learn/datasets/mldata.py|scikits/learn/datasets/mldata.py|scikits/learn/utils/fixes.py|scikits/learn/utils/testing.py|scikits/learn/datasets/mldata.py|.gitignore|doc/contents.rst|doc/datasets/index.rst|doc/datasets/labeled_faces.rst|doc/datasets/labeled_faces_fixture.py|doc/datasets/mldata.rst|doc/datasets/mldata_fixture.py|doc/datasets/twenty_newsgroups.rst|doc/datasets/twenty_newsgroups_fixture.py|doc/modules/datasets.rst|doc/datasets/index.rst|scikits/learn/datasets/_svmlight_format.cpp|doc/developers/index.rst|scikits/learn/base.py|scikits/learn/preprocessing/__init__.py|doc/modules/linear_model.rst|scikits/learn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_path_crossval.py|doc/modules/linear_model.rst|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|doc/modules/classes.rst|scikits/learn/cross_val.py|doc/modules/neighbors.rst|doc/developers/index.rst|scikits/learn/linear_model/least_angle.py|doc/modules/linear_model.rst|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/ridge.py|scikits/learn/externals/joblib/__init__.py|scikits/learn/externals/joblib/parallel.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|doc/modules/decomposition.rst|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/tests/data/svmlight_classification.txt|scikits/learn/base.py|scikits/learn/feature_extraction/text.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/tests/test_svmlight_format.py|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/_svmlight_format.cpp|scikits/learn/datasets/tests/data/svmlight_classification.txt|scikits/learn/datasets/tests/data/svmlight_classification.txt|scikits/learn/svm/classes.py|scikits/learn/naive_bayes.py|scikits/learn/externals/joblib/__init__.py|scikits/learn/externals/joblib/disk.py|scikits/learn/externals/joblib/format_stack.py|scikits/learn/externals/joblib/func_inspect.py|scikits/learn/externals/joblib/hashing.py|scikits/learn/externals/joblib/logger.py|scikits/learn/externals/joblib/memory.py|scikits/learn/externals/joblib/my_exceptions.py|scikits/learn/externals/joblib/numpy_pickle.py|scikits/learn/externals/joblib/parallel.py|scikits/learn/externals/joblib/test/common.py|scikits/learn/externals/joblib/test/test_format_stack.py|scikits/learn/externals/joblib/test/test_func_inspect.py|scikits/learn/externals/joblib/test/test_logger.py|scikits/learn/externals/joblib/test/test_memory.py|scikits/learn/externals/joblib/test/test_my_exceptions.py|scikits/learn/externals/joblib/test/test_numpy_pickle.py|scikits/learn/externals/joblib/test/test_parallel.py,68,0.009287925696594427,0,0,false,Robust estimation of covariance (Minimum Covariance Determinant) Implement the FastMCD algorithm [1] to compute a robust estimator of (location and) covariance: the Minimum Covariance DeterminantThis is a first version that we may found to be somewhat slow but I wanted to have a simple non-optimized version first For the same reasons examples must be a bit slow tooI added examples showing that the robust estimator is better both in term of MSE and with regard to the Mahalanobis distances relevance,,135,0.8296296296296296,0.18808049535603716,13672,298.34698654183734,30.500292568753657,86.8929198361615,961,28,313,18,unknown,VirgileFritsch,VirgileFritsch,true,,1,1.0,1,0,419,true,true,false,false,5,7,1,0,243,0,99
625123,scikit-learn/scikit-learn,python,222,1309051731,1309378644,1309378644,5448,5448,github,false,false,false,21,3,0,0,5,0,5,0,3,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,224,0,0,0.0,0,,,0,0.0,0,1,false,NeighborsClassifier: changed window_size to leaf_size & updated documentation A few minor changes as discussed in a hrefhttps://githubcom/scikit-learn/scikit-learn/commit/b4a6b57669a7c46aa7d6e21d6009f497003e8095#commentsb4a6b57/a and a hrefhttps://githubcom/scikit-learn/scikit-learn/issues/195Issue 195/a,,134,0.8283582089552238,0.18778979907264295,13684,292.8237357497808,30.181233557439345,85.79362759427069,960,28,312,19,unknown,jakevdp,fabianp,false,fabianp,3,1.0,310,0,45,true,true,false,false,7,21,3,0,13,0,484
625124,scikit-learn/scikit-learn,python,221,1309002095,1316443459,1316443459,124022,124022,github,false,false,false,25,190,47,0,50,10,60,7,4,11,1,15,44,14,0,1,15,2,31,48,28,0,1,1947,512,5477,970,285.1281829849452,10.876099542219404,391,wardefar@iro.umontreal.ca,scikits/learn/linear_model/omp.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/tests/test_omp.py|examples/linear_model/plot_omp.py|examples/linear_model/plot_omp.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/decomposition/sparse_pca.py|.gitignore|doc/contents.rst|doc/datasets/index.rst|doc/datasets/labeled_faces.rst|doc/datasets/labeled_faces_fixture.py|doc/datasets/mldata.rst|doc/datasets/mldata_fixture.py|doc/datasets/twenty_newsgroups.rst|doc/datasets/twenty_newsgroups_fixture.py|doc/modules/datasets.rst|doc/datasets/index.rst|scikits/learn/datasets/_svmlight_format.cpp|doc/developers/index.rst|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/tests/test_omp.py|scikits/learn/linear_model/tests/test_omp.py|examples/linear_model/plot_omp.py|examples/linear_model/plot_omp.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|examples/decomposition/plot_dict_learning.py|examples/decomposition/plot_dict_learning.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/decomposition/dict_learning.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/dict_learning.py|examples/decomposition/plot_dict_learning.py|scikits/learn/decomposition/dict_learning.py|examples/decomposition/plot_dict_learning.py|scikits/learn/decomposition/dict_learning.py,101,0.027258566978193146,0,12,false,Dictionary learning Pull request contains: (UPDATED)BaseDictionaryLearning object implementing transform methodsDictionaryLearning and OnlineDictionaryLearning implementing fit in different waysDictionary learning exampleImage denoising example,,133,0.8270676691729323,0.18925233644859812,18120,300.49668874172187,31.788079470198674,89.95584988962473,957,28,312,25,unknown,vene,ogrisel,false,ogrisel,5,1.0,19,15,440,true,true,true,true,10,32,9,0,169,0,545
625129,scikit-learn/scikit-learn,python,219,1308744831,1309133453,1309133453,6477,6477,github,false,false,false,80,11,1,0,17,0,17,0,7,0,0,4,7,4,0,0,0,0,7,7,6,0,0,81,0,152,70,18.247399454410296,0.8134166170172745,74,vlad@vene.ro,examples/document_classification_20newsgroups.py|scikits/learn/metrics/pairwise.py|scikits/learn/neighbors.py|scikits/learn/utils/__init__.py,32,0.01973164956590371,0,11,false,Sparse k-nearest neighbor classifier This is an early pull request for an adapted kNN classifier that handles sparse input My motivation for this is that I use several kNN-based NLP tools in day-to-day use which perform quite well so it may be of use in my fieldGood performance on such categorical tasks would require different distance metrics though which are not included in this pull requestTODO before merging:- write tests ✓- document the sparsity option ✓,,132,0.8257575757575758,0.19494869771112866,13672,298.34698654183734,30.500292568753657,86.8929198361615,952,28,309,18,unknown,larsmans,larsmans,true,larsmans,11,0.7272727272727273,34,25,339,true,true,false,false,26,58,17,0,165,6,5
625130,scikit-learn/scikit-learn,python,218,1308659420,1311101485,1311101485,40701,40701,github,false,false,false,15,10,2,0,6,0,6,0,3,0,0,2,4,2,0,0,0,0,4,4,3,0,0,2,11,15,70,8.806200054514512,0.3854282344171139,39,vlad@vene.ro,scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_least_angle.py,31,0.02437106918238994,0,2,false,Fix lars Gaels patch and my test -- for discussion there are still missing bits,,131,0.8244274809160306,0.19418238993710693,14984,274.5595301655099,29.097704217832355,81.55365723438335,950,28,308,23,unknown,fabianp,fabianp,true,fabianp,12,0.8333333333333334,63,19,402,true,true,false,false,48,104,21,0,123,0,36635
625131,scikit-learn/scikit-learn,python,216,1308409616,1308416510,1308416510,114,114,github,false,false,false,9,3,0,0,2,0,2,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,minor doc typos Fixes minor problems in the documentation,,130,0.823076923076923,0.2014331210191083,13652,298.4178142396718,30.398476413712277,86.87371813653677,946,28,305,14,unknown,ametaireau,GaelVaroquaux,false,GaelVaroquaux,0,0,117,60,815,false,false,false,false,0,0,0,0,2,0,108
625132,scikit-learn/scikit-learn,python,212,1308094203,1310839930,1310839930,45762,45762,github,false,false,false,49,127,37,0,34,0,34,0,5,3,0,7,13,5,0,0,4,2,8,14,7,0,0,1693,114,3636,246,236.7599833290723,10.446051864320383,88,vlad@vene.ro,doc/modules/classes.rst|doc/modules/decomposition.rst|examples/decomposition/plot_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparsepca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|examples/decomposition/plot_sparse_pca.py|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py|doc/modules/classes.rst|doc/modules/decomposition.rst|examples/decomposition/plot_sparse_pca.py|doc/modules/decomposition.rst|scikits/learn/decomposition/sparse_pca.py|scikits/learn/decomposition/tests/test_sparse_pca.py,76,0.0,0,7,false,Sparse PCA Sparse principal components analysisThis computes a sparse matrix decomposition with L1 penalties on the dictionary atoms and unitary L2 penalties on the data columnsThere is a pretty-looking (purple) example on the digits datasetTest coverage was 86% at the moment this pull request was sent,,129,0.8217054263565892,0.20802620802620803,14760,274.2547425474255,28.929539295392953,81.70731707317073,940,28,301,21,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,4,1.0,19,15,429,true,true,true,false,9,32,8,0,113,0,70
625133,scikit-learn/scikit-learn,python,211,1308038797,1309341054,1309341054,21704,21704,github,false,false,false,111,19,5,0,12,0,12,0,3,2,0,6,14,6,0,0,5,3,9,17,10,0,0,742,0,3451,60,41.2471982982312,1.8386660647131428,49,vlad@vene.ro,scikits/learn/manifold/HLLE.py|scikits/learn/manifold/HLLE.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/MLLE.py|scikits/learn/datasets/samples_generator.py|scikits/learn/manifold/MLLE.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/locally_linear.py|scikits/learn/setup.py,25,0.012335526315789474,0,0,false,Hessian and Modified Locally Linear Embedding  - implemented hessian LLE modified LLE and LTSA (local tangent space alignment) as keywords in locally_linear_embedding - added a module scipy_futurepy which wraps the shift-invert mode of the scipy ARPACK wrapper  This is based on a recent commit in scipy: https://githubcom/scipy/scipy/commit/eea4ac1  scipy_futurepy will be superseded when the new functionality is included in a scipy releaseThe ARPACK eigensolver is about a factor of three faster than LOBPCG  The three modified methods are about a factor of two slower than the standard embedding algorithm mainly because constructing the weight matrix is more involved  They produce embeddings with much better warping (run examples/manifold/compare_methodspy to see a comparison),,128,0.8203125,0.21052631578947367,13684,292.8237357497808,30.181233557439345,85.79362759427069,938,28,301,20,unknown,jakevdp,fabianp,false,fabianp,2,1.0,297,0,34,true,true,false,false,6,16,2,0,8,0,104
625134,scikit-learn/scikit-learn,python,210,1307822249,1307965857,1307965857,2393,2393,github,false,false,false,138,13,9,0,8,0,8,0,3,0,0,3,5,2,0,0,0,0,5,5,3,0,0,430,0,457,55,41.870487061384765,1.9166746342951662,49,vlad@vene.ro,scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|examples/document_classification_20newsgroups.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py,38,0.014119601328903655,0,2,false,Naive Bayes classifier for multivariate Bernoulli (binary features) Heres a version of the naive Bayes classifier for data following multivariate Bernoulli distributions in short: categorical data with binary features It does binarizing if handed any other kind of data with a threshold that can be set by the userBernoulliNB is a separate class rather than a different mode of MultinomialNB mostly because the parameter interaction needed to implement both models in a single class would I think be too complicated to describe So complex is better than complicated beats flat is better than nested hereBernoulliNB performs slightly better in the document classification example on four classes this may be a coincidence Its also slower but a pipeline using it might be a lot faster with something like an OccurrenceVectorizer (TODO)The documentation may need some expanding,,127,0.8188976377952756,0.21677740863787376,13383,292.38586266158563,29.73922140028394,83.98714787416873,936,28,298,14,unknown,larsmans,larsmans,true,larsmans,10,0.7,34,25,328,true,true,false,false,24,50,14,0,150,6,345
625135,scikit-learn/scikit-learn,python,209,1307730064,1308062196,1308062196,5535,5535,github,false,false,false,320,30,0,0,31,0,31,0,4,0,0,0,6,0,0,0,5,0,6,11,5,0,0,0,0,1346,93,0,0.0,0,,,0,0.0,2,10,true,Fast loader for the svmlight / libsvm format Heres a pull-request that implements a fast and memory efficient loader for the svmlight / libsvm sparse dataset format By memory efficient I mean that it loads the dataset directly into a scipy sparse CSR without any memory copyingIt should make playing with scikit-learn much easier as a wealth of datasets are availabel in this format see eg http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/You can use it like this:preimport sysimport numpy as npfrom scikitslearndatasets import load_svmlight_formatfrom scikitslearnlinear_modelsparse import SGDClassifierX_train y_train X_test y_test  load_svmlight_format(sysargv[1] sysargv[2])y_pred  SGDClassifier()fit(X_train y_train)predict(X_test)print Accuracy npmean(y_pred  y_test)/preTiming on the MNIST dataset:preAccuracy 08731real    0m3104suser    0m2920ssys     0m0180s/preTiming on the news20 dataset:preAccuracy 0845730027548real    0m1134suser    0m1070ssys     0m0060s/pre(loading 2 datasets learning and prediction)The loader is coded in 250 lines of C++ and a few lines of Python (would be nice if experienced C++ programmers like @larsmans or @jakevdp could review)Pure-python was out-of-the-question for me as it is really slow for parsing text (I think it would take 10 to 100 times slower and it would require memory copying) I didnt use Cython as I think the code would look just like a C++ dialectAlso I had to use the trick described by Travis Oliphant (http://blogenthoughtcom/p62) to manage memory deallocation As mentioned in the comments of this blog post if we assume that the memory allocator is malloc quite a few lines of code can be spared by using arrayflags | NPY_OWNDATA However it seems to me that it is a dangerous assumption so I went for the solution suggested by TravisWhat still needs to be done:- narrative doc and doctests　[done]- check error handling [done]- return a scipy sparse matrix for Y if theres more than 1 label per row (multilabel) [for another pull request],,126,0.8174603174603174,0.22345890410958905,13531,297.8346020249797,30.079077673490502,85.72906658783535,933,28,297,14,unknown,mblondel,mblondel,true,mblondel,3,1.0,68,21,437,true,false,false,false,30,82,14,0,48,4,12
625136,scikit-learn/scikit-learn,python,207,1307462418,1307527081,1307527081,1077,1077,github,false,false,false,38,3,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,16,0,0,0.0,0,,,0,0.0,1,0,false,Fixes for MiniBatchKMeans I spotted some minor bugs in MiniBatchKMeans - it would be great if the original authors (@NelleV) could double-check and pull if ok:   * random_state allways None   * reuse sample norms squared instead of recompute,,125,0.816,0.22388059701492538,13389,292.1801478825902,29.72589439091792,83.42669355441033,927,28,294,13,unknown,pprett,ogrisel,false,ogrisel,9,0.8888888888888888,33,22,672,true,true,true,true,14,36,9,0,51,0,1077
625137,scikit-learn/scikit-learn,python,206,1307448811,,1307455439,110,,unknown,false,false,false,56,1,1,0,1,0,1,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,13,0,13,0,4.570178940262288,0.20920705604115275,34,vlad@vene.ro,scikits/learn/naive_bayes.py,34,0.029982363315696647,0,0,false,Restore Numpy 12 compatibility Yesterday I accidentally broke Numpy 12 compatibility by using the return_inverse keyword arg of numpyunique It seems this functionality has been around since Numpy 12 but was only in the now-deprecated numpyunique1dThe attached commit should fix the issue Can anyone please check whether it actually works with Numpy 12 and 13,,124,0.8225806451612904,0.22486772486772486,13388,292.2019719151479,29.728114729608606,83.43292500746938,927,28,294,13,unknown,larsmans,larsmans,true,,9,0.7777777777777778,34,25,324,true,true,false,false,21,37,13,0,110,6,52
625138,scikit-learn/scikit-learn,python,205,1307387709,1311025750,1311025750,60634,60634,github,false,false,false,269,17,1,0,34,0,34,0,6,6,0,1,21,7,0,0,8,4,11,23,12,0,0,572,0,2157,77,27.44459590507741,1.2234198281851874,13,vlad@vene.ro,examples/document_clustering.py|scikits/learn/cluster/setup.py|scikits/learn/cluster/sparse/__init__.py|scikits/learn/cluster/sparse/_fast_kmeans.c|scikits/learn/cluster/sparse/_fast_kmeans.pyx|scikits/learn/cluster/sparse/k_means_.py|scikits/learn/cluster/sparse/setup.py,13,0.0,0,25,false,Sparse MiniBatchKmeans clustering Ive created an initial version of a sparse implementation of MiniBatchKmeans clustering based on scikitslearnclusterMiniBatchKmeans - Id like to use this as an sparse clustering prototype to come up with a proper API for sparse clustering algorithms You can find an document clustering example in examples/document_clusteringpy Ive implemented the sparse version analogous to the sparse svm or linear_model modules This however requires that the user picks the appropriate implementation for her task at hand (either clusterMiniBatchKmeans or clustersparseMiniBatchKmeans) First I wanted to integrate the functionality into the existing code base but it turned out that the code base of both versions are fundamentally different due to the different data structures that they use maybe the original authors of MiniBatchKmeans can comment on this and prove me wrongCurrently it only supports initrandom and only has a n_iter stopping conditionAnother conceptual difference to the dense version is that it uses an index array to create the mini batches instead of shuffling the data array and splitting it (my gut feeling says that this should be faster)The mini-batch updates are written in Cython using similar tricks as in SGDClassifier (eg centroids are represented by a dense vector + a scaling vactor)) - see sparse/_fast_kmeanspyx The current implementation clusters the training set of the 20 newsgroups corpus (~11000 samples) in about 2 minutes or less depending on how you tune chunk_size - most of the time is spent on distance computations (basically a product: csr_matrix * ndarray ) - maybe we can optimize this furtherNOTE: not ready for merging yet - no tests - not pep8ed,,123,0.8211382113821138,0.22458001768346597,13668,298.4342990927714,30.509218612818263,86.91834942932397,927,28,293,22,unknown,pprett,ogrisel,false,ogrisel,8,0.875,33,22,671,true,true,true,true,13,35,8,0,45,0,143
625139,scikit-learn/scikit-learn,python,203,1307360798,1307360871,1307360871,1,1,github,false,false,false,6,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,rst fix Proper rst for https://githubcom/amueller/scikit-learn/commit/ecbb8b5b857821e1b34e094d08e96eea7b910460#commitcomment-416063,,122,0.819672131147541,0.21750902527075813,13363,288.7824590286612,29.48439721619397,82.09234453341315,926,28,293,14,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,7,1.0,142,19,227,true,true,true,false,13,23,7,0,4,0,-1
625140,scikit-learn/scikit-learn,python,201,1307291998,1310136268,1310136268,47404,47404,commit_sha_in_comments,false,false,false,27,4,2,0,7,0,7,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,31,0,50,12,8.969119121567214,0.4098276383302197,10,vlad@vene.ro,scikits/learn/feature_extraction/text.py|scikits/learn/feature_extraction/text.py,10,0.00909090909090909,0,2,false,Vectorizer and CountVectorizer inverse transform This is on issue 137: https://githubcom/scikit-learn/scikit-learn/issues/173I added a inverse_vocabulary to CountVectorizer Its a nparray of all the words in the vocabulary,,121,0.8181818181818182,0.21818181818181817,13363,287.8844570829903,29.409563720721394,82.01751103794058,925,28,292,23,unknown,amueller,larsmans,false,larsmans,6,1.0,141,19,226,true,true,false,false,10,18,6,0,3,0,5
625141,scikit-learn/scikit-learn,python,200,1307214730,1307216675,1307216675,32,32,github,false,false,false,8,4,4,0,0,0,0,0,1,0,0,5,5,5,0,0,0,0,5,5,5,0,0,13,0,13,0,22.991417293980188,1.0505515954123181,11,vlad@vene.ro,examples/plot_rfe_digits.py|examples/plot_classification_probability.py|examples/plot_pls.py|examples/decomposition/plot_pca_vs_lda.py|examples/decomposition/plot_ica_vs_pca.py,5,0.0036463081130355514,0,0,false,Minor docs Minor additions/corrections in docstrings of examples,,120,0.8166666666666667,0.2187784867821331,13361,287.9275503330589,29.413966020507445,82.02978818950677,924,28,291,16,unknown,amueller,mblondel,false,mblondel,5,1.0,140,19,225,true,true,true,false,9,13,5,0,3,0,-1
625142,scikit-learn/scikit-learn,python,199,1307214492,1307214510,1307214510,0,0,commits_in_master,false,false,false,10,7,7,0,0,3,3,0,0,0,0,8,8,8,0,0,0,0,8,8,8,0,0,38,7,38,7,41.48765492628932,1.8957040149125888,21,vlad@vene.ro,examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/tests/test_fastica.py|examples/plot_rfe_digits.py|examples/plot_classification_probability.py|examples/plot_pls.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_pca_vs_lda.py,10,0.004557885141294439,0,0,false,Minor changes in example docstrings Minor additions/corrections of the examples,,119,0.8151260504201681,0.2187784867821331,13361,287.9275503330589,29.413966020507445,82.02978818950677,924,28,291,16,unknown,amueller,amueller,true,amueller,4,1.0,140,19,225,true,true,false,false,8,13,4,0,3,0,-1
625143,scikit-learn/scikit-learn,python,198,1307214428,1307370385,1307370385,2599,2599,github,false,false,false,10,11,9,0,15,0,15,0,5,0,0,4,4,4,0,0,0,0,4,4,4,0,0,44,7,76,7,50.23519412998439,2.2954071366848945,15,vlad@vene.ro,examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/fastica_.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/tests/test_fastica.py,10,0.004557885141294439,0,2,false,Fast ica transposed transposed fast ica  (in its  own branch),,118,0.8135593220338984,0.2187784867821331,13361,287.9275503330589,29.413966020507445,82.02978818950677,924,28,291,16,unknown,amueller,ogrisel,false,ogrisel,3,1.0,140,19,225,true,true,true,false,7,13,3,0,3,0,9
625144,scikit-learn/scikit-learn,python,197,1307204554,1307218118,1307218118,226,226,commits_in_master,false,false,false,54,11,5,0,5,0,5,0,2,0,0,4,5,4,0,0,0,0,5,5,5,0,0,31,7,54,7,27.806712725177412,1.2705778875256974,15,vlad@vene.ro,examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|scikits/learn/decomposition/fastica_.py|scikits/learn/decomposition/tests/test_fastica.py,10,0.004591368227731864,0,0,false,Transposing input and output of fastICA This transposes input and output of fastICA as discussed on the mailing listI tried to transpose everything in the algorithm but gave up Now everything is  just transposed in the beginning and the end - which is a free operation in numpy afaikHope this is ok,,117,0.811965811965812,0.22130394857667585,13361,287.9275503330589,29.413966020507445,82.02978818950677,923,28,291,15,unknown,amueller,amueller,true,amueller,2,1.0,140,19,225,true,true,false,false,6,11,2,0,3,0,26
625145,scikit-learn/scikit-learn,python,196,1307136069,,1324299423,286055,,unknown,false,false,false,206,1,1,0,2,6,8,0,2,2,0,0,2,2,0,0,2,0,0,2,2,0,0,273,0,273,0,9.299120936839113,0.3409553096178775,0,,examples/cluster/clustering_compare.py|examples/cluster/plot_k_means.py,0,0.0,0,0,true,Two examples for clustering Hi First I would apologize that I sent my last message twice I have made two changes here and added two files:1- examples/cluster/plot_k_meanspyI noticed that an example of how to use the k-means algorithm is missing in the clustering webpage:http://scikit-learnsourceforgenet/modules/clusteringhtmlSo I wrote this simple example illustrating how to use k-means The code is in accordance to those of the other examples Im wondering if it is possible for me to add this to the project and also put it on that webpage If yes can anybody help me telling how to do that 2- examples/cluster/clustering_comparepyAs you advised I wrote a code that serves as a framework for comparing different clustering methods Unfortunately I have only managed to write it for two clustering methods namely k-means and mean-shift Nonetheless the code is written such that any new clustering method can be easily added without changing the preamble parameters etc The instructions of how to add a new method is commented within the code Can you please consider also adding this code to the project or in the case you think it does not suit it can you please advise me how to improve it All the bestMostafa,,116,0.8189655172413793,0.23033175355450236,19329,296.23881214755033,31.093176056702365,88.20942625071136,923,28,290,36,unknown,mosi,agramfort,false,,0,0,0,0,44,false,true,false,false,0,0,0,0,0,0,864
625146,scikit-learn/scikit-learn,python,194,1307040771,1307353295,1307353295,5208,5208,github,false,false,false,63,4,1,0,3,0,3,0,3,0,0,4,4,4,0,0,0,0,4,4,4,0,0,146,18,1868,73,13.61314494256602,0.6220230519031559,48,vlad@vene.ro,scikits/learn/src/BallTree.h|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|scikits/learn/tests/test_neighbors.py,31,0.02759276879162702,0,0,false,query_radius updates I added two small but useful features to the query_radius function in BallTree - the ability to specify a different search radius for each query point - the ability to return distances as well as indices of neighborsThis was in response to a request from a researcher at Argonne NL whos using the BallTree code to process climate-related radar maps,,115,0.8173913043478261,0.22645099904852523,13338,288.1241565452092,29.389713600239915,82.09626630679261,922,28,289,15,unknown,jakevdp,ogrisel,false,ogrisel,1,1.0,291,0,22,true,true,false,false,3,15,1,0,2,0,18
625147,scikit-learn/scikit-learn,python,193,1306988091,1307361147,1307361147,6217,6217,github,false,false,false,60,35,4,0,24,0,24,0,5,2,1,5,17,8,0,0,3,2,13,18,11,0,0,173,61,737,230,31.259422017581922,1.4283432488016297,35,vlad@vene.ro,scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/sparse/__init__.py|scikits/learn/utils/__init__.py|scikits/learn/preprocessing/setup.py|scikits/learn/preprocessing/sparse/setup.py|scikits/learn/preprocessing/src/_preprocessing.c|scikits/learn/preprocessing/src/_preprocessing.pyx|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/sparse/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py,21,0.005741626794258373,0,2,false,Preprocessing simplification Here is some early progress report on a refactoring of the preprocessing package to make it simpler by combining dense and sparse variant into consistent classesAs usual early feedback welcomedTODO before merge:- delmore tests for pathological cases/del- delnarrative documentation and more usage examples/delSparse variant for the Scaler is left for another pull request,,114,0.8157894736842105,0.23062200956937798,13361,287.9275503330589,29.413966020507445,82.02978818950677,922,28,289,14,unknown,ogrisel,mblondel,false,mblondel,11,0.7272727272727273,292,105,736,true,false,true,true,48,216,26,4,241,10,62
625148,scikit-learn/scikit-learn,python,188,1306226764,,1324557443,305511,,unknown,false,false,false,235,23,14,0,4,0,4,0,3,1,0,6,8,6,0,0,2,0,7,9,7,0,0,399,38,533,38,68.52036654619293,2.5123216430421897,250,vmic@crater2.logilab.fr,scikits/learn/grid_search.py|scikits/learn/grid_search.py|scikits/learn/tests/test_grid_search.py|scikits/learn/grid_search.py|examples/grid_search_text_feature_extraction.py|scikits/learn/grid_search.py|scikits/learn/grid_search.py|examples/grid_search_digits.py|scikits/learn/cluster/k_means_.py|scikits/learn/grid_search.py|scikits/learn/tests/test_grid_search.py|examples/cluster/plot_grid_search_clustering.py|scikits/learn/cluster/k_means_.py|examples/cluster/plot_grid_search_clustering.py|scikits/learn/tests/test_grid_search.py,168,0.0504950495049505,0,0,false,WIP: Grid search improvements Hi all I worked a bit in the plane on grid search improvements and before going further I would like to get some feedbackHere is what I changed:- the grid search now collects the raw scores and durations to be able to compute various statistics a posteriori (eg variance of the score across folds for each parameter set)- store the best set of parameters as this is what we are interested in general- added a way to handle model with unsupervised predictions (eg clustering) based on the agreement strategy we discussed earlier on the V-Measure pull request I also introduced an example for this caseTODO:- extensive test suite : the coverage is not good enough- update the documentation- how to handle unsupervised clustering scores that do not work with pairs labels (eg Calinski index)- discuss which fitted attributes to we want to keep- should we normalize all fitted attribute names to have a trailing _ as in the rested of the scikit (gsbest_score_ gsbest_estimator_)- is the iid option really important it is apparently used to weight the mean of the score per number of examples in the test folds (I think there was a bug) but makes the code more complicated for nothing since all of our cross validation iterators have equal sized test sets if I am not mistaken,,113,0.8230088495575221,0.2396039603960396,19329,296.23881214755033,31.093176056702365,88.20942625071136,911,28,280,38,unknown,ogrisel,ogrisel,true,,10,0.8,291,105,727,true,false,false,false,45,215,26,5,233,10,2217
625150,scikit-learn/scikit-learn,python,186,1306184761,1308047179,1308047179,31040,31040,github,false,false,false,114,2,0,0,24,0,24,0,5,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,171,0,0.0,0,,,0,0.0,0,9,false,Mldata The branch mldata contains a new function fetch_mldata to download  data sets from the mldataorg repository The data set is stored in cache as usual in scikitslearnMikio Braun confirmed that there is no fixed convention in mldataorg for distinguishing between target values and data nor is there a convention for the column names fetch_mldata uses default values that work with the most common data sets and accepts keyword arguments to adapt to the other casesFor example it is possible to write:from scikitslearn import datasets as dtmnist  dtfetch_mldata(MNIST original)iris  dtfetch_mldata(iris)news20  dtfetch_mldata(news20binary)but also:iris2  dtfetch_mldata(datasets-UCI iris target_name1 data_name0) iris3  dtfetch_mldata(datasets-UCI iris target_nameclass data_namedouble0) ,,112,0.8214285714285714,0.24169184290030213,13407,293.6525695532184,29.760572835086148,83.98597747445365,910,28,279,18,unknown,pberkes,ogrisel,false,ogrisel,0,0,11,4,332,false,true,false,true,0,0,0,0,8,0,1159
625151,scikit-learn/scikit-learn,python,185,1306090049,1306223926,1306223926,2231,2231,github,false,false,false,77,4,3,0,5,0,5,0,3,0,0,1,2,1,0,0,0,0,2,2,2,0,0,23,0,23,2,13.178935477396287,0.6113388924274358,58,vlad@vene.ro,scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py,58,0.05858585858585859,0,0,false,Fixing issue 82 and 183 This should fix a bug in kmeans initialisation as explained in issue 82See this thread for a demo of the bug (err Im not so familiar with unit tests sorry)I also added something to the doc-string and changed the default initialisation from random to kmeans++This should rather be seen as a suggestion I just wanted to have a little more consistency between default argumentsHope this helpsCheersAndy,,111,0.8198198198198198,0.25252525252525254,13061,288.1862032003675,29.247377689304034,81.69359160860577,906,29,278,12,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,1,1.0,137,19,212,true,true,true,false,3,1,1,0,1,0,241
625152,scikit-learn/scikit-learn,python,184,1306089460,1306354334,1306354334,4414,4414,github,false,false,false,66,27,18,0,27,0,27,0,4,4,4,11,20,10,0,0,4,4,12,20,12,0,0,1231,91,1536,119,146.97901072631714,6.818000268809195,73,vlad@vene.ro,scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/__init__.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|doc/modules/classes.rst|doc/modules/naive_bayes.rst|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/__init__.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|doc/modules/classes.rst|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py,54,0.00303951367781155,2,13,false,Multinomial naive Bayes again Dear allHeres a cleaned up version of @amitibos multinomial naive Bayes classifier It incorporates some of the features of @mblondels earlier attempt but is *much* faster on sparse data On the slowish machine Im currently behind it takes about 8s to train on 11314 documents and test on 7532 a fraction of the time it takes to vectorize themPlease check,,110,0.8181818181818182,0.25329280648429586,13061,288.1862032003675,29.247377689304034,81.69359160860577,906,29,278,13,unknown,larsmans,larsmans,true,larsmans,8,0.75,34,25,308,true,true,false,false,12,20,10,0,62,6,29
625153,scikit-learn/scikit-learn,python,182,1306003553,1306060155,1306060155,943,943,github,false,false,false,44,16,1,0,13,0,13,0,4,0,0,1,5,1,0,0,0,0,5,5,3,0,0,97,0,225,18,4.318909575230318,0.20034071364237344,10,vlad@vene.ro,scikits/learn/cross_val.py,10,0.010121457489878543,0,2,false,Boostrapping cross validator Hi I started the design of a new CV generator to implement bootstrapping Before writing the documentation and maybe more tests I would like to have your feedback on the proposed APIStatus / TODO:- delwrite documentation/del- delmore tests/del,,109,0.8165137614678899,0.2550607287449393,13009,287.87762318394954,29.05680682604351,81.17457145053424,905,29,277,12,unknown,ogrisel,ogrisel,true,ogrisel,9,0.7777777777777778,289,105,724,true,false,false,false,40,196,21,5,215,10,36
625154,scikit-learn/scikit-learn,python,181,1305936179,1305936566,1305936566,6,6,github,false,false,false,4,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.45865877572877,0.20685207709087733,19,vlad@vene.ro,scikits/learn/__init__.py,19,0.01890547263681592,0,0,true,Fix minor spelling errors ,,108,0.8148148148148148,0.2656716417910448,12965,292.4797531816429,28.769764751253376,80.98727342846124,903,29,276,12,unknown,lucaswiman,agramfort,false,agramfort,0,0,0,2,623,false,false,false,false,0,0,0,0,0,0,-1
625155,scikit-learn/scikit-learn,python,180,1305906961,1305906976,1305906976,0,0,commits_in_master,false,false,false,17,14,13,0,0,0,0,0,2,4,0,11,15,10,0,0,4,0,11,15,10,0,0,770,65,900,65,120.19859603382834,5.576414456372199,73,vlad@vene.ro,scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|doc/modules/classes.rst|doc/modules/naive_bayes.rst|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py|scikits/learn/naive_bayes/tests/__init__.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|scikits/learn/naive_bayes/tests/test_naive_bayes.py|doc/modules/classes.rst|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py|doc/modules/naive_bayes.rst|scikits/learn/naive_bayes.py|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/naive_bayes/__init__.py|scikits/learn/naive_bayes/naive_bayes.py|scikits/learn/naive_bayes/sparse/__init__.py|scikits/learn/naive_bayes/sparse/naive_bayes.py,54,0.003,0,4,true,Naive bayes Ive enhanced your naive Bayes code with optional priors and cleaned it up a bit,,107,0.8130841121495327,0.269,12965,292.4797531816429,28.769764751253376,80.98727342846124,902,29,276,12,unknown,larsmans,larsmans,true,larsmans,7,0.7142857142857143,33,25,306,true,true,false,false,11,17,9,0,52,6,-1
626149,scikit-learn/scikit-learn,python,178,1305831082,1305831428,1305831428,5,5,github,false,false,false,6,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Minor typos in covariance sphinx doc ,,106,0.8113207547169812,0.26774847870182555,12963,292.29345059014116,28.774203502275707,80.99976857208979,899,28,275,12,unknown,kwgoodman,agramfort,false,agramfort,0,0,15,2,346,false,false,false,false,0,0,0,0,0,0,-1
625158,scikit-learn/scikit-learn,python,176,1305759092,1306027056,1306027056,4466,4466,github,false,false,false,32,10,5,0,20,0,20,0,4,0,0,5,6,4,0,0,0,0,6,6,5,0,0,155,0,1150,19,31.914629605949724,1.4807642374109418,38,vlad@vene.ro,scikits/learn/src/BallTree.h|scikits/learn/src/BallTreePoint.h|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|scikits/learn/src/BallTree.h|scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx|doc/modules/neighbors.rst|scikits/learn/src/ball_tree.pyx,20,0.0163098878695209,0,1,false,Balltree wrapper I updated the cython ball_tree wrapperTwo main additions:  knn_brute: a brute-force neighbor algorithm  BallTreequery_ball : query points by specifying a radius r rather than a number of neighbors k,,105,0.8095238095238095,0.2640163098878695,12988,291.73082845703726,28.718817369879886,80.84385586695412,897,28,274,12,unknown,jakevdp,ogrisel,false,ogrisel,0,0,284,0,7,true,true,false,false,2,8,0,0,1,0,23
625159,scikit-learn/scikit-learn,python,168,1305310293,1305317301,1305317301,116,116,github,false,false,false,46,3,1,0,5,1,6,0,2,0,0,1,2,1,0,0,0,0,2,2,2,0,0,2,0,2,17,4.256710749795036,0.19929159200757054,8,vlad@vene.ro,scikits/learn/linear_model/least_angle.py,8,0.008733624454148471,0,0,true,08x: FIX: lars_path -- assure that at least some features get added if necessary  Otherwise can run into situation when max_features  n_active so add_featuresbecomes  0 thus no actual resize happens and then next assignment to coefsfailsplease cherry into master upon acceptance,,104,0.8076923076923077,0.2740174672489083,12597,308.08922759387156,27.86377708978328,82.71810748590934,895,27,269,13,unknown,yarikoptic,GaelVaroquaux,false,GaelVaroquaux,3,1.0,29,7,883,true,true,false,false,2,1,2,0,7,0,2
625160,scikit-learn/scikit-learn,python,167,1305240118,1305276486,1305276486,606,606,github,false,false,false,19,3,0,0,2,0,2,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,6,11,0,0.0,0,,,0,0.0,0,0,false,add support for n_components in KernelPCA Support for n_components in KernelPCA along with an associated test case Fixes #166,,103,0.8058252427184466,0.27673649393605293,12594,308.4008257900588,28.029220263617596,83.21422899793552,892,26,268,13,unknown,bsilverthorn,mblondel,false,mblondel,0,0,8,12,704,false,true,false,false,1,0,0,0,0,0,11
625161,scikit-learn/scikit-learn,python,164,1305179230,1305184489,1305184489,87,87,github,false,false,false,69,2,2,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,29,0,29,0,8.92369223939053,0.4177909884457169,11,vlad@vene.ro,scikits/learn/pls.py|scikits/learn/pls.py,11,0.012141280353200883,0,0,false,08x (to be cp to master as well) PLS -- use string comparisons (instead of identity checking) and few spell fixes real-life example of strings not always (I am not even talking about possible unicode here) being singletones thus inappropriate for is whenever  is intended:    (Pdb) selfalgorithm is nipals    False    *(Pdb) selfalgorithm  nipals    True    *(Pdb) print selfalgorithm    nipals    *(Pdb) print repr(selfalgorithm)    nipals    *(Pdb) print selfalgorithm__class__    type str,,102,0.803921568627451,0.27704194260485654,12594,308.16261711926313,27.870414483087185,82.73781165634429,890,26,268,13,unknown,yarikoptic,GaelVaroquaux,false,GaelVaroquaux,2,1.0,29,7,882,true,true,false,false,1,1,1,0,6,0,87
625162,scikit-learn/scikit-learn,python,163,1304946032,1304946346,1304946347,5,5,github,false,false,false,6,3,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,16,22,0,0.0,0,,,0,0.0,0,0,false,Revert yNone parameter removal in preprocessing ,,101,0.801980198019802,0.2968197879858657,12576,308.44465648854964,27.989821882951656,83.17430025445293,886,26,265,14,unknown,paolo-losi,mblondel,false,mblondel,2,1.0,5,1,370,true,true,false,false,1,14,1,0,6,0,-1
625163,scikit-learn/scikit-learn,python,162,1304936632,,1307353854,40287,,unknown,false,false,false,57,35,12,0,27,15,42,13,4,1,0,2,7,2,0,0,4,0,6,10,5,0,0,1182,0,2066,145,57.85427438768996,2.6959977026872766,14,vlad@vene.ro,scikits/learn/perceptron.py|scikits/learn/__init__.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py|scikits/learn/perceptron.py,14,0.01652892561983471,0,16,false,(Averaged) perceptrons As promised heres my library of (averaged) perceptrons adapted from initial code by Leif JohnsonStill TODO:* ~~Design an online learning interface~~* ~~Rewrite the sparse version using scipysparse (previous attempts failed as they got very slow) or Cython~~* ~~Write tests~~* ~~Add bias vector~~* Adapt to linear classifier interface (coef_ etc),,100,0.81,0.2975206611570248,12746,298.8388514043622,29.26408284952142,82.61415345990899,886,26,265,18,unknown,larsmans,larsmans,true,,6,0.8333333333333334,31,25,295,true,true,false,false,8,6,8,0,18,5,5
625164,scikit-learn/scikit-learn,python,161,1304864794,1305631947,1305631947,12785,12785,github,false,false,false,72,26,7,0,25,1,26,0,3,2,0,6,10,5,0,0,2,0,8,10,6,0,0,397,58,542,94,44.81610889621025,2.0979919174317025,30,vlad@vene.ro,scikits/learn/metrics/cluster.py|scikits/learn/metrics/__init__.py|scikits/learn/metrics/cluster.py|examples/cluster/kmeans_digits.py|examples/cluster/plot_affinity_propagation.py|scikits/learn/metrics/cluster.py|scikits/learn/metrics/tests/test_cluster.py|scikits/learn/metrics/tests/test_cluster.py|doc/modules/clustering.rst|scikits/learn/metrics/__init__.py,24,0.002380952380952381,0,2,false,V-Measure homogeneity and completeness clustering metrics Hi all I have implemented entropy based clustering metrics from the following paper:V-Measure: A conditional entropy-based external cluster evaluation measure by Andrew Rosenberg and Julia Hirschberg 2007 http://aclldcupennedu/D/D07/D07-1043pdfTest coverage is 100% some existing clustering examples have been updated to use the new metrics I wrote a short summary of the paper in the clustering documentation emphasizing the pros and cons and pep8 is happy ,,99,0.8080808080808081,0.3011904761904762,12642,300.7435532352476,27.60639139376681,80.99984179718399,886,26,264,12,unknown,ogrisel,ogrisel,true,ogrisel,8,0.75,283,105,711,true,false,false,false,34,172,17,5,182,8,1112
625165,scikit-learn/scikit-learn,python,160,1304701398,1304701498,1304701498,1,1,github,false,false,false,16,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,true,Updated READMErst Separate build and install for Unix to prevent directories cluttered with root-owned object files,,98,0.8061224489795918,0.3057553956834532,12576,308.44465648854964,27.989821882951656,83.17430025445293,882,26,262,11,unknown,larsmans,larsmans,true,larsmans,5,0.8,31,25,292,true,true,false,false,7,6,5,0,15,4,-1
625166,scikit-learn/scikit-learn,python,157,1304594251,1304602820,1304602820,142,142,github,false,false,false,36,1,0,0,2,0,2,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,1,0,false,FIX: joblib paths Think its trivial but I dont know if @GaelVaroquaux uses some script to change the import paths in which case it should be fixed thereAlso I wonder why this wasnt triggered before,,97,0.8041237113402062,0.29914529914529914,12419,313.9544246718737,28.1021016184878,84.30630485546341,881,26,261,13,unknown,fabianp,fabianp,true,fabianp,11,0.8181818181818182,62,19,355,true,true,false,false,28,106,19,2,125,0,5
625167,scikit-learn/scikit-learn,python,155,1304535519,1304599430,1304599430,1065,1065,github,false,false,false,99,4,0,0,2,0,2,0,2,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,153,44,0,0.0,0,,,0,0.0,0,0,false,ROC fixes for trivial classifiers (always predict one class) and input ch ROC fixes for trivial classifiers (always predict one class) and input checks (raise ValueError in case of multi-class)metricsroc_curve now always returns arrays of size at least 3 That is in the case of decision classifiers (classifiers which do not return confidence scores or probability estimates) (00) and (11) are included to ensure that metricsauc gives right results Additional tests for AUCTested AUC values agains SIGKDD evaluation tool perf [1] - give same results on test data supplied by perf + other boundary cases[1] http://wwwsigkddorg/kddcup/indexphpsection2004&methodsoft,,96,0.8020833333333334,0.2972636815920398,12420,313.9291465378422,28.09983896940419,84.29951690821257,880,26,260,13,unknown,pprett,pprett,true,pprett,7,0.8571428571428571,32,20,638,true,true,false,false,6,13,5,0,26,0,8
625168,scikit-learn/scikit-learn,python,154,1304521966,1304522720,1304522720,12,12,github,false,false,false,4,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,small typo in docs ,,95,0.8,0.2997512437810945,12420,313.9291465378422,28.09983896940419,84.29951690821257,879,26,260,12,unknown,larsmans,ogrisel,false,ogrisel,4,0.75,31,25,290,true,true,true,true,4,4,4,0,13,3,-1
625169,scikit-learn/scikit-learn,python,153,1304509727,1306752851,1306752851,37385,37385,github,false,false,false,114,21,1,0,41,0,41,0,7,6,0,5,19,5,0,0,8,0,13,21,8,0,0,248,49,387,76,50.485902109120985,2.3414024616416995,89,vlad@vene.ro,doc/contents.rst|doc/index.rst|doc/modules/classes.rst|doc/modules/manifold.rst|doc/unsupervised_learning.rst|examples/manifold/README.txt|examples/manifold/plot_lle_digits.py|examples/manifold/plot_swissroll.py|scikits/learn/datasets/__init__.py|scikits/learn/manifold.py|scikits/learn/tests/test_manifold.py,68,0.0,0,10,false,Initial implementation of Locally Linear Embedding First method in the manifold moduleSome things are still not implemented   - LocallyLinearEmbedding does not implement a predict method yet First because it will take me time but also because it involves refactoring some other methods in neighbors and I think its easier to review if those changes are kept separateOthers would need improvement:  - Image from plot_swissroll is a bit too flat Maybe someone with more matplotlib experience can fix that  - Olivier suggested to try fast_svd as sparse eigensolver I didnt try it Also Im interested only in the smallest nonzero eigenvalues it is not immediately obvious how to get that information with fast_svd,,94,0.7978723404255319,0.2968553459119497,13193,287.9557341014174,29.48533313120594,82.316379898431,879,26,260,15,unknown,fabianp,fabianp,true,fabianp,10,0.8,62,19,354,true,true,false,false,25,103,18,2,122,0,1101
626153,scikit-learn/scikit-learn,python,152,1304507123,1304510623,1304510623,58,58,merged_in_comments,false,false,false,8,1,1,0,2,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,17,0,17,0,4.2198446525926,0.19846135891544367,10,vlad@vene.ro,scikits/learn/src/ball_tree.cpp|scikits/learn/src/ball_tree.pyx,10,0.012626262626262626,0,0,false,prevent SEGV when BallTree initialization fails Fixes #146,,93,0.7956989247311828,0.29545454545454547,12420,313.9291465378422,28.09983896940419,84.29951690821257,879,26,260,12,unknown,thouis,,false,,1,1.0,5,7,177,true,false,false,false,2,0,1,0,5,0,57
625171,scikit-learn/scikit-learn,python,151,1304502294,,1304504771,41,,unknown,false,false,false,34,1,1,0,9,0,9,0,3,0,0,1,1,1,0,0,0,0,1,1,1,0,0,6,0,6,0,3.901117858837448,0.18347356697702993,14,vlad@vene.ro,scikits/learn/pipeline.py,14,0.017834394904458598,0,2,false,Allow methods that dont implement transform (but do implement a fit_transoform) in Pipeline I think this is just a bug in the __init__ logic Someone with more experience in the Pipeline can hopefully tell,,92,0.8043478260869565,0.2968152866242038,12419,313.9544246718737,28.1021016184878,84.30630485546341,879,26,260,11,unknown,fabianp,fabianp,true,,9,0.8888888888888888,62,19,354,true,true,false,false,24,101,17,2,121,0,2
625172,scikit-learn/scikit-learn,python,150,1304458139,1304597697,1304597697,2325,2325,github,false,false,false,55,10,3,0,15,0,15,0,3,1,0,9,11,9,0,0,1,0,10,11,9,0,0,386,0,957,0,66.87068396490866,3.1449890693999394,24,vlad@vene.ro,benchmarks/bench_sgd_regression.py|doc/modules/sgd.rst|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pxd|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sgd_fast_sparse.pyx|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py,16,0.011479591836734694,0,10,false,Learningrate Different learning rate schedules for SGD moduleFeatures:  * constant: via parameter eta0  * optimal: the schedule that was currently used (similar to Leon Bottous sgd and PEGASOS)  * invscaling: inverse scaling schedule used by vowpal wabitFor classification optimal is default for regression invscaling is default sgdrst includes some documentation in Math formulation,,91,0.8021978021978022,0.29846938775510207,12420,313.9291465378422,28.09983896940419,84.29951690821257,879,26,259,13,unknown,pprett,ogrisel,false,ogrisel,6,0.8333333333333334,32,20,637,true,true,true,true,5,8,4,0,23,0,769
625173,scikit-learn/scikit-learn,python,149,1304455408,,1304455730,5,,unknown,false,false,false,52,43,43,0,0,0,0,0,1,5,4,48,57,49,0,1,5,4,48,57,49,0,1,3788,493,3788,493,455.2879496791782,21.41265841022057,93,vlad@vene.ro,scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|doc/modules/sgd.rst|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_face_recognition_convolutional_features.py|examples/applications/plot_face_recognition_convolutional_features.py|examples/applications/plot_face_recognition_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_face_recognition_convolutional_features.py|scikits/learn/feature_extraction/image.py|.gitignore|examples/applications/plot_face_recognition_convolutional_features.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|examples/applications/image_classification_convolutional_features.py|examples/applications/test_image_classification_convolutional_features.py|examples/applications/plot_image_classification_convolutional_features.py|examples/applications/image_classification_convolutional_features.py|examples/applications/plot_image_classification_convolutional_features.py|examples/applications/test_image_classification_convolutional_features.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|examples/applications/plot_image_classification_convolutional_features.py|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pxd|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sgd_fast_sparse.pyx|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|benchmarks/bench_sgd_regression.py|doc/modules/sgd.rst|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|.gitignore|examples/applications/plot_image_classification_convolutional_features.py|scikits/learn/feature_extraction/image.py|scikits/learn/feature_extraction/tests/test_image.py|doc/sphinxext/gen_rst.py|examples/applications/plot_species_distribution_modeling.py|examples/cluster/plot_ward_structured_vs_unstructured.py|examples/covariance/plot_lw_vs_oas.py|examples/plot_rfe_with_cross_validation.py|scikits/learn/__init__.py|scikits/learn/covariance/empirical_covariance_.py|scikits/learn/covariance/shrunk_covariance_.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/externals/joblib/__init__.py|scikits/learn/externals/joblib/disk.py|scikits/learn/externals/joblib/format_stack.py|scikits/learn/externals/joblib/func_inspect.py|scikits/learn/externals/joblib/hashing.py|scikits/learn/externals/joblib/logger.py|scikits/learn/externals/joblib/memory.py|scikits/learn/externals/joblib/my_exceptions.py|scikits/learn/externals/joblib/numpy_pickle.py|scikits/learn/externals/joblib/parallel.py|scikits/learn/externals/joblib/test/common.py|scikits/learn/externals/joblib/test/test_format_stack.py|scikits/learn/externals/joblib/test/test_func_inspect.py|scikits/learn/externals/joblib/test/test_logger.py|scikits/learn/externals/joblib/test/test_memory.py|scikits/learn/externals/joblib/test/test_my_exceptions.py|scikits/learn/externals/joblib/test/test_numpy_pickle.py|scikits/learn/externals/joblib/test/test_parallel.py|scikits/learn/feature_selection/tests/test_feature_select.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/tests/test_least_angle.py|scikits/learn/linear_model/tests/test_sgd.py|scikits/learn/svm/sparse/classes.py|scikits/learn/svm/tests/test_sparse.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/tests/test_pls.py,16,0.00510204081632653,0,0,false,Learningrate Different learning rate schedules for SGD module Features:  constant: via parameter eta0  optimal: the schedule that was currently used (similar to Leon Bottous sgd and PEGASOS)  invscaling: inverse scaling schedule used by vowpal wabitFor classification optimal is default for regression invscaling is default sgdrst includes some documentation in Math formulation ,,90,0.8111111111111111,0.29846938775510207,12419,313.9544246718737,28.1021016184878,84.30630485546341,879,26,259,10,unknown,pprett,pprett,true,,5,1.0,32,20,637,true,true,false,false,4,7,3,0,23,0,-1
625174,scikit-learn/scikit-learn,python,148,1304446071,1304446369,1304446369,4,4,github,false,false,false,34,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,5,0,0,0.0,0,,,0,0.0,0,0,false,RF: use jobliblogger submodule itself while accessing its function in grid_search That would make it easier to allow using system-wide installed joblib on Debianinstallations which provide only rudimentary externals/joblib/__init__pyPlease cherry-pick into 08X,,89,0.8089887640449438,0.29653401797175866,12419,313.9544246718737,28.1021016184878,84.30630485546341,879,26,259,10,unknown,yarikoptic,GaelVaroquaux,false,GaelVaroquaux,1,1.0,29,7,873,true,true,false,false,0,0,0,0,1,0,4
625175,scikit-learn/scikit-learn,python,147,1304434961,1304435338,1304435338,6,6,github,false,false,false,9,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0.0,0,,,0,0.0,0,0,false,Copy editing in developers docs Some really minor stuff,,88,0.8068181818181818,0.2966321243523316,12419,313.9544246718737,28.1021016184878,84.30630485546341,879,26,259,10,unknown,larsmans,ogrisel,false,ogrisel,3,0.6666666666666666,31,25,289,true,true,true,true,3,4,3,0,7,3,-1
625176,scikit-learn/scikit-learn,python,144,1304337476,1304364883,1304364883,456,456,github,false,false,false,51,1,0,0,1,0,1,0,1,0,0,0,3,0,0,0,0,0,3,3,3,0,0,0,0,516,0,0,0.0,0,,,0,0.0,0,0,false,Make ball tree code safer and 64-bit clean Use size_t instead of a mixture of int/long int On most 64 bit platformssize_t is 64 bits while int is 31 bits + sign The size of long int differsper platform even among 64 bit onesRebuilt with cython --cplus ball_treepyx,,87,0.8045977011494253,0.299738219895288,12415,314.0555779299235,28.11115585984696,84.33346757954088,879,26,258,10,unknown,larsmans,fabianp,false,fabianp,2,0.5,31,25,288,false,true,true,false,2,3,2,0,0,0,456
625177,scikit-learn/scikit-learn,python,143,1304333599,,1343390841,650954,,unknown,false,false,false,30,2,1,0,8,0,8,0,5,0,0,3,6,3,0,0,1,0,6,7,7,0,0,206,0,672,0,13.901506461944601,0.5097032797057439,49,vlad@vene.ro,scikits/learn/svm/src/liblinear/liblinear_helper.c|scikits/learn/svm/src/libsvm/libsvm_helper.c|scikits/learn/svm/src/libsvm/libsvm_sparse_helper.c,36,0.03795811518324607,0,3,false,Cleanup lib{linearsvm} C helper routines I deleted my repo and reapplied my changes in proper order Heres two patch to the lib{linearsvm} helper routines and the actual codebase of liblinear,,86,0.813953488372093,0.299738219895288,19329,296.23881214755033,31.093176056702365,88.20942625071136,879,26,258,110,unknown,larsmans,larsmans,true,,1,1.0,31,25,288,false,true,false,false,1,2,1,0,0,0,52
625178,scikit-learn/scikit-learn,python,141,1304005372,1304331916,1304331916,5442,5442,github,false,false,false,56,3,1,0,5,0,5,0,4,0,0,1,2,1,0,0,0,0,2,2,1,0,0,7,0,7,0,4.367612437939955,0.2054121177274048,39,vmic@crater2.logilab.fr,scikits/learn/datasets/lfw.py,39,0.048628428927680795,0,4,false,Fixed a whole load of memory leaks in libsvm/liblinear interface HiI caught a bunch of potential memory leaks in the C and C++ code that interfaces with liblinear and libsvm and fixed them I also took the liberty of cleaning that code while I was at itRegardsLars BuitinckScientific programmer University of Amsterdam,,85,0.8117647058823529,0.314214463840399,12413,314.10617900588096,28.11568516877467,84.34705550632401,875,26,254,10,unknown,larsmans,ogrisel,false,ogrisel,0,0,30,25,284,false,true,true,true,0,0,0,0,0,0,1891
625179,scikit-learn/scikit-learn,python,140,1303988570,1304086341,1304086341,1629,1629,github,false,false,false,41,1,0,0,5,0,5,0,3,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,7,0,0,0.0,0,,,0,0.0,1,0,false,Do not open file write file until download is complete @ogrisel: is this OKThis way we avoid (or more precisely minimize) the need to deal withpartially downloaded files and the errors that arise when youControl-C a started download,,84,0.8095238095238095,0.315,12384,312.82299741602066,28.18152454780362,83.81782945736434,875,26,254,10,unknown,fabianp,GaelVaroquaux,false,GaelVaroquaux,8,0.875,62,19,348,true,true,true,true,19,107,15,2,137,0,2
625180,scikit-learn/scikit-learn,python,137,1302965163,1302968476,1302968476,55,55,github,false,false,false,36,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,8,0,0,0.0,0,,,0,0.0,0,0,false,Removed typo from nearest neighbours example The nearest neighbours example seems to have been copied from the SVM exampleThere were some references to SVMs and support vectors in this example which I removedCheersAndy,,82,0.8170731707317073,0.35974025974025975,12305,299.2279561154003,28.199918732222674,80.86143843965868,853,24,242,10,unknown,amueller,GaelVaroquaux,false,GaelVaroquaux,0,0,127,17,176,false,true,true,false,0,0,0,0,0,0,-1
625181,scikit-learn/scikit-learn,python,136,1302695240,,1309046491,105854,,unknown,false,false,false,64,2,1,0,8,2,10,0,5,0,0,1,2,1,0,0,0,0,2,2,2,0,0,157,0,207,4,4.42388196851372,0.21083264948223487,42,vlad@vene.ro,scikits/learn/linear_model/ridge.py,42,0.05419354838709677,0,1,false,Refactoring in ridgepy Id like to expose the computational routines behind Ridge*  as they might be useful in some manifold routines (among others) Also minor changes like lazy import of scipysparse and docstring changesAs a future note it might be useful to add the auto to {cg default} as keywords for solve and rename default to something more explicit like cholesky or dense ,,81,0.8271604938271605,0.3574193548387097,12305,298.4152783421373,28.11865095489638,80.53636733035351,850,26,239,20,unknown,fabianp,fabianp,true,,7,1.0,60,19,333,true,true,false,false,18,95,10,2,142,0,8
625182,scikit-learn/scikit-learn,python,135,1302608992,1303813141,1303813141,20069,20069,github,false,false,false,51,63,42,0,40,0,40,0,6,11,2,39,56,25,1,3,13,2,43,58,28,1,3,409,120,645,305,370.1567249590202,17.55861371225403,193,vlad@vene.ro,scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/linear_model/sparse/tests/test_logistic.py|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py|doc/images/plot_face_recognition_1.png|doc/images/plot_face_recognition_2.png|doc/modules/datasets.rst|doc/modules/decomposition.rst|doc/whats_new.rst|examples/applications/face_recognition.py|examples/applications/plot_face_recognition.py|doc/modules/classes.rst|doc/modules/decomposition.rst|doc/modules/clustering.rst|doc/conf.py|doc/modules/classes.rst|doc/modules/clustering.rst|scikits/learn/covariance/__init__.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/sparse/__init__.py|scikits/learn/svm/__init__.py|scikits/learn/svm/sparse/__init__.py|doc/modules/classes.rst|doc/modules/gaussian_process.rst|doc/modules/sgd.rst|doc/sphinxext/numpy_ext/docscrape_sphinx.py|doc/whats_new.rst|examples/applications/README.txt|examples/cluster/README.txt|examples/mixture/README.txt|scikits/learn/decomposition/fastica_.py|scikits/learn/feature_extraction/text.py|doc/developers/performance.rst|doc/developers/performance.rst|scikits/learn/decomposition/nmf.py|doc/developers/index.rst|doc/developers/performance.rst|doc/developers/performance.rst|doc/developers/performance.rst|scikits/learn/decomposition/nmf.py|scikits/learn/decomposition/tests/test_nmf.py|doc/modules/linear_model.rst|doc/developers/performance.rst|doc/developers/index.rst|doc/developers/performance.rst|doc/index.rst|doc/performance.rst|doc/modules/decomposition.rst|examples/decomposition/README.txt|examples/decomposition/plot_ica_blind_source_separation.py|examples/decomposition/plot_ica_vs_pca.py|examples/decomposition/plot_kernel_pca.py|examples/decomposition/plot_nmf.py|examples/decomposition/plot_pca_vs_lda.py|doc/sphinxext/gen_rst.py|doc/sphinxext/gen_rst.py|doc/conf.py|doc/user_guide.rst|doc/whats_new.rst|doc/developers/index.rst|doc/conf.py|doc/includes/big_toc_css.rst|doc/includes/bigger_toc_css.rst|doc/index.rst|doc/model_selection.rst|doc/modules/classes.rst|doc/supervised_learning.rst|doc/unsupervised_learning.rst|doc/modules/clustering.rst|doc/developers/index.rst|doc/developers/index.rst|doc/themes/scikit-learn/static/nature.css_t|doc/developers/index.rst|doc/developers/index.rst|doc/themes/scikit-learn/layout.html|doc/themes/scikit-learn/static/nature.css_t|doc/developers/index.rst|doc/developers/neighbors.rst|doc/index.rst|scikits/learn/decomposition/nmf.py|scikits/learn/decomposition/tests/test_nmf.py|scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/tests/test_logistic.py,41,0.006402048655569782,0,12,false,L1 logreg min c the minimum value for C that yields a not null modelYou can find the calculation on [1]The advantage of having a lower bound on Cobviously allows for an easier search on the parameterspacePS: Thanks Alexandre and Oliver for the review[1] https://docsgooglecom/leafid0B71d1mificKaNWY3NDY5YzktOTkzYi00ZmYyLWFmMzYtMTMwNTEzZjIwMTE4,,80,0.825,0.3546734955185659,12238,303.3175355450237,28.1091681647328,80.97728386991339,848,26,238,10,unknown,paolo-losi,fabianp,false,fabianp,1,1.0,5,1,343,false,true,false,false,0,0,0,0,0,0,5
625183,scikit-learn/scikit-learn,python,134,1302253207,1302412151,1302412151,2649,2649,commits_in_master,false,false,false,43,41,36,0,21,0,21,0,7,30,12,20,63,35,0,0,30,12,21,63,35,0,0,4522,173,4528,195,540.0528324131132,25.97020650952499,13,vlad@vene.ro,scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/bagging/__init__.py|scikits/learn/ensemble/bagging/bagging.py|scikits/learn/ensemble/boosting.py|scikits/learn/ensemble/boosting/__init__.py|scikits/learn/ensemble/boosting/gradboost.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/committee.py|scikits/learn/ensemble/bagging.py|scikits/learn/ensemble/boosting.py|testbdt.py|scikits/learn/decisiontree/tests/test_decisiontree.py|scikits/learn/boosting/__init__.py|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/base.py|scikits/learn/decisiontree/__init__.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/src/BDT.cpp|scikits/learn/decisiontree/src/BDT.h|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/bagging/__init__.py|scikits/learn/bagging/bagging.py|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/gradboost.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/decisiontree/src/tmp/BDT.cpp|scikits/learn/decisiontree/src/tmp/BDT.h|scikits/learn/decisiontree/tests/__init__.py|scikits/learn/setup.py|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.c|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/boosting/adaboost.py|scikits/learn/boosting/base.py|scikits/learn/bagging/bagging.py|scikits/learn/boosting/__init__.py|scikits/learn/boosting/gradboost.py|scikits/learn/ensemble/__init__.py|scikits/learn/ensemble/bagging/__init__.py|scikits/learn/ensemble/bagging/bagging.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/__init__.py|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/ensemble/boosting/gradboost.py|scikits/learn/ensemble/committee.py|scikits/learn/ensemble/base.py|scikits/learn/ensemble/base.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.c|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/setup.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/decisiontree/__init__.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/decisiontree.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|testbdt.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py|scikits/learn/decisiontree/decisiontree.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/ensemble/base.py|scikits/learn/ensemble/boosting/adaboost.py|scikits/learn/ensemble/committee.py|testbdt.py|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree.pyx|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/src/Histogram.h|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/tmp/BDT.cpp|scikits/learn/decisiontree/src/tmp/BDT.h|testbdt.py|scikits/learn/decisiontree/libdecisiontree.cpp|scikits/learn/decisiontree/libdecisiontree_helper.cpp|scikits/learn/decisiontree/setup.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/decisiontree/src/Node.h|scikits/learn/decisiontree/src/Object.h|testbdt.py|scikits/learn/decisiontree/src/Node.cpp|scikits/learn/ensemble/boosting/adaboost.py|testbdt.py,13,0.0,0,2,true,Decision Trees AdaBoost and Bagging I have implemented decision trees from scratch in C++ which is interfaced with CythonAdaBoost and bagging are implemented in a new module scikitslearnensemble An implementation of gradient boosting will come soonFor preliminary tests/sample usage see:scikit-learn/scikits/learn/decisiontree/tests/test_decisiontreepy,,79,0.8227848101265823,0.3763586956521739,12328,298.83192731992216,28.309539260220635,80.54834523036989,846,26,234,9,unknown,ndawe,ndawe,true,ndawe,0,0,8,26,420,true,true,false,false,0,0,0,0,35,0,62
625184,scikit-learn/scikit-learn,python,133,1302233412,1312048074,1312048074,163577,163577,commits_in_master,false,false,false,46,19,13,0,14,0,14,0,8,14,0,11,31,17,0,0,21,3,14,38,23,0,1,1980,0,3583,335,152.2832806305148,6.502717504394022,86,vlad@vene.ro,scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/base.py|scikits/learn/setup.py|scikits/learn/tree_model/_tree.cpp|scikits/learn/tree_model/normalise.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/base.py|scikits/learn/tree_model/classifier.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/setup.py|scikits/learn/tree_model/tests/__init__.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/base.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/datasets/base.py|examples/tree_model/plot_decision_tree_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/tree.py|scikits/learn/tree_model/criteria.py|scikits/learn/tree_model/randomforest.py|examples/tree_model/plot_decision_tree_iris.py|examples/tree_model/plot_random_forest_iris.py|scikits/learn/tree_model/__init__.py|scikits/learn/tree_model/_tree.c|scikits/learn/tree_model/_tree.pyx|scikits/learn/__init__.py,47,0.0,0,5,false,Enh/randomforest first pass at decision tree and random forest classifier derived from milk im sort of working on this in spare moments so just let me know what else i should take care of before a merge to master can be done (ie docs tests etc) ,,78,0.8205128205128205,0.37945205479452054,15492,284.79215078750326,29.43454686289698,83.13968499870901,846,26,233,26,unknown,satra,satra,true,satra,1,1.0,31,2,445,true,true,false,false,0,0,1,0,12,0,24
625185,scikit-learn/scikit-learn,python,132,1302215462,1305679595,1305679595,57735,57735,github,false,false,false,168,100,0,0,30,1,31,0,4,0,0,0,94,0,0,0,3,0,94,97,74,1,2,0,0,2093,460,0,0.0,0,,,0,0.0,0,1,false,Minibatch k-means I havent written documentations yet but I would appreciate feedback on the codeThis patch implements the minibatch k-means (or fast k-means) of : http://wwweecstuftsedu/~dsculley/papers/fastkmeanspdfI implemented a new class MiniBatchKMeans that inherits from KMeans and takes an extra arguments: chunk This arguments sets the size of the sample data (in the article chosen randomly among the data) Ive modified the function k_means to take two extra arguments: chunk and batch (thought we might want to set chunk to 0 per default and get rid of batch and use the batch k means step computation only when chunk is not set to 0) The kmeans algorithm now computes the step differently whether it is a batch k-means or the Lloyds oneThe data from the example is a bit small IMO to run the batch k-means algorithm but we still have some interesting results (here is the results of the example file: http://bpastenet/show/15236/)ThanksEDIT: OG: I fixed the confusing name of the pull request,,77,0.8181818181818182,0.3841886269070735,12757,304.06835462883123,27.59269420710198,81.99419926314964,846,26,233,13,unknown,NelleV,ogrisel,false,ogrisel,0,0,18,13,444,false,true,false,false,0,0,0,0,0,0,340
625186,scikit-learn/scikit-learn,python,131,1302084998,1302179431,1302179431,1573,1573,commit_sha_in_comments,false,false,false,4,4,4,0,1,0,1,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,4,0,4,0,8.928261254961086,0.43032786527794026,16,vlad@vene.ro,scikits/learn/linear_model/sparse/logistic.py|scikits/learn/cluster/affinity_propagation_.py,12,0.01694915254237288,0,0,false,Update docstring in linear_modellogistic ,,76,0.8157894736842105,0.3898305084745763,12325,298.8235294117647,28.316430020283978,80.48681541582151,846,26,232,8,unknown,fannix,GaelVaroquaux,false,GaelVaroquaux,1,1.0,1,0,84,true,true,false,false,0,7,1,0,1,0,1572
625187,scikit-learn/scikit-learn,python,130,1301940549,1302265465,1326758139,413626,5415,github,false,false,false,87,9,1,0,39,3,42,0,5,1,0,0,5,1,0,0,3,0,4,7,5,0,0,181,0,274,227,4.879337624297443,0.23253850086348535,0,,scikits/learn/utils/logger.py,0,0.0,0,0,false,WIP : logger utility for scikit learn This is made to fit where print is used and allows for several loggers(rework of https://githubcom/scikit-learn/scikit-learn/pull/125 )2 examples of how this could be used : - https://githubcom/feth/scikit-learn/commit/e32c54372875c83deaa84a8129839bdcb93eb0cf (simple)- and https://githubcom/feth/scikit-learn/commit/b744f8cc97f32ddf0a81df2431c9d6ba6c65adc4 (more elaborate) -what do you thinkAs suggested by Gael Varoquaux there is the drop in replacement for print : log() but this leaves room for people who want to fine tune their logging (domain criticity and threshold) or benefit of the string formatting being performed late,,75,0.8133333333333334,0.40822320117474303,12305,299.2279561154003,28.199918732222674,80.86143843965868,846,27,230,46,unknown,feth,fabianp,false,fabianp,1,0.0,8,3,85,true,true,false,false,1,2,1,0,2,0,1786
625188,scikit-learn/scikit-learn,python,129,1301934022,1302200868,1302200868,4447,4447,github,false,false,false,44,13,4,0,7,0,7,0,3,6,0,16,27,14,0,0,7,0,21,28,19,0,0,49,34,127,78,75.86589259026105,3.6565911663992248,65,vlad@vene.ro,doc/modules/classes.rst|doc/modules/decomposition.rst|doc/unsupervised_learning.rst|examples/applications/plot_face_recognition.py|examples/cluster/kmeans_digits.py|examples/plot_ica_vs_pca.py|examples/plot_nmf.py|examples/plot_pca.py|scikits/learn/__init__.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/fastica.py|scikits/learn/decomposition/nmf.py|scikits/learn/decomposition/pca.py|scikits/learn/decomposition/tests/test_fastica.py|scikits/learn/decomposition/tests/test_nmf.py|scikits/learn/decomposition/tests/test_pca.py|scikits/learn/tests/test_pipeline.py|scikits/learn/decomposition/__init__.py|scikits/learn/decomposition/fastica.py|scikits/learn/decomposition/nmf.py|scikits/learn/decomposition/pca.py|scikits/learn/decomposition/tests/test_fastica.py|scikits/learn/decomposition/tests/test_nmf.py|scikits/learn/decomposition/tests/test_pca.py,28,0.0,1,0,false,Decomposition Here it is I think I fixed every reference except in an old changelog that I dont think is appropriate to touchAs for the components_ shape and the PEP8 of decomposition/test/test_pcapy I am waiting for the merge of @mblondel s Kernel PCA,,74,0.8108108108108109,0.41002949852507375,12325,298.8235294117647,28.316430020283978,80.48681541582151,846,27,230,9,unknown,vene,vene,true,vene,3,1.0,17,14,358,true,true,false,false,4,24,5,0,66,0,36
625189,scikit-learn/scikit-learn,python,128,1301931922,1303897839,1303897839,32765,32765,github,false,false,false,78,11,4,0,30,0,30,0,4,9,4,5,23,13,0,0,9,4,10,23,14,0,0,1426,324,2034,543,88.7047740346537,4.2656591388794105,4,vlad@vene.ro,examples/covariance/README.txt|examples/covariance/plot_covariance_estimation.py|examples/covariance/plot_lw_vs_oas.py|examples/plot_covariance_estimation.py|scikits/learn/covariance/__init__.py|scikits/learn/covariance/base_covariance_.py|scikits/learn/covariance/covariance.py|scikits/learn/covariance/ledoit_wolf.py|scikits/learn/covariance/shrunk_covariance_.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/covariance/tests/test_ledoit_wolf.py|examples/plot_covariance_estimation.py|examples/plot_lw_vs_oas.py|scikits/learn/covariance/__init__.py|scikits/learn/covariance/covariance.py|scikits/learn/covariance/ledoit_wolf.py|scikits/learn/covariance/oas.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/covariance/__init__.py|scikits/learn/covariance/covariance.py|scikits/learn/covariance/ledoit_wolf.py|scikits/learn/covariance/shrunk_covariance.py|scikits/learn/covariance/tests/test_covariance.py|scikits/learn/covariance/tests/test_ledoit_wolf.py,4,0.0,0,3,false,Covariance Refactoring of the covariance module:- Basic (MLE) covariance -- covariance_py- Shrunk covariances (Ledoit-Wolf OAS) -- shrunk_covariance_py- Examples -- examples/covarianceNovelties:- OAS (shrinkage type) (see Chen et al paper Shrinkage Algorithm for MMSE Covariance Estimation)- New example (Ledoit-Wolf and OAS comparison  replication of a Chens experiment can be seen as a test)I wanted to have only two commits one for refactoring one for adding OAS but git wished for four ),,73,0.8082191780821918,0.4108983799705449,12328,298.83192731992216,28.309539260220635,80.54834523036989,846,27,230,12,unknown,VirgileFritsch,fabianp,false,fabianp,0,0,1,0,336,true,true,false,false,0,0,0,0,12,0,2489
625190,scikit-learn/scikit-learn,python,127,1301914598,1301924111,1301924111,158,158,github,false,false,false,29,1,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,14,0,0,0.0,0,,,0,0.0,0,0,false,FIX: very confusing internal naming in NMF Minor fix that fixes some weird internal naming in my code that did not affect the interface(n_features - n_samples in fit_transform),,72,0.8055555555555556,0.4157973174366617,12159,301.17608355950324,28.456287523645038,81.17443868739205,845,27,230,7,unknown,vene,vene,true,vene,2,1.0,17,14,358,true,true,false,false,3,19,2,0,61,0,38
625191,scikit-learn/scikit-learn,python,125,1301702936,,1301940608,3961,,unknown,false,false,false,22,1,1,0,3,1,4,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,0,123,0,123,0,4.912805542376388,0.2379635290820171,0,,scikits/learn/utils/logger.py,0,0.0,0,0,true,logger utility for scikit learn This is made to fit where print is usedShould imho be extended as written in docstring,,71,0.8169014084507042,0.4200680272108844,11913,304.0376059766641,27.616889112733986,82.01124821623438,843,27,227,15,unknown,feth,feth,true,,0,0,8,3,82,true,true,false,false,0,0,0,0,2,0,714
626157,scikit-learn/scikit-learn,python,124,1301701784,1301743279,1301743279,691,691,github,false,false,false,15,11,0,0,0,0,0,0,1,0,0,0,13,0,0,0,0,0,13,13,13,0,0,0,0,348,0,0,0.0,0,,,0,0.0,0,0,true,more pep8 safe files not much here sorry I came in late to the sprint,,70,0.8142857142857143,0.4186440677966102,11913,304.0376059766641,27.616889112733986,82.01124821623438,843,27,227,12,unknown,npinto,GaelVaroquaux,false,GaelVaroquaux,0,0,35,34,834,false,true,false,false,0,0,0,0,0,0,-1
625193,scikit-learn/scikit-learn,python,123,1301696253,1301801376,1301801376,1752,1752,github,false,false,false,26,71,34,0,30,5,35,5,4,5,1,5,13,6,0,0,5,1,7,13,6,0,0,2040,211,2564,321,182.2847890945308,8.848951351340904,9,ronweiss@gmail.com,benchmarks/bench_plot_nmf.py|examples/plot_nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py|examples/plot_nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/tests/test_nmf.py|benchmarks/bench_plot_nmf.py|scikits/learn/nmf.py|scikits/learn/__init__.py|scikits/learn/cro.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/cro.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py,9,0.0,0,2,true,NMF lite branch I created a branch with all of the NMF code except for CRO hierarchical clustering which is slow and impractical at the moment,,69,0.8115942028985508,0.4265975820379965,11858,305.44779895429247,27.744982290436834,82.39163433968629,843,27,227,11,unknown,vene,ogrisel,false,ogrisel,1,1.0,17,14,355,true,true,true,true,2,3,1,0,21,0,58
625194,scikit-learn/scikit-learn,python,122,1301684477,1301763239,1301763239,1312,1312,github,false,false,false,10,2,0,0,1,0,1,0,2,0,0,0,12,0,0,0,0,0,12,12,1,0,0,0,0,67,0,0,0.0,0,,,0,0.0,0,0,true,Completed task Multiple figures in documentation examples Yes I did,,68,0.8088235294117647,0.42702702702702705,11913,304.0376059766641,27.616889112733986,82.01124821623438,843,27,227,11,unknown,mike-perdide,GaelVaroquaux,false,GaelVaroquaux,1,0.0,9,2,243,true,true,false,false,1,0,1,0,1,0,1153
625196,scikit-learn/scikit-learn,python,121,1301684210,,1301684347,2,,unknown,false,false,false,7,3,3,0,0,0,0,0,0,0,0,5,5,5,0,0,0,0,5,5,5,0,0,101,0,101,0,22.957313348387327,1.111992578477149,40,thouis@gmail.com,doc/sphinxext/gen_rst.py|scikits/learn/externals/joblib/__init__.py|scikits/learn/externals/joblib/parallel.py|scikits/learn/neighbors.py|scikits/learn/src/ball_tree.pyx,37,0.0036036036036036037,0,0,true,Completed task Multiple figures in documentation examples ,,67,0.8208955223880597,0.42702702702702705,11913,304.0376059766641,27.616889112733986,82.01124821623438,843,27,227,9,unknown,mike-perdide,mike-perdide,true,,0,0,9,2,243,true,true,false,false,0,0,0,0,1,0,-1
626158,scikit-learn/scikit-learn,python,120,1301676751,1301677495,1301680298,59,12,github,false,false,false,16,2,0,0,0,0,0,0,0,0,0,0,7,0,0,0,2,1,6,9,7,0,0,0,0,1346,0,0,0.0,0,,,0,0.0,0,0,true,Wrapped BallTree in Cython Note that this wrapper does not include the BruteForceNeighbors()function from BallTreeh,,66,0.8181818181818182,0.43609022556390975,11620,306.79862306368335,27.796901893287437,83.30464716006884,843,25,227,11,unknown,thouis,fabianp,false,fabianp,0,0,5,6,144,false,false,false,true,0,0,0,0,0,0,-1
625198,scikit-learn/scikit-learn,python,119,1301674988,1301803508,1301803508,2142,2142,github,false,false,false,73,1,0,0,6,0,6,0,4,0,0,0,3,0,0,0,0,2,1,3,4,0,0,0,0,308,13,0,0.0,0,,,0,0.0,0,0,true,[feature_extraction] Refactor text/* to textpy Hi everybodyAfter discussion with Olivier Mathieu Gaël and Nicolas we propose the following pull requestthat refactor the feature_extraction/text module in a single textpy fileWe also remove the dense version and keep only the sparse version of each method This allows to remove some abstract classes eg BaseCountVectorizer to keep only CountVectorizerWe are also planning to refactor some functions to expose a simple APIVincent,,65,0.8153846153846154,0.4339622641509434,11595,307.46011211729194,27.85683484260457,83.48426045709357,843,25,227,13,unknown,vmichel,ogrisel,false,ogrisel,1,0.0,5,1,302,true,true,false,true,1,3,1,0,2,0,8
625199,scikit-learn/scikit-learn,python,118,1301674858,,1301676434,26,,unknown,false,false,false,72,1,1,0,0,0,0,0,1,11,0,1,12,12,0,0,11,0,1,12,12,0,0,891,111,891,111,50.89113209606233,2.5479388467293433,0,,examples/cluster/plot_lena_ward.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|examples/plot_ward_feature_agglomeration.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/_inertia.c|scikits/learn/cluster/_inertia.pyx|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/feature_agglomeration/__init__.py|scikits/learn/feature_agglomeration/feature_agglomeration.py|scikits/learn/feature_agglomeration/test/test_feature_agglomeration.py,0,0.0,0,0,true,Refactor text feature extraction Hi everybodyAfter discussion with Olivier Mathieu Gaël and Nicolas we propose the following pull requestthat refactor the feature_extraction/text module in a single textpy fileWe also remove the dense version and keep only the sparse version of each method This allows to remove some abstract classes eg BaseCountVectorizer to keep only CountVectorizerWe are also planning to refactor some functions to expose a simple APIVincent,,64,0.828125,0.4339622641509434,11595,307.46011211729194,27.85683484260457,83.48426045709357,843,25,227,12,unknown,vmichel,vmichel,true,,0,0,5,1,302,true,true,false,false,0,3,0,0,2,0,-1
625200,scikit-learn/scikit-learn,python,117,1301673975,1301675324,1302289594,10260,22,github,false,false,false,76,1,0,0,1,0,1,0,2,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,12,0,0,0.0,0,,,0,0.0,0,0,true,Initial implementation of cross validated SVC Ok this still needs some work This pull request is just to share some ideas and because of the lookie what I did factorOn the API Im still not sure that its a great idea to pass multiple parameters as lists or tuples which is how its implemented right now Its handy but might give some conflicts in the long run:     clf  SVCCV(C[1 2])     clffit(X y),,63,0.8253968253968254,0.4339622641509434,12328,298.83192731992216,28.309539260220635,80.54834523036989,843,25,227,18,unknown,fabianp,fabianp,true,fabianp,6,1.0,58,19,321,true,true,false,false,5,80,4,2,125,0,75
625201,scikit-learn/scikit-learn,python,116,1301672109,1310989072,1310989072,155282,155282,github,false,false,false,113,122,5,0,156,12,168,0,7,2,0,1,24,1,1,0,11,2,20,33,18,1,1,538,0,3986,607,22.710884195755103,1.0679505492458612,0,,doc/dp-derivation/dp-derivation.tex|scikits/learn/dpgmm.py|scikits/learn/dpgmm.py|scikits/learn/dpgmm.py|scikits/learn/dpgmm.py,0,0.0,0,62,true,Variational infinite gmm This implementation seems to workThe DP variational GMM has the following advantages:  1 You dont need to specify a priori the number of components (just an upper bound on its value)  2 The covariance matrix never diverges so the full model is safe even with one or two examples per cluster  3 All estimates are regularizedIt has the following disadvantages:  1 It tends to create uneven distributions between the clusters  2 It might take longer than EM  3 Intializing is harder (the components are not exchangeable) so restarts might be a good ideaRight now Id like:  1 comments on code quality  2 suggestions for things to test,,62,0.8225806451612904,0.4339622641509434,12420,313.9291465378422,28.09983896940419,84.29951690821257,843,25,227,25,unknown,alextp,fabianp,false,fabianp,1,1.0,20,2,1092,true,false,false,false,0,1,0,0,5,0,4
625207,scikit-learn/scikit-learn,python,114,1301669084,1301669199,1301669199,1,1,github,false,false,false,8,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,9.1232916691974,0.4567728944405175,12,ronweiss@gmail.com,doc/tutorial.rst|doc/install.rst,8,0.015444015444015444,0,0,true,SPRINT: Doc Pull request Update the installation doc,,61,0.819672131147541,0.4343629343629344,11595,307.46011211729194,27.85683484260457,83.48426045709357,843,25,227,10,unknown,yml,GaelVaroquaux,false,GaelVaroquaux,1,1.0,12,0,567,true,true,false,false,1,0,1,0,2,0,-1
625208,scikit-learn/scikit-learn,python,113,1301668971,1301669199,1301669199,3,3,github,false,false,false,13,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.594034144967159,0.23000802228433215,8,ronweiss@gmail.com,doc/install.rst,8,0.015444015444015444,0,0,true,SPRINT: Doc Pull request Small pull request to improved install doc on Ubuntu,,60,0.8166666666666667,0.4343629343629344,11595,307.46011211729194,27.85683484260457,83.48426045709357,843,25,227,9,unknown,yml,GaelVaroquaux,false,GaelVaroquaux,0,0,12,0,567,true,true,false,false,0,0,0,0,2,0,-1
625209,scikit-learn/scikit-learn,python,112,1301663450,1301935834,1301935834,4539,4539,github,false,false,false,35,14,0,0,27,0,27,0,4,0,0,0,4,0,0,0,1,0,4,5,5,0,0,0,0,155,67,0,0.0,0,,,0,0.0,0,11,true,Kernel PCA This pull request implements the following:- linear_kernel polynomial_kernel and rbf_kernel (in pairwise module)- KernelCenterer (in preprocessing module)- KernelPCA (in pca module)- inverse_transform (pre-image)- minimal documentation- plot_kpca example,,59,0.8135593220338984,0.4339250493096647,11726,305.3044516459151,27.80146682585707,82.80743646597305,842,23,227,13,unknown,mblondel,mblondel,true,mblondel,2,1.0,62,20,367,true,false,false,false,2,34,2,2,73,0,315
625211,scikit-learn/scikit-learn,python,109,1301617180,1301748800,1301748800,2193,2193,commits_in_master,false,false,false,111,41,19,0,8,5,13,4,2,4,1,3,10,5,0,0,5,1,5,11,6,0,0,1485,86,2162,273,84.93962145475469,4.2423600974839015,8,ronweiss@gmail.com,scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/tests/test_nmf.py|scikits/learn/tests/test_nmf.py|benchmarks/bench_plot_nmf.py|scikits/learn/nmf.py|scikits/learn/__init__.py|scikits/learn/cro.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/cro.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py|scikits/learn/nmf.py,8,0.0,0,0,false,NMF HiHere is my pull request containing:Non-negative matrix factorizationTwo initialization methods (NNDSVD and CRO based hierarchical clustering) apart from random initializationFor those who have been watching my local code NNDSVD has been updated according to the reference MATLAB implementation from the authors homepage It now has 3 variants of its ownTests are incomplete benchmark needs to be extended to compare init methods and to consider sparsity (see next paragraph)NMF sparseness constraints seem to work at a glance but rigurous evaluation has not been made yet It seems to keep error low enough while indeed setting more values to zero in the corresponding factorBestVlad,,58,0.8103448275862069,0.4317718940936864,11583,304.0663040663041,26.936026936026934,79.340412673746,842,22,226,14,unknown,vene,GaelVaroquaux,false,GaelVaroquaux,0,0,17,14,354,true,true,false,false,0,0,0,0,11,0,884
625212,scikit-learn/scikit-learn,python,107,1300752765,1306089549,1306089549,88946,88946,commits_in_master,false,false,false,16,11,2,0,10,0,10,0,5,0,0,3,11,2,0,0,4,0,11,15,10,0,0,144,18,729,65,17.933789740807292,0.8396200628532071,48,vlad@vene.ro,doc/modules/classes.rst|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/naive_bayes.py,43,0.01276595744680851,0,5,false,Added Multinomial Naive Bayes classifier Added Multinomial Naive Bayes classifier Updated the documents and testsAmit,,57,0.8070175438596491,0.4340425531914894,12594,308.16261711926313,28.029220263617596,83.21422899793552,824,21,216,16,unknown,amitibo,larsmans,false,larsmans,1,1.0,2,0,10,true,true,false,false,1,0,1,0,4,0,517
625213,scikit-learn/scikit-learn,python,106,1300702227,1300746046,1300746046,730,730,github,false,false,false,32,6,3,0,3,0,3,0,2,1,0,12,13,11,0,0,1,0,12,13,11,0,0,1653,161,1716,192,94.6775207263606,4.765460083053317,12,ronweiss@gmail.com,doc/modules/sgd.rst|examples/linear_model/plot_sgd_weighted_samples.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/tests/test_sgd.py|examples/linear_model/plot_sgd_weighted_classes.py|examples/linear_model/plot_sgd_weighted_samples.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sgd_fast_sparse.pyx|scikits/learn/linear_model/tests/test_sgd.py|benchmarks/bench_sgd_covertype.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sgd_fast_sparse.pyx|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/tests/test_sgd.py,12,0.006550218340611353,0,0,false,Sgdsampleweight Features: * Added sample weights to SGDClassifier and SGDRegressor classes * Major refactoring in SGD module * More test cases for class and sample weights See examples/linear_model/plot_sgd_weighted_samplespy for sample weight example,,56,0.8035714285714286,0.4497816593886463,11587,296.7981358418918,26.06369206869768,78.36368343833607,824,21,216,7,unknown,pprett,pprett,true,pprett,4,1.0,31,20,594,true,true,false,false,0,0,0,0,5,0,34
625214,scikit-learn/scikit-learn,python,105,1300631078,1300636725,1300636725,94,94,github,false,false,false,25,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0.0,0,,,0,0.0,0,0,false,Fix to the sparse SVM poly kernel implementation Fixes the bug that I reported in issue 104Includes a fix to the test_parsepy testAmit,,55,0.8,0.4497816593886463,11587,296.7981358418918,26.06369206869768,78.36368343833607,824,20,215,7,unknown,amitibo,ogrisel,false,ogrisel,0,0,2,0,9,false,true,false,false,0,0,0,0,0,0,-1
625215,scikit-learn/scikit-learn,python,103,1299861186,1301905063,1301905063,34064,34064,github,false,false,false,123,9,3,0,12,0,12,0,5,0,0,1,2,2,0,0,0,0,2,2,2,0,0,73,0,105,0,9.011534745329477,0.4354587358765233,9,vlad@vene.ro,scikits/learn/lda.py|examples/plot_pca_vs_lda.py|scikits/learn/lda.py,9,0.017543859649122806,0,3,true,LDA improvements Heres a pull request which does the following: * implement transform * properly implement predict_log_proba * plot PCA vs LDAI dont fully understand the code in fit so I would like someone to check the commit that implements transformUsually the text book way to implement LDA is:1) Centers the data2) Compute Sw (within covariance matrix) and Sb (between covariance matrix)3) Compute the pseudo inverse Sw^-1 by the SVD method (and take care of near-zero singular values)4) Compute weight matrix W  eig(Sw^-1 Sb)By using Scaler and the covariance module for 1) and 2) this could make the code really shortHowever I guess the current implementation has some numerical advantages Can someone document them,,54,0.7962962962962963,0.4775828460038986,12137,301.72200708577077,28.507868501277088,81.32157864381642,806,20,206,11,unknown,mblondel,mblondel,true,mblondel,1,1.0,58,20,346,true,false,false,false,0,28,1,2,70,0,157
625216,scikit-learn/scikit-learn,python,102,1299690475,1299716709,1299716709,437,437,github,false,false,false,61,4,2,1,2,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,8,0,14,0,8.694482538758631,0.4457239556723527,11,ronweiss@gmail.com,scikits/learn/mixture.py|scikits/learn/mixture.py,11,0.020872865275142316,0,0,false,add converged_ attribute to GMM as discussed in the ML I have added an attribute converged_ to GMM (in mixturepy) to indicate whether the fit() method ended in convergence or not (max_iter reached) The attribute is initialized to False at class initialization and set to True if convergence is reached in GMMfit() It is reset to False when calling fit() again,,53,0.7924528301886793,0.476280834914611,11222,291.65924077704506,25.57476385671003,76.72429157013009,806,20,204,6,unknown,vincentschut,,false,,0,0,0,0,0,false,true,false,false,0,0,0,0,0,0,22
625217,scikit-learn/scikit-learn,python,99,1299217700,1299431745,1299431745,3567,3567,merged_in_comments,false,false,false,157,3,1,10,11,0,21,0,4,0,0,1,1,1,0,0,0,0,1,1,1,0,0,97,0,270,0,4.902386247178873,0.25198834632986145,22,olivier.grisel@ensta.org,scikits/learn/cluster/k_means_.py,22,0.037996545768566495,0,2,true,Replacement for wrong k-means++ initialization As noted in Issue scikit-learn/scikit-learn#98 the k-means++ initialization in scikitslearn is based on a widespread implementation for k-means++ in Python which is short and simple but wrong (ie it is not k-means++)I have now ported and optimized the original C++ implementation of the authors of the 2007 k-means++ paper This does not only correctly implement k-means++ it also reduces the computational complexity from O(k * n_samples**2) to O(k * n_samples * numLocalTries)I therefore removed the max_samples parameter -- it is now fast enough even with large data sets (on my system it takes around a minute to choose 64 centers for 1e6 data points) Alternatively we could just leave it there (with a high default value) for backwards compatibilityAs scikitslearn depends on scipy anyway I am using scipyspatialdistancescdist for distance calculations It is a lot faster than scikitslearnmetricspairwiseeuclidean_distances -- it may pay to make _e_step() use cdist() as well,,52,0.7884615384615384,0.5008635578583766,11180,293.2021466905188,25.670840787119857,77.10196779964222,792,20,199,7,unknown,f0k,,false,,0,0,9,0,11,false,true,false,false,0,1,0,0,0,0,205
318500,scikit-learn/scikit-learn,python,97,1299092021,1338217615,1338217615,652093,652093,merged_in_comments,false,false,false,78,26,0,3,34,33,70,0,6,0,0,0,3,0,0,0,1,0,3,4,3,0,0,0,0,744,0,0,0.0,0,,,0,0.0,0,7,false,WIP : Add functionality to gp Hi to all This is my first commit to scikit-learnAs detailed in the description of my commit I added some functionality to the GP framework:*) Calculate the predictive covariance matrix for a given set of evaluation points*) Draw a number of sample functions from a fitted GP distributionThe support for batch processing (as done in the predict method) still needs to be addedBest to you allD,,51,0.7843137254901961,0.5207612456747405,19329,296.23881214755033,31.093176056702365,88.20942625071136,786,20,197,103,unknown,demianw,agramfort,false,agramfort,0,0,5,3,296,false,true,true,true,0,0,0,0,0,0,959
625218,scikit-learn/scikit-learn,python,94,1298999698,1301675969,1301675969,44604,44604,merged_in_comments,false,false,false,16,5,5,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,202,0,202,0,22.32090208411015,1.1473307063134257,11,vincent.dubourg@gmail.com,scikits/learn/grid_search.py|scikits/learn/grid_search.py|scikits/learn/grid_search.py|scikits/learn/grid_search.py|scikits/learn/grid_search.py,11,0.018803418803418803,0,0,false,Grid search This branch implements a more aggressive parallel dispatching of the jobs in the GridSearchCV,,50,0.78,0.517948717948718,11180,293.2021466905188,25.670840787119857,77.10196779964222,784,20,196,14,unknown,GaelVaroquaux,fabianp,false,fabianp,1,1.0,109,2,372,true,true,false,false,0,55,0,0,27,7,44308
625220,scikit-learn/scikit-learn,python,93,1298909357,1301675988,1301675988,46110,46110,merged_in_comments,false,false,false,27,1,1,0,2,0,2,0,2,1,0,4,5,5,0,0,1,0,4,5,5,0,0,141,0,141,0,24.129487596637233,1.2403812911837084,7,ronweiss@gmail.com,scikits/learn/qda.py|scikits/learn/utils/sparsetools/csgraph.py|scikits/learn/utils/sparsetools/csgraph_wrap.cxx|scikits/learn/utils/sparsetools/py3k.h|setup.py,6,0.0,0,0,false,Py3k This should add the basic infrastructure to support py3k Only the Joblib part [0] lacks this support but It should first be done upstream[0] https://githubcom/joblib/joblib/pull/2,,49,0.7755102040816326,0.524451939291737,11171,292.36415719273117,25.60200519201504,77.16408557873064,782,20,195,13,unknown,fabianp,fabianp,true,fabianp,5,1.0,57,19,289,true,true,false,false,0,83,4,2,178,2,32
625221,scikit-learn/scikit-learn,python,92,1298900873,1299355720,1299355720,7580,7580,github,false,false,false,71,21,12,3,3,0,6,0,2,1,0,14,15,12,0,0,1,0,14,15,12,0,0,261,18,288,18,97.72718340787783,5.023290719411909,67,vincent.dubourg@gmail.com,doc/modules/datasets.rst|scikits/learn/datasets/twenty_newsgroups.py|doc/modules/datasets.rst|examples/applications/plot_face_recognition.py|examples/document_classification_20newsgroups.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/datasets/lfw.py|scikits/learn/datasets/tests/test_lfw.py|scikits/learn/metrics/metrics.py|scikits/learn/metrics/tests/test_metrics.py|doc/modules/classes.rst|scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/datasets/mlcomp.py|scikits/learn/datasets/twenty_newsgroups.py|examples/grid_search_text_feature_extraction.py|scikits/learn/datasets/twenty_newsgroups.py|examples/document_classification_20newsgroups.py|examples/document_classification_20newsgroups.py|examples/document_classification_20newsgroups.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/twenty_newsgroups.py,19,0.011904761904761904,0,0,false,20newsgroups dataset Hi all here is some work on the dataset loader this time factoring the boilerplate from the examples + some function renaming and more documentation improvementsBTW if someone knows the official citation reference for this dataset please specify it in the comments or directly add it to the comments I gave the link to the most official website I could find but there is no official bibtex AFAIK,,48,0.7708333333333334,0.5255102040816326,11180,293.2021466905188,25.670840787119857,77.10196779964222,781,20,195,7,unknown,ogrisel,,false,,6,0.8333333333333334,250,104,642,true,false,false,false,0,53,3,0,139,0,220
626170,scikit-learn/scikit-learn,python,90,1298402861,,1298865721,7714,,unknown,false,false,false,93,2,2,1,12,0,13,0,4,2,0,1,3,3,0,0,2,0,1,3,3,0,0,531,0,531,0,13.362030133794866,0.6935346398043233,16,satra@mit.edu,scikits/learn/cross_val.py|scikits/learn/pls/PLS.py|scikits/learn/pls/__init__.py,16,0.0,0,0,false,Changes to cross_val and new algorithm Hi allIve made a small change to LeaveOneLabelOut in cross_val  I just included the name of the label in the yield line of the __iter__ method  Im writing some automated reports and I thought it would be useful to know the performance of the cross_validation based on the label being usedAlso  Ive written a Projection to Latent Structures (aka Partial Least Squares) algorithm that I have been using in my work  Please have a look and let me know what you thinkBest RegardsAman,,47,0.7872340425531915,0.5368782161234992,11010,288.73751135331514,25.61307901907357,75.56766575840146,773,20,189,7,unknown,aman-thakral,,false,,0,0,1,0,8,false,true,false,false,0,0,0,0,0,0,9
625225,scikit-learn/scikit-learn,python,89,1298396151,1298465825,1298465825,1161,1161,merged_in_comments,false,false,false,59,8,2,2,3,0,5,0,4,0,0,4,5,3,0,0,0,0,5,5,4,0,0,104,40,160,93,18.384003206500154,0.9541920587153008,27,vincent.dubourg@gmail.com,scikits/learn/src/BallTree.cpp|doc/modules/neighbors.rst|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py,23,0.022413793103448276,0,0,false,make Neighbors* faster in high dimensional spaces I get some 10x speedup in high dimensional (500 features) spaces For this added keyword strategy{auto btree brute inplace} to Neighbors*This implements the possibility of using a brute-force algorithm whenthe ambient space becomes too big It also implements a strategyautothat uses a simple heuristic to use the best method,,46,0.782608695652174,0.5379310344827586,11010,288.73751135331514,25.61307901907357,75.56766575840146,773,19,189,7,unknown,fabianp,,false,,4,1.0,56,19,283,true,true,false,false,0,80,3,2,170,2,1
625227,scikit-learn/scikit-learn,python,86,1298352495,1301683404,1301683404,55515,55515,github,false,false,false,84,11,0,0,6,5,11,0,3,0,0,0,10,0,0,0,0,0,10,10,10,0,0,0,0,987,80,0,0.0,0,,,0,0.0,0,0,false,Hcluster v2 Hi folksIve update the current hierarchial clustering with the changes in masterA quick update on the status (I hope I did not forget anything from previous comments):done----- pipeline and GridSearchCV have been cleaned- feat agglo inherits from TransformerMixin- s / n_comp / n_componentstodo----- s / memory / cache - check cython inertia necessary- scipy comparison and auto select best implementation- rst doc for feature agglomeration Where should it appear,,45,0.7777777777777778,0.5483870967741935,11373,283.7421964301416,25.23520619009936,73.7712125208828,772,19,189,13,unknown,agramfort,agramfort,true,agramfort,8,0.75,44,130,447,true,true,false,false,0,66,5,4,52,2,207
625228,scikit-learn/scikit-learn,python,85,1298294466,1298849075,1298877875,9723,9243,github,false,false,false,183,5,0,0,16,0,16,0,2,0,0,0,3,0,0,0,0,0,3,3,2,0,0,0,0,114,0,0,0.0,0,,,0,0.0,0,0,false,Dataset API for loading the labeled faces datasets Hi allHere is a preliminary request for comments for a new dataset loader for the Labeled Faces in the Wild dataset that download the original data (the jpeg files tarball and the labels text files and official CV splits)I use joblib to memoize the result of the jpeg extraction (using scipymiscimread) in a folder that is a subfolder of ~/scikit_learn_data/lfw_home/ that also keep a cached copy of the original dataI am willing to write some doc and tests for this but running the doc and tests will imply downloading ~230MB the first time Is this acceptable Is there a way to implement a global switch to skip tests that would require such a heavy network access Especially how to implement such a switch so as not to pollute the doctests in the documentationEdit: there is no example for the useage of the load_lfw_pairs function that is used for the official face verification task I plan to use it with another feature extractor that will arrive later with its own pull request,,44,0.7727272727272727,0.5421686746987951,11196,291.71132547338334,25.544837441943553,76.99178277956413,771,19,188,8,unknown,ogrisel,,false,,5,0.8,246,104,635,true,false,false,false,0,43,2,0,120,0,5208
625229,scikit-learn/scikit-learn,python,84,1298291610,1298848175,1298848175,9276,9276,merged_in_comments,false,false,false,23,4,1,0,6,0,6,0,4,0,0,1,2,1,0,0,0,0,2,2,2,0,0,23,0,88,14,4.343027727088815,0.22542699585170559,6,vincent.dubourg@gmail.com,scikits/learn/mixture.py,6,0.010327022375215147,0,2,false,Improve performance of GMM sampling Patch contributed by f0k: http://sourceforgenet/apps/trac/scikit-learn/ticket/164The patch looks good to me but variable names should be more explicit,,43,0.7674418604651163,0.5421686746987951,11030,288.0326382592928,25.475974614687217,75.52130553037172,771,19,188,7,unknown,fabianp,,false,,3,1.0,56,19,282,true,true,false,false,0,78,2,2,167,2,2925
625230,scikit-learn/scikit-learn,python,74,1296836771,,1297171553,5579,,unknown,false,false,false,49,15,12,2,5,8,15,8,3,0,0,7,8,7,0,0,0,0,8,8,8,0,0,127,157,302,184,72.55595121222967,3.785373995577309,55,vincent.dubourg@gmail.com,scikits/learn/svm/base.py|scikits/learn/svm/sparse/base.py|scikits/learn/svm/src/liblinear/_liblinear.c|scikits/learn/svm/src/liblinear/_liblinear.pyx|scikits/learn/svm/src/liblinear/liblinear_helper.c|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/src/liblinear/linear.cpp|scikits/learn/svm/tests/test_svm.py,37,0.01037037037037037,0,2,true,Fortran ordering stuff used explicit fortran ordering in various places to correct problems Various files in my commit dont pass pep8 (like basepy )  but I these had many pep8 errors before I began working on them  I can fix all the errors later today if thats required,,42,0.7857142857142857,0.5881481481481482,10859,291.83166037388344,26.429689658347915,77.8156367989686,754,19,171,5,unknown,yamins81,,false,,3,0.3333333333333333,6,1,315,false,true,false,false,0,1,3,0,10,0,32
625231,scikit-learn/scikit-learn,python,72,1296597179,,1296636015,647,,unknown,false,false,false,51,10,10,0,1,8,9,8,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,50,146,50,146,50.642363796907176,2.642165225275179,27,vincent.dubourg@gmail.com,scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/src/liblinear/linear.cpp|scikits/learn/svm/tests/test_svm.py,27,0.03896103896103896,0,0,false,Same pull request as before The previous pull request got closed before it was accepted  I fixed ALL the pep8 errors in the file so that I could see what was going wrong in my code section and now everything is pep8-compliant   Yay   (Ok I know its important)    ,,41,0.8048780487804879,0.6089466089466089,10834,292.0435665497508,26.49067749676943,77.99520029536644,751,19,168,6,unknown,yamins81,,false,,2,0.5,6,1,312,false,true,false,false,0,1,2,0,4,0,57
625232,scikit-learn/scikit-learn,python,71,1296594314,,1296596569,37,,unknown,false,false,false,34,8,5,0,0,8,8,3,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,50,90,50,100,27.609991030800185,1.4404887362531433,27,vincent.dubourg@gmail.com,scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/src/liblinear/linear.cpp|scikits/learn/svm/tests/test_svm.py,27,0.03879310344827586,0,0,false,Simplified SVM prediction function I was able to simplify the SVM prediction function More important though I realized what was really going on --- the coef_ matrix is situated wrong  (See email over list),,40,0.825,0.610632183908046,10834,292.0435665497508,26.49067749676943,77.99520029536644,751,19,168,6,unknown,yamins81,,false,,1,1.0,6,1,312,false,true,false,false,0,0,1,0,4,0,-1
625233,scikit-learn/scikit-learn,python,70,1296578890,1296612077,1296612077,553,553,merged_in_comments,false,false,false,26,4,2,0,1,3,4,0,1,0,0,2,2,2,0,0,0,0,2,2,2,0,0,50,44,50,60,13.742399166810614,0.7169698974687564,23,vincent.dubourg@gmail.com,scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/src/liblinear/linear.cpp|scikits/learn/svm/tests/test_svm.py,23,0.032904148783977114,0,0,false,Patch for order as well as liblinear test This is essentially the same as what I did previously  but now starting with the updated repo ,,39,0.8205128205128205,0.6151645207439199,10834,291.39745246446375,26.49067749676943,77.99520029536644,751,19,168,6,unknown,yamins81,,false,,0,0,6,1,312,false,true,false,false,0,0,0,0,0,0,73
625234,scikit-learn/scikit-learn,python,69,1296541251,1298352388,1298352388,30185,30185,commits_in_master,false,false,false,56,30,25,0,28,2,30,2,6,16,5,19,42,25,0,0,17,5,21,43,27,0,0,2990,288,3130,288,326.36450559245844,17.027123377203747,59,vincent.dubourg@gmail.com,examples/cluster/plot_lena_ward.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|examples/plot_ward_feature_agglomeration.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/_inertia.c|scikits/learn/cluster/_inertia.pyx|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/feature_agglomeration/__init__.py|scikits/learn/feature_agglomeration/feature_agglomeration.py|scikits/learn/feature_agglomeration/test/test_feature_agglomeration.py|examples/cluster/plot_lena_ward.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|scikits/learn/cluster/_inertia.c|scikits/learn/cluster/_inertia.pyx|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/setup.py|doc/modules/clustering.rst|doc/modules/clustering.rst|examples/cluster/plot_lena_feature_agglomeration.py|examples/cluster/plot_lena_ward.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/tests/test_feature_agglomeration.py|scikits/learn/feature_agglomeration/__init__.py|scikits/learn/feature_agglomeration/feature_agglomeration.py|scikits/learn/cluster/feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_feature_agglomeration.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/pipeline.py|scikits/learn/grid_search.py|scikits/learn/feature_selection/univariate_selection.py|examples/plot_classification_probability.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/cluster/plot_lena_feature_agglomeration.py|examples/cluster/plot_lena_ward_segmentation.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/k_means_.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|scikits/learn/cluster/feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/_feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/hierarchical.py|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_feature_agglomeration.py|scikits/learn/cluster/tests/test_hierarchical.py|doc/modules/clustering.rst|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py|examples/cluster/plot_lena_ward_segmentation.py|examples/cluster/plot_ward_structured.py|examples/cluster/plot_ward_unstructured.py|examples/plot_ward_feature_agglomeration.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/tests/test_pipeline.py|examples/cluster/plot_lena_ward_segmentation.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/cluster/hierarchical.py|scikits/learn/cluster/tests/test_hierarchical.py|scikits/learn/cluster/hierarchical.py|doc/modules/clustering.rst|examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,24,0.0,0,16,false,Hierarchical clustering and feature agglomeration here is the result of our efforts to provide a simple though efficienthierarchical clustering in the scikit One key feature is its abilityto get clusters of connected samples (regions of images etc)This patch also provided a way to achieve dimensionalityreduction using feature agglomeration rather than featureselection,,38,0.8157894736842105,0.6284916201117319,10834,291.39745246446375,26.49067749676943,77.99520029536644,750,19,168,6,unknown,agramfort,,false,,7,0.7142857142857143,42,126,426,true,true,false,false,0,67,5,0,74,2,223
625235,scikit-learn/scikit-learn,python,68,1296484981,1299691795,1301151451,77774,53446,github,false,false,false,36,36,16,0,21,0,21,0,5,1,0,9,13,6,0,2,2,0,12,14,7,0,2,1990,341,3225,473,178.16267195903464,9.13353618743074,43,vincent.dubourg@gmail.com,examples/plot_pls.py|scikits/learn/datasets/__init__.py|scikits/learn/datasets/base.py|scikits/learn/datasets/data/linnerud_exercise.csv|scikits/learn/datasets/data/linnerud_physiological.csv|scikits/learn/datasets/samples_generator.py|scikits/learn/pls.py|examples/plot_pls.py|scikits/learn/datasets/descr/linnerud.rst|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/tests/test_pls.py|examples/plot_pls.py|scikits/learn/pls.py|scikits/learn/datasets/data/linnerud_exercise.csv|scikits/learn/datasets/data/linnerud_physiological.csv,21,0.0027472527472527475,0,1,false,PLS Partial least square PLS class implements :- regression PLS: PLS2 (multidimensional response) PLS1 (one dimensional response)- canonical PLS (PLS CA) with symetric deflationsWe also add a simple PLS_SVD (not fully tested yet),,37,0.8108108108108109,0.6414835164835165,11222,291.65924077704506,25.57476385671003,76.72429157013009,750,19,167,11,unknown,duchesnay,GaelVaroquaux,false,GaelVaroquaux,0,0,2,0,189,true,true,false,false,0,0,0,0,18,0,229
625262,scikit-learn/scikit-learn,python,67,1296474445,1297975188,1297975188,25012,25012,merged_in_comments,false,false,false,221,12,2,4,10,0,14,0,2,0,0,3,9,3,0,0,1,0,9,10,5,0,0,379,114,657,181,13.714003062475172,0.7154255586821853,19,vincent.dubourg@gmail.com,scikits/learn/src/BallTree.cpp|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py,18,0.015214384508990318,0,0,false,Neighbors refactoring Heavy refactoring in scikitslearnneighbors: most of loops could be eliminated in kneighbors_graph cleaner implementationThe patch is difficult to follow as it touches a lot of paths most illuminating maybe to take a look into the resulting kneighbors_graphThe biggest change is that the output of kneighbors_graph is modified: now it always returns a sparse matrix with exactly n_neighbors * n_samples nonzero coefficients As a side-effect drop_fist keyword is dropped and places where drop_first where used (NeighborBarycenter) now use the (improved) barycenter functionDetailed API changes:   - barycenter_weights has been renamed to barycenters and accepts     now X as 2D array and 3D array Y This way iteration happens     inside barycenters rendering kneighbor_graph considerably     cleaner Also NeighborRegressor directly uses barycenters     instead of kneighbor_graph Apart from being conceptually     clearer this lets us drop keyword drop_first in kneighbor_graph     making the output of kneighbor_graph more consistent   - weight keyword was renamed to mode This is in the spirit of some     scipy function (qr) that perform slightly different algorithms     based on that keyword   - added keyword eps to control how much regularization is     needed Better names anyone    - dropped keyword drop_first By default the first is not used on     distance and barycenter so that exactly n_neighbors are     nonzero on each row This is what is expected in my opinion and     makes API simpler,,36,0.8055555555555556,0.6445366528354081,10822,291.72056921086676,26.52005174644243,78.0816854555535,750,19,167,4,unknown,fabianp,,false,,2,1.0,54,19,261,true,true,false,false,0,53,2,0,137,2,14
625267,scikit-learn/scikit-learn,python,59,1295532661,1295549181,1295549181,275,275,merged_in_comments,false,false,false,30,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,2,0,2,0,4.438073775024322,0.2320674876727423,8,vincent.dubourg@gmail.com,scikits/learn/cluster/affinity_propagation_.py,8,0.01092896174863388,0,0,false,Fix  a bug of affinity propagation This bug is caused by incorrect index usageBTW the illustrating examples should also be changed accordingly since the result would probably be wrong,,35,0.8,0.7049180327868853,10649,288.3838858108743,26.38745422105362,77.47206310451686,737,16,156,4,unknown,fannix,,false,,0,0,1,0,8,false,true,false,false,0,5,0,0,0,0,227
625269,scikit-learn/scikit-learn,python,57,1295524358,1296197887,1296226687,11705,11225,github,false,false,false,57,44,15,0,21,0,21,0,3,2,0,9,17,9,0,0,2,2,13,17,13,0,0,342,222,1085,563,126.19306435264107,6.598652679197883,29,vincent.dubourg@gmail.com,scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/sparse/__init__.py|scikits/learn/linear_model/sparse/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|scikits/learn/preprocessing/__init__.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|scikits/learn/linear_model/ridge.py|scikits/learn/linear_model/tests/test_ridge.py|examples/document_classification_20newsgroups.py|scikits/learn/linear_model/sparse/__init__.py|scikits/learn/linear_model/sparse/ridge.py|scikits/learn/linear_model/sparse/ridge.py|scikits/learn/linear_model/sparse/tests/test_ridge.py|scikits/learn/linear_model/sparse/ridge.py|scikits/learn/linear_model/sparse/tests/test_ridge.py|scikits/learn/linear_model/sparse/ridge.py|scikits/learn/linear_model/sparse/tests/test_ridge.py|scikits/learn/preprocessing/__init__.py|scikits/learn/preprocessing/tests/test_preprocessing.py,14,0.004081632653061225,0,1,false,Ridge improvements - LabelBinarizer: easy and clean way to binarize the target vector using the transformer paradigm- RidgeLOO: Ridge regression with efficient built-in leave-one-out cross-val- RidgeClassifier: use ridge regression in a one-vs-all fashion- Sparse classes- A few additional testsI didnt implement fit_intercept for the sparse case yet but will do it later,,34,0.7941176470588235,0.7129251700680272,10649,288.3838858108743,26.38745422105362,77.47206310451686,737,16,156,4,unknown,mblondel,,false,,0,0,55,18,296,true,false,false,false,0,8,0,0,41,0,75
625270,scikit-learn/scikit-learn,python,55,1295459509,1295488668,1295488668,485,485,commits_in_master,false,false,false,4,1,1,0,3,0,3,0,2,0,0,1,1,1,0,0,0,0,1,1,1,0,0,4,0,4,0,4.214567319282006,0.220385649494859,20,vincent.dubourg@gmail.com,scikits/learn/cross_val.py,20,0.027359781121751026,0,0,false,Fixes/kfoldchecks minor assertion checks,,33,0.7878787878787878,0.7181942544459644,10606,286.5359230624175,26.400150858004903,77.31472751272865,737,16,155,4,unknown,satra,,false,,0,0,25,2,367,false,true,false,false,0,2,0,0,0,0,5
625272,scikit-learn/scikit-learn,python,54,1295399753,1295463536,1295492336,1543,1063,github,false,false,false,13,5,1,0,2,0,2,0,1,0,0,1,4,1,0,0,0,0,4,4,3,0,0,2,0,94,22,4.684305830186435,0.24494893653616243,10,vincent.dubourg@gmail.com,doc/conf.py,10,0.013550135501355014,0,0,false,Cross-val indices cv objects can now return integers indices (useful for sparse matrices),,32,0.78125,0.7249322493224932,10606,286.5359230624175,26.400150858004903,77.31472751272865,735,16,154,6,unknown,agramfort,,false,,6,0.6666666666666666,42,123,412,true,true,false,false,0,54,4,0,66,2,186
625273,scikit-learn/scikit-learn,python,53,1295375027,1295375926,1295404726,494,14,github,false,false,false,25,1,1,0,1,0,1,0,1,1,0,1,2,2,0,0,1,0,1,2,2,0,0,7,25,7,25,8.970700357174533,0.4702277463929125,25,vincent.dubourg@gmail.com,scikits/learn/__init__.py|scikits/learn/tests/test_init.py,25,0.033967391304347824,0,0,false,FIX: removed obsolete entries and added current ones for top-level __all__ + added a unittest for such import Otherwise from scikitslearn import * would fail,,31,0.7741935483870968,0.7282608695652174,10603,285.5795529567104,26.313307554465716,77.05366405734226,735,16,154,5,unknown,yarikoptic,,false,,0,0,25,7,768,false,true,false,false,0,0,0,0,0,0,15
625274,scikit-learn/scikit-learn,python,52,1295347279,,1295549222,3365,,unknown,false,false,false,3,1,1,0,6,0,6,0,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,4.500095960630268,0.23588682018864243,3,vincent.dubourg@gmail.com,doc/modules/cross_validation.rst,3,0.0040595399188092015,0,2,false,Small documentation update ,,30,0.8,0.7320703653585927,10603,285.5795529567104,26.313307554465716,77.05366405734226,733,16,154,3,unknown,turian,,false,,0,0,95,46,666,false,true,false,false,0,0,0,0,0,0,449
625275,scikit-learn/scikit-learn,python,49,1295224631,,1295225986,22,,unknown,false,false,false,19,1,1,0,0,0,0,0,1,0,0,2,2,1,0,0,0,0,2,2,1,0,0,2,0,2,0,8.771085275046133,0.45976066784889913,10,vincent.dubourg@gmail.com,doc/tutorial.rst|examples/plot_digits_classification.py,8,0.010869565217391304,0,0,false,Pull request: typos in the documentation HelloI corrected a few (very few) typos in the documentation CheersEmmanuelle,,29,0.8275862068965517,0.7309782608695652,10602,285.6064893416336,26.31578947368421,77.06093189964159,730,16,152,2,unknown,emmanuelle,,false,,0,0,13,3,258,false,true,false,false,0,0,0,0,0,0,-1
625276,scikit-learn/scikit-learn,python,48,1295159604,,1295159666,1,,unknown,false,false,false,3,4,3,0,0,0,0,0,0,4,0,5,9,5,0,0,4,0,5,9,5,0,0,558,68,769,75,42.65045978437587,2.2356416862477864,0,,examples/cluster/som_digits.py|scikits/learn/cluster/som_.py|scikits/learn/cluster/tests/test_som.py|examples/cluster/plot_som_colormap.py|examples/cluster/plot_som_colormap.py|examples/cluster/som_digits.py|scikits/learn/cluster/__init__.py|scikits/learn/cluster/som_.py|scikits/learn/cluster/tests/test_som.py,0,0.0,0,0,false,code review SOM ,,28,0.8571428571428571,0.7329700272479565,10602,285.6064893416336,26.31578947368421,77.06093189964159,730,16,152,2,unknown,agramfort,,false,,5,0.8,42,123,410,true,true,false,false,0,51,3,0,65,2,-1
626175,scikit-learn/scikit-learn,python,47,1294955838,1295156227,1295156227,3339,3339,merged_in_comments,false,false,false,131,75,66,0,74,0,74,0,4,140,95,69,316,152,0,7,146,96,80,322,158,0,7,32549,1209,33406,1301,1333.7185326521853,69.90655065803898,50,vincent.dubourg@gmail.com,scikits/learn/manifold/__init__.py|scikits/learn/manifold/compression/NLM/__init__.py|scikits/learn/manifold/compression/NLM/cost_function.py|scikits/learn/manifold/compression/__init__.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/compression/cca_function.py|scikits/learn/manifold/compression/cca_multiresolution_dimensionality_reduction.py|scikits/learn/manifold/compression/cost_function/__init__.py|scikits/learn/manifold/compression/cost_function/cost_function.cpp|scikits/learn/manifold/compression/cost_function/cost_function.h|scikits/learn/manifold/compression/cost_function/cost_function.py|scikits/learn/manifold/compression/cost_function/modifiedCompression.h|scikits/learn/manifold/compression/cost_function/setup.py|scikits/learn/manifold/compression/dimensionality_reduction.py|scikits/learn/manifold/compression/distances.py|scikits/learn/manifold/compression/euclidian_mds.py|scikits/learn/manifold/compression/geodesic_mds.py|scikits/learn/manifold/compression/isomap_function.py|scikits/learn/manifold/compression/multiresolution_dimensionality_reduction.py|scikits/learn/manifold/compression/pca.py|scikits/learn/manifold/compression/quadratic_dimensionality_reduction.py|scikits/learn/manifold/compression/robust_dimensionality_reduction.py|scikits/learn/manifold/compression/setup.py|scikits/learn/manifold/compression/similarities.py|scikits/learn/manifold/compression/similarities_mds.py|scikits/learn/manifold/compression/stochastic_dimensionality_reduction.py|scikits/learn/manifold/compression/tests/__init__.py|scikits/learn/manifold/compression/tests/test_barycenters.py|scikits/learn/manifold/compression/tests/test_similarities.py|scikits/learn/manifold/compression/tools.py|scikits/learn/manifold/examples/compression.py|scikits/learn/manifold/examples/projection.py|scikits/learn/manifold/examples/regression.py|scikits/learn/manifold/examples/swissroll.isomap.pickled|scikits/learn/manifold/examples/swissroll.pickled|scikits/learn/manifold/examples/swissroll.projected.pickled|scikits/learn/manifold/examples/swissroll.regressed.pickled|scikits/learn/manifold/examples/swissroll.samples.pickled|scikits/learn/manifold/projection/MAP_projection.py|scikits/learn/manifold/projection/ML_projection.py|scikits/learn/manifold/projection/__init__.py|scikits/learn/manifold/projection/grid_MAP_projection.py|scikits/learn/manifold/projection/grid_ML_projection.py|scikits/learn/manifold/regression/CPLMR.py|scikits/learn/manifold/regression/MLPLMR.py|scikits/learn/manifold/regression/PCA.py|scikits/learn/manifold/regression/PLMR.py|scikits/learn/manifold/regression/__init__.py|scikits/learn/manifold/regression/cluster/ModifiedGeneralClustering.h|scikits/learn/manifold/regression/cluster/ModifiedGeneralClustering.i|scikits/learn/manifold/regression/cluster/__init__.py|scikits/learn/manifold/regression/cluster/clusterInfos.h|scikits/learn/manifold/regression/cluster/setup.py|scikits/learn/manifold/regression/setup.py|scikits/learn/manifold/regression/tests/__init__.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/src/matrix/container_iterator.h|scikits/learn/manifold/src/matrix/diagonal_matrix_iterator.h|scikits/learn/manifold/src/matrix/diagonal_matrix_lib.h|scikits/learn/manifold/src/matrix/dynamic_container_matrix.h|scikits/learn/manifold/src/matrix/expression_template.h|scikits/learn/manifold/src/matrix/expression_template_iterators.h|scikits/learn/manifold/src/matrix/inversion.h|scikits/learn/manifold/src/matrix/iterator_based_functions.h|scikits/learn/manifold/src/matrix/matrix_functions.h|scikits/learn/manifold/src/matrix/matrix_lib.h|scikits/learn/manifold/src/matrix/matrix_traits.h|scikits/learn/manifold/src/matrix/pointer_matrix.h|scikits/learn/manifold/src/matrix/static_container_matrix.h|scikits/learn/manifold/src/matrix/sub_matrix_iterator.h|scikits/learn/manifold/src/matrix/sub_matrix_lib.h|scikits/learn/manifold/src/matrix/sub_vector_lib.h|scikits/learn/manifold/src/matrix/sub_vector_matrix_iterator.h|scikits/learn/manifold/src/matrix/sub_vector_matrix_lib.h|scikits/learn/manifold/src/matrix/type_traits.h|scikits/learn/manifold/src/numpy.i|scikits/learn/manifold/stats/__init__.py|scikits/learn/manifold/stats/gaussian.py|scikits/learn/manifold/stats/gm.py|scikits/learn/manifold/stats/kernels/__init__.py|scikits/learn/manifold/stats/kernels/gaussian.py|scikits/learn/manifold/stats/kernels/gm.py|scikits/learn/manifold/stats/kernels/laplace.py|scikits/learn/manifold/stats/laplace.py|scikits/learn/manifold/stats/radial_basis_functions_field.py|scikits/learn/manifold/stats/setup.py|scikits/learn/setup.py|scikits/learn/manifold/setup.py|scikits/learn/setup.py|doc/modules/manifold.rst|scikits/learn/manifold/__init__.py|scikits/learn/manifold/compression/cost_function/modifiedCompression.h|scikits/learn/manifold/compression/cost_function/setup.py|scikits/learn/manifold/compression/euclidian_mds.py|scikits/learn/manifold/compression/setup.py|scikits/learn/manifold/isomap.py|doc/modules/manifold.rst|scikits/learn/manifold/isomap.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/compression/__init__.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/compression/distances.py|scikits/learn/manifold/compression/euclidian_mds.py|scikits/learn/manifold/compression/isomap_function.py|scikits/learn/manifold/compression/pca.py|scikits/learn/manifold/compression/quadratic_dimensionality_reduction.py|scikits/learn/manifold/compression/similarities.py|scikits/learn/manifold/compression/similarities_mds.py|scikits/learn/manifold/compression/tools.py|scikits/learn/manifold/isomap.py|scikits/learn/svm.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/tests/test_barycenters.py|scikits/learn/manifold/compression/tests/test_distances.py|scikits/learn/manifold/compression/tests/test_similarities.py|scikits/learn/manifold/compression/distances.py|scikits/learn/manifold/compression/tests/test_distances.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/setup.py|examples/plot_digits_classification_manifold.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/compression/similarities.py|scikits/learn/manifold/compression/tests/test_barycenters.py|scikits/learn/manifold/compression/tests/test_similarities.py|scikits/learn/manifold/compression/tools.py|scikits/learn/neighbors.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/compression/tests/test_barycenters.py|scikits/learn/manifold/compression/tests/test_similarities.py|doc/modules/manifold.rst|scikits/learn/manifold/compression/tools.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/mapping/__init__.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/regression/cluster/ModifiedGeneralClustering.h|scikits/learn/manifold/setup.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/tools.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/isomap.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/mapping/barycenter.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/barycenters.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/barycenters.py|scikits/learn/manifold/compression/cca_function.py|scikits/learn/manifold/compression/cca_multiresolution_dimensionality_reduction.py|scikits/learn/manifold/compression/cost_function/__init__.py|scikits/learn/manifold/compression/dimensionality_reduction.py|scikits/learn/manifold/compression/distances.py|scikits/learn/manifold/compression/euclidian_mds.py|scikits/learn/manifold/compression/geodesic_mds.py|scikits/learn/manifold/compression/isomap_function.py|scikits/learn/manifold/compression/multiresolution_dimensionality_reduction.py|scikits/learn/manifold/compression/pca.py|scikits/learn/manifold/compression/quadratic_dimensionality_reduction.py|scikits/learn/manifold/compression/robust_dimensionality_reduction.py|scikits/learn/manifold/compression/similarities.py|scikits/learn/manifold/compression/similarities_mds.py|scikits/learn/manifold/compression/stochastic_dimensionality_reduction.py|scikits/learn/manifold/compression/tests/test_barycenters.py|scikits/learn/manifold/compression/tests/test_similarities.py|scikits/learn/manifold/compression/tools.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/mapping/barycenter.py|doc/modules/manifold.rst|examples/plot_digits_classification.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/compression/__init__.py|scikits/learn/manifold/embedding/NLM/__init__.py|scikits/learn/manifold/embedding/NLM/cost_function.py|scikits/learn/manifold/embedding/__init__.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/cca_function.py|scikits/learn/manifold/embedding/cca_multiresolution_dimensionality_reduction.py|scikits/learn/manifold/embedding/cost_function/__init__.py|scikits/learn/manifold/embedding/cost_function/cost_function.cpp|scikits/learn/manifold/embedding/cost_function/cost_function.h|scikits/learn/manifold/embedding/cost_function/cost_function.py|scikits/learn/manifold/embedding/cost_function/modifiedCompression.h|scikits/learn/manifold/embedding/cost_function/setup.py|scikits/learn/manifold/embedding/dimensionality_reduction.py|scikits/learn/manifold/embedding/distances.py|scikits/learn/manifold/embedding/euclidian_mds.py|scikits/learn/manifold/embedding/geodesic_mds.py|scikits/learn/manifold/embedding/isomap_function.py|scikits/learn/manifold/embedding/multiresolution_dimensionality_reduction.py|scikits/learn/manifold/embedding/pca.py|scikits/learn/manifold/embedding/quadratic_dimensionality_reduction.py|scikits/learn/manifold/embedding/robust_dimensionality_reduction.py|scikits/learn/manifold/embedding/setup.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/stochastic_dimensionality_reduction.py|scikits/learn/manifold/embedding/tests/__init__.py|scikits/learn/manifold/embedding/tests/test_barycenters.py|scikits/learn/manifold/embedding/tests/test_distances.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/setup.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/distances.py|scikits/learn/manifold/embedding/euclidian_mds.py|scikits/learn/manifold/embedding/geodesic_mds.py|scikits/learn/manifold/embedding/tests/test_barycenters.py|scikits/learn/manifold/embedding/tests/test_distances.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/examples/compression.py|scikits/learn/manifold/examples/projection.py|scikits/learn/manifold/examples/regression.py|scikits/learn/manifold/examples/swissroll.isomap.pickled|scikits/learn/manifold/examples/swissroll.pickled|scikits/learn/manifold/examples/swissroll.projected.pickled|scikits/learn/manifold/examples/swissroll.regressed.pickled|scikits/learn/manifold/examples/swissroll.samples.pickled|scikits/learn/manifold/isomap.py|examples/plot_digits_classification_manifold.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/embedding/geodesic_mds.py|scikits/learn/manifold/isomap.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/mapping/setup.py|scikits/learn/manifold/mapping/tests/__init__.py|scikits/learn/manifold/mapping/tests/test_barycenter.py|scikits/learn/manifold/src/matrix/container_iterator.h|scikits/learn/manifold/src/matrix/diagonal_matrix_iterator.h|scikits/learn/manifold/src/matrix/diagonal_matrix_lib.h|scikits/learn/manifold/src/matrix/dynamic_container_matrix.h|scikits/learn/manifold/src/matrix/expression_template.h|scikits/learn/manifold/src/matrix/expression_template_iterators.h|scikits/learn/manifold/src/matrix/inversion.h|scikits/learn/manifold/src/matrix/iterator_based_functions.h|scikits/learn/manifold/src/matrix/matrix_functions.h|scikits/learn/manifold/src/matrix/matrix_lib.h|scikits/learn/manifold/src/matrix/matrix_traits.h|scikits/learn/manifold/src/matrix/pointer_matrix.h|scikits/learn/manifold/src/matrix/static_container_matrix.h|scikits/learn/manifold/src/matrix/sub_matrix_iterator.h|scikits/learn/manifold/src/matrix/sub_matrix_lib.h|scikits/learn/manifold/src/matrix/sub_vector_lib.h|scikits/learn/manifold/src/matrix/sub_vector_matrix_iterator.h|scikits/learn/manifold/src/matrix/sub_vector_matrix_lib.h|scikits/learn/manifold/src/matrix/type_traits.h|scikits/learn/manifold/src/numpy.i|doc/modules/manifold.rst|scikits/learn/manifold/embedding/geodesic_mds.py|examples/plot_digits_classification_manifold.py|examples/plot_manifold_embeddings.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/embedding/NLM/__init__.py|scikits/learn/manifold/embedding/NLM/cost_function.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/cca_function.py|scikits/learn/manifold/embedding/cca_multiresolution_dimensionality_reduction.py|scikits/learn/manifold/embedding/cost_function/__init__.py|scikits/learn/manifold/embedding/cost_function/cost_function.cpp|scikits/learn/manifold/embedding/cost_function/cost_function.h|scikits/learn/manifold/embedding/cost_function/cost_function.py|scikits/learn/manifold/embedding/cost_function/modifiedCompression.h|scikits/learn/manifold/embedding/cost_function/setup.py|scikits/learn/manifold/embedding/dimensionality_reduction.py|scikits/learn/manifold/embedding/distances.py|scikits/learn/manifold/embedding/euclidian_mds.py|scikits/learn/manifold/embedding/geodesic_mds.py|scikits/learn/manifold/embedding/isomap_function.py|scikits/learn/manifold/embedding/multiresolution_dimensionality_reduction.py|scikits/learn/manifold/embedding/pca.py|scikits/learn/manifold/embedding/quadratic_dimensionality_reduction.py|scikits/learn/manifold/embedding/robust_dimensionality_reduction.py|scikits/learn/manifold/embedding/setup.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/stochastic_dimensionality_reduction.py|scikits/learn/manifold/embedding/tests/test_distances.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tests/test_similarities_mds.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/mapping/__init__.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/mapping/builder.py|scikits/learn/manifold/mapping/tests/test_barycenter.py|scikits/learn/manifold/mapping/tests/test_builder.py|scikits/learn/manifold/projection/MAP_projection.py|scikits/learn/manifold/projection/ML_projection.py|scikits/learn/manifold/projection/__init__.py|scikits/learn/manifold/projection/grid_MAP_projection.py|scikits/learn/manifold/projection/grid_ML_projection.py|scikits/learn/manifold/regression/CPLMR.py|scikits/learn/manifold/regression/MLPLMR.py|scikits/learn/manifold/regression/PCA.py|scikits/learn/manifold/regression/PLMR.py|scikits/learn/manifold/regression/__init__.py|scikits/learn/manifold/regression/cluster/ModifiedGeneralClustering.h|scikits/learn/manifold/regression/cluster/ModifiedGeneralClustering.i|scikits/learn/manifold/regression/cluster/__init__.py|scikits/learn/manifold/regression/cluster/clusterInfos.h|scikits/learn/manifold/regression/cluster/setup.py|scikits/learn/manifold/regression/setup.py|scikits/learn/manifold/regression/tests/__init__.py|scikits/learn/manifold/stats/__init__.py|scikits/learn/manifold/stats/gaussian.py|scikits/learn/manifold/stats/gm.py|scikits/learn/manifold/stats/kernels/__init__.py|scikits/learn/manifold/stats/kernels/gaussian.py|scikits/learn/manifold/stats/kernels/gm.py|scikits/learn/manifold/stats/kernels/laplace.py|scikits/learn/manifold/stats/laplace.py|scikits/learn/manifold/stats/radial_basis_functions_field.py|scikits/learn/manifold/stats/setup.py|doc/modules/manifold.rst|examples/plot_manifold_embeddings.py|scikits/learn/manifold/embedding/embedding.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/embedding.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tests/pseudo_neighbor.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tests/test_similarities_mds.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/mapping/builder.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/embedding.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tests/test_similarities_mds.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/tests/test_barycenters.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/setup.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tests/test_barycenters.py|scikits/learn/manifold/embedding/tests/test_similarities.py|scikits/learn/manifold/embedding/tests/test_similarities_mds.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/base_embedding.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tools.py|examples/plot_swissroll.py|scikits/learn/datasets/samples_generator.py|doc/modules/manifold.rst|doc/modules/manifold.rst|examples/plot_swissroll.py|scikits/learn/manifold/embedding/barycenters.py|examples/plot_swissroll.py|examples/plot_swissroll.py|scikits/learn/datasets/samples_generator.py|scikits/learn/manifold/embedding/tests/test_similarities_mds.py|examples/plot_swissroll.py|scikits/learn/benchmarks/bench_manifold.py|scikits/learn/datasets/samples_generator.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/benchmarks/bench_manifold.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/utils/euclidian_distance.py|scikits/learn/utils/tests/test_euclidian_distance.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/utils/euclidian_distance.py|scikits/learn/utils/euclidian_distances.py|scikits/learn/manifold/embedding/similarities.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/hessian_map.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/lle.py|scikits/learn/manifold/embedding/similarities_mds.py|scikits/learn/manifold/embedding/tests/test_hessian_map.py|scikits/learn/manifold/embedding/tests/test_laplacian_map.py|scikits/learn/manifold/embedding/tests/test_lle.py|scikits/learn/manifold/embedding/hessian_map.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/lle.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/tests/test_laplacian_map.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/tests/test_lle.py|benchmarks/bench_manifold.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/utils/euclidian_distance.py|scikits/learn/utils/euclidian_distances.py|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py|benchmarks/bench_manifold.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/embedding/barycenters.py|scikits/learn/manifold/embedding/base_embedding.py|scikits/learn/manifold/embedding/hessian_map.py|scikits/learn/manifold/embedding/laplacian_map.py|scikits/learn/manifold/embedding/lle.py|scikits/learn/manifold/embedding/tests/test_barycenters.py|scikits/learn/manifold/embedding/tests/test_hessian_map.py|scikits/learn/manifold/embedding/tests/test_laplacian_map.py|scikits/learn/manifold/embedding/tests/test_lle.py|scikits/learn/manifold/embedding/tests/test_tools.py|scikits/learn/manifold/embedding/tools.py|scikits/learn/manifold/mapping/__init__.py|scikits/learn/manifold/mapping/barycenter.py|scikits/learn/manifold/mapping/builder.py|scikits/learn/manifold/mapping/setup.py|scikits/learn/manifold/mapping/tests/__init__.py|scikits/learn/manifold/mapping/tests/test_barycenter.py|scikits/learn/manifold/mapping/tests/test_builder.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/embedding/lle.py|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py|benchmarks/bench_manifold.py|scikits/learn/manifold/__init__.py|scikits/learn/manifold/base_embedding.py|scikits/learn/manifold/embedding/__init__.py|scikits/learn/manifold/embedding/setup.py|scikits/learn/manifold/hessian_map.py|scikits/learn/manifold/laplacian_map.py|scikits/learn/manifold/lle.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/tests/__init__.py|scikits/learn/manifold/tests/pseudo_neighbor.py|scikits/learn/manifold/tests/test_hessian_map.py|scikits/learn/manifold/tests/test_laplacian_map.py|scikits/learn/manifold/tests/test_lle.py|scikits/learn/manifold/hessian_map.py|scikits/learn/manifold/laplacian_map.py|scikits/learn/manifold/lle.py|scikits/learn/manifold/lle.py|scikits/learn/manifold/setup.py|scikits/learn/manifold/lle.py,25,0.0,0,9,false,Manifold-light merge Four algorithms are available :- Laplacian Eigenmaps- Diffusion Maps- LLE- HessianMapsIt uses now the code from Alexandre which means that the number of lines is far shorter now Some modifications are proposed for the neighbors code without any test failure There is some **kwargs in some part of the code it will be fixed in the second iteration (inclusion of Isomap )Only the first two algorithms are optimized by using sparse matrices LLE and Hessian Maps use dense matrices (I tried optimizing LLE but I couldnt get something working I hope the inclusion of this code inside master will help fix this issue)Documentation refers to the two examples as to the different publications The next algorithms are also already in the documentation,,27,0.8518518518518519,0.7397820163487738,10517,293.33460112199293,27.194066749072928,79.49034895882856,728,16,149,3,unknown,mbrucher,,false,,0,0,4,0,194,true,false,false,false,0,13,0,0,36,0,84
625280,scikit-learn/scikit-learn,python,43,1294814316,1294876880,1294876880,1042,1042,commits_in_master,false,false,false,18,3,0,0,4,0,4,0,2,0,0,0,5,0,0,0,1,0,5,6,5,0,0,0,0,149,109,0,0.0,0,,,0,0.0,0,0,false,Permutations on cross val score Some code to test a cross val score with permutations in supervised settings,,26,0.8461538461538461,0.7669902912621359,10476,288.8507063764796,27.20504009163803,79.13325696830852,727,16,148,3,unknown,agramfort,,false,,4,0.75,42,123,406,true,true,false,false,0,34,2,0,64,2,371
625282,scikit-learn/scikit-learn,python,42,1294814281,1294975815,1295004615,3172,2692,github,false,false,false,32,19,5,0,2,0,2,0,2,0,0,1,4,1,0,0,0,0,4,4,4,0,0,69,0,242,18,22.118609565146105,1.1620877587892364,5,vincent.dubourg@gmail.com,scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py,5,0.006934812760055479,0,1,false,improvements to k-means (and one PCA thing) Thanks for the feedback from the pull request I sent this afternoon - I made those changes and more hopefully you like this set better,,25,0.84,0.7669902912621359,10476,288.8507063764796,27.20504009163803,79.13325696830852,727,16,148,4,unknown,jaberg,,false,,1,1.0,42,13,385,true,true,false,false,0,0,1,0,5,0,1387
625286,scikit-learn/scikit-learn,python,40,1294773673,1294799505,1294799505,430,430,commits_in_master,false,false,false,9,4,4,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,44,0,44,0,17.573686684842116,0.9233024396539422,5,vincent.dubourg@gmail.com,scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py|scikits/learn/cluster/k_means_.py,5,0.006944444444444444,0,0,false,k-means tweaks minor interface and performance improvements to kmeans,,24,0.8333333333333334,0.7680555555555556,10476,288.8507063764796,27.20504009163803,79.13325696830852,726,16,147,2,unknown,jaberg,,false,,0,0,42,13,384,true,true,false,false,0,0,0,0,5,0,-1
318497,scikit-learn/scikit-learn,python,39,1294771697,,1338215925,724070,,unknown,false,false,false,113,7,0,0,10,2,12,0,6,0,0,0,5,0,0,0,4,0,5,9,5,0,0,0,0,877,94,0,0.0,0,,,0,0.0,0,1,false,WIP : Self-Organizing Map i added the SOM clustering (using a square grid in 2 dimensions) in my branch : https://githubcom/scampion/scikit-learnI also added the calinski_index mesure to evaluate clustering qualityI dont know if i made a mistake but Im a bit surprised by the comparaison between KMeans and SOM using digits dataset in exampleSelf-Organizing Map done in 2328scalinski index 1619 | 9418%KMeans done in 15618scalinski index 902 | 9002%Due to the SOM shape i also use 16 clusters in the kmeansI dont know if my implemention is good or notAt last the unit test test_sompy produce a color map of the neurons as example,,23,0.8695652173913043,0.7680555555555556,19329,296.23881214755033,31.093176056702365,88.20942625071136,726,16,147,99,unknown,scampion,agramfort,false,,0,0,6,3,2,false,false,false,true,0,0,0,0,0,0,53
625289,scikit-learn/scikit-learn,python,34,1292376536,1292483446,1292483446,1781,1781,merged_in_comments,false,false,false,25,23,23,0,2,0,2,0,3,1,0,19,20,19,0,0,1,0,19,20,19,0,0,550,161,550,161,159.06096813188918,8.357878747419925,177,vincent.dubourg@gmail.com,scikits/learn/linear_model/logistic.py|scikits/learn/linear_model/sparse/logistic.py|scikits/learn/svm/base.py|scikits/learn/svm/liblinear.py|scikits/learn/svm/sparse/liblinear.py|scikits/learn/svm/tests/test_sparse.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/base.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/linear_model/cd_fast.c|scikits/learn/linear_model/cd_fast.pyx|examples/applications/plot_face_recognition.py|scikits/learn/pca.py|scikits/learn/tests/test_pca.py|benchmarks/bench_lasso.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/linear_model/least_angle.py|scikits/learn/linear_model/tests/test_coordinate_descent.py|scikits/learn/grid_search.py|scikits/learn/pca.py|scikits/learn/tests/test_pca.py|examples/applications/plot_face_recognition.py|benchmarks/bench_lasso.py|scikits/learn/linear_model/coordinate_descent.py|examples/applications/plot_face_recognition.py|examples/linear_model/plot_lasso_path_crossval.py|scikits/learn/linear_model/coordinate_descent.py|examples/linear_model/plot_lasso_lars.py|examples/linear_model/plot_lasso_path_crossval.py|examples/linear_model/plot_lasso_path_crossval.py|scikits/learn/linear_model/tests/test_coordinate_descent.py|examples/linear_model/plot_lasso_path_crossval.py|examples/linear_model/plot_lasso_lars.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/svm/base.py|scikits/learn/svm/base.py,62,0.01608910891089109,0,0,false,liblinear bias/intercept handling As discussed on the mailing listPS: pls tell me if the pull request is correct My git fu is embarassing :-),,22,0.8636363636363636,0.8316831683168316,10480,287.59541984732823,26.908396946564885,78.72137404580153,682,15,119,3,unknown,paolo-losi,,false,,0,0,4,0,224,true,true,false,false,0,0,0,0,21,0,608
626177,scikit-learn/scikit-learn,python,33,1292356131,1292357576,1292386376,504,24,github,false,false,false,38,1,0,0,2,0,2,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,4,0,0,0.0,0,,,0,0.0,0,0,false,_lmvnpdffull can result in overflow Computing the determinant in _lmvnpdffull has a potential overflow problem (npprod ( ))I suggest to pull the logarithm into the computation (which is done in line 566 currently)to resolve this problem,,21,0.8571428571428571,0.8318804483188045,10486,287.4308601945451,26.893000190730497,78.6763303452222,682,14,119,4,unknown,osdf,,false,,0,0,12,1,317,false,true,false,false,0,0,0,0,0,0,24
625306,scikit-learn/scikit-learn,python,31,1292207759,,1292500940,4886,,unknown,false,false,true,55,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,14,0,14,4.929245391092549,0.2583377453739164,13,vincent.dubourg@gmail.com,scikits/learn/tests/test_grid_search.py,13,0.0161892901618929,0,0,false,Linear svc refit instability I open the pull request for tracking the resolution of this bug even though I dont have a fix for this right now This is just a way to mark this as an issue in the githubcom issue trackerHere is the thread on the mailing list discussing this issue: http://sourceforgenet/mailarchive/forumphpthread_nameAANLkTim0sgVeo_uANDdOr%2BPJy591F7%2BBp3U%3D%3DC_sb%2BtM%40mailgmailcom&forum_namescikit-learn-general,,20,0.9,0.838107098381071,10574,285.0387743521846,26.669188575751846,78.02156232267826,680,15,117,2,unknown,ogrisel,,false,,4,1.0,222,99,564,true,false,false,false,0,25,3,0,215,0,-1
625308,scikit-learn/scikit-learn,python,30,1292026582,1292176751,1292176751,2502,2502,github,false,false,false,139,7,2,0,4,0,4,0,3,0,0,8,8,8,0,0,0,0,8,8,8,0,0,289,76,293,88,41.665329650829584,2.199408917472199,42,vincent.dubourg@gmail.com,examples/applications/plot_face_recognition.py|examples/cluster/kmeans_digits.py|examples/plot_pca.py|scikits/learn/fastica.py|scikits/learn/pca.py|scikits/learn/tests/test_fastica.py|scikits/learn/utils/_csgraph.py|scikits/learn/pca.py|scikits/learn/tests/test_pca.py,29,0.008728179551122194,0,0,true,Extract randomized PCA impl in a dedicated toplevel class I wanted to make PCA able to handle sparse data (scipysparse matrices) using the fast_svd implementation Since computing PCA for sparse dataset (with big n_features) is only feasable with truncated SVD the API of the existing PCA module (with automated mle strategy for finding n_components)  is not very well suited to such an evolutionI hence decided to wrap the fast_svd method in a dedicated RandomizedPCA that makes it explicit that it is able to handle both sparse and dense input provided that you are willing to truncate the singular spectrum to an arbitrary levelHere is a branch that does just that along with updated docstring tests and examples along with a renaming of n_comp to n_components to be consistent with the overall scikit-learn naming conventions for dimension parameters,,19,0.8947368421052632,0.8466334164588528,10552,282.0318423047764,25.96664139499621,77.14177407126611,678,15,115,3,unknown,ogrisel,,false,,3,1.0,222,99,562,true,false,false,false,0,22,2,0,206,0,81
625311,scikit-learn/scikit-learn,python,29,1291892135,1292217030,1292245830,5894,5414,github,false,false,false,69,4,1,0,3,0,3,0,3,0,0,7,7,7,0,0,0,0,7,7,7,0,0,113,3,241,3,32.28037249813059,1.704000933920718,21,vincent.dubourg@gmail.com,examples/gaussian_process/plot_gp_diabetes_dataset.py|examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.py|examples/gaussian_process/plot_gp_regression.py|scikits/learn/gaussian_process/correlation_models.py|scikits/learn/gaussian_process/gaussian_process.py|scikits/learn/gaussian_process/regression_models.py|scikits/learn/gaussian_process/tests/test_gaussian_process.py,15,0.007177033492822967,0,1,false,GaussianProcess score function Hi listAfter a discussion with Alex (Gramfort) who reviewed the gaussian_process module we decided to re-implement the score function so that it natively performs a leave-one-out estimate of the determination coefficient (as usually done with this kind of model) without exposing the internals to the user(See https://githubcom/agramfort/scikit-learn/commit/f068df4f62aae7febd0d9fc2319c100af4dd17df)There is an example of usage of this new score function in the plot_gp_diabetes_datasetpy exampleCheersVincent,,18,0.8888888888888888,0.8552631578947368,10563,285.05159519076017,26.602291015809904,78.00814162643188,677,16,114,5,unknown,dubourg,,false,,1,1.0,3,4,84,true,true,false,false,0,15,1,0,3,0,3651
625321,scikit-learn/scikit-learn,python,28,1291830878,1298846706,1298846706,116930,116930,merged_in_comments,false,false,false,72,7,5,0,12,4,16,0,4,0,0,4,4,4,0,0,0,0,4,4,4,0,0,53,74,58,74,30.64043023496604,1.6253151066760667,83,vincent.dubourg@gmail.com,scikits/learn/svm/setup.py|scikits/learn/svm/src/libsvm/svm.cpp|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/base.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/svm/base.py|scikits/learn/svm/tests/test_svm.py,55,0.04561824729891957,0,0,false,Libsvm labels Sort labels by increasing order This ensures that support vectors coefs etc are also ordered in this fashionAlso added some test one of them reconstructs the decision_function from the classifierDue to convention in libsvm original_decision function is the negative of our decision_function For now i just invert the sign although it should probably be fixed in libsvm so that dual_coef_ is not the negative of what users expect,,17,0.8823529411764706,0.8571428571428571,10416,282.8341013824885,26.20967741935484,77.86098310291858,676,16,113,8,unknown,fabianp,,false,,1,1.0,51,19,207,true,true,false,false,0,11,1,0,243,0,94
625326,scikit-learn/scikit-learn,python,27,1291310830,,1292029287,11974,,unknown,false,false,false,5,5,4,0,6,4,10,2,3,0,0,6,7,6,0,0,0,0,7,7,7,0,0,39,14,47,14,34.97718786996551,1.854017206731789,86,virgile.fritsch@gmail.com,scikits/learn/linear_model/base.py|scikits/learn/linear_model/bayes.py|scikits/learn/linear_model/coordinate_descent.py|scikits/learn/tests/test_metrics.py|scikits/learn/base.py|scikits/learn/linear_model/base.py|scikits/learn/metrics.py|scikits/learn/tests/test_metrics.py,47,0.03837209302325582,0,0,false,addition of r2_score function (reworked) ,,16,0.9375,0.8697674418604651,10250,286.6341463414634,26.4390243902439,76.09756097560975,670,15,107,7,unknown,AnneLaureF,,false,,1,1.0,1,0,6,false,true,false,false,0,0,1,0,0,0,333
625327,scikit-learn/scikit-learn,python,26,1291295891,1291408239,1291408239,1872,1872,commits_in_master,false,false,false,24,10,7,0,4,0,4,0,3,22,7,29,66,46,0,0,22,7,37,66,47,0,0,566,78,584,82,141.50730739762034,7.500802630812853,170,vm.michel@gmail.com,benchmarks/bench_sgd_covertype.py|doc/modules/sgd.rst|examples/linear_model/plot_sgd_iris.py|examples/linear_model/plot_sgd_loss_functions.py|examples/linear_model/plot_sgd_ols.py|examples/linear_model/plot_sgd_penalties.py|examples/linear_model/plot_sgd_separating_hyperplane.py|examples/linear_model/plot_sgd_weighted_classes.py|examples/sgd/README.txt|scikits/learn/sgd/tests/test_sgd.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/base.py|scikits/learn/linear_model/setup.py|scikits/learn/linear_model/sgd_fast.c|scikits/learn/linear_model/sgd_fast.pxd|scikits/learn/linear_model/sgd_fast.pyx|scikits/learn/linear_model/sgd_fast_sparse.c|scikits/learn/linear_model/sgd_fast_sparse.pyx|scikits/learn/linear_model/sparse/__init__.py|scikits/learn/linear_model/sparse/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient.py|scikits/learn/linear_model/stochastic_gradient/__init__.py|scikits/learn/linear_model/stochastic_gradient/base.py|scikits/learn/linear_model/stochastic_gradient/setup.py|scikits/learn/linear_model/stochastic_gradient/sparse/__init__.py|scikits/learn/linear_model/stochastic_gradient/sparse/setup.py|scikits/learn/linear_model/stochastic_gradient/tests/__init__.py|scikits/learn/linear_model/tests/test_sgd.py|scikits/learn/setup.py|doc/modules/classes.rst|doc/modules/sgd.rst|examples/document_classification_20newsgroups.py|examples/grid_search_text_feature_extraction.py|examples/mlcomp_sparse_document_classification.py|examples/sgd/README.txt|examples/sgd/covertype_dense_sgd.py|examples/sgd/plot_iris.py|examples/sgd/plot_loss_functions.py|examples/sgd/plot_ols_sgd.py|examples/sgd/plot_penalties.py|examples/sgd/plot_separating_hyperplane.py|examples/sgd/plot_weighted_classes.py|scikits/learn/__init__.py|scikits/learn/linear_model/__init__.py|scikits/learn/linear_model/setup.py|scikits/learn/linear_model/stochastic_gradient/__init__.py|scikits/learn/linear_model/stochastic_gradient/base.py|scikits/learn/linear_model/stochastic_gradient/setup.py|scikits/learn/linear_model/stochastic_gradient/sgd.py|scikits/learn/linear_model/stochastic_gradient/sgd_fast.c|scikits/learn/linear_model/stochastic_gradient/sgd_fast_sparse.c|scikits/learn/linear_model/stochastic_gradient/sparse/__init__.py|scikits/learn/linear_model/stochastic_gradient/sparse/sgd.py|scikits/learn/linear_model/stochastic_gradient/tests/test_sgd.py|scikits/learn/setup.py|scikits/learn/linear_model/stochastic_gradient/__init__.py|scikits/learn/linear_model/stochastic_gradient/base.py|scikits/learn/linear_model/stochastic_gradient/setup.py|scikits/learn/linear_model/stochastic_gradient/sgd.py|scikits/learn/linear_model/stochastic_gradient/sgd_fast.c|scikits/learn/linear_model/stochastic_gradient/sgd_fast.pxd|scikits/learn/linear_model/stochastic_gradient/sgd_fast.pyx|scikits/learn/linear_model/stochastic_gradient/sgd_fast_sparse.c|scikits/learn/linear_model/stochastic_gradient/sgd_fast_sparse.pyx|scikits/learn/linear_model/stochastic_gradient/sparse/__init__.py|scikits/learn/linear_model/stochastic_gradient/sparse/setup.py|scikits/learn/linear_model/stochastic_gradient/sparse/sgd.py|scikits/learn/linear_model/stochastic_gradient/tests/__init__.py|scikits/learn/linear_model/stochastic_gradient/tests/test_sgd.py,46,0.0,0,0,false,Sgd rename * Moved sgd examples to examples/linear_model* Prefixed examples with sgd* Moved covertype example to benchmarks* Updated documentation (linear_modelrst classesrst),,15,0.9333333333333333,0.8717948717948718,10250,286.6341463414634,26.4390243902439,76.09756097560975,670,15,107,4,unknown,pprett,,false,,3,1.0,30,19,485,true,true,false,false,0,1,3,0,80,1,15
625328,scikit-learn/scikit-learn,python,25,1291126697,1291212926,1291212926,1437,1437,commits_in_master,false,false,false,33,3,2,0,3,0,3,0,3,14,0,1,41,15,0,0,14,0,27,41,29,0,0,0,31,111,59,4.44466078255629,0.23564130152091606,21,vincent.dubourg@gmail.com,scikits/learn/sgd/tests/test_sgd.py|scikits/learn/linear_model/stochastic_gradient/__init__.py|scikits/learn/linear_model/stochastic_gradient/base.py|scikits/learn/linear_model/stochastic_gradient/setup.py|scikits/learn/linear_model/stochastic_gradient/sgd.py|scikits/learn/linear_model/stochastic_gradient/sgd_fast.c|scikits/learn/linear_model/stochastic_gradient/sgd_fast.pxd|scikits/learn/linear_model/stochastic_gradient/sgd_fast.pyx|scikits/learn/linear_model/stochastic_gradient/sgd_fast_sparse.c|scikits/learn/linear_model/stochastic_gradient/sgd_fast_sparse.pyx|scikits/learn/linear_model/stochastic_gradient/sparse/__init__.py|scikits/learn/linear_model/stochastic_gradient/sparse/setup.py|scikits/learn/linear_model/stochastic_gradient/sparse/sgd.py|scikits/learn/linear_model/stochastic_gradient/tests/__init__.py|scikits/learn/linear_model/stochastic_gradient/tests/test_sgd.py,21,0.0,0,0,false,SGD module renaming Finalized the renaming of SGD module Moved sgd into linear_modelRenamed sgd to stochastic_gradientRenamed ClassifierSGD to SGDClassifier (same for Regressor and Base classes)Updated examples docs and build files ,,14,0.9285714285714286,0.8818076477404403,10236,285.16998827667055,26.47518561938257,75.5177803829621,666,14,105,3,unknown,pprett,,false,,2,1.0,30,19,483,true,true,false,false,0,0,2,0,76,0,1379
625329,scikit-learn/scikit-learn,python,24,1291053441,1292000518,1292000518,15784,15784,merged_in_comments,false,false,false,4,5,1,0,3,4,7,0,3,0,0,4,7,4,0,0,0,0,7,7,7,0,0,29,12,47,14,17.83510746408624,0.9453760618280105,89,vm.michel@gmail.com,scikits/learn/base.py|scikits/learn/linear_model/base.py|scikits/learn/metrics.py|scikits/learn/tests/test_metrics.py,52,0.05011655011655012,0,0,false,addition of r2_score function ,,13,0.9230769230769231,0.8846153846153846,10250,286.6341463414634,26.4390243902439,76.09756097560975,666,14,104,5,unknown,AnneLaureF,,false,,0,0,1,0,3,false,true,false,false,0,0,0,0,0,0,31
625332,scikit-learn/scikit-learn,python,23,1290978189,1291332163,1291332163,5899,5899,github,false,false,false,19,10,5,0,38,0,38,0,5,1,0,5,10,4,0,0,1,0,9,10,7,0,0,501,66,2368,76,41.457102847798765,2.1978109407347004,51,vm.michel@gmail.com,doc/modules/classes.rst|doc/modules/neighbors.rst|examples/plot_neighbors_regression.py|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py|scikits/learn/neighbors.py|scikits/learn/neighbors.py|scikits/learn/tests/test_neighbors.py|examples/svm/plot_weighted_samples.py,41,0.009400705052878966,0,8,false,Neighbor barycenter new estimator for k-NN based regression+kneighbors_graph function to build the eventually weighted graph of neighbors,,12,0.9166666666666666,0.8848413631022327,10254,283.3040764579676,26.33118782913985,75.09264677199141,664,14,103,5,unknown,agramfort,,false,,3,0.6666666666666666,40,119,361,true,true,false,false,0,17,3,0,98,0,101
625335,scikit-learn/scikit-learn,python,22,1290903634,1290957780,1290986580,1382,902,github,false,false,false,49,2,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,25,0,0,0.0,0,,,0,0.0,0,0,false,Implementation of the power iteration method Its probably not needed but in some experiments with a similar I did note a loss in quality when going to very large very sparse matrices so it cant hurt to have it there Also a light refactoring to unify the sparsity testing,,11,0.9090909090909091,0.8861209964412812,10195,284.9435998038254,26.4835703776361,75.52721922511036,663,13,102,3,unknown,alextp,,false,,0,0,17,2,967,false,false,false,false,0,0,0,0,1,0,903
625336,scikit-learn/scikit-learn,python,19,1290473651,1290605752,1290605752,2201,2201,github,false,false,false,34,10,9,0,0,0,0,0,0,3,0,17,20,17,0,0,3,0,17,20,17,0,0,1297,60,1357,60,141.93967092524326,8.176106284954965,110,ronweiss@gmail.com,doc/modules/sgd.rst|examples/sgd/plot_loss_functions.py|examples/sgd/plot_weighted_classes.py|scikits/learn/sgd/__init__.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sgd.py|scikits/learn/sgd/sgd_fast.c|scikits/learn/sgd/sgd_fast.pxd|scikits/learn/sgd/sgd_fast.pyx|scikits/learn/sgd/sgd_fast_sparse.c|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/tests/test_sgd.py|examples/sgd/covertype_dense_sgd.py|examples/sgd/plot_loss_functions.py|examples/sgd/plot_ols_sgd.py|examples/sgd/plot_separating_hyperplane.py|scikits/learn/sgd/__init__.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sgd.py|scikits/learn/sgd/sgd_fast.c|scikits/learn/sgd/sgd_fast.pyx|scikits/learn/sgd/sgd_fast_sparse.c|scikits/learn/sgd/sparse/__init__.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/tests/test_sgd.py|examples/sgd/plot_weighted_classes.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sgd.py|scikits/learn/sgd/sgd_fast.c|scikits/learn/sgd/sgd_fast.pyx|scikits/learn/sgd/sgd_fast_sparse.c|scikits/learn/sgd/sgd_fast_sparse.pyx|scikits/learn/sgd/sparse/sgd.py|examples/svm/svm_gui.py|examples/svm/svm_gui.py,51,0.015568862275449102,0,0,false,SGD regression and class weights  * changed SGD to ClassifierSGD and implemented RegressorSGD - Im not overly convinced of the naming if you have better ideasplease let me know * importance (class) weighting,,10,0.9,0.9053892215568863,9316,305.1738943752684,28.231000429368827,79.75525976814083,660,12,97,2,unknown,pprett,,false,,1,1.0,29,19,475,true,true,false,false,0,0,1,0,73,0,-1
625337,scikit-learn/scikit-learn,python,17,1289854592,1291330533,1291330533,24599,24599,commits_in_master,false,false,false,19,10,8,0,5,0,5,0,3,0,0,12,13,12,0,0,0,0,13,13,13,0,0,146,25,146,55,52.69948006371399,3.0601636678028865,109,vm.michel@gmail.com,scikits/learn/glm/logistic.py|scikits/learn/pipeline.py|scikits/learn/lda.py|scikits/learn/qda.py|scikits/learn/tests/test_lda.py|scikits/learn/tests/test_qda.py|scikits/learn/naive_bayes.py|scikits/learn/tests/test_naive_bayes.py|scikits/learn/svm/base.py|scikits/learn/svm/libsvm.py|scikits/learn/svm/tests/test_svm.py|scikits/learn/glm/sparse/logistic.py,47,0.012235817575083427,0,0,false,Log proba here a quick attempt to support predict_log_proba for classifiers that support probabilistic outputsSee ticket :http://sourceforgenet/apps/trac/scikit-learn/ticket/157,,9,0.8888888888888888,0.9110122358175751,8949,315.5659850262599,28.941781204603863,81.46161582299698,653,12,90,5,unknown,agramfort,,false,,2,0.5,40,117,348,true,true,false,false,0,10,2,0,101,0,191
625339,scikit-learn/scikit-learn,python,16,1289531129,1289531444,1289531444,5,5,commits_in_master,false,false,true,16,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,44,0,44,0,4.5249078380653,0.26354990843185916,28,vm.michel@gmail.com,scikits/learn/glm/coordinate_descent.py,28,0.03317535545023697,0,0,false,Untitled Sorry just trying to undo a stupid mistake by which I override this commit ,,8,0.875,0.9135071090047393,8826,315.5449807387265,30.704736007251302,87.24223883979153,648,12,86,1,unknown,fabianp,,false,,0,0,50,19,180,true,true,false,false,0,1,0,0,284,1,-1
625340,scikit-learn/scikit-learn,python,15,1288990529,1289176118,1289176118,3093,3093,github,false,false,false,38,10,8,0,0,0,0,0,0,4,0,20,25,23,0,0,5,0,21,26,24,0,0,753,31,1034,31,103.90779926439632,6.071488588932047,68,ronweiss@gmail.com,scikits/learn/svm/liblinear.py|scikits/learn/svm/sparse/liblinear.py|doc/modules/sgd.rst|examples/sgd/plot_loss_functions.py|examples/sgd/covertype_dense_sgd.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sgd_fast.c|scikits/learn/sgd/sgd_fast.pyx|scikits/learn/sgd/sgd_fast_sparse.c|scikits/learn/sgd/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sgd.py|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sgd_fast.c|scikits/learn/sgd/sgd_fast.pxd|scikits/learn/sgd/sgd_fast.pyx|scikits/learn/sgd/sgd_fast_sparse.c|scikits/learn/sgd/sgd_fast_sparse.pyx|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/src/sgd_fast.c|scikits/learn/sgd/src/sgd_fast_sparse.c|scikits/learn/sgd/src/sgd_fast_sparse.pyx|scikits/learn/sgd/__init__.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sgd.py|scikits/learn/sgd/sparse/__init__.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py,33,0.008706467661691543,0,0,true,Dense Stochastic Gradient Descent - new dense implementation of SGD (sgd/sgd_fastpyx and sgd/sgdpy)- moved sparse extension module from sgd/sparse/src to sgd/- sgd/sgd_fast_sparsepyx now imports declarations (LossFunctions) from sgd/sgd_fastpxd- new example: examples/sgd/covertype_dense_sgdpy to showcase dense SGD ,,7,0.8571428571428571,0.917910447761194,8672,332.679889298893,31.25,89.59870848708488,634,12,80,1,unknown,pprett,,false,,0,0,29,18,458,true,true,false,false,0,0,0,0,47,0,-1
625341,scikit-learn/scikit-learn,python,14,1288576962,1290636414,1290665214,34804,34324,github,false,false,false,94,29,0,0,45,0,45,0,6,0,0,0,20,0,0,0,17,3,17,37,21,0,0,0,0,6912,259,0,0.0,0,,,0,0.0,0,11,false,Kriging model class Hello listId like to commit a new feature to the scikits-learn projectI implemented a kriging model class which is able to perform both regression and probabilistic classificationI will send an e-mail with a script that performs a demoSince I have no clue where to put my contribution I simply created a singlefile named krigingpy with the class and other functions such as correlation and regression models and I added an import instruction to the main __init__py module of scikitlearnI hope youll enjoy this contributionVincent,,6,0.8333333333333334,0.9193324061196105,9534,298.4057058946927,27.795259072792113,79.29515418502203,628,12,75,2,unknown,dubourg,,false,,0,0,2,3,45,false,true,false,false,0,0,0,0,0,0,728
625342,scikit-learn/scikit-learn,python,10,1287963256,1287963280,1287963280,0,0,commits_in_master,false,false,false,2,22,21,0,0,0,0,0,0,12,1,11,24,14,1,0,12,1,11,24,14,1,0,2184,390,2194,390,206.675217788297,13.299920644875183,61,virgile.fritsch@gmail.com,scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/tests/test_sparse.py|examples/sgd/mlcomp_sparse_document_classification_sgd.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|examples/sgd/mlcomp_sparse_document_classification_sgd.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|examples/sgd/mlcomp_sparse_document_classification_sgd.py|examples/sgd/mlcomp_sparse_document_classification_sgd.py|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/base.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/base.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/__init__.py|scikits/learn/setup.py|scikits/learn/sgd/__init__.py|scikits/learn/sgd/base.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/__init__.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/__init__.py|scikits/learn/sgd/tests/test_sparse.py,39,0.0,0,0,false,broken doctests ,,5,0.8,0.922237380627558,7950,314.0880503144654,31.823899371069185,86.28930817610063,617,12,68,1,unknown,ogrisel,,false,,2,1.0,200,96,515,true,false,false,false,0,6,2,0,93,3,-1
625344,scikit-learn/scikit-learn,python,9,1287941393,1287973622,1287973622,537,537,commits_in_master,false,false,false,2,17,11,0,1,0,1,0,1,12,1,10,24,14,1,0,12,1,11,24,14,1,0,1737,232,1939,290,128.34920829679405,8.259501566987472,65,virgile.fritsch@gmail.com,examples/sgd/mlcomp_sparse_document_classification_sgd.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/base.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/base.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/test_sparse.py|scikits/learn/sgd/base.py|examples/mlcomp_sparse_document_classification.py|scikits/learn/__init__.py|scikits/learn/setup.py|scikits/learn/sgd/__init__.py|scikits/learn/sgd/base.py|scikits/learn/sgd/setup.py|scikits/learn/sgd/sparse/__init__.py|scikits/learn/sgd/sparse/setup.py|scikits/learn/sgd/sparse/sgd.py|scikits/learn/sgd/sparse/src/sgd_fast_sparse.c|scikits/learn/sgd/sparse/src/sgd_fast_sparse.html|scikits/learn/sgd/sparse/src/sgd_fast_sparse.pyx|scikits/learn/sgd/tests/__init__.py|scikits/learn/sgd/tests/test_sparse.py,43,0.0,0,0,false,Some cleanups ,,4,0.75,0.9347222222222222,7950,314.0880503144654,31.823899371069185,86.28930817610063,617,12,68,0,unknown,ogrisel,,false,,1,1.0,200,96,515,true,false,false,false,0,5,1,0,85,3,117
625350,scikit-learn/scikit-learn,python,6,1284572895,1284680385,1284680385,1791,1791,github,false,false,false,278,3,2,0,1,0,1,0,1,0,0,4,6,4,0,0,0,0,6,6,6,0,0,66,19,103,33,22.310044635387953,1.5740161249249662,104,vm.michel@gmail.com,scikits/learn/grid_search.py|scikits/learn/base.py|scikits/learn/cross_val.py|scikits/learn/grid_search.py|scikits/learn/tests/test_base.py,45,0.06074074074074074,0,0,false,Work on cross val and pipelines Code review: I am interested in suggestions on how to make this code stink less or criticism that finds problems or bugs with this codeEnhancements* Cross validation in parallel (with several CPUs) works better* The GridSearchCV stores its scores on the grid I dont like the data structure that they are stored in I think it    is something that will need to be revisited at some point* The score stored are now scaled even in the case iidTrue Alex could you check that the code is right You   are the number one person who wants iidTrue to work wellBug fixes* Make clone work on pipelines  The clone code is fragile I dont like it The alternative is to add a _clone method to an object that would   enable to override the clone behavior I think that we need to keep this option in mind (the con is that it   makes the estimator contract heavier the pro is that it makes this contract more explicit I still think that we   need the clone behavior in order to cross validate* Make sure that classifiers are indeed recognized as so even when wrapped in CV or Pipeline objects  I am very unhappy with this code (but I would still like it merged because it gives a temporary solution)   It raises the issue of how to recognize a classifier from a regressor Finding the duck-typing signature for   classifiers is partly the problem because once we have found it we need to find a good way to propagate in   the case of nested objects as this commit shows,,3,0.6666666666666666,0.9274074074074075,7790,310.012836970475,31.065468549422338,82.67008985879333,567,10,29,1,unknown,GaelVaroquaux,,false,,0,0,86,2,205,true,true,false,false,0,0,0,0,198,2,308
625351,scikit-learn/scikit-learn,python,4,1284055433,1284476443,1284476443,7016,7016,github,false,false,false,27,25,20,0,2,1,3,0,1,8,0,9,20,13,0,0,8,0,12,20,16,0,0,2061,192,2121,264,141.09848386648835,10.326428578824839,63,vm.michel@gmail.com,scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/glm/base.py|scikits/learn/glm/sparse/__init__.py|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/tests/__init__.py|scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/sparse/tests/test_svm.py|scikits/learn/glm/setup.py|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/setup.py|scikits/learn/glm/sparse/src/cd_fast.c|scikits/learn/glm/sparse/src/cd_fast.pyx|scikits/learn/sparse/svm.py|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/setup.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/src/cd_fast_sparse.pyx|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/src/cd_fast_sparse.pyx|scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/src/cd_fast_sparse.pyx|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/src/cd_fast_sparse.pyx|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/glm/sparse/tests/test_coordinate_descent.py|scikits/learn/glm/sparse/coordinate_descent.py|scikits/learn/glm/sparse/src/cd_fast_sparse.c|scikits/learn/glm/sparse/src/cd_fast_sparse.pyx|scikits/learn/glm/sparse/tests/test_coordinate_descent.py,37,0.0,0,0,false,Issue 77 sparse cd Implementation of CD for elastic net on scipysparse dataMissing: centered intercept regularization path + example combining sparse and dense API at once,,2,0.5,0.9276094276094277,7570,305.94451783355345,31.307793923381766,81.24174372523117,565,9,23,1,unknown,ogrisel,,false,,0,0,184,95,470,true,false,false,false,0,3,0,0,69,2,1254
625355,scikit-learn/scikit-learn,python,3,1283454835,,1287855873,73350,,unknown,false,false,false,40,1,1,0,1,10,11,0,1,2,0,0,2,2,0,0,2,0,0,2,2,0,0,31,20,31,20,9.146464408676255,0.6029623287763897,7,olivier.grisel@ensta.org,scikits/learn/preprocessing.py|scikits/learn/tests/test_preprocessing.py,7,0.014314928425357873,0,0,false,Scaling and preprocessing this is just a draft but maybe a good start to think about this ticket:http://sourceforgenet/apps/trac/scikit-learn/ticket/117the scaling of data is either done by hand right now or handledby the different estimators in a non-uniform way,,1,1.0,0.9468302658486708,7747,316.89686330192336,32.14147411901381,86.0978443268362,558,7,16,0,unknown,agramfort,,false,,1,1.0,29,104,274,true,true,false,false,0,0,1,0,88,0,2397
625358,scikit-learn/scikit-learn,python,2,1283353560,1283443816,1283443816,1504,1504,github,false,false,false,6,9,1,0,2,0,2,0,1,0,0,2,7,2,0,0,0,0,7,7,5,0,0,7,33,36,75,9.046063002539896,0.7365974613051098,8,gael.varoquaux@normalesup.org,scikits/learn/glm/lars.py|scikits/learn/glm/tests/test_lars.py,7,0.01417004048582996,0,0,false,more on LARS (were getting there) ,,0,0,0.9473684210526315,6316,314.28119062697914,33.72387587080431,87.23875870804306,557,8,15,0,unknown,agramfort,,false,,0,0,29,103,273,true,true,false,false,0,0,0,0,88,0,543
